commit 74fe0261b8d6f3d263af3133e2584f3be1b83e34
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Tue Jan 28 03:12:07 2025 +0530

    added the function SIM marker op node at ggml_compute_backward() in ggml.c file

commit 08228aa4452e2807e81d1392f50152fd5642ca6d
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Tue Jan 28 03:07:33 2025 +0530

    added switch case in ggml_get_n_task

commit 84db07607ca8fbe0f16e883330e8e9f969cbef69
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Tue Jan 28 03:02:30 2025 +0530

    added switch case in ggml_compute_forward()

commit f32e8710997a553b128fbac03be53da7ac42fe58
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Tue Jan 28 02:46:27 2025 +0530

    modified header file after adding SIMROI dummy impl functions and modified enum ggml_op accordingly

commit 0f8bc1e18552f9ca9bc84b845aa540b8ceeeaa8c
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Tue Jan 28 02:36:11 2025 +0530

    Added SniperSim ROI markers

commit 13dfb291efb01fbfb017ccd53e9bed2a6e987987
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Mon Jan 27 12:56:20 2025 +0530

    Formatted the readme file in markdown format untill computation

commit d667d3c49103c16cb22a5d1f03bf34c9fe6e33a0
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Mon Jan 27 12:51:40 2025 +0530

    Formatted the readme file in markdown format untill computation

commit 986b3925be6877b80763b7d60a27c3ae4b220db0
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Mon Jan 27 12:47:55 2025 +0530

    Formatted the readme file in markdown format untill computation

commit eab2f7c454013473d9f626abea447b8f689c97ae
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Mon Jan 27 12:46:42 2025 +0530

    Formatted the readme file in markdown format untill computation

commit 127a921f6ed851c5d162fba9cf99826dc300c596
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Mon Jan 27 12:44:04 2025 +0530

    Formatted the readme file in markdown format untill computation

commit 1a53eee53cff7862749a001b04669c12c1b1428f
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Mon Jan 27 02:57:36 2025 +0530

    rough document untill computation of graph completed

commit a999baf44c2967df72afffc753de482c7501c8f7
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Mon Jan 27 01:12:21 2025 +0530

    documentation update until graph scheduling

commit fbdfb30b7fde9122bca64948d243e3a09f1cdb9f
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Sat Jan 25 03:51:45 2025 +0530

    read.md me modifed up untill exact compuation tracking in code base

commit 1f2f46fd33b3749afb5e1b05c40b73c8dd3018d0
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Sat Jan 25 03:29:20 2025 +0530

    read.md me modifed up untill exact compuation tracking in code base

commit fdf1f8d6d7917b06a40a6df5bf276ff8881ece89
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Sat Jan 25 03:27:57 2025 +0530

    read.md me modifed up untill exact compuation tracking in code base

commit beffd47a7387d1708568923ff28aa28eb07e2128
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Sat Jan 25 03:24:12 2025 +0530

    read.md me modifed up untill exact compuation tracking in code base

commit 6cd159ef3f5015775864a6b46342bb0b1fb7e349
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Sat Jan 25 03:09:04 2025 +0530

    read.md me modifed up untill exact compuation tracking in code base

commit 8b8b0c403f4d1c7549887107622f59d692042858
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Fri Jan 24 23:12:49 2025 +0530

    Computational graph built understanding

commit 2f786c0c71b655b0d96f2800596fddb4e2a0fbdd
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Fri Jan 24 14:15:25 2025 +0530

    Add GGML lib learning  notes

commit 6e043a9a0f23ce06abe3f339c39f35c935957789
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Fri Jan 24 01:12:03 2025 +0530

    Add ggmlt.c for ggml basic testing and CMake file to build it in ./llmworkload folder

commit 064df790dbda96fd6166f622273e6a9668388882
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Thu Jan 23 07:59:24 2025 +0530

    modified default GGML_DEFAULT_N_THREADS = 1 for single thread execution

commit 5d17353d050d7d5e7110381cea206d631b1eafe7
Author: Abhishek Kumar <Singhabhisheknitp@gmail.com>
Date:   Fri Jan 17 14:49:46 2025 +0530

    Add llmworkload/binary for sniper run

commit 0d52a69e4bf0d6181beec7853307bdcdeec9905b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 8 11:29:34 2025 +0200

    ci : fix cmake option (#11125)

commit 02f04301417e7fb44fa1025bc1b0aef866e2ca89
Author: Mathieu Baudier <mbaudier@argeo.org>
Date:   Wed Jan 8 09:18:13 2025 +0100

    Disable GL_KHR_cooperative_matrix Vulkan extension if not available. (#11117)
    
    * Disable GL_KHR_cooperative_matrix Vulkan extension if not available.
    
    * Perform Vulkan extensions checks in a more sensible order
    
    * Remove unnecessary #ifdef directive

commit bec2183f2c8d37cf1278c11d1adb9311e9eaa242
Author: ag2s20150909 <19373730+ag2s20150909@users.noreply.github.com>
Date:   Wed Jan 8 16:17:29 2025 +0800

    fix: Vulkan shader gen binary path when Cross-compiling (#11096)
    
    * fix: Vulkan shader gen binary path when cross compiling

commit 53ff6b9b9fb25ed0ec0a213e05534fe7c3d0040f
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Jan 7 18:01:58 2025 +0100

    GGUF: C++ refactor, backend support, misc fixes (#11030)
    
    * GGUF: C++ refactor, backend support, misc fixes
    
    remove ggml_tensor.backend
    
    update CODEOWNERS [no ci]
    
    remove gguf_get_data from API
    
    revise GGUF API data types

commit 017cc5f446863316d05522a87f25ec48713a9492
Author: Diego Devesa <slarengh@gmail.com>
Date:   Tue Jan 7 16:11:57 2025 +0100

    ggml-backend : only offload from host buffers (fix) (#11124)

commit a3d50bc022bedd6c7754c24749a1fef4d2d60c7c
Author: Diego Devesa <slarengh@gmail.com>
Date:   Tue Jan 7 12:38:05 2025 +0100

    ggml-backend : only offload from host buffers (#11120)

commit a4dd490069a66ae56b42127048f06757fc4de4f7
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Tue Jan 7 08:37:02 2025 +0200

    rpc : code cleanup (#11107)
    
    Remove duplicated macros, use GGML_LOG_ERROR for errors

commit c0d6f790d07aa78be15584ec394ac20739ade93b
Author: Akarshan Biswas <akarshan.biswas@gmail.com>
Date:   Tue Jan 7 11:56:07 2025 +0530

    SYCL: Use get_multi_ptr instead of deprecated get_pointer in wkv6 (#11087)
    
    * SYCL: Use get_multi_ptr instead of deprecated get_pointer in wkv6
    
    * Revert "SYCL: Use get_multi_ptr instead of deprecated get_pointer in wkv6"
    
    This reverts commit f62dc45f318e48d375e7734b34cbddee81deed52.
    
    * Reland: Use get_multi_ptr instead of deprecated get_pointer in wkv6

commit dc7cef9f373f2a24b851f0df7a618c5209e593fa
Author: Eric Curtin <ecurtin@redhat.com>
Date:   Mon Jan 6 22:45:28 2025 +0000

    llama-run : fix context size (#11094)
    
    Set `n_ctx` equal to `n_batch` in `Opt` class. Now context size is
    a more reasonable 2048.
    
    Signed-off-by: Eric Curtin <ecurtin@redhat.com>

commit ecebbd292d741ac084cf248146b2cfb17002aa1d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 6 17:52:35 2025 +0200

    llama : remove unused headers (#11109)
    
    ggml-ci

commit 96be8c32649378a23031630a48c440f3a5d0839b
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Jan 6 16:34:49 2025 +0100

    github : add cmd line field to bug report (#11090)
    
    * github : cmd line to bug report
    
    * codeowners : (@ngxson) only watch dockerfile
    
    * Apply suggestions from code review [no ci]
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    * rm cmd in log output [no ci]
    
    * rm 2 [no ci]
    
    * no need backticks [no ci]
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

commit e6e7c75d94adf4d39e846d30807c531ff22865e7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 6 15:36:08 2025 +0200

    server : fix extra BOS in infill endpoint (#11106)
    
    * server : fix extra BOS in infill endpoing
    
    ggml-ci
    
    * server : update infill tests

commit 09186fabbe05236f2b9446ba6c643cb737540d10
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Jan 6 13:41:12 2025 +0100

    llama : remove check flash_attn with lora (#11104)

commit 96a1dc27c3f09bf1ed83a26292d571795bcf27fa
Author: Asghar Ghorbani <a-ghorbani@users.noreply.github.com>
Date:   Mon Jan 6 12:21:46 2025 +0100

    llama : prevent system info string accumulation across calls (#11101)

commit 6369f867a410416239d9f20ec27c2b1d6a9fee52
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Mon Jan 6 10:28:17 2025 +0100

    llama : rename missed batch params/vars to ubatch (#10059)
    
    This commit renames the `batch` parameter to `ubatch` in the
    `llama_kv_cache_find_slot`, `llm_build_inp_embd`, and
    `llm_build_mamba` functions.
    
    The motivation for this is that this should have been done as part of
    Commit 19d900a7565b8f6b0a708836a57d26966cb9efe2 ("llama : rename batch
    to ubatch (#9950)") but for some reason I missed these functions in
    that commit and only noticed them now (sorry).

commit 47182dd03fe04a4ffda5d7f4c8a109ae0056cf56
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 6 10:55:18 2025 +0200

    llama : update llama_model API names (#11063)
    
    * llama : deprecate llama_free_model, add llama_model_free
    
    ggml-ci
    
    * llama : change `llama_load_model_from_file` -> `llama_model_load_from_file`
    
    ggml-ci

commit 3e6e7a6bc2c4b980a0cf0fcb5cb3b79a965b5f14
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 6 10:54:25 2025 +0200

    tokenize : escape the prompt (#11058)
    
    * tokenize : escape the prompt
    
    * tokenize : update help

commit ae2f606bb598b287f5fb69c9fdfc98b86598c6cc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 6 10:52:38 2025 +0200

    mmap : fix fileno macro clash (#11076)
    
    * mmap : fix fileno macro clash
    
    ggml-ci
    
    * cont
    
    ggml-ci

commit 727368c60f2ebf2d6a7473a4a9f80957ab063a8e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 6 10:52:15 2025 +0200

    llama : use LLAMA_TOKEN_NULL (#11062)
    
    ggml-ci

commit 5047dd3546951dea3d65c02257d06c46c8662338
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 6 10:52:01 2025 +0200

    llama : use _impl suffix instead of _internal (#11060)
    
    ggml-ci

commit 46e3556e01b824e52395fb050b29804b6cff2a7c
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jan 6 02:33:52 2025 +0100

    CUDA: add BF16 support (#11093)
    
    * CUDA: add BF16 support

commit b56f079e28fda692f11a8b59200ceb815b05d419
Author: 0cc4m <picard12@live.de>
Date:   Sat Jan 4 21:09:59 2025 +0100

    Vulkan: Add device-specific blacklist for coopmat for the AMD proprietary driver (#11074)
    
    * Vulkan: Add device-specific blacklist for coopmat for the AMD proprietary driver
    
    * Add (TM) to AMD name check

commit 9394bbd484f802ce80d2858033583af3ef700d25
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Sat Jan 4 21:06:11 2025 +0100

    llama : Add support for DeepSeek V3 (#11049)
    
    * convert : extend DEEPSEEK2 model architecture to support DeepseekV3ForCausalLM by adding EXPERT_WEIGHTS_NORM and EXPERT_GATING_FUNC model parameters and FFN_EXP_PROBS_B tensor type
    
    * vocab : add DeepSeek V3 pre-tokenizer regexes
    
    * unicode : handle ACCENT_MARK and SYMBOL categories in regex
    
    * llama : add DeepSeek V3 chat template, handle new model parameters and tensor types
    
    ---------
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

commit f922a9c542ee117550a168395c63ea79261f5c99
Author: matt23654 <matthew.webber@protonmail.com>
Date:   Sat Jan 4 16:10:30 2025 +0000

    [GGML][RPC] Support for models with non-512-aligned tensors over RPC. (#11047)
    
    * Added init tensor calling code
    
    * Added get_alloc_size forwarding
    
    * Cleaned up and improved type/error handling.
    
    * fix: remove trailing whitespaces.
    
    * Cleanup and use GGML error logging functions.
    
    * Handle potentially dangerous edge cases.
    
    * Apply suggestions from code review
    
    Co-authored-by: Diego Devesa <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: Diego Devesa <slarengh@gmail.com>

commit 46be942214e295cd34660bbbd6b846155d1c36a0
Author: DAN™ <dranger003@gmail.com>
Date:   Sat Jan 4 09:33:31 2025 -0500

    llama : add support for the cohere2 model architecture (#10900)

commit 78c678517530d411b4263341cdb4dc28c9d117c8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jan 4 10:54:01 2025 +0200

    sync : ggml

commit 5e3b08d606b5b0caaea16541b504c3bba8f3ec1d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jan 4 10:53:54 2025 +0200

    ggml : do not install metal source when embed library (ggml/1054)

commit db68c93b57bfdf6da1fbdae81080382d6998cbc9
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Dec 19 03:50:12 2024 +0100

    ggml : improve inputs log sched_print_assignments (ggml/1053)
    
    This commit attempts to improve the log message for the inputs of the
    splits in the sched_print_assignments function.
    
    The motivation for this change is that currently even if there are no
    inputs a colon is displayed at the end of the line, which can make it a
    little confusing when reading the output as it could be interpreted as
    the line below are inputs when they are in fact nodes. With this change
    the colon will only be printed if there actually are inputs.

commit c31fc8b966817b2f0b277fd28e04a189e388972a
Author: Gilad S. <7817232+giladgd@users.noreply.github.com>
Date:   Sat Jan 4 10:17:31 2025 +0200

    fix: Vulkan shader gen binary path (#11037)

commit 4b0c638b9a68f577cb2066b638c9f622d91ee661
Author: Molly Sophia <mollysophia379@gmail.com>
Date:   Fri Jan 3 20:13:18 2025 +0800

    common : disable KV cache shifting automatically for unsupported models (#11053)
    
    * Disable KV cache shifting automatically for unsupported models
    
    instead of exiting directly
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * Update common/common.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit e7da954eccdf39ee795a6135bdb86f0978902681
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 3 11:26:14 2025 +0200

    metal : avoid uint (#11019)

commit f66f5829276650cd83a087ab2cfed1a760183ea1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 3 10:18:53 2025 +0200

    llama : refactor `src/llama.cpp` (#10902)
    
    * llama : scatter llama.cpp into multiple modules (wip)
    
    * llama : control-vector -> adapter
    
    * llama : arch
    
    * llama : mmap
    
    ggml-ci
    
    * ci : remove BUILD_SHARED_LIBS=OFF
    
    ggml-ci
    
    * llama : arch (cont)
    
    ggml-ci
    
    * llama : chat
    
    ggml-ci
    
    * llama : model
    
    ggml-ci
    
    * llama : hparams
    
    ggml-ci
    
    * llama : adapter
    
    ggml-ci
    
    * examples : fix
    
    ggml-ci
    
    * rebase
    
    ggml-ci
    
    * minor
    
    * llama : kv cache
    
    ggml-ci
    
    * llama : impl
    
    ggml-ci
    
    * llama : batch
    
    ggml-ci
    
    * cont
    
    ggml-ci
    
    * llama : context
    
    ggml-ci
    
    * minor
    
    * llama : context (cont)
    
    ggml-ci
    
    * llama : model loader
    
    ggml-ci
    
    * common : update lora
    
    ggml-ci
    
    * llama : quant
    
    ggml-ci
    
    * llama : quant (cont)
    
    ggml-ci
    
    * minor [no ci]

commit 2f0ee84b9b02d2a98742308026f060ebdc2423f1
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Thu Jan 2 18:06:12 2025 +0100

    server: bench: minor fixes (#10765)
    
    * server/bench:
    - support openAI streaming standard output with [DONE]\n\n
    - export k6 raw results in csv
    - fix too many tcp idle connection in tcp_wait
    - add metric time to emit first token
    
    * server/bench:
    - fix when prometheus not started
    - wait for server to be ready before starting bench

commit 0da5d860266c6928b8c9408efbd264ae59fedda6
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Jan 2 15:05:18 2025 +0100

    server : allow using LoRA adapters per-request (#10994)
    
    * slot.can_batch_with
    
    * lora per request
    
    * test: force disable cache prompt
    
    * move can_batch_with check
    
    * fix condition
    
    * add slow test with llama 8b
    
    * update docs
    
    * move lora change task to queue
    
    * Apply suggestions from code review
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * lora_base
    
    * remove redundant check
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit a45433ba209ee0b33d02c7dc4c31f29894ad83a6
Author: Benson Wong <mostlygeek@gmail.com>
Date:   Wed Jan 1 23:14:54 2025 -0800

    readme : add llama-swap to infrastructure section (#11032)
    
    * list llama-swap under tools in README
    
    * readme: add llama-swap to Infrastructure

commit 0827b2c1da299805288abbd556d869318f2b121e
Author: Srihari-mcw <96763064+Srihari-mcw@users.noreply.github.com>
Date:   Tue Dec 31 19:53:33 2024 +0530

    ggml : fixes for AVXVNNI instruction set with MSVC and Clang (#11027)
    
    * Fixes for clang AVX VNNI
    
    * enable AVX VNNI and alder lake build for MSVC
    
    * Apply suggestions from code review
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 45095a61bfd164e87563a0dc0fbd7b0e9891590b
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Dec 31 15:22:01 2024 +0100

    server : clean up built-in template detection (#11026)
    
    * server : clean up built-in template detection
    
    * fix compilation
    
    * add chat template test
    
    * fix condition

commit 5896c65232c7dc87d78426956b16f63fbf58dcf6
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Dec 31 12:34:13 2024 +0100

    server : add OAI compat for /v1/completions (#10974)
    
    * server : add OAI compat for /v1/completions
    
    * add test
    
    * add docs
    
    * better docs

commit bc7b1f86324279a3dabb705c04ad754a2b27df16
Author: ymcki <84055651+ymcki@users.noreply.github.com>
Date:   Tue Dec 31 19:04:48 2024 +0800

    convert : fix Llama-3_1-Nemotron-51B rope settings (#11008)
    
    * conflict resolution
    
    * move comments after bracket to its own line
    
    * DeciLMCausalModel now reads rope_theta from config.json properly

commit 6e1531aca5ed17f078973b4700fcdadbda4a34a5
Author: Peter <peter277@users.noreply.github.com>
Date:   Tue Dec 31 11:46:06 2024 +1100

    common, examples, ggml : fix MSYS2 GCC compiler errors and warnings when building with LLAMA_CURL=ON and GGML_OPENCL=ON (#11013)
    
    In common/common.cpp:
    * Convert usage of stat() function call to check if file exists to standard library function std::filesystem::exists (error unable to match to correct function signature)
    * Additional conditions to check if PATH_MAX is already defined in WIN32 environment (warning it is already defined in MSYS2)
    
    In examples/run/run.cpp:
    * Add io.h header inclusion (error cannot find function _get_osfhandle)
    * Change initialisers for OVERLAPPED to empty struct (warning about uninitialised members)
    * Add initialiser for hFile (warning it may be uninitialised)
    * Add cast for curl_off_t percentage value to long int in generate_progress_prefix function (warning that curl_off_t is long long int)
    
    In ggml/src/ggml-opencl/ggml-opencl.cpp:
    * Initialise certain declared cl_mem variables to nullptr for greater safety (warning about B_d variable possibly used unassigned)

commit 716bd6dec3e044e5c325386b5b0483392b24cefe
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Mon Dec 30 11:27:11 2024 -0600

    vulkan: optimize mul_mat for small values of N (#10991)
    
    Make the mul_mat_vec shaders support N>1 (as a spec constant, NUM_COLS) where
    the batch_strides are overloaded to hold the row strides. Put the loads from the
    B matrix in the innermost loop because it should cache better.
    
    Share some code for reducing the result values to memory in mul_mat_vec_base.

commit c250ecb3157f3bae0a45f44c3c953b5414d4c2f7
Author: ag2s20150909 <19373730+ag2s20150909@users.noreply.github.com>
Date:   Mon Dec 30 20:35:13 2024 +0800

    android : fix llama_batch free (#11014)

commit a813badbbdf0d38705f249df7a0c99af5cdee678
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Sun Dec 29 03:16:34 2024 -0600

    vulkan: im2col and matmul optimizations for stable diffusion (#10942)
    
    * tests: Add im2col perf tests
    
    * vulkan: optimize im2col, more elements per thread
    
    * vulkan: increase small tile size for NV_coopmat2
    
    * vulkan: change im2col to 512 elements per workgroup

commit fdd21889123bec62b1db3b2fc22b5a4abab32174
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Sun Dec 29 02:35:11 2024 -0600

    vulkan: Use push constant offset to handle misaligned descriptors (#10987)

commit f865ea149d71ef883e3780fced8a20a1464eccf4
Author: Isaac McFadyen <isaac@imcf.me>
Date:   Sat Dec 28 10:09:19 2024 -0500

    server: added more docs for response_fields field (#10995)

commit 16cdce7b68218959e0658e2f95b4572573d5008e
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Sat Dec 28 15:08:54 2024 +0000

    server : fix token duplication when streaming with stop strings (#10997)

commit d79d8f39b4da6deca4aea8bf130c6034c482b320
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Thu Dec 26 10:54:44 2024 -0500

    vulkan: multi-row k quants (#10846)
    
    * multi row k quant shaders!
    
    * better row selection
    
    * more row choices
    
    * readjust row selection
    
    * rm_kq=2 by default

commit d283d02bf254a7f2991e1502066330cc0d4321a6
Author: Peter <peter277@users.noreply.github.com>
Date:   Fri Dec 27 00:59:11 2024 +1100

    examples, ggml : fix GCC compiler warnings (#10983)
    
    Warning types fixed (observed under MSYS2 GCC 14.2.0):
    * format '%ld' expects argument of type 'long int', but argument has type 'size_t'
    * llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/vulkan-shaders-gen.cpp:81:46: warning: missing initializer for member '_STARTUPINFOA::lpDesktop' [-Wmissing-field-initializers]  (emitted for all struct field except first)

commit 9ba399dfa7f115effc63d48e6860a94c9faa31b2
Author: Reza Kakhki <rezakakhki.de@gmail.com>
Date:   Tue Dec 24 21:33:04 2024 +0100

    server : add support for "encoding_format": "base64" to the */embeddings endpoints (#10967)
    
    * add support for base64
    
    * fix base64 test
    
    * improve test
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <son@huggingface.co>

commit 2cd43f4900ba0e34124fdcbf02a7f9df25a10a3d
Author: Djip007 <3705339+Djip007@users.noreply.github.com>
Date:   Tue Dec 24 18:54:49 2024 +0100

    ggml : more perfo with llamafile tinyblas on x86_64 (#10714)
    
    * more perfo with llamafile tinyblas on x86_64.
    
    - add bf16 suport
    - change dispache strategie (thanks:
    https://github.com/ikawrakow/ik_llama.cpp/pull/71 )
    - reduce memory bandwidth
    
    simple tinyblas dispache and more cache freindly
    
    * tinyblas dynamic dispaching
    
    * sgemm: add M blocs.
    
    * - git 2.47 use short id of len 9.
    - show-progress is not part of GNU Wget2
    
    * remove not stable test

commit 09fe2e76137dde850b13313f720e7ffa17efdefa
Author: NeverLucky <92274250+nvrxq@users.noreply.github.com>
Date:   Tue Dec 24 19:39:49 2024 +0300

    server:  allow filtering llama server response fields (#10940)
    
    * llama_server_response_fields
    
    * llama_server_response_fields_fix_issues
    
    * params fixes
    
    * fix
    
    * clarify docs
    
    * change to "response_fields"
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <son@huggingface.co>

commit 30caac3a68a54de8396b21e20ba972554c587230
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Dec 24 09:44:20 2024 +0200

    llama : the WPM vocabs use the CLS token as BOS (#10930)
    
    * llama : the WPM vocabs use the CLS token as BOS
    
    ggml-ci
    
    * llama : add comment

commit 60cfa728e27c28537657d4e627ed432508eb9537
Author: Diego Devesa <slarengh@gmail.com>
Date:   Tue Dec 24 04:05:27 2024 +0100

    ggml : use wstring for backend search paths (#10960)
    
    ggml-ci

commit 3327bb0f8dea381118f8e66c18ea14db56d3b942
Author: Diego Devesa <slarengh@gmail.com>
Date:   Tue Dec 24 04:05:17 2024 +0100

    ggml : fix arm enabled features check (#10961)

commit 32d6ee6385b3fc908b283f509b845f757a6e7206
Author: Diego Devesa <slarengh@gmail.com>
Date:   Mon Dec 23 20:25:52 2024 +0100

    ggml : fix const usage in SSE path (#10962)

commit 14b699ecde8f1e9e251ebff9eca39ebc5603b83b
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Dec 23 12:52:25 2024 +0100

    server : fix missing model id in /model endpoint (#10957)
    
    * server : fix missing model id in /model endpoint
    
    * fix ci

commit 485dc01214f266afff7004bc702498b491abc404
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Dec 23 12:02:44 2024 +0100

    server : add system_fingerprint to chat/completion (#10917)
    
    * server : add system_fingerprint to chat/completion
    
    * update README

commit 86bf31cfe684849157f0875b4f0ebccac7034547
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Mon Dec 23 10:39:30 2024 +0200

    rpc-server : add support for the SYCL backend (#10934)

commit b92a14a841fb4dfaf27b29d982ec8ba5289a3bff
Author: Yun Dou <dixyes@gmail.com>
Date:   Mon Dec 23 08:35:44 2024 +0800

    llama : support InfiniAI Megrez 3b (#10893)
    
    * Support InfiniAI Megrez 3b
    
    * Fix tokenizer_clean_spaces for megrez

commit 6f0c9e034bb398915a6617ee4acc62adb87d387d
Author: ymcki <84055651+ymcki@users.noreply.github.com>
Date:   Mon Dec 23 08:22:33 2024 +0800

    llama : support for Llama-3_1-Nemotron-51B (#10669)
    
    * conflict resolution
    
    * move comments after bracket to its own line

commit dab76c92cc63072d9495ba87f2f3f3a4872d4f57
Author: Eric Curtin <ecurtin@redhat.com>
Date:   Mon Dec 23 00:21:40 2024 +0000

    llama-run : include temperature option (#10899)
    
    This commit updates the `examples/run/README.md` file to include a new
    option for setting the temperature and updates the `run.cpp` file to
    parse this option.
    
    Signed-off-by: Eric Curtin <ecurtin@redhat.com>

commit 7024d59e6a572730626cb11896829d115043a1b1
Author: yuri@FreeBSD <yurivict@users.noreply.github.com>
Date:   Sun Dec 22 16:20:11 2024 -0800

    ggml : fix run-time on FreeBSD in get_executable_path() (#10948)

commit 7c0e28585843b366864b43b48f92425e2ea17df6
Author: Rudi Servo <rudiservo@gmail.com>
Date:   Sun Dec 22 21:22:58 2024 -0100

    devops : add docker-multi-stage builds (#10832)

commit 7ae33a616f44ecc081f3dcb589be20962d1d4a92
Author: Billel Mokeddem <billel.mokeddem.ml@gmail.com>
Date:   Mon Dec 23 01:09:58 2024 +0300

    llama : add Falcon3 support (#10883)
    
    * Add Falcon3 model support
    
    * Add fix for adding bos to added special tokens
    
    * Add comment explaining the logic behind the if statement
    
    * Add a log message to better track the when the following line of code is triggered
    
    * Update log to only print when input and output characters are different
    
    * Fix handling pre-normalized tokens
    
    * Refactoring

commit ebdee9478ca7ba65497b9b96f7457698c6ee5115
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Sun Dec 22 03:44:01 2024 -0600

    vulkan: build fixes for 32b (#10927)
    
    * vulkan: build fixes for 32b
    
    Should fix #10923
    
    * vulkan: initialize some buffer/offset variables

commit 5cd85b5e008de2ec398d6596e240187d627561e3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Dec 21 10:10:18 2024 +0200

    convert : add BertForMaskedLM (#10919)

commit a91a41364b25705dbb81ae996bc35c3440c63b35
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Sat Dec 21 01:04:45 2024 -0600

    vulkan: optimize coopmat2 dequant functions (#10855)
    
    Change the code to do 16b loads when possible and extract the appropriate
    component late, so the code is effectively decoding a pair of elements and
    then selecting one. This can allow more commoning to happen in the compiler
    when neighboring elements are loaded.

commit e34c5af43f941f0ddb92466776339897295aca11
Author: Adrien Gallouët <angt@huggingface.co>
Date:   Sat Dec 21 00:33:37 2024 +0100

    ggml-cpu: replace NEON asm with intrinsics in ggml_gemv_q4_0_4x8_q8_0() (#10874)
    
    * ggml-cpu: replace NEON asm with intrinsics in ggml_gemv_q4_0_4x8_q8_0()
    
    Signed-off-by: Adrien Gallouët <angt@huggingface.co>
    
    * ggml-cpu: format code
    
    Signed-off-by: Adrien Gallouët <angt@huggingface.co>
    
    ---------
    
    Signed-off-by: Adrien Gallouët <angt@huggingface.co>

commit eb5c3dc64bd967f2e23c87d9dec195f45468de60
Author: Akarshan Biswas <akarshan.biswas@gmail.com>
Date:   Fri Dec 20 21:01:28 2024 +0530

    SYCL: Migrate away from deprecated ggml_tensor->backend (#10840)
    
    * Migrate to tensor->buffer for checking backend buffer type: 1
    
    * SYCL: common.cpp try to migrate away from tensor->backend
    
    * SYCL: fix assertions and add proper comments
    
    * SYCL: remove extra space
    
    * SYCL: Add back static to ggml_backend_buffer_is_sycl_split function
    
    * SYCL: Add pragma directive to suppress warning spam
    
    * SYCL: Integrate debug logs with GGML_LOG and other fixes
    
    * Revert "SYCL: Integrate debug logs with GGML_LOG and other fixes"
    
    This reverts commit 2607b7de0f0d2f4f1f690226f86fa861aa39cb97.
    Let's keep the current SYCL specific logging mechanism for now
    
    * SYCL: Use GGML_SYCL_DEBUG after reverting
    
    * SYCL: reg_get_proc_address func, update to the current func signature
    
    * SYCL: Refactor SYCL buffer checks in ggml_sycl_cpy_tensor_2d

commit 0ca416c91aea4549d9c77e3efeca403e15aa6c75
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Dec 20 14:12:06 2024 +0100

    server : (UI) fix copy to clipboard function (#10916)

commit 21ae3b9be83820565d1a720999b7f63ce95b4920
Author: Diego Devesa <slarengh@gmail.com>
Date:   Fri Dec 20 13:31:28 2024 +0100

    ggml : add test for SVE and disable when it fails (#10906)

commit 0a11f8b7b5c39fdf6e91ef9674bc68ff08681af7
Author: Molly Sophia <mollysophia379@gmail.com>
Date:   Fri Dec 20 17:44:58 2024 +0800

    convert : fix RWKV v6 model conversion (#10913)
    
    * Enable --no-context-shift for llama-perplexity example
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * RWKV 6: Fix error in ggml_cuda_op_bin_bcast
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    ---------
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>

commit d408bb9268a988c5a60a5746d3a6430386e7604d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Dec 19 18:47:15 2024 +0200

    clip : disable GPU support (#10896)
    
    ggml-ci

commit 5cab3e4aaa892df4620b720f20a503a1bbbe7a52
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Dec 19 17:42:13 2024 +0200

    llama : minor grammar refactor (#10897)
    
    ggml-ci

commit 36319dec5d75a7dfe3e3de37b9ca2a76cd52b7b2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Dec 19 17:35:15 2024 +0200

    tts : small QoL for easy model fetch (#10903)

commit 57bb2c40cd94c5a09f5210ed8264cc93b21c4b7e
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Dec 19 15:40:08 2024 +0100

    server : fix logprobs, make it OAI-compatible (#10783)
    
    * server : fix logprobs, make it openai-compatible
    
    * update docs
    
    * add std::log
    
    * return pre-sampling p
    
    * sort before apply softmax
    
    * add comment
    
    * fix test
    
    * set p for sampled token
    
    * update docs
    
    * add --multi-token-probs
    
    * update docs
    
    * add `post_sampling_probs` option
    
    * update docs [no ci]
    
    * remove --multi-token-probs
    
    * "top_probs" with "post_sampling_probs"
    
    * resolve review comments
    
    * rename struct token_prob to prob_info
    
    * correct comment placement
    
    * fix setting prob for sampled token

commit a3c33b1dce2d4f25040b75f66629104bd1e40128
Author: Adrien Gallouët <angt@huggingface.co>
Date:   Thu Dec 19 14:20:41 2024 +0100

    ggml: fix arm build with gcc (#10895)
    
    Signed-off-by: Adrien Gallouët <angt@huggingface.co>

commit 2fffc52b50992ac5bc64db19d33c39cbc06f52cf
Author: Sukriti Sharma <Ssukriti@users.noreply.github.com>
Date:   Thu Dec 19 06:04:51 2024 -0700

    llama : fix Roberta embeddings (#10856)
    
    * fix: Use gpt2 tokenizer for roberta and add eos/bos tokens
    
    Branch: RobertaTokenizer
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * fixes to position embeddings
    
    Signed-off-by: Sukriti-Sharma4 <sukriti.sharma4@ibm.com>
    
    * map roberta-bpe to gpt-2
    
    Signed-off-by: Sukriti-Sharma4 <sukriti.sharma4@ibm.com>
    
    * fix linting
    
    Signed-off-by: Sukriti-Sharma4 <sukriti.sharma4@ibm.com>
    
    ---------
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    Signed-off-by: Sukriti-Sharma4 <sukriti.sharma4@ibm.com>
    Co-authored-by: Gabe Goodhart <ghart@us.ibm.com>

commit 7585edbdebd02861e0994dae67c9338731fb3fc5
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Thu Dec 19 10:37:12 2024 +0100

    convert : Add support for Microsoft Phi-4 model  (#10817)
    
    * convert : use GPT2 vocab for Phi-4 model
    
    * convert : use null value of sliding_window to distinguish Phi-4 from other PHI3-based models
    
    * llama : do not use sliding window attention mask for Phi-4 model
    
    ---------
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

commit cd920d0ac38ec243605a5a57c50941140a193f9e
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Dec 19 08:53:58 2024 +0100

    tests: disable GGUF test for bad value size (#10886)

commit 7909e8588ddf70820adf1f325490eb3f67b32875
Author: Eric Curtin <ecurtin@redhat.com>
Date:   Thu Dec 19 02:58:00 2024 +0000

    llama-run : improve progress bar (#10821)
    
    Set default width to whatever the terminal is. Also fixed a small bug around
    default n_gpu_layers value.
    
    Signed-off-by: Eric Curtin <ecurtin@redhat.com>

commit 9177484f589d770ffc4e655b9819124d6a22c1d9
Author: Diego Devesa <slarengh@gmail.com>
Date:   Wed Dec 18 23:21:42 2024 +0100

    ggml : fix arm build (#10890)
    
    * ggml: GGML_NATIVE uses -mcpu=native on ARM
    
    Signed-off-by: Adrien Gallouët <angt@huggingface.co>
    
    * ggml: Show detected features with GGML_NATIVE
    
    Signed-off-by: Adrien Gallouët <angt@huggingface.co>
    
    * remove msvc support, add GGML_CPU_ARM_ARCH option
    
    * disable llamafile in android example
    
    * march -> mcpu, skip adding feature macros
    
    ggml-ci
    
    ---------
    
    Signed-off-by: Adrien Gallouët <angt@huggingface.co>
    Co-authored-by: Adrien Gallouët <angt@huggingface.co>

commit 0bf2d10c5514ff61b99897a4a5054f846e384e1e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Dec 18 19:27:21 2024 +0200

    tts : add OuteTTS support (#10784)
    
    * server : add "tokens" output
    
    ggml-ci
    
    * server : output embeddings for all tokens when pooling = none
    
    ggml-ci
    
    * server : be explicit about the pooling type in the tests
    
    ggml-ci
    
    * server : do not normalize embeddings when there is no pooling
    
    ggml-ci
    
    * llama : add OuteTTS support (wip)
    
    * wip
    
    * extract features
    
    * first conv
    
    * group norm
    
    * resnet conv
    
    * resnet
    
    * attn
    
    * pos net
    
    * layer norm
    
    * convnext
    
    * head
    
    * hann window
    
    * fix n_embd + remove llama.cpp hacks
    
    * compute hann window
    
    * fft
    
    * spectrum processing
    
    * clean-up
    
    * tts : receive input text and generate codes
    
    * clip : fix new conv name
    
    * tts : minor fix
    
    * tts : add header + minor fixes
    
    ggml-ci
    
    * tts : add matchematical constant
    
    ggml-ci
    
    * tts : fix sampling + cut initial noise
    
    * tts : fixes
    
    * tts : update default samplers
    
    ggml-ci
    
    * tts : text pre-processing
    
    * tts : outetts-voc -> wavtokenizer-dec
    
    * tts : remove hardcoded constants
    
    ggml-ci
    
    * tts : fix tensor shapes
    
    * llama : refactor wavtokenizer tensors
    
    ggml-ci
    
    * cont
    
    ggml-ci
    
    * cont [no ci]
    
    * llama : update WavTokenizer to non-causal attn
    
    * llama : handle no-vocab detokenization
    
    * tts : add Python example for OuteTTS (wip)
    
    * tts : extend python example to generate spectrogram
    
    ggml-ci
    
    * server : fix rebase artifacts
    
    * tts : enable "return_tokens" in Python example
    
    ggml-ci
    
    * tts : minor fixes
    
    * common : support HF download for vocoder

commit 7bbb5acf125d1d2840cac7d31b9aaa72210dd5ec
Author: Gaetan Bisson <gaetan@fenua.org>
Date:   Wed Dec 18 04:00:07 2024 -1000

    server: avoid overwriting Authorization header (#10878)
    
    * server: avoid overwriting Authorization header
    
    If no API key is set, leave the Authorization header as is. It may be
    used by another part of the Web stack, such as an authenticating proxy.
    
    Fixes https://github.com/ggerganov/llama.cpp/issues/10854
    
    * rebuild
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <son@huggingface.co>

commit 152610eda91217ac409342cd976d05f5114ad39f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Dec 18 13:01:41 2024 +0200

    server : output embeddings for all tokens when pooling = none (#10861)
    
    * server : add "tokens" output
    
    ggml-ci
    
    * server : output embeddings for all tokens when pooling = none
    
    ggml-ci
    
    * server : update readme [no ci]
    
    * server : fix spacing [no ci]
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    * server : be explicit about the pooling type in the tests
    
    ggml-ci
    
    * server : update /embeddings and /v1/embeddings endpoints
    
    ggml-ci
    
    * server : do not normalize embeddings when there is no pooling
    
    ggml-ci
    
    * server : update readme
    
    ggml-ci
    
    * server : fixes
    
    * tests : update server tests
    
    ggml-ci
    
    * server : update readme [no ci]
    
    * server : remove rebase artifact
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>

commit 0e70ba686e6c717a0aa41d88284e2a392c2bd0cd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Dec 18 11:05:29 2024 +0200

    server : add "tokens" output (#10853)
    
    * server : add "tokens" output
    
    ggml-ci
    
    * server : update readme
    
    ggml-ci
    
    * server : return tokens ids only if requested
    
    ggml-ci
    
    * tests : improve "tokens" type check
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    * server : remove "tokens" from the OAI endpoint
    
    ggml-ci
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>

commit 46828872c31b25df16169cbbf5c2225fa9cb0675
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed Dec 18 09:55:09 2024 +0100

    server : (embeddings) using same format for "input" and "content" (#10872)
    
    * server : (embeddings) using same format for "input" and "content"
    
    * fix test case
    
    * handle empty input case
    
    * fix test

commit 6b064c92b4c0228e333193c167f2c98d2ec8d9bc
Author: redbeard <bharrington@alticon.net>
Date:   Wed Dec 18 00:35:00 2024 -0800

    docs: Fix HIP (née hipBLAS) in README (#10880)
    
    Related to #10524 / be0e350c references to hipBLAS have been removed
    across the repository.  This fixes the link from the repositories
    `README.md`.
    
    Signed-off-by: Brian 'redbeard' Harrington <redbeard@dead-city.org>

commit 4da69d1abd15263061aff94c10f205836a96a4bc
Author: Diego Devesa <slarengh@gmail.com>
Date:   Wed Dec 18 01:36:46 2024 +0100

    Revert "llama : add Falcon3 support (#10864)" (#10876)
    
    This reverts commit 382bc7f2e8ffd0b89f23e840d097e21f301197ba.

commit d62b532c52e0118323277eaa5f442e11ce6505ed
Author: DAN™ <dranger003@gmail.com>
Date:   Tue Dec 17 17:24:22 2024 -0500

    Use model->gguf_kv for loading the template instead of using the C API. (#10868)
    
    * Bump model_template to 16384 bytes to support larger chat templates.
    
    * Use `model->gguf_kv` for efficiency.

commit 081b29bd2a3d91e7772e3910ce223dd63b8d7d26
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Dec 17 19:09:35 2024 +0100

    tests: add tests for GGUF (#10830)

commit 5437d4aaf5132c879acda0bb67f2f8f71da4c9fe
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Dec 17 18:36:02 2024 +0200

    sync : ggml

commit 78f766768d94e9c8b30ce10ca76f568e710d84f7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Dec 17 18:34:32 2024 +0200

    cmake : fix "amd64" processor string (whisper/2638)

commit 8dd19a48129d578972ea3d98896edbbf492891a9
Author: gn64 <yukikaze.jp@gmail.com>
Date:   Mon Dec 16 19:34:38 2024 +0900

    vulkan : fix soft_max.comp division by zero (whisper/2633)
    
    This change prevents a division by zero error when p.KY is 0.

commit 130d0c90bd26b25e3a713b17a61a5d4eb0a66405
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Sat Dec 14 03:23:08 2024 +0100

    ggml : remove return from ggml_gallocr_allocate_node (ggml/1048)
    
    This commit removes the return statement from ggml_gallocr_allocate_node
    function.
    
    The motivation behind this change is to make the code more readable and
    consistent.

commit 3919da8e335dbfd0741d239932a9b9e72061bf27
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Dec 13 08:19:38 2024 +0100

    ggml : add check for grad_accs (ggml/1046)
    
    * ggml : add check for grad_accs
    
    This commit adds a check for grad_accs in ggml_graph_get_grad and
    ggml_graph_get_grad_acc functions. This is necessary to avoid segfaults
    when grad_accs is not initialized.
    
    The motivation for this change is that I find it nice to be able to
    print out a computation graph using ggml_graph_print but this function
    segfaults when grad_accs is not initialized:
    ```console
    (gdb) p g1
    $2 = (ggml_cgraph *) 0x7ffff66004b0
    (gdb) p *g1
    $3 = {size = 2048, n_nodes = 1, n_leafs = 2, nodes = 0x7ffff6600500,
    grads = 0x0, grad_accs = 0x0, leafs = 0x7ffff6604500,
    visited_hash_set = {size = 4099, used = 0x7ffff6610518,
    keys = 0x7ffff6608500}, order = GGML_CGRAPH_EVAL_ORDER_LEFT_TO_RIGHT}
    (gdb) p ggml_graph_print(g1)
    === GRAPH ===
    n_nodes = 1
    
    Program received signal SIGSEGV, Segmentation fault.
    0x0000555555579775 in ggml_graph_get_grad
    (cgraph=0x7ffff66004b0,node=0x7ffff6600340)
        at /ggml/ggml/src/ggml.c:5990
    5990  return igrad != GGML_HASHSET_FULL &&
              ggml_bitset_get(cgraph->visited_hash_set.used, igrad) ?
              cgraph->grads[igrad] : NULL;
    ```
    
    * squash! ggml : add check for grad_accs
    
    Fix the check in ggml_graph_get_grad. The check was incorrectly using
    cgraph->grad_accs instead of cgraph->grads.

commit 0006f5a74a59945f22ff966e723c3d566b3ca8d0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Dec 17 18:35:42 2024 +0200

    ggml : update ggml_backend_cpu_device_supports_op (#10867)
    
    * ggml : fix cpy op for IQ-quants to use reference impl
    
    ggml-ci
    
    * ggml : disable tests involving i-matrix quantization
    
    * ggml : update ggml_backend_cpu_device_supports_op
    
    ggml-ci

commit 05c3a444b8b017b01b366b725ba22c740aae6b38
Author: krystiancha <krystian@krystianch.com>
Date:   Tue Dec 17 16:00:24 2024 +0000

    server : fill usage info in embeddings and rerank responses (#10852)
    
    * server : fill usage info in embeddings response
    
    * server : fill usage info in reranking response

commit 382bc7f2e8ffd0b89f23e840d097e21f301197ba
Author: Billel Mokeddem <billel.mokeddem.ml@gmail.com>
Date:   Tue Dec 17 19:24:56 2024 +0400

    llama : add Falcon3 support (#10864)

commit 4f51968aca049080dc77e26603aa0681ea77fe45
Author: Ruan <47767371+ruanych@users.noreply.github.com>
Date:   Tue Dec 17 17:47:20 2024 +0800

    readme : update typos (#10863)

commit 227d7c5a7f4cc7734fde8a4ef4382d613486d3c8
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Dec 17 09:52:09 2024 +0100

    server : (UI) fix missing async generator on safari (#10857)
    
    * server : (UI) fix missing async generator on safari
    
    * fix

commit 7b1ec53f56bb72c49e4c8434157895f94d3709c2
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Tue Dec 17 05:52:55 2024 +0000

    vulkan: bugfixes for small subgroup size systems + llvmpipe test (#10809)
    
    * ensure mul mat shaders work on systems with subgroup size less than 32
    
    more fixes
    
    add test
    
    * only s_warptile_mmq needs to be run with 32 threads or more

commit 160bc039c862c10b9938269a90ddb09d794a7b0d
Author: Zhiyuan Li <uniartisan2017@gmail.com>
Date:   Tue Dec 17 05:00:46 2024 +0800

    rwkv6: add wkv6 support for Vulkan backend (#10829)
    
    * rwkv_wkv6 vulkan shader
    
    * RWKV_WKV6 Vulkan op tests passed
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * Apply code format changes
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * add [[unroll]] and remove unnecessary conditions
    
    * add uma support
    
    * fix erros in EditorConfig Checker
    
    ---------
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    Co-authored-by: Molly Sophia <mollysophia379@gmail.com>

commit 08ea539df211e46bb4d0dd275e541cb591d5ebc8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Dec 16 12:31:45 2024 +0200

    unicode : improve naming style (#10838)
    
    * unicode : improve naming style
    
    ggml-ci
    
    * cont [no ci]

commit 644fd71b44c4cdbfc6482fbf0353d289c3bc29e6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Dec 16 12:31:14 2024 +0200

    sampling : refactor + optimize penalties sampler (#10803)
    
    * sampling : refactor + optimize penalties sampler
    
    ggml-ci
    
    * common : apply ignore_eos as logit bias
    
    ggml-ci
    
    * batched : remove penalties sampler
    
    * params : allow penalty_last_n == -1 to be equal to context size
    
    ggml-ci
    
    * common : by default, move the penalties at the end of the sampling chain
    
    ggml-ci
    
    * common : ignore all EOG tokens
    
    Co-authored-by: Diego Devesa <slarengh@gmail.com>
    
    * common : move back the penalties at the front of the sampling chain
    
    ggml-ci
    
    * readme : restore hint about --ignore-eos flag [no ci]
    
    * llama : minor
    
    ggml-ci
    
    * webui : update
    
    ---------
    
    Co-authored-by: Diego Devesa <slarengh@gmail.com>

commit 4ddd199f6f6b980e0a7ed9f9b44efeae2fbdf5c4
Author: Bartowski <ckealty1182@gmail.com>
Date:   Sun Dec 15 15:43:25 2024 -0500

    llava : Allow locally downloaded models for QwenVL (#10833)
    
    * Allow locally downloaded models for QwenVL
    
    * Define model_path
    
    * rm trailing space
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>

commit a0974156f334acf8af5858d7ede5ab7d7490d415
Author: Valentin Mamedov <45292985+Inf1delis@users.noreply.github.com>
Date:   Mon Dec 16 00:02:46 2024 +0700

    llama : add Deepseek MoE v1 & GigaChat models (#10827)
    
    * Add deepseek v1 arch & gigachat template
    
    * improve template code
    
    * add readme
    
    * delete comments
    
    * remove comment
    
    * fix format
    
    * lint llama.cpp
    
    * fix order of deepseek and deepseek2, move gigachat temlate to the end of func
    
    * fix order of deepseek and deepseek2 in constants; mark shared exp as deepseek arch need
    
    * remove comments
    
    * move deepseek above deepseek2
    
    * change placement of gigachat chat template

commit 87cf323cef80f6aa530f047ab05b539ebc6b7e3c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Dec 15 18:44:47 2024 +0200

    scripts : change build path to "build-bench" for compare-commits.sh (#10836)

commit 5478bbcd173e7027af7689493c7421719f5c43df
Author: Vinesh Janarthanan <36610342+VJHack@users.noreply.github.com>
Date:   Sun Dec 15 05:55:54 2024 -0600

    server: (UI) add syntax highlighting and latex math rendering (#10808)
    
    * add code highlighting and math formatting
    
    * code cleanup
    
    * build public/index.html
    
    * rebuild public/index.html
    
    * fixed coding style
    
    * fixed coding style
    
    * style fixes
    
    * highlight: smaller bundle size, fix light & dark theme
    
    * remove katex
    
    * add bundle size check
    
    * add more languages
    
    * add php
    
    * reuse some langs
    
    * use gzip
    
    * Revert "remove katex"
    
    This reverts commit c0e5046accd10be3f83018cffdc29a652849fc61.
    
    * use better maintained @vscode/markdown-it-katex
    
    * fix gzip non deterministic
    
    * ability to add a demo conversation for dev
    
    * fix latex rendering
    
    * add comment
    
    * latex codeblock as code
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <son@huggingface.co>

commit b5ae1ddff93403300fc79c7ab5ee73b8cfbb3457
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Dec 15 13:16:42 2024 +0200

    gguf-py : bump to v0.13.0

commit 89d604f2c87af9db657d8a27a1528bc4b7579c29
Author: Michelle Tan <41475767+MichelleTanPY@users.noreply.github.com>
Date:   Sat Dec 14 22:29:45 2024 +0000

    server: Fix `has_next_line` in JSON response (#10818)
    
    * Update server JSON response.
    
    * Add unit test to check `has_new_line` JSON response
    
    * Remove `has_new_line` unit test changes.
    
    * Address code review comment: type check for `has_new_line` in unit test

commit e52aba537a34d51a65cddec6bc6dafc9031edc63
Author: Evgeny Kurnevsky <kurnevsky@gmail.com>
Date:   Sat Dec 14 18:17:36 2024 +0000

    nix: allow to override rocm gpu targets (#10794)
    
    This allows to reduce compile time when you are building for a single GPU.

commit ba1cb19cdd0d92e012e0f6e009e0620f854b6afd
Author: HimariO <dsfhe49854@gmail.com>
Date:   Sat Dec 14 20:43:46 2024 +0800

    llama : add Qwen2VL support + multimodal RoPE (#10361)
    
    * Barebone Qwen2VL LLM convertor
    
    * Add Qwen2VL cli entrypoint
    
    * [WIP] add qwen2vl arch
    
    * Verify m-rope output
    
    * Add vl-rope/2d-rope support for qwen2vl ViT
    
    * update qwen2vl cli tool
    
    * update 5D tensor op workaround
    
    * [WIP] qwen2vl vision model
    
    * make batch and clip utils compatible with qwen2vl
    
    * [WIP] create inference workflow, gguf convert script but fix
    
    * correcting vision-rope behavior, add the missing last layer back to ViT
    
    * add arg parser to qwen2vl_surgery
    
    * replace variable size array with vector
    
    * cuda-gdb cmake preset
    
    * add fp32 mrope, vision rope kernel
    
    * add fp16 support for qwen2vl and m-rope
    
    * add `GGML_ROPE_TYPE_MROPE`, `GGML_ROPE_TYPE_VISION`
    
    * fix rope op mode switching, out dated func args
    
    * update `llama_hparams`
    
    * update to keep up stream changes
    
    * resolve linter, test errors
    
    * add makefile entry, update speical image padding token
    
    * add mrope unit test, fix few compiler warnings
    
    * rename `mrope` related function, params
    
    * minor updates on debug util, bug fixs
    
    * add `m-rope` testcase to `test-backend-ops`
    
    * Apply suggestions from code review
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * fix traililng whitespce
    
    * store `llama_hparams.rope_sections` with fixed size array
    
    * update position id tensor size check in GGML_OP_ROPE
    
    * minor updates
    
    * update `ggml_backend_*_supports_op` of unsupported backends
    
    * remote old `rope_section` compare operator
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 56eea0781cbd2608ed8ff524955495569b9264be
Author: cduk <19917266+cduk@users.noreply.github.com>
Date:   Fri Dec 13 23:21:49 2024 +0100

    Removes spurious \r in output that causes logging in journalctl to treat lines as binary and therefore hidden by default (#10771)
    
    Signed-off-by: Charles Darke <s.cduk@toodevious.com>
    Co-authored-by: Charles Darke <s.cduk@toodevious.com>

commit a76c56fa1a3d27467eb97468d8c3b2fe1243b61a
Author: lhez <quic_lih@quicinc.com>
Date:   Fri Dec 13 12:23:52 2024 -0800

    Introducing experimental OpenCL backend with support for Qualcomm Adreno GPUs (#10693)
    
    * [cl][adreno] Add Adreno GPU support
    
    Add new OpenCL backend to support Adreno GPUs
    
    ---------
    
    Co-authored-by: Skyler Szot <quic_sszot@quicinc.com>
    Co-authored-by: Shangqing Gu <quic_shawngu@quicinc.com>
    Co-authored-by: Alexander Angus <quic_aangus@quicinc.com>
    Co-authored-by: Hongqiang Wang <quic_wangh@quicinc.com>
    Co-authored-by: Max Krasnyansky <quic_maxk@quicinc.com>
    
    * [cl][ci] Add workflow for CL
    
    * [cl][adreno] Fix memory leak for non SMALL_ALLOC path
    
    * opencl: integrate backend dyn.load interface and fix compiler and format warnings
    
    * opencl: remove small-alloc support and fix build errors for non-opencl platforms
    
    * opencl: fixed merge conflict (MUSA added twice in cmake)
    
    * opencl-ci: use RUNNER_TEMP instead of github.workspace
    
    * opencl: fix embed tool invocation with python3
    
    * opencl: CI workflow fixes
    
    * opencl: Clean up small-alloc in CMake files
    
    * opencl: cleanup ggml-opencl2 header file
    
    * opencl: use ulong for offsets and strides in ADD kernel
    
    * opencl: use cl_ulong for all offsets
    
    * opencl: use cl_ulong for sizes and strides
    
    * opencl: use `GGML_LOG_xxx` instead of `fprintf(stderr, ...)`
    
    * opencl: rename backend `opencl2` -> `opencl`
    
    * opencl: rename kernel files `ggml-opencl2` -> `ggml-opencl`
    
    * opencl: make OpenCL required, remove redundant lib and inc directories
    
    * `ggml-base`, `..` and `.` are added by `ggml_add_backend_library`
    
    * opencl: rename backend - funcs, structs, etc `opencl2` -> `opencl`
    
    * opencl: remove copyright marker since main license already covers
    
    * opencl: replace some more OPENCL2 leftovers
    
    * opencl: remove limits on `tensor_extra`
    
    * opencl: use pools for `tensor_extra`
    
    * opencl: fix compiler warnings with GCC and Clang
    
    Still getting the warning about clCreateCmdQueue being obsolete.
    Will fix that separately.
    
    * opencl: fail gracefully if opencl devices are not available
    
    Also for unsupported GPUs.
    
    * opencl: fix MSVC builds (string length error)
    
    * opencl: check for various requirements, allow deprecated API
    
    * opencl: update log message for unsupported GPUs
    
    ---------
    
    Co-authored-by: Skyler Szot <quic_sszot@quicinc.com>
    Co-authored-by: Shangqing Gu <quic_shawngu@quicinc.com>
    Co-authored-by: Alexander Angus <quic_aangus@quicinc.com>
    Co-authored-by: Hongqiang Wang <quic_wangh@quicinc.com>
    Co-authored-by: Max Krasnyansky <quic_maxk@quicinc.com>

commit c27ac678dd393af0da9b8acf10266e760c8a0912
Author: Eric Curtin <ecurtin@redhat.com>
Date:   Fri Dec 13 18:34:25 2024 +0000

    Opt class for positional argument handling (#10508)
    
    Added support for positional arguments `model` and `prompt`. Added
    functionality to download via strings like:
    
      llama-run llama3
      llama-run ollama://granite-code
      llama-run ollama://granite-code:8b
      llama-run hf://QuantFactory/SmolLM-135M-GGUF/SmolLM-135M.Q2_K.gguf
      llama-run huggingface://bartowski/SmolLM-1.7B-Instruct-v0.2-GGUF/SmolLM-1.7B-Instruct-v0.2-IQ3_M.gguf
      llama-run https://example.com/some-file1.gguf
      llama-run some-file2.gguf
      llama-run file://some-file3.gguf
    
    Signed-off-by: Eric Curtin <ecurtin@redhat.com>

commit 11e07fd63bac1cb642380a7a3eac03fd8703e948
Author: Corentin REGAL <corentin.regal@gmail.com>
Date:   Fri Dec 13 18:23:50 2024 +0100

    fix: graceful shutdown for Docker images (#10815)

commit 4601a8bb6784d2ab8b4b605354b51979fbeea1d3
Author: Jett Janiak <jettjaniak@gmail.com>
Date:   Fri Dec 13 15:48:44 2024 +0100

    gguf-py : numpy 2 newbyteorder fix (#9772)

commit 9f35e44592a7646a5803620eb6a3f0ed5ac90553
Author: 谢乃闻 <sienaiwun@users.noreply.github.com>
Date:   Fri Dec 13 12:56:07 2024 +0000

    Fix crash caused by ggml_backend_load_all when launching on Android Activity (#10812)
    
    * Fix crash caused by ggml_backend_load_all when launching on AndroidActivity.
    
    Details:
    Calling ggml_backend_load_all during initialization in the AndroidActivity project leads to a crash with the error:
    terminating with uncaught exception of type std::__ndk1::__fs::filesystem::filesystem_error: filesystem error: in directory_iterator::directory_iterator(...): Permission denied [./].
    This issue occurs because AndroidActivity restricts file access due to sandboxing.
    
    Reproduction:
    In the example folder, the LlamaAndroid project can reproduce the crash by calling ggml_backend_load_all first in Java_android_llama_cpp_LLamaAndroid_backend_1init.
    
    * Update ggml/src/ggml-backend-reg.cpp
    
    ---------
    
    Co-authored-by: Diego Devesa <slarengh@gmail.com>

commit 64ae0655114f84f11a724bc6878c6f8f4a55560b
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Fri Dec 13 08:42:04 2024 +0000

    vulkan: small mul_mat_vec optimizations (#10665)
    
    * double the number of rows per workgroup
    
    * Update ggml-vulkan.cpp
    
    * Vulkan: Add VK_EXT_subgroup_size_control support to ensure full subgroups for coopmats
    
    * only increase the number of rows for amd and subgroup size 64
    
    * fix missing NUM_ROWS for mul_mat_vec_iq4_nl_f16_f32, untested
    
    * use subgroup min and max to check for gcn (requires https://github.com/ggerganov/llama.cpp/pull/10721)
    
    * manual merge ggml-vulkan.cpp
    
    * set min and max subgroup size in any case
    
    * Also double the number of rows for Intel GPUs

commit 83ed24a97b500ccdb32b90b94e6f9621ad8db79e
Author: Akarshan Biswas <akarshan.biswas@gmail.com>
Date:   Fri Dec 13 12:12:15 2024 +0530

    SYCL: Reduce most of the compiler warnings (#10748)
    
    * Try to reduce some unused and typecast warnings
    
    * Reduce compiler warnings step 2
    
    * add a newline at the end of the file
    
    * Initialize nreduce as size_t
    
    * [SYCL] Remove pragma directives from mmq.cpp
    
    * SYCL: mmq add condition to prevent blocks_per_tile_x_row variable from becoming 0
    
    * SYCL softmax: Initialize nreduce as size_t
    
    * ggml-sycl.cpp: fix some trailing whitespaces
    
    * SYCL: remove the unused variables instead of commenting it out
    
    * SYCL poo2d kernel: set NAN for invalid pooling op
    
    * SYCL gemm.hpp: remove pragma directives
    
    * SYCL gemm.hpp: use const cast to properly support dnnl::memory
    
    * SYCL: wkv6 remove a comment
    
    * SYCL: clean comments step 2
    
    * SYCL: clean comments and variables step 3
    
    * SYCL: Use GGML_UNUSED for unused variables
    
    * SYCL: remove extra empty lines and a comment
    
    * Remove TODO
    
    * cleanup spaces
    
    * add a stdout for unsupported op
    
    * use sycl printf over fprintf
    
    * remove prints for CI
    
    * SYCL ggml-sycl: pool2D use sycl::nan and remove if-else block
    
    ---------
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>

commit d583cd03f663701858ba152a334cbb467fabe342
Author: Karol Kontny <82021046+kkontny@users.noreply.github.com>
Date:   Fri Dec 13 01:04:19 2024 +0100

    ggml : Fix compilation issues on ARM platform when building without fp16 (#10811)

commit adffa6ffd59997da59f62b72d2b79fc37e085e84
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Dec 12 22:53:05 2024 +0100

    common : improve -ctv -ctk CLI arguments (#10806)
    
    * common : improve ctv ctk cli argument
    
    * regenerate docs
    
    * even better approach
    
    * use std::vector

commit 274ec65af6e54039eb95cb44904af5c945dca1fa
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Dec 12 20:52:28 2024 +0100

    contrib : add ngxson as codeowner (#10804)

commit 8faa1d4dd42f6cb26088ce7f5bbca5996b921685
Author: a3sh <38979186+A3shTnT@users.noreply.github.com>
Date:   Fri Dec 13 02:09:50 2024 +0800

    CUDA: faster non-contiguous concat (#10760)
    
    * faster uncontiguous concat
    
    * Use a lambda to avoid code duplication
    
    Co-authored-by: Diego Devesa <slarengh@gmail.com>
    
    * Update ggml/src/ggml-cuda/concat.cu
    
    * add constexpr  and static assert
    
    ---------
    
    Co-authored-by: Diego Devesa <slarengh@gmail.com>

commit cb13ef85a444eb52a3f1b82dce198ceb25606583
Author: Diego Devesa <slarengh@gmail.com>
Date:   Thu Dec 12 19:02:49 2024 +0100

    remove CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS (#10797)
    
    other windows build fixes

commit 4064c0e3b6c27440f5d12b7caaf90b4088c28c61
Author: 0cc4m <picard12@live.de>
Date:   Thu Dec 12 18:36:00 2024 +0100

    Vulkan: Use improved q4_k and q5_k dequant code in dequant shaders (#10798)

commit dc5301d565b7d0b2c691f7df13099aab3997479c
Author: 0cc4m <picard12@live.de>
Date:   Thu Dec 12 18:35:37 2024 +0100

    Vulkan: Add VK_EXT_subgroup_size_control support to ensure full subgroups for coopmats (#10721)
    
    * Vulkan: Add VK_EXT_subgroup_size_control support to ensure full subgroups for coopmats
    
    * Fix subgroup size control extension support check
    
    Add accf32 and accf16 checks for coopmats
    
    * Also disable coopmats on amdvlk

commit 9fdb1243049aa7e8211693f116daf2052d47507d
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Dec 12 16:57:32 2024 +0100

    common : add missing env var for speculative (#10801)

commit 5555c0c1f66d766c544c30699190dec0285bcbfc
Author: CentricStorm <CentricStorm@users.noreply.github.com>
Date:   Wed Dec 11 22:40:40 2024 +0000

    docs: update server streaming mode documentation (#9519)
    
    Provide more documentation for streaming mode.

commit 973f328b1e92a6406030442dfd15b29449e89747
Merge: 235f6e14 fb18934a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Dec 11 23:14:46 2024 +0200

    Merge pull request #10788 from ggerganov/gg/gguf-py-0.11.0

commit fb18934a97425c42c8b32a1baaa94f0080eb051d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Dec 11 23:13:31 2024 +0200

    gguf-py : bump version to 0.11.0

commit 235f6e14bf0ed0211c51aeff14139038ae1000aa
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed Dec 11 20:52:14 2024 +0100

    server : (UI) add tok/s, get rid of completion.js (#10786)
    
    * get rid of completion.js
    
    * extract chat bubble to a component
    
    * add tok/s info
    
    * sync
    
    * fix BASE_URL
    
    * only extract timings when it's enabled
    
    * fix auto scroll

commit 1a31d0dc00ba946d448e16ecc915ce5e8355994e
Author: qingy1337 <qxli2@students.everettcc.edu>
Date:   Wed Dec 11 07:16:32 2024 -0800

    Update README.md (#10772)

commit 92f77a640f763c0af73554fb810a85a7d4c85e5e
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed Dec 11 14:59:41 2024 +0100

    ci : pin nodejs to 22.11.0 (#10779)

commit 484d2f31aed34ff9f096e3961125762e81d9b7d6
Author: kallewoof <kalle.alm@gmail.com>
Date:   Wed Dec 11 22:48:04 2024 +0900

    bug-fix: snprintf prints NULL in place of the last character (#10419)
    
    * bug-fix: snprintf prints NULL in place of the last character
    
    We need to give snprintf enough space to print the last character and the null character, thus we allocate one extra byte and then ignore it when converting to std::string.
    
    * add comment about extra null-term byte requirement

commit 4b4d92b0986e3b627b9a9ef4782973108bf47691
Author: CentricStorm <CentricStorm@users.noreply.github.com>
Date:   Wed Dec 11 10:47:43 2024 +0000

    docs: fix server documentation formatting (#10776)

commit 43041d2eb32623d5b6ca734313327abe96f73146
Author: Gilad S. <7817232+giladgd@users.noreply.github.com>
Date:   Wed Dec 11 02:47:21 2024 +0200

    ggml: load all backends from a user-provided search path (#10699)
    
    * feat: load all backends from a user-provided search path
    
    * fix: Windows search path
    
    * refactor: rename `ggml_backend_load_all_in_search_path` to `ggml_backend_load_all_from_path`
    
    * refactor: rename `search_path` to `dir_path`
    
    * fix: change `NULL` to `nullptr`
    
    Co-authored-by: Diego Devesa <slarengh@gmail.com>
    
    * fix: change `NULL` to `nullptr`
    
    ---------
    
    Co-authored-by: Diego Devesa <slarengh@gmail.com>

commit b685daf3867c54e42a9db484d7b92619021d4510
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Tue Dec 10 14:23:17 2024 -0600

    vulkan: request round-to-even for fp16 in im2col/rope_head (#10767)
    
    Vulkan doesn't mandate a specific rounding mode, but the shader_float_controls
    feature allows rounding mode to be requested if the implementation supports it.

commit dafae66cc242eb766797194d3c85c5e502625623
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Tue Dec 10 19:33:23 2024 +0000

    vulkan: dynamic subgroup size for the remaining k quants (#10745)
    
    * q5_k
    
    q4_k
    
    q3_k
    
    q2_k
    
    q6_k multi row example
    
    * revert as multi row isnt faster for k quants

commit ae4b922614d452477cf5d2fb8cad247c9c12596c
Author: Bartowski <ckealty1182@gmail.com>
Date:   Tue Dec 10 12:23:50 2024 -0500

    imatrix : Add imatrix to --no-context-shift (#10766)
    
    This allows for setting the --no-context-shift value in llama-imatrix which is required for models like DeepSeek

commit 750cb3e246f7e544124cf18cb0c5e0c8b7e38738
Author: Andreas Kieslinger <47689530+aendk@users.noreply.github.com>
Date:   Tue Dec 10 18:23:24 2024 +0100

    CUDA: rename macros to avoid conflicts with WinAPI (#10736)
    
    * Renames NVIDIA GPU-architecture flags to avoid name clashes with WinAPI. (e.g. CC_PASCAL, GPU architecture or WinAPI pascal compiler flag?)
    
    * Reverts erroneous rename in SYCL-code.
    
    * Renames GGML_CUDA_MIN_CC_DP4A to GGML_CUDA_CC_DP4A.
    
    * Renames the rest of the compute capability macros for consistency.

commit a86ad841f103e471ac0fd7ee8852d1eb5015ce89
Author: Yüg <eugeniosegalaweb@gmail.com>
Date:   Tue Dec 10 17:22:34 2024 +0000

    server : add flag to disable the web-ui (#10762) (#10751)
    
    Co-authored-by: eugenio.segala <esegala@deloitte.co.uk>

commit a05e2afcc241c1ecd38ec5cb4c579d90cdf3f918
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Tue Dec 10 11:22:20 2024 -0600

    vulkan: disable spirv-opt for coopmat shaders (#10763)
    
    There are some bugs in the 1.3.296 SDK, so disable this. It isn't strictly
    necessary anyway.
    
    Add missing dependency on vulkan-shaders-gen, so shaders get recompiled when it
    changes.
    
    Fix coopmat support reporting when glslc doesn't support NV_coopmat2.

commit 26a8406ba9198eb6fdd8329fa717555b4f77f05f
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Dec 9 20:07:12 2024 +0100

    CUDA: fix shared memory access condition for mmv (#10740)

commit c37fb4cf62ddf0d33562c4c4a4d6fb45e32ad3b6
Author: Srihari-mcw <96763064+Srihari-mcw@users.noreply.github.com>
Date:   Mon Dec 9 23:10:19 2024 +0530

    Changes to CMakePresets.json to add ninja clang target on windows (#10668)
    
    * Update cmakepreset.json to use clang with ninja by default
    
    * Update cmakepreset.json to add clang and ninja based configs
    
    * Updates to build.md file
    
    * Make updates to rename preset targets
    
    * Update with .cmake file
    
    * Remove additional whitespaces
    
    * Add .cmake file for x64-windows-llvm
    
    * Update docs/build.md
    
    * Update docs/build.md
    
    ---------
    
    Co-authored-by: Max Krasnyansky <max.krasnyansky@gmail.com>

commit 3d98b4cb226c3140bd1ae6c65ed126b7d90332fa
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Mon Dec 9 01:24:01 2024 -0600

    vulkan: fix compile warnings (#10731)

commit 1a05004743e00aca400833be625f0ec8cce176a7
Author: Borislav Stanimirov <b@ibob.bg>
Date:   Mon Dec 9 09:15:13 2024 +0200

    cmake : simplify msvc charsets (#10672)

commit ce8784bdb153ff7794dde5a50b0ebfa51baa6171
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sun Dec 8 23:04:29 2024 +0100

    server : fix format_infill (#10724)
    
    * server : fix format_infill
    
    * fix
    
    * rename
    
    * update test
    
    * use another model
    
    * update test
    
    * update test
    
    * test_invalid_input_extra_req

commit e52522b8694ae73abf12feb18d29168674aa1c1b
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sun Dec 8 20:38:51 2024 +0100

    server : bring back info of final chunk in stream mode (#10722)
    
    * server : bring back into to final chunk in stream mode
    
    * clarify a bit
    
    * traling space

commit 06d70147e6480c021e493c442ae0f0d83ae366de
Author: stduhpf <stephduh@live.fr>
Date:   Sun Dec 8 19:19:19 2024 +0100

    Vulkan: fix NaN in tanh.comp with AMD proprietary driver on Windows (#10723)
    
    * Vulkan: fix NaN in tanh.comp
    
    * Faster NaN-free tanh

commit 43ed389a3f102517e6f7d5620d8e451e88afbf27
Author: Diego Devesa <slarengh@gmail.com>
Date:   Sun Dec 8 12:14:54 2024 +0100

    llama : use cmake for swift build (#10525)
    
    * llama : use cmake for swift build
    
    * swift : <> -> ""
    
    * ci : remove make
    
    * ci : disable ios build
    
    * Revert "swift : <> -> """
    
    This reverts commit d39ffd9556482b77d4ea5b118b453fc1c097a31d.
    
    * ci : try fix ios build
    
    * ci : cont
    
    * ci : cont
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit ecc93d0558fc3ecb8a5af69d2ece02fae4710ade
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Sun Dec 8 02:05:55 2024 -0600

    vulkan: compile a test shader in cmake to check for coopmat2 support (#10713)

commit 62e84d984875372f4b0fb89a67658e012ff0cc9f
Author: Robert Collins <roberto.tomas.cuentas@gmail.com>
Date:   Sat Dec 7 16:12:27 2024 -0500

    llama : add 128k yarn context for Qwen (#10698)
    
    * add 128k yarn context for Qwen
    
    * added property for model tensors
    
    * removing useless line

commit 3573fa8e7b7f0865638b52b4e9b4d2006f0558a2
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat Dec 7 20:21:09 2024 +0100

    server : (refactor) no more json in server_task input (#10691)
    
    * server : (refactor) no more json in server_task input
    
    * add test for slots endpoint
    
    * add tests for /props and /slots
    
    * remove task inf_type
    
    * fix CI by adding safe_json_to_str
    
    * add "model_path" to /props
    
    * update readme

commit d9c3ba2b7749c00df477599aa141a98b4521aa2c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Dec 7 18:38:15 2024 +0200

    ggml : disable iq4_nl interleave size 8 (#10709)
    
    ggml-ci

commit ce4a7b849388b67d45ad420bdd82d5efcd55647a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Dec 7 18:02:05 2024 +0200

    server : various fixes (#10704)
    
    * server : various fixes
    
    ggml-ci
    
    * server : show curent seed in slot_params
    
    ggml-ci
    
    * fix /slots endpoint
    
    * Update examples/server/server.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * server : reflect endpoint response changes in the readme
    
    ggml-ci
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <son@huggingface.co>
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>

commit 19d8762ab61df8286367588a80b9c7db4cb568db
Author: Djip007 <3705339+Djip007@users.noreply.github.com>
Date:   Sat Dec 7 13:37:50 2024 +0100

    ggml : refactor online repacking (#10446)
    
    * rename ggml-cpu-aarch64.c to .cpp
    
    * reformat extra cpu backend.
    
    - clean Q4_0_N_M and IQ4_0_N_M
      - remove from "file" tensor type
      - allow only with dynamic repack
    
    - extract cpu extra bufts and convert to C++
      - hbm
      - "aarch64"
    
    - more generic use of extra buffer
      - generalise extra_supports_op
      - new API for "cpu-accel":
         - amx
         - aarch64
    
    * clang-format
    
    * Clean Q4_0_N_M ref
    
    Enable restrict on C++
    
    * add op GGML_OP_MUL_MAT_ID for Q4_0_N_M with runtime repack
    
    * added/corrected control on tensor size for Q4 repacking.
    
    * Update ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * add debug logs on repacks.
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit c2a16c0bdbe2e51adf318918bad82f0c3e3d6f3b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Dec 7 11:52:44 2024 +0200

    server : fix free of spec context and batch (#10651)
    
    ggml-ci

commit 3df784b3050f657ea681f804187ce5bddb433e88
Author: 0cc4m <picard12@live.de>
Date:   Sat Dec 7 10:24:15 2024 +0100

    Vulkan: VK_KHR_cooperative_matrix support to speed up prompt processing (#10597)
    
    * Vulkan: Implement VK_KHR_cooperative_matrix support in the matrix matrix multiplication shader
    
    * Improve performance with better q4_k and q5_k dequant and store unrolling
    
    * Add Vulkan MUL_MAT and MUL_MAT_ID accumulator precision selection
    
    * Rework mulmat shader selection and compilation logic, avoid compiling shaders that won't get used by device
    
    * Vulkan: Implement accumulator switch for specific mul mat mat shaders
    
    * Vulkan: Unroll more loops for more mul mat mat performance
    
    * Vulkan: Add VK_AMD_shader_core_properties2 support to read Compute Unit count for split_k logic
    
    * Disable coopmat support on AMD proprietary driver
    
    * Remove redundant checks
    
    * Add environment variable GGML_VK_DISABLE_COOPMAT to disable VK_KHR_cooperative_matrix support
    
    * Fix rebase typo
    
    * Fix coopmat2 MUL_MAT_ID pipeline selection

commit 86a1934978ce5daffc7e5b4251f6540ee5e7b47b
Author: Robert Ormandi <52251610+ormandi@users.noreply.github.com>
Date:   Sat Dec 7 01:55:01 2024 -0600

    metal : Extend how Llama.cpp locates metal resources (#10676)
    
    * metal : Extend how Llama.cpp locates metal resources (#10675)
    
      * It searches the resource file in the directory where the current
        binary is located as well.
      * Resolves symbolic links.
    
    Rationale:
    
    When we plug this dependency into a Bazel build and run it in the
    context of Bazel (e.g. testing):
    
      * the execution directory is often very different from where the files
        are located and no direct control over this (Bazel sandboxing),
      * the Bazel sandbox often use symbolic links to make files available.
    
    With this patch, we can have the resource file added to the target,
    can build and run tests in the context of Bazel.
    
    * Update ggml/src/ggml-metal/ggml-metal.m
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml/src/ggml-metal/ggml-metal.m
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 784a14aa496a66cc43e76014a1bf4333b4a2a55a
Author: Sukriti Sharma <Ssukriti@users.noreply.github.com>
Date:   Sat Dec 7 00:02:14 2024 -0700

    convert : add support for Roberta embeddings (#10695)

commit c5ede3849fc021174862f9c0bf8273808d8f0d39
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Dec 6 21:33:15 2024 +0200

    convert : add custom attention mapping

commit f162d45a21b27f7613f14539f9a4932d6ff3ca48
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Dec 6 13:29:05 2024 +0100

    common : bring back --no-warmup to server (#10686)

commit 6c5bc0625fae6909cb40def15bc4bb45db6f7f4d
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Dec 6 11:14:32 2024 +0100

    server : (refactoring) do not rely on JSON internally (#10643)
    
    * server : (refactoring) reduce usage of json internally
    
    * move all response types to struct
    
    * wip [no ci]
    
    * many fixes
    
    * add virtual function
    
    * fix index
    
    * minor style fix
    
    * add std::move
    
    * refactor handle_completions_generic
    
    * add virtual functions
    
    * remove server.hpp
    
    * clarify server_sent_event RFC specs
    
    * apply review comments
    
    * fix model_alias and completion_probabilities
    
    * small clean up
    
    * remove virtual for to_json_oai_compat()
    
    * naming oai_compat --> oaicompat
    
    * fix unwanted recursive call
    
    * update docs

commit 7736837d62efed1dbebfe579472fca041eda12d6
Author: Plamen Minev <pacominev@gmail.com>
Date:   Thu Dec 5 23:36:41 2024 +0200

    fix(server) : not show alert when DONE is received (#10674)

commit c9c6e01daedac542b174c235872569fce5385982
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Thu Dec 5 13:15:05 2024 -0600

    vulkan: Add VK_NV_cooperative_matrix2 support for mul_mat and flash attention (#10206)

commit 6fe624783166e7355cec915de0094e63cd3558eb
Author: Riccardo Orlando <Riccorl@users.noreply.github.com>
Date:   Thu Dec 5 19:30:59 2024 +0100

    llama : add Minerva 7B model support (#10673)
    
    * Support for Minerva 7B
    
    * Update convert_hf_to_gguf_update.py

commit 0cd182ebcc2c45907240e727e80e8fbf0f5fdee9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Dec 5 13:27:42 2024 +0200

    sync : ggml

commit a8cbab201dcf4c76c30b8028427494d71f02bb57
Author: PAB <pierreantoine.bannier@gmail.com>
Date:   Wed Dec 4 09:19:30 2024 +0100

    ggml: add `GGML_SET` Metal kernel + i32 CPU kernel (ggml/1037)
    
    * implemented cpu kernel
    
    * add i32 test cases in test-backend-ops
    
    * typedef `ggml_metal_kargs_set`
    
    * implemented `kernel_set`
    
    * memcpy

commit c2082d93a82d66dca112098c63775d7a97e6d47b
Author: PAB <pierreantoine.bannier@gmail.com>
Date:   Tue Dec 3 20:20:04 2024 +0100

    ggml : add `GGML_PAD_REFLECT_1D` operation (ggml/1034)
    
    * ggml_pad_reflect_1d defined in header
    
    * implemented on CPU
    
    * called the forward pass
    
    * impl Metal kernel
    
    * added Metal kernel
    
    * added OP_PAD_REFLECT_1D in test-backend-ops.cpp
    
    * add test-pad-reflect-1d test case
    
    * test case support multiple backend

commit d405804be84cfdac936f28b3446ce8cb988408d8
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Dec 5 08:47:55 2024 +0100

    py : update outdated copy-paste instructions [no ci] (#10667)
    
    This commit updates the copy-paste instruction in
    convert_hf_to_gguf_update.py to reflect that convert_hf_to_gguf.py
    will have already been updated with the new get_vocab_base_pre()
    function when this script completes.

commit f112d198cd9953b633ac662c2705d6e423438717
Author: aryantandon01 <80969509+aryantandon01@users.noreply.github.com>
Date:   Thu Dec 5 03:49:20 2024 +0530

    Update deprecation-warning.cpp (#10619)
    
    Fixed Path Separator Handling for Cross-Platform Support (Windows File Systems)

commit 1da7b765692764a8b33b08da61cbee63812a7bd9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Dec 4 22:38:20 2024 +0200

    server : fix speculative decoding with context shift (#10641)
    
    * server : fix speculative decoding with context shift
    
    ggml-ci
    
    * server : take into account speculative limits
    
    ggml-ci
    
    * server : add tests

commit 59f4db10883a4f3e855cffbf2c3ab68430e95272
Author: Diego Devesa <slarengh@gmail.com>
Date:   Wed Dec 4 14:45:40 2024 +0100

    ggml : add predefined list of CPU backend variants to build (#10626)
    
    * ggml : add predefined list of CPU backend variants to build
    
    * update CPU dockerfiles

commit 2803540814bf0a4e44d0960ff6afda6bac971c17
Author: Diego Devesa <slarengh@gmail.com>
Date:   Wed Dec 4 14:40:44 2024 +0100

    ggml-cpu : fix HWCAP2_I8MM value (#10646)

commit 253b7fde910731104670724391bfbcb94d97d0c3
Author: ltoniazzi <61414566+ltoniazzi@users.noreply.github.com>
Date:   Wed Dec 4 09:45:48 2024 +0000

    Fix HF repo commit to clone lora test models (#10649)

commit 8d0cfd554a9ae545ff94d27e04458f537b4e8c0e
Author: JFLFY2255 <JFLFY2255@163.com>
Date:   Wed Dec 4 17:42:50 2024 +0800

    llama: Support MiniCPM-1B (with & w/o longrope) (#10559)

commit 2759916d86b70e7aceaed4d0b4e7ed126f0f9e51
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Wed Dec 4 01:28:59 2024 -0600

    vulkan: Implement "fast divide" (mul+shift) for unary ops like copy (#10642)

commit 40c6d79fb52f995f47507fedfeaae2ac05d9b35c
Author: Nicolò Scipione <nicolo.scipione@codeplay.com>
Date:   Wed Dec 4 02:29:20 2024 +0100

    SYCL : Move to compile time oneMKL interface backend selection for NVIDIA backend (#10584)
    
    * [SYCL] Move to Compile Time backend selection on oneMKL Interface for NVIDIA backend
    
    Move to compile time selection to backend to avoid latency at run time.
    Add it to all mkl gemm calls and only for NVIDIA backend.
    
    Signed-off-by: nscipione <nicolo.scipione@codeplay.com>
    
    * Formatting
    
    * Address PR comments to increase readibility
    
    ---------
    
    Signed-off-by: nscipione <nicolo.scipione@codeplay.com>

commit 98036d5670f21e9b9a99d5e3dbb3bf7589f5c4e3
Author: Wang Ran (汪然) <wangr@smail.nju.edu.cn>
Date:   Wed Dec 4 09:22:50 2024 +0800

    fix typo of README.md (#10605)

commit cd2f37b304f8e88b9de8424b31078b97f9cf7c60
Author: Frankie Robertson <frankier@users.noreply.github.com>
Date:   Wed Dec 4 02:41:37 2024 +0200

    Avoid using __fp16 on ARM with old nvcc (#10616)

commit da6aac91f150a3b0bcc26d3fd50288accb15f179
Author: Benson Wong <mostlygeek@gmail.com>
Date:   Tue Dec 3 16:40:36 2024 -0800

    Add docs for creating a static build (#10268) (#10630)
    
    * Add notes for a static build
    
    * Update docs/build.md
    
    ---------
    
    Co-authored-by: Diego Devesa <slarengh@gmail.com>

commit 01e6d9bb71eb71fe1f811f2fdef15753232cd0f2
Author: piDack <104877312+piDack@users.noreply.github.com>
Date:   Wed Dec 4 08:26:37 2024 +0800

    clip : add sycl support (#10574)
    
    Co-authored-by: piDack <pcdack@hotmail.co>

commit cc98896db858df7aa40d0e16a505883ef196a482
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Tue Dec 3 13:29:54 2024 -0600

    vulkan: optimize and reenable split_k (#10637)
    
    Use vector loads when possible in mul_mat_split_k_reduce. Use split_k
    when there aren't enough workgroups to fill the shaders.

commit 91c36c269bca75b2d08119c653512cd20b4ea2ba
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Dec 3 19:38:44 2024 +0100

    server : (web ui) Various improvements, now use vite as bundler (#10599)
    
    * hide buttons in dropdown menu
    
    * use npm as deps manager and vite as bundler
    
    * fix build
    
    * fix build (2)
    
    * fix responsive on mobile
    
    * fix more problems on mobile
    
    * sync build
    
    * (test) add CI step for verifying build
    
    * fix ci
    
    * force rebuild .hpp files
    
    * cmake: clean up generated files pre build

commit 1cd3df46bd49a0c1c78da8b68c956448a73b7476
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Dec 3 19:42:30 2024 +0200

    scripts : remove amx sync
    
    ggml-ci

commit c5054718575d37f43cc4e6f61ea7c4014ad2c0cf
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Dec 3 19:40:25 2024 +0200

    sync : ggml

commit e9e661bd59364e5d4fce035834b6cadcadf8c2ef
Author: mahorozte <41834471+mahorozte@users.noreply.github.com>
Date:   Tue Dec 3 21:11:43 2024 +0800

    CUDA: remove unnecessary warp reduce in FA (ggml/1032)
    
    * kqmax_new_j in every thread within warp is same after operate at line 199,this reduce can be omit
    
    * same problem in vec32
    
    ---------
    
    Co-authored-by: ZhaoXiaoYu <zhao.xiaoyu@zte.com.cn>

commit efb6ae963031709fc331e6e48cc4606ac8f9c3a7
Author: PAB <pierreantoine.bannier@gmail.com>
Date:   Mon Dec 2 19:27:24 2024 +0100

    feat: add `GGML_UNARY_OP_ARGMAX` Metal kernel (ggml/1019)
    
    * implemented argmax kernel
    
    * tpig -> tgpig
    
    * change to strides
    
    * contiguous assertions
    
    * kernel working and tested
    
    * argmax simd parallel implementation
    
    * added 2 new tests for argmax in test-backend-ops
    
    * cosmit
    
    * added 3 tests cases for perf eval
    
    * add test_argmax in make_test_cases_perf
    
    * Update test-backend-ops.cpp
    
    Co-authored-by: Diego Devesa <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: Diego Devesa <slarengh@gmail.com>

commit 667d70d1704dfa6977505f5d01d4638669b90dce
Author: PAB <pierreantoine.bannier@gmail.com>
Date:   Thu Nov 28 09:25:06 2024 +0100

    metal : add `GGML_OP_CONV_TRANSPOSE_1D` kernels (ggml/1026)
    
    * wip
    
    * wip implementation f32
    
    * kernel conv transpose 1d f32 working
    
    * initial commit

commit 3b4f2e33e2cbfca621e623c4b92b88da57a8c2f4
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Dec 3 12:54:30 2024 +0100

    llama : add missing LLAMA_API for llama_chat_builtin_templates (#10636)

commit 82bca2257b3cec72676abb26011f1b99fcdab29d
Author: Nikolaos Pothitos <pothitos@di.uoa.gr>
Date:   Tue Dec 3 12:50:08 2024 +0200

    readme : add option, update default value, fix formatting (#10271)
    
    * readme : document --no-display-prompt
    
    * readme : update default prompt context size
    
    * readme : remove unnecessary indentation
    
    Indenting a line with four spaces makes Markdown treat that section as
    plain text.
    
    * readme : indent commands under bullets
    
    * readme : indent commands in lettered list

commit 0115df2f65ac7c64dd0e5915c72ecc4a9343a130
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Dec 3 11:52:33 2024 +0200

    metal : small-batch mat-mul kernels (#10581)
    
    * metal : small-batch mat-mul kernels
    
    ggml-ci
    
    * metal : add rest of types
    
    ggml-ci
    
    * metal : final adjustments
    
    ggml-ci
    
    * metal : add comments
    
    ggml-ci

commit 515d4e53727924e48774f45ecb15bdacbf851e13
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Dec 3 11:21:43 2024 +0200

    github : minify link [no ci] (revert)
    
    this doesn't work as expected

commit 844e2e1feec887c803845a3b8762f3b15979b095
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Dec 3 11:20:35 2024 +0200

    github : minify link [no ci]

commit 70b98fadbc8c07a0144f3b50a4d7ab7e2aeff878
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Dec 3 11:20:00 2024 +0200

    server : fix default draft model parameters (#10586)
    
    * server : force F16 KV cache for the draft model
    
    ggml-ci
    
    * server : fix draft params
    
    ggml-ci
    
    * server : various params fixes
    
    ggml-ci

commit 642330ac7cea11dc1f9d3df2b8f3dbd10b5e3f3e
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Dec 2 22:10:19 2024 +0100

    llama : add enum for built-in chat templates (#10623)
    
    * llama : add enum for supported chat templates
    
    * use "built-in" instead of "supported"
    
    * arg: print list of built-in templates
    
    * fix test
    
    * update server README

commit 8648c521010620c2daccfa1d26015c668ba2c717
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Dec 2 21:22:53 2024 +0200

    make : deprecate (#10514)
    
    * make : deprecate
    
    ggml-ci
    
    * ci : disable Makefile builds
    
    ggml-ci
    
    * docs : remove make references [no ci]
    
    * ci : disable swift build
    
    ggml-ci
    
    * docs : remove obsolete make references, scripts, examples
    
    ggml-ci
    
    * basic fix for compare-commits.sh
    
    * update build.md
    
    * more build.md updates
    
    * more build.md updates
    
    * more build.md updates
    
    * Update Makefile
    
    Co-authored-by: Diego Devesa <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 64ed2091b24b2f9747148fdf49a34ed5938762c3
Author: haopeng <657407891@qq.com>
Date:   Mon Dec 2 21:45:54 2024 +0800

    server: Add "tokens per second" information in the backend (#10548)
    
    * add cmake rvv support
    
    * add timings
    
    * remove space
    
    * update readme
    
    * fix
    
    * fix code
    
    * remove empty line
    
    * add test
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <son@huggingface.co>

commit 991f8aabeec89d801300bb179e52013fb0eb0584
Author: Akarshan Biswas <akarshan.biswas@gmail.com>
Date:   Mon Dec 2 12:34:11 2024 +0530

    SYCL: Fix and switch to GGML_LOG system instead of fprintf (#10579)
    
    * Switched to GGML_LOG
    
    * Fix missing semicolon

commit 4cb003dd8d1f37523120a21e4b1a50a2adcb8c84
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Dec 2 08:53:27 2024 +0200

    contrib : refresh (#10593)
    
    * contrib : refresh
    
    * contrib : expand [no ci]
    
    * contrib : expand test-backend-ops instructions
    
    * contrib : add CODEOWNERS
    
    * prs : update template to not have checkbox [no ci]

commit 917786f43d0f29b7c77a0c56767c0fa4df68b1c5
Author: Juk Armstrong <69222624+jukofyork@users.noreply.github.com>
Date:   Sun Dec 1 22:09:49 2024 +0000

    Add `mistral-v1`, `mistral-v3`, `mistral-v3-tekken` and `mistral-v7` chat template types (#10572)
    
    * Templates: `mistral-v1`, `mistral-v2`, `mistral-v3`, `mistral-v3-tekken`
    
    * Changed system message logic and added tests for all 4
    
    * Invalid `system_message` instead of `content` fixed
    
    * Removed tab-indented lines
    
    * Added template code and test for `mistral-v7`
    
    * Added all tests. Fixed bug with `tmpl == "llama2"` test.
    
    * Replaced tabs with spaces.
    
    * Removed `'mistral-v2'` option as no (open) models ever used it
    
    * Removed all references to 'v2' template from comments
    
    * Update llama.cpp
    
    Fixed `trim_assistant_message` bug

commit 5e1ed95583ca552a98d8528b73e1ff81249c2bf9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Dec 1 21:37:54 2024 +0200

    grammars : add English-only grammar (#10612)

commit 5c7a5aa0c32eb19ce03e178560797db5875d7692
Author: Wang Qin <37098874+wangqin0@users.noreply.github.com>
Date:   Sun Dec 1 10:11:42 2024 -0800

    ci: add error handling for Python venv creation in run.sh (#10608)

commit 3420909dffa50e70660524797a1e715a717684d2
Author: Diego Devesa <slarengh@gmail.com>
Date:   Sun Dec 1 16:12:41 2024 +0100

    ggml : automatic selection of best CPU backend (#10606)
    
    * ggml : automatic selection of best CPU backend
    
    * amx : minor opt
    
    * add GGML_AVX_VNNI to enable avx-vnni, fix checks

commit 86dc11c5bcf34db2749d8bd8d4fa07a542c94f84
Author: alek3y <44779186+alek3y@users.noreply.github.com>
Date:   Sun Dec 1 12:33:12 2024 +0100

    server : bind to any port when specified (#10590)

commit 6acce3971098772a8aacb10fe8550b4119110581
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Dec 1 11:25:17 2024 +0200

    readme : update the usage section with examples (#10596)
    
    * readme : update the usage section with examples
    
    * readme : more examples

commit 43957ef203b4c9ceaee42c176b3ef44ea4359c85
Author: Wang Qin <37098874+wangqin0@users.noreply.github.com>
Date:   Sat Nov 30 19:19:44 2024 -0800

    build: update Makefile comments for C++ version change (#10598)

commit 0c39f44d70d058940fe2afe50cfc789e3e44d756
Author: Adrien Gallouët <adrien@gallouet.fr>
Date:   Sat Nov 30 18:13:18 2024 +0100

    ggml-cpu: replace AArch64 NEON assembly with intrinsics in ggml_gemv_q4_0_4x4_q8_0() (#10567)
    
    Signed-off-by: Adrien Gallouët <angt@huggingface.co>

commit 3e0ba0e604b1ac5b2cbca9a3f38f91f2be4ef1cd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 30 10:09:21 2024 +0200

    readme : remove old badge

commit abadba05be52cccf6c0da49534e37f6062ce8ded
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 30 09:47:07 2024 +0200

    readme : refresh (#10587)
    
    * readme : refresh
    
    * readme : move section [no ci]
    
    * readme : clarify [no ci]
    
    * readme : fixes [no ci]
    
    * readme : more fixes [no ci]
    
    * readme : simplify [no ci]
    
    * readme : clarify GGUF

commit 0533e7fb3842a523f64dc533bd7bd7147ec2c63a
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Sat Nov 30 07:00:02 2024 +0000

    vulkan: Dynamic subgroup size support for Q6_K mat_vec (#10536)
    
    * subgroup 64 version with subgroup add. 15% faster
    
    scalable version
    
    tested for subgroup sizes 16-128
    
    * check for subgroup multiple of 16 and greater than 16
    
    * subgroup sizes are always a power of 2 (https://github.com/KhronosGroup/GLSL/issues/45)
    
    * force 16 sequential threads per block
    
    * make 16 subgroup size a constant

commit 7cc2d2c88908fc92b97b28acafb82f7d6e425b85
Author: Diego Devesa <slarengh@gmail.com>
Date:   Fri Nov 29 21:54:58 2024 +0100

    ggml : move AMX to the CPU backend (#10570)
    
    * ggml : move AMX to the CPU backend
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit b782e5c7d453b3f1fa8dc6c34cde7e2fa946af93
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Nov 29 21:48:56 2024 +0100

    server : add more test cases (#10569)
    
    * server : add split model test
    
    * add test speculative
    
    * add invalid cases

commit 3a8e9af402f7893423bdab444aa16c5d9a2d429a
Author: Robert Collins <roberto.tomas.cuentas@gmail.com>
Date:   Fri Nov 29 12:21:37 2024 -0500

    imatrix : support combine-only (#10492)
    
    * imatrix-combine-only idea
    
    * ensured that behavior consistent with log

commit a3a3048e7a0f9464d0d625a29257d8bce5da5090
Author: Diego Devesa <slarengh@gmail.com>
Date:   Fri Nov 29 17:45:08 2024 +0100

    cleanup UI link list (#10577)
    
    * cleanup UI link list
    
    * sort list alphabetically
    
    * add missing licenses

commit f0678c5ff4cb8873d6ff48801475ff270db656fa
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Nov 29 16:25:39 2024 +0200

    ggml : fix I8MM Q4_1 scaling factor conversion (#10562)
    
    ggml-ci

commit 4b3242bbea172ac0980378496fbc676d44c4f459
Author: Shupei Fan <dymarkfan@outlook.com>
Date:   Fri Nov 29 21:49:02 2024 +0800

    ggml-cpu: fix typo in gemv/gemm iq4_nl_4_4 (#10580)

commit 0f77aae5608f16780a49926b67be6d56ec4b09bd
Author: Alberto Cabrera Pérez <alberto.cabrera@codeplay.com>
Date:   Fri Nov 29 12:38:45 2024 +0000

    sycl : offload of get_rows set to 0 (#10432)

commit 266b8519ee6d21e7ba2bf56f5629e20a181fee8b
Author: Alberto Cabrera Pérez <alberto.cabrera@codeplay.com>
Date:   Fri Nov 29 09:49:43 2024 +0000

    sycl : Reroute permuted mul_mats through oneMKL (#10408)
    
    This PR fixes the failing MUL_MAT tests for the sycl backend.

commit 938f6087421889a3af7d0786c64406ced2be81b8
Author: Chenguang Li <87689256+noemotiovon@users.noreply.github.com>
Date:   Fri Nov 29 14:46:55 2024 +0800

    CANN: RoPE operator optimization (#10563)
    
    * [cann] RoPE operator optimization
    
    * [CANN]Code Formatting
    
    ---------
    
    Co-authored-by: noemotiovon <noemotiovon@gmail.com>

commit f095a649ec390e04dfab1b04e646ae8549dafaef
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Fri Nov 29 00:18:02 2024 -0600

    vulkan: get the first command buffer submitted sooner (#10499)
    
    This is an incremental improvement over #9118 to get work to the GPU a bit
    sooner. The first part is to start with a smaller number of nodes before
    the first submit, and ramp it up to the current 100 nodes/submit. The
    second part is to reduce the dryrun overhead for all the nodes that just
    need to request descriptor space.
    
    With these changes I get around 1-2% speedup on RTX 4070 combined with my
    old Haswell-era CPU.

commit 678d7994f4da0af3d29046be99950ac999ee9762
Author: Ting Lou <louting@189.cn>
Date:   Fri Nov 29 08:09:46 2024 +0800

    llava: return false instead of exit (#10546)

commit dc22344088a7ee81a1e4f096459b03a72f24ccdc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 28 20:46:40 2024 +0200

    ggml : remove redundant copyright notice + update authors

commit 4c0a95b1074907ce7efe6f5bb6ae3351c01429ab
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 28 20:45:07 2024 +0200

    llama : add missing model types

commit 6c595676899013102fdb0aa4b06a49954300c94a
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Nov 28 19:17:49 2024 +0100

    server : (tests) don't use thread for capturing stdout/stderr, bump openai client library (#10568)
    
    * server : (tests) don't use thread for capturing stdout/stderr
    
    * test: bump openai to 1.55.2
    
    * bump openai to 1.55.3

commit 890719311b6535e572f15965c6d7ec4ac2537f60
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Nov 28 18:15:25 2024 +0100

    common: fix warning message when no GPU found (#10564)

commit 7281cf13addfae9b64bb2be87e3b5b1914505d63
Author: Random Fly <renfei8@live.cn>
Date:   Thu Nov 28 23:03:11 2024 +0800

    docs: fix outdated usage of llama-simple (#10565)

commit e90688edd004fdb7063f463bd18408ba9ae008dd
Author: Diego Devesa <slarengh@gmail.com>
Date:   Thu Nov 28 15:58:54 2024 +0100

    ci : fix tag name in cuda and hip releases (#10566)

commit 76b27d29c22af03172cf211a8a31025c7c828a57
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 28 14:56:37 2024 +0200

    ggml : fix row condition for i8mm kernels (#10561)
    
    ggml-ci

commit eea986f215e1dc490654d012ccf2ab62fe8f606d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 28 14:56:23 2024 +0200

    cmake : fix ARM feature detection (#10543)
    
    ggml-ci

commit c202cef1686182a78f8f4e253ab8d0c0ffe2fcc8
Author: Shupei Fan <dymarkfan@outlook.com>
Date:   Thu Nov 28 20:52:03 2024 +0800

    ggml-cpu: support IQ4_NL_4_4 by runtime repack (#10541)
    
    * ggml-cpu: support IQ4_NL_4_4 by runtime repack
    
    * ggml-cpu: add __ARM_FEATURE_DOTPROD guard

commit 2025fa67e94358deda4740a74fe9803916cb2f60
Author: Sergio López <slp@redhat.com>
Date:   Thu Nov 28 12:51:38 2024 +0100

    kompute : improve backend to pass test_backend_ops (#10542)
    
    * kompute: op_unary: reject unsupported parameters
    
    Signed-off-by: Sergio Lopez <slp@redhat.com>
    
    * kompute: softmax: implement ALiBi support
    
    Signed-off-by: Sergio Lopez <slp@redhat.com>
    
    * kompute: rope: implement neox and phi3 support
    
    Signed-off-by: Sergio Lopez <slp@redhat.com>
    
    * kompute: op_mul_mat_q4_k permutted support
    
    Signed-off-by: Sergio Lopez <slp@redhat.com>
    
    * kompute: op_mul_mat_[q4_0|q4_1|q8_0] permutted support
    
    Signed-off-by: Sergio Lopez <slp@redhat.com>
    
    * kompute: op_mul_mat_f16 permutted support
    
    Signed-off-by: Sergio Lopez <slp@redhat.com>
    
    * kompute: op_mul_mat_q6_k permutted support
    
    Signed-off-by: Sergio Lopez <slp@redhat.com>
    
    ---------
    
    Signed-off-by: Sergio Lopez <slp@redhat.com>

commit c6bc73951ed52466392b1abda98c28ecbe522c7f
Author: Ruixin Huang <18860020911@163.com>
Date:   Thu Nov 28 15:27:11 2024 +0800

    CANN: Update cann.md to display correctly in CLion (#10538)

commit 605fa66c509f9f117bd654cf0b9b3ea08bb86e80
Author: leo-pony <nengjunma@outlook.com>
Date:   Thu Nov 28 15:25:24 2024 +0800

    CANN: Fix SOC_TYPE compile bug (#10519)
    
    * CANN: Fix the bug build fail on Ascend310P under two cases:
    1) Manual specify SOC_TYPE
    2) Under some unusual compile environment
    
    * Update the cann backend News content: Support F16 and F32 data type model for Ascend 310P NPU.
    
    * fix CANN  compile fail bug: the assert in ascend kernel function doesn't supportted on some CANN version

commit b7420131bf8ab3e067bc660439ab1ab18be7edbd
Author: Chenguang Li <87689256+noemotiovon@users.noreply.github.com>
Date:   Thu Nov 28 14:24:46 2024 +0800

    CANN: ROPE operator optimization (#10540)
    
    * [cann] ROPE operator optimization
    
    Co-authored-by: noemotiovon <noemotiovon@gmail.com>

commit 9f912511bc9414fa7a3c521378b6388cd932b58d
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed Nov 27 22:30:52 2024 +0100

    common : fix duplicated file name with hf_repo and hf_file (#10550)

commit 3ad5451f3b75809e3033e4e577b9f60bcaf6676a
Author: uvos <devnull@uvos.xyz>
Date:   Wed Nov 27 17:10:08 2024 +0100

    Add some minimal optimizations for CDNA (#10498)
    
    * Add some minimal optimizations for CDNA
    
    * ggml_cuda: set launch bounds also for GCN as it helps there too

commit 46c69e0e752ff16206347bb12f96ed69f4a01abf
Author: Diego Devesa <slarengh@gmail.com>
Date:   Wed Nov 27 11:03:25 2024 +0100

    ci : faster CUDA toolkit installation method and use ccache (#10537)
    
    * ci : faster CUDA toolkit installation method and use ccache
    
    * remove fetch-depth
    
    * only pack CUDA runtime on master

commit 9e2301f4a4ef1690bd99360c11de43fe830b1c8d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 27 11:22:14 2024 +0200

    metal : fix group_norm support condition (#0)

commit fee824a1a1e35b5c49d482f654613addade61764
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 27 11:10:42 2024 +0200

    sync : ggml

commit 9150f8fef95327474d39ccd6c6e30787e85f3529
Author: Frankie Robertson <frankier@users.noreply.github.com>
Date:   Tue Nov 26 15:50:26 2024 +0200

    Do not include arm_neon.h when compiling CUDA code (ggml/1028)

commit c31ed2abfce05c38a2a5189586bfae45a139a547
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Wed Nov 27 01:32:54 2024 -0600

    vulkan: define all quant data structures in types.comp (#10440)

commit 5b3466bedfa84aa29c6871c7254467550186ecc6
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Wed Nov 27 01:30:27 2024 -0600

    vulkan: Handle GPUs with less shared memory (#10468)
    
    There have been reports of failure to compile on systems with <= 32KB
    of shared memory (e.g. #10037). This change makes the large tile size
    fall back to a smaller size if necessary, and makes mul_mat_id fall
    back to CPU if there's only 16KB of shared memory.

commit 249a7902ec710c8d027b9cc0ed10219d2b4184f8
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Wed Nov 27 01:21:59 2024 -0600

    vulkan: further optimize q5_k mul_mat_vec (#10479)

commit 71a64989a5d2e25c13507efada145f12cf358914
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Wed Nov 27 01:08:54 2024 -0600

    vulkan: skip integer div/mod in get_offsets for batch_idx==0 (#10506)

commit 4a57d362e1948ada50af997a92c3cbff9711e78b
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Wed Nov 27 01:00:50 2024 -0600

    vulkan: optimize Q2_K and Q3_K mul_mat_vec (#10459)

commit c9b00a70b080d5c0668608024afc3e0e2fed822f
Author: Diego Devesa <slarengh@gmail.com>
Date:   Tue Nov 26 22:12:10 2024 +0100

    ci : fix cuda releases (#10532)

commit de5097351caffb3deaea3393633609df49ef41d0
Author: Shane A <shanea@allenai.org>
Date:   Tue Nov 26 12:55:29 2024 -0800

    Add OLMo 2 model in docs (#10530)
    
    * Add link to OLMo 2 model in docs
    
    * Change link to landing page

commit 5a349f2809dc825960dfcfdf8f76b19cd0345be7
Author: Diego Devesa <slarengh@gmail.com>
Date:   Tue Nov 26 21:13:54 2024 +0100

    ci : remove nix workflows (#10526)

commit 30ec39832165627dd6ed98938df63adfc6e6a21a
Author: Diego Devesa <slarengh@gmail.com>
Date:   Tue Nov 26 21:01:47 2024 +0100

    llama : disable warnings for 3rd party sha1 dependency (#10527)

commit be0e350c8b69632b27d5fb41fa064fa256dd7fbf
Author: Tristan Druyen <tristan@vault81.mozmail.com>
Date:   Tue Nov 26 19:27:28 2024 +0100

    Fix HIP flag inconsistency & build docs (#10524)
    
    * Fix inconsistency of HIP flags in cmake & make
    
    * Fix docs regarding GGML_HIP

commit 249cd93da3df9c8fa78869b0522526d1625aca91
Author: R0CKSTAR <xiaodong.ye@mthreads.com>
Date:   Wed Nov 27 00:00:41 2024 +0800

    mtgpu: Add MUSA_DOCKER_ARCH in Dockerfiles && update cmake and make (#10516)
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>

commit 904109ed0d97c9b656a5e8bf612925f739bb8166
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Tue Nov 26 09:45:05 2024 -0600

    vulkan: fix group_norm (#10496)
    
    Fix bad calculation of the end of the range. Add a backend test that
    covers the bad case (taken from stable diffusion).
    
    Fixes https://github.com/leejet/stable-diffusion.cpp/issues/439.

commit 45abe0f74ee281aea6e5283c1e738061256cfcae
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Nov 26 16:20:18 2024 +0100

    server : replace behave with pytest (#10416)
    
    * server : replace behave with pytest
    
    * fix test on windows
    
    * misc
    
    * add more tests
    
    * more tests
    
    * styling
    
    * log less, fix embd test
    
    * added all sequential tests
    
    * fix coding style
    
    * fix save slot test
    
    * add parallel completion test
    
    * fix parallel test
    
    * remove feature files
    
    * update test docs
    
    * no cache_prompt for some tests
    
    * add test_cache_vs_nocache_prompt

commit 0bbd2262a3263f37385297b30de37941836e57f7
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Tue Nov 26 21:43:47 2024 +0800

    restore the condistion to build & update pacakge when merge (#10507)
    
    Co-authored-by: arthw <14088817+arthw@users.noreply.github.com>

commit ab96610b1e58684bc5e8b810130c4cf6d8252e21
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Nov 26 14:18:08 2024 +0200

    cmake : enable warnings in llama (#10474)
    
    * cmake : enable warnings in llama
    
    ggml-ci
    
    * cmake : add llama_get_flags and respect LLAMA_FATAL_WARNINGS
    
    * cmake : get_flags -> ggml_get_flags
    
    * speculative-simple : fix warnings
    
    * cmake : reuse ggml_get_flags
    
    ggml-ci
    
    * speculative-simple : fix compile warning
    
    ggml-ci

commit 7db3846a94ce7683b3e8120abe427457edf840c9
Author: Diego Devesa <slarengh@gmail.com>
Date:   Tue Nov 26 13:05:20 2024 +0100

    ci : publish the docker images created during scheduled runs (#10515)

commit c6807b3f28cc3dbfda3ec390bcb87e69fb5634e2
Author: Diego Devesa <slarengh@gmail.com>
Date:   Tue Nov 26 13:05:07 2024 +0100

    ci : add ubuntu cuda build, build with one arch on windows (#10456)

commit 25669aa92caaddff09f39b54a5173e5cb2680fa3
Author: Charles Xu <charles.xu@arm.com>
Date:   Tue Nov 26 12:37:05 2024 +0100

    ggml-cpu: cmake add arm64 cpu feature check for macos (#10487)
    
    * ggml-cpu: cmake add arm64 cpu feature check for macos
    
    * use vmmlaq_s32 for compile option i8mm check

commit 84e1c33cde9e0a7aafcda2d4f21ba51c300482d7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Nov 26 13:36:40 2024 +0200

    server : fix parallel speculative decoding (#10513)
    
    ggml-ci

commit 811872a59daefb25fc0c4326bcb6d8ae893c2f7c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Nov 26 12:29:38 2024 +0200

    speculative : simplify the implementation (#10504)
    
    ggml-ci

commit 9a4b79bcfa4338b922fa8cf903bd5ac058aaf46f
Author: Shanshan Shen <467638484@qq.com>
Date:   Tue Nov 26 18:08:37 2024 +0800

    CANN: Improve the Inferencing Performance for Ascend NPU Device (#10454)
    
    * improve inferencing performance for ascend npu.
    
    Co-authored-by: Frank Mai <thxCode@thxcode0824@gmail.com>
    
    * some modification after review
    
    * some modifications after review
    
    * restore some modifications
    
    * restore some modifications
    
    ---------
    
    Co-authored-by: shanshan shen <shanshanshen333@gmail.com>
    Co-authored-by: Frank Mai <thxCode@thxcode0824@gmail.com>

commit 7066b4cce2898993e943ad6af5d8f1de5840c8e9
Author: Chenguang Li <87689256+noemotiovon@users.noreply.github.com>
Date:   Tue Nov 26 17:31:05 2024 +0800

    CANN: RoPE and CANCAT operator optimization (#10488)
    
    Co-authored-by: noemotiovon <noemotiovon@gmail.com>

commit 0eb4e12beebabae46d37b78742f4c5d4dbe52dc1
Author: Junil Kim <logyourself@gmail.com>
Date:   Tue Nov 26 10:47:20 2024 +0900

    vulkan: Fix a vulkan-shaders-gen arugment parsing error (#10484)
    
    The vulkan-shaders-gen was not parsing the --no-clean argument correctly.
    Because the previous code was parsing the arguments which have a value only
    and the --no-clean argument does not have a value, it was not being parsed
    correctly. This commit can now correctly parse arguments that don't have values.

commit 0cc63754b831d3a6c37bc5d721d12ce9540ffe76
Author: Eric Curtin <ecurtin@redhat.com>
Date:   Mon Nov 25 16:56:24 2024 -0500

    Introduce llama-run (#10291)
    
    It's like simple-chat but it uses smart pointers to avoid manual
    memory cleanups. Less memory leaks in the code now. Avoid printing
    multiple dots. Split code into smaller functions. Uses no exception
    handling.
    
    Signed-off-by: Eric Curtin <ecurtin@redhat.com>

commit 50d5cecbda3b0d03344eed326287adc1f6c7f3ef
Author: Diego Devesa <slarengh@gmail.com>
Date:   Mon Nov 25 22:05:39 2024 +0100

    ci : build docker images only once daily (#10503)

commit 9fd8c2687f5aa2f095ac6e12a376e1c0583888e8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Nov 25 22:28:27 2024 +0200

    server : add more information about error (#10455)

commit 47f931c8f9a26c072d71224bc8013cc66ea9e445
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Nov 25 21:50:07 2024 +0200

    server : enable cache_prompt by default (#10501)
    
    ggml-ci

commit 106964e3d266740f571b5aad7b57545b4a901ac9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Nov 25 21:49:31 2024 +0200

    metal : enable mat-vec kernels for bs <= 4 (#10491)

commit 80acb7b430d826a7685326603a07342e9abc1b45
Author: Shane A <shanea@allenai.org>
Date:   Mon Nov 25 10:36:09 2024 -0800

    Rename Olmo1124 to Olmo2 (#10500)

commit 10bce0450f0c4d80087e06312b9dbbab3e87f16b
Author: Diego Devesa <slarengh@gmail.com>
Date:   Mon Nov 25 19:30:06 2024 +0100

    llama : accept a list of devices to use to offload a model (#10497)
    
    * llama : accept a list of devices to use to offload a model
    
    * accept `--dev none` to completely disable offloading
    
    * fix dev list with dl backends
    
    * rename env parameter to LLAMA_ARG_DEVICE for consistency

commit 1f922254f0c984a8fb9fbaa0c390d7ffae49aedb
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Nov 25 19:18:37 2024 +0100

    Github: update issue templates [no ci] (#10489)

commit a9a678a6b2264e5895533217b0cf8f250534cc58
Author: brucepro <git@brucepro.net>
Date:   Mon Nov 25 08:11:55 2024 -0800

    Add download chat feature to server chat (#10481)
    
    * Add download chat feature to server chat
    
    Add a download feature next to the delete chat feature in the server vue chat interface.
    
    * code style
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <son@huggingface.co>

commit 9ca2e677626fce759d5d95c407c03677b9c87a26
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Nov 25 16:31:38 2024 +0200

    server : add speculative decoding support (#10455)
    
    * server : add speculative decoding support
    
    ggml-ci
    
    * server : add helper function slot.can_speculate()
    
    ggml-ci

commit 5931c1f233c616083d64e41a228249d58e039aa5
Author: Diego Devesa <slarengh@gmail.com>
Date:   Mon Nov 25 15:13:39 2024 +0100

    ggml : add support for dynamic loading of backends (#10469)
    
    * ggml : add support for dynamic loading of backends
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit f6d12e7df8fe64384f1939976871252e6422a01e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Nov 25 15:17:32 2024 +0200

    tests : fix compile warning

commit b756441104ca8384640d6df22ba4ea6dab7ad799
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Nov 25 15:08:04 2024 +0200

    metal : minor code formatting

commit 5a8987793f3e7c1fbfa6806bfcd17d578071b6c9
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Mon Nov 25 17:31:10 2024 +0800

    [SYCL] Fix building Win package for oneAPI 2025.0 update (#10483)
    
    * fix build package for 2025.0
    
    * debug
    
    * debug
    
    * fix
    
    * rm debug
    
    ---------
    
    Co-authored-by: arthw <14088817+arthw@users.noreply.github.com>

commit d9d54e498d38ec99bbc0031022f9c92711e97bbc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Nov 25 09:58:41 2024 +0200

    speculative : refactor and add a simpler example (#10362)
    
    * speculative : refactor and add a simpler example
    
    ggml-ci
    
    * speculative : clean-up and add comments and TODOs [no ci]
    
    * speculative : manage context in common_speculative
    
    ggml-ci
    
    * speculative : simplify
    
    ggml-ci
    
    * speculative : simplify (cont)
    
    ggml-ci
    
    * speculative : add --draft-min CLI arg
    
    * speculative : minor fixup
    
    * make : build fixes
    
    * speculative : do not redraft previous drafts
    
    ggml-ci
    
    * speculative : fix the draft sampling
    
    ggml-ci
    
    * speculative : fix compile warning
    
    * common : refactor args
    
    ggml-ci
    
    * common : change defaults [no ci]
    
    * common : final touches
    
    ggml-ci

commit cce5a9007572c6e9fa522296b77571d2e5071357
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Nov 24 18:03:25 2024 +0200

    flake.lock: Update (#10470)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/5e4fbfb6b3de1aa2872b76d49fafc942626e2add?narHash=sha256-OZiZ3m8SCMfh3B6bfGC/Bm4x3qc1m2SVEAlkV6iY7Yg%3D' (2024-11-15)
      → 'github:NixOS/nixpkgs/23e89b7da85c3640bbc2173fe04f4bd114342367?narHash=sha256-y/MEyuJ5oBWrWAic/14LaIr/u5E0wRVzyYsouYY3W6w%3D' (2024-11-19)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit dc39012cbaf8752fabecaeb60af78ccdd1dfb73b
Author: Diego Devesa <slarengh@gmail.com>
Date:   Sun Nov 24 16:10:26 2024 +0100

    llama : fix op mul check with command-r-plus (#10476)

commit 9336db462c0c34bbe2055413fe4e16442626c38b
Author: Gabe Goodhart <ghart@us.ibm.com>
Date:   Sun Nov 24 02:02:34 2024 -0700

    convert : XLMRoberta Type Vocab Size (#10458)
    
    This matches the key in common bert-based embedding models and may have a
    value other than 1 in it.
    
    Branch: XLMRobertaTypeVocabSize
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>

commit 96fa2c5e2d6cb5319f34c3a7fb0cec05694b22f1
Author: momonga <146910567+mmngays@users.noreply.github.com>
Date:   Sun Nov 24 09:09:22 2024 +0900

    fix gguf-py:  Conversion error when multiple licenses are configured (#9807)
    
    * fix general.license list to str
    
    * fix join license list
    
    ---------
    
    Co-authored-by: momonga <115213907+mmnga@users.noreply.github.com>

commit 55ed008b2de01592659b9eba068ea01bb2f72160
Author: Diego Devesa <slarengh@gmail.com>
Date:   Sat Nov 23 14:41:12 2024 +0100

    ggml : do not use ARM features not included in the build (#10457)

commit 6dfcfef0787e9902df29f510b63621f60a09a50b
Author: 蕭澧邦 <45505768+shou692199@users.noreply.github.com>
Date:   Fri Nov 22 17:44:08 2024 +0800

    ci: Update oneAPI runtime dll packaging (#10428)
    
    This is the minimum runtime dll dependencies for oneAPI 2025.0

commit 599b3e0cd40432cd1975a8906f3db70bbe53b627
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Nov 22 08:32:40 2024 +0100

    GitHub: ask for more info in issue templates (#10426)
    
    * GitHub: ask for more info in issues [no ci]
    
    * refactor issue templates to be component-specific
    
    * more understandable issue description
    
    * add dropdown for llama.cpp module

commit c18610b4ee29ca056bb4f2d375a4ad1b16f44ef7
Author: leo-pony <nengjunma@outlook.com>
Date:   Fri Nov 22 14:07:20 2024 +0800

    CANN: Support Ascend310P to accelerate F32 and F16 Model (#10216)
    
    * CANN Support Ascend310P to accelerate F32 and F16 Model
    
    * Add compile option soc type macro ASCEND_310P to ggml-cann lib
    
    * Remove unused code
    
    * Remove the ascend soc_type hard code compile option in CMakelist.txt

commit a5e47592b6171ae21f3eaa1aba6fb2b707875063
Author: Diego Devesa <slarengh@gmail.com>
Date:   Thu Nov 21 18:18:50 2024 +0100

    cuda : optimize argmax (#10441)
    
    * cuda : optimize argmax
    
    * remove unused parameter
    
    ggml-ci
    
    * fixup : use full warps
    
    ggml-ci
    
    * Apply suggestions from code review
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    * fix ub
    
    * ggml : check ne00 <= INT32_MAX in argmax and argsort
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

commit 1bb30bf28cb5a7adf111bc41c935bdaf128397e7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 21 10:22:47 2024 +0200

    llama : handle KV shift for recurrent models (#10402)
    
    ggml-ci

commit 87a533be57e602f8ca469d14ad15ee851265b655
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 21 09:22:11 2024 +0200

    sync : ggml

commit 59b917282236eadfb82bf1f46a31eb119941da08
Author: slaren <slarengh@gmail.com>
Date:   Wed Nov 20 13:25:08 2024 +0100

    ggml/sched : do not skip views in pre-assignments

commit 02e4eaf22f229a114054b053a9eff61483653670
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Nov 20 14:56:04 2024 +0100

    ggml-opt: fix data corruption (ggml/1022)

commit 9abe9eeae98b11fa93b82632b264126a010225ff
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Wed Nov 20 13:47:36 2024 -0600

    vulkan: predicate max operation in soft_max shaders/soft_max (#10437)
    
    Fixes #10434

commit f95caa79546271722ada703da20ffb1cfcd21fed
Author: bandoti <141645996+bandoti@users.noreply.github.com>
Date:   Wed Nov 20 12:22:19 2024 -0400

    cmake: add link dependencies to cmake find pkg (#10433)
    
    * cmake pkg: find accelerate, openmp, memkind libs
    
    * cmake pkg: find BLAS libs
    
    * try BLAS_LIBRARIES instead
    
    * Add BLAS link opts
    
    * Add more link deps. and set GGML_ vars

commit fab5d30ff6729ff6ff615c41e8c0215d6bc30393
Author: Diego Devesa <slarengh@gmail.com>
Date:   Wed Nov 20 12:57:53 2024 +0100

    llama : add .clang-format file (#10415)

commit 8fd4b7fa29c3061b2e02e897d818dfcbc593430a
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Wed Nov 20 01:40:18 2024 -0600

    vulkan: copy iq4_nl LUT into shared memory (#10409)

commit 1bacb9f62514b520bdf74ed6feb46c80508dad38
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Wed Nov 20 01:11:00 2024 -0600

    vulkan: further optimize mul_mat_vec using larger loads (#10387)
    
    * vulkan: Use pipeline_robustness to disable robustness in mul_mat_vec.
    
    Add some early returns for nonexistent rows in mul_mat_vec shaders. These
    can only be hit when dispatching a 2D grid of workgroups. Fix the logic
    for the 2D grid of workgroups to round up.
    
    Enable the pipeline robustness extension if it's available, and use it to
    disable robustness for these pipelines. The instructions to do the bounds
    checking contend for the same ALU resources as the bit twiddling dequant
    instructions.
    
    * vulkan: Add GLSL structure aliases for quant types to allow larger loads
    
    In Vulkan it's not possible to cast pointer types, so instead you have to
    declare an aliased binding for the memory with a different type. This
    commit adds aliases for the quant formats using 16b ints, and in a few
    places where the struct size is a multiple of 4 also using 32b ints.
    Currently only q4_k's aliases are used, but others will be used in
    subsequent commits.
    
    * vulkan: use larger loads in q5_k and q6_k shaders.
    
    Similar to the optimization I did in q4_k recently, this vectorizes some loads
    and reduces the number of bit twiddling instructions.
    
    * vulkan: use larger K step per iteration in mul_mat_vec.
    
    Add vec4 dequantization functions, and use them to do K=8 per iteration in
    mul_mat_vec. This uses 16b loads for the quant values and 128b loads for B
    which helps reduce the load on the memory system.
    
    The K_PER_ITER==2 logic is still there, just for F16/F32, and really only
    because they support unaligned sizes.
    
    Tweak the num_iters/unrolling logic to be simpler and catch a couple missed
    unrolling opportunities.

commit ad21c9e1f14d82b8c15ae369a8839019e3d498b4
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Wed Nov 20 13:54:25 2024 +0800

    update rel to 4040 (#10395)
    
    Co-authored-by: arthw <14088817+arthw@users.noreply.github.com>

commit 3952a221af54b8a6549bc2bd4a7363ef7ad3081e
Author: Anthony Van de Gejuchte <anthonyvdgent@gmail.com>
Date:   Tue Nov 19 23:18:17 2024 +0100

    Fix missing file renames in Makefile due to changes in commit ae8de6d50a (#10413)

commit 42ae10bbcd7b56f29a302c86796542a6dadf46c9
Author: haopeng <657407891@qq.com>
Date:   Wed Nov 20 04:10:31 2024 +0800

    add cmake rvv support (#10411)

commit 9fe0fb062630728e3c21b5839e3bce87bff2440a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Nov 19 19:15:50 2024 +0200

    sync : ggml

commit 611fabd7922050e1e99bd276d3544527cd46047b
Author: Plamen Minev <pacominev@gmail.com>
Date:   Mon Nov 18 15:02:27 2024 +0200

    metal : fox offset integer overflows in im2col (ggml/1015)
    
    -- While running StableDiffusion.cpp locally with Metal some offsets overflow and results in incorrect calculations

commit 12b0ad953a59563ea8d973708760d747321d8432
Author: PAB <pierreantoine.bannier@gmail.com>
Date:   Mon Nov 18 10:02:49 2024 +0100

    metal : add `GGML_UNARY_OP_ELU` kernel (ggml/1018)

commit 342397dc7edb311e0373205134d0d3a928b891b3
Author: 蕭澧邦 <45505768+shou692199@users.noreply.github.com>
Date:   Wed Nov 20 01:42:00 2024 +0800

    cmake: force MSVC compiler charset to utf-8 (#9989)

commit 2a11b6b0946c1abab2ab150725610e5ee736b3af
Author: bandoti <141645996+bandoti@users.noreply.github.com>
Date:   Tue Nov 19 12:10:30 2024 -0400

    Add required ggml-base and backend libs to cmake pkg (#10407)

commit 3ee6382d48b07b31e64983969c16019490e19740
Author: Diego Devesa <slarengh@gmail.com>
Date:   Tue Nov 19 14:29:38 2024 +0100

    cuda : fix CUDA_FLAGS not being applied (#10403)

commit 8e752a777b272606f22cb741b03e062de4ddb8fe
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Nov 19 13:29:26 2024 +0200

    llama : add check for KV cache shifts (#10401)
    
    ggml-ci

commit a88ad007de3eb5d92cb538fd269ff94c4bf0c8d2
Author: Shane A <shanea@allenai.org>
Date:   Tue Nov 19 01:04:08 2024 -0800

    llama : add OLMo November 2024 support (#10394)
    
    * Add OLMo November 2024 constants
    
    * Add OLMo November 2024 converter
    
    * Add loading of OLMo November 2024 tensors and hyper parameters
    
    * Add building of OLMo November 2024 model

commit 2a1507c1629975d9d20a503d6a14f44eff292c25
Author: Romain Biessy <romain.biessy@codeplay.com>
Date:   Tue Nov 19 09:02:23 2024 +0100

    sycl : Add option to set the SYCL architecture for all targets (#10266)
    
    * Add option to set the SYCL architecture for all targets
    * Convert GGML_SYCL_HIP_TARGET to the more generic GGML_SYCL_ARCH option
    * Document that setting GGML_SYCL_ARCH can improve the performance

commit b3e585988fc65d3a8083c6d94dfc0629f9ce226d
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Tue Nov 19 01:25:17 2024 -0600

    vulkan: Optimize soft_max (#10301)
    
    * vulkan: Optimize soft_max
    
    Large soft_max could already saturate memory, but small/medium sizes were
    pretty slow. The bulk of the gains for them comes from using a smaller
    workgroup size, and making the workgroup size match the subgroup size also
    makes the barriers much cheaper.
    
    Cache some values in locals to avoid refetching/recomputing. And stamp
    out a few "template instantiations" so smaller cases will fully unroll.
    
    Add a missing early return for OOB rows. This happens when there are more
    than 512 rows and the dispatch is 512 x H.
    
    * vulkan: Further soft_max optimizations
    
    Restore the workgroup size of 512 case, use it for >1024.
    
    Use unrollable loops for more iteration counts.

commit 557924f22237c76387a39c4db5abae154d57e754
Author: Alberto Cabrera Pérez <alberto.cabrera@codeplay.com>
Date:   Tue Nov 19 00:50:04 2024 +0000

    sycl: Revert MUL_MAT_OP support changes (#10385)

commit d3481e631661b5e9517f78908cdd58cee63c4903
Author: Diego Devesa <slarengh@gmail.com>
Date:   Mon Nov 18 18:43:40 2024 +0100

    cuda : only use native when supported by cmake (#10389)

commit 531cb1c233800e6acb021dc56d69595e314db072
Author: bandoti <141645996+bandoti@users.noreply.github.com>
Date:   Mon Nov 18 11:23:58 2024 -0400

    Skip searching root path for cross-compile builds (#10383)

commit f139d2ea611c5604395c95160d3c53f7c4eaf220
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Mon Nov 18 08:28:42 2024 -0600

    vulkan: remove use of null initializer (#10372)
    
    Seems like this isn't working for vulkan-over-metal when the array is sized
    by a spec constant. Maybe a spirv-cross limitation?

commit 2eb76b2a5e4ea395b971a419c95b473ab6f253e4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Nov 18 16:08:20 2024 +0200

    flake.lock: Update (#10346)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/4aa36568d413aca0ea84a1684d2d46f55dbabad7?narHash=sha256-Zwl8YgTVJTEum%2BL%2B0zVAWvXAGbWAuXHax3KzuejaDyo%3D' (2024-11-05)
      → 'github:NixOS/nixpkgs/5e4fbfb6b3de1aa2872b76d49fafc942626e2add?narHash=sha256-OZiZ3m8SCMfh3B6bfGC/Bm4x3qc1m2SVEAlkV6iY7Yg%3D' (2024-11-15)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit 9b75f03cd2ec9cc482084049d87a0f08f9f01517
Author: 0cc4m <picard12@live.de>
Date:   Mon Nov 18 11:02:43 2024 +0100

    Vulkan: Fix device info output format specifiers (#10366)
    
    * Vulkan: Fix device info output format specifiers
    
    * Vulkan: Use zu printf specifier for size_t instead of ld

commit 75207b3a887f91f813de1eb6e9fd135d3cb2b8c6
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Nov 18 00:21:53 2024 +0100

    docker: use GGML_NATIVE=OFF (#10368)

commit 76e9e58b7847112848aa1f40b65d1cbcd6d5f5a3
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Nov 17 23:20:42 2024 +0100

    CUDA: fix MMV kernel being used for FP16 src1 (#10357)

commit ce2e59ba107cf71ed566040ff20a15d1c58e09c2
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Nov 17 12:59:38 2024 +0100

    CMake: fix typo in comment [no ci] (#10360)

commit be5caccef945546ee9fd25a151330a88d785faf9
Author: Diego Devesa <slarengh@gmail.com>
Date:   Sun Nov 17 12:25:45 2024 +0100

    llama : only use default buffer types for the KV cache (#10358)

commit 20a780c7b64c38071fe70ad23e16e80c23c0147b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Nov 17 13:12:22 2024 +0200

    gitignore : ignore local run scripts [no ci]

commit cf32a9b93ad859ea592c31785b6bd3b4b2121463
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Nov 17 11:23:01 2024 +0200

    metal : refactor kernel args into structs (#10238)
    
    * metal : add kernel arg structs (wip)
    
    * metal : fattn args
    
    ggml-ci
    
    * metal : cont + avoid potential int overflow [no ci]
    
    * metal : mul mat struct (wip)
    
    * cont : mul mat vec
    
    * cont : pass by reference
    
    * cont : args is first argument
    
    * cont : use char ptr
    
    * cont : shmem style
    
    * cont : thread counters style
    
    * cont : mul mm id
    
    ggml-ci
    
    * cont : int safety + register optimizations
    
    ggml-ci
    
    * metal : GGML_OP_CONCAT
    
    ggml-ci
    
    * metal : GGML_OP_ADD, GGML_OP_SUB, GGML_OP_MUL, GGML_OP_DIV
    
    * metal : GGML_OP_REPEAT
    
    * metal : GGML_OP_CPY
    
    * metal : GGML_OP_RMS_NORM
    
    * metal : GGML_OP_NORM
    
    * metal : add TODOs for rest of ops
    
    * ggml : add ggml-metal-impl.h
    
    ggml-ci

commit a43178299c2f74f14bcfc659bfd9fd32d931d1f4
Author: FirstTimeEZ <179362031+FirstTimeEZ@users.noreply.github.com>
Date:   Sun Nov 17 21:39:22 2024 +1300

    ggml : fix undefined reference to 'getcpu' (#10354)
    
    https://github.com/ggerganov/llama.cpp/issues/10352

commit c3ea58aca406911eb4d409cdbfc76683393442b6
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Nov 17 09:09:55 2024 +0100

    CUDA: remove DMMV, consolidate F16 mult mat vec (#10318)

commit 467576b6cc7d2b9220f55bc635aa51469cf26fb5
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Nov 17 09:06:34 2024 +0100

    CMake: default to -arch=native for CUDA build (#10320)

commit eda7e1d4f54711de1c9b40502d6c88bbc217da60
Author: Diego Devesa <slarengh@gmail.com>
Date:   Sun Nov 17 07:31:17 2024 +0100

    ggml : fix possible buffer use after free in sched reserve (#9930)

commit 24203e9dd7355a4a10bc32d959fd0148d37bf666
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 16 23:40:39 2024 +0200

    ggml : inttypes.h -> cinttypes (#0)
    
    ggml-ci

commit 5d9e59979c2fd91c99d03c23e8df9c50c9d55a85
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 16 21:38:01 2024 +0200

    ggml : adapt AMX to tensor->grad removal (#0)
    
    ggml-ci

commit a4200cafadebb7576a9a3905039858f5e73ce4cc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 16 21:35:31 2024 +0200

    make : add ggml-opt (#0)
    
    ggml-ci

commit 84274a10c399d843cff39feb2b86dc8224f613d8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 16 21:34:03 2024 +0200

    tests : remove test-grad0

commit 68fcb4759c0ca2874b59d9c1e6a35ceca1cc04ce
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 16 21:32:41 2024 +0200

    ggml : fix compile warnings (#0)
    
    ggml-ci

commit 8a43e940ab0daaff198809bf9277289994ec62f5
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Nov 16 22:17:59 2024 +0200

    ggml: new optimization interface (ggml/988)

commit 5c9a8b22b10132db620529435e3cfa49304b65cc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 16 22:16:04 2024 +0200

    scripts : update sync

commit 0fff7fd79818980763a601660f25b01a0cf4b87a
Author: FirstTimeEZ <179362031+FirstTimeEZ@users.noreply.github.com>
Date:   Sun Nov 17 12:29:18 2024 +1300

    docs : vulkan build instructions to use git bash mingw64 (#10303)

commit 4e54be0ec6cb5cd6ed56e52e927b80b2796ec844
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Nov 16 23:00:41 2024 +0100

    llama/ex: remove --logdir argument (#10339)

commit db4cfd5dbc31c90f0d5c413a2e182d068b8ee308
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 16 17:58:56 2024 +0200

    llamafile : fix include path (#0)
    
    ggml-ci

commit 8ee0d09ae6928d0501765cfc4e430b9236730caf
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 16 17:58:32 2024 +0200

    make : auto-determine dependencies (#0)

commit bcdb7a23862b61aa307fc462fadfe1e2e653d010
Author: MaggotHATE <clay1326@gmail.com>
Date:   Sat Nov 16 18:26:54 2024 +0500

    server: (web UI) Add samplers sequence customization (#10255)
    
    * Samplers sequence: simplified and input field.
    
    * Removed unused function
    
    * Modify and use `settings-modal-short-input`
    
    * rename "name" --> "label"
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <son@huggingface.co>

commit f245cc28d4eb900efad0bc740145f58d713c6e4f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 16 10:32:50 2024 +0200

    scripts : fix missing key in compare-llama-bench.py (#10332)

commit 772703c8fffdd83d2e28f60119e83525f1189412
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Sat Nov 16 00:26:57 2024 -0600

    vulkan: Optimize some mat-vec mul quant shaders (#10296)
    
    Compute two result elements per workgroup (for Q{4,5}_{0,1}). This reuses
    the B loads across the rows and also reuses some addressing calculations.
    This required manually partially unrolling the loop, since the compiler
    is less willing to unroll outer loops.
    
    Add bounds-checking on the last iteration of the loop. I think this was at
    least partly broken before.
    
    Optimize the Q4_K shader to vectorize most loads and reduce the number of
    bit twiddling instructions.

commit dd3a6ce9f84d21ba05fe98af9f983bdea0398e6c
Author: FirstTimeEZ <179362031+FirstTimeEZ@users.noreply.github.com>
Date:   Sat Nov 16 14:59:33 2024 +1300

    vulkan : add cmake preset debug/release (#10306)

commit 1e58ee1318429a3e97aa66f3034cdfd65ffc6c34
Author: Dan Johansson <dan.johansson@arm.com>
Date:   Sat Nov 16 01:53:37 2024 +0100

    ggml : optimize Q4_0 into Q4_0_X_Y repack (#10324)

commit 89e4caaaf081f4712af61a3e08cb67b406c02b80
Author: FirstTimeEZ <179362031+FirstTimeEZ@users.noreply.github.com>
Date:   Sat Nov 16 13:42:13 2024 +1300

    llama : save number of parameters and the size in llama_model (#10286)
    
    fixes #10285

commit 74d73dc85cc2057446bf63cc37ff649ae7cebd80
Author: Srihari-mcw <96763064+Srihari-mcw@users.noreply.github.com>
Date:   Sat Nov 16 02:57:00 2024 +0530

    Make updates to fix issues with clang-cl builds while using AVX512 flags (#10314)

commit 4047be74da398acb8717a4d21b77b929ad7ed4f7
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Nov 15 21:19:03 2024 +0100

    scripts: update compare-llama-bench.py (#10319)

commit 883d206fbd2c5b2b9b589a9328503b9005e146c9
Author: slaren <slarengh@gmail.com>
Date:   Fri Nov 15 20:20:54 2024 +0100

    ggml : fix some build issues

commit 09ecbcb596ed8fa97d503d7440f0b3eff872e8f1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Nov 15 15:35:22 2024 +0200

    cmake : fix ppc64 check (whisper/0)
    
    ggml-ci

commit 3225008973579cc6a784890c237e1bfc9de41819
Author: thewh1teagle <61390950+thewh1teagle@users.noreply.github.com>
Date:   Fri Nov 15 15:33:53 2024 +0200

    ggml : vulkan logs (whisper/2547)

commit cbf5541a82952bcd7c4fceb55f5e332cafbf1720
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Nov 15 15:31:16 2024 +0200

    sync : ggml

commit 18429220bdb344da1bc7df9bc580c7b41b3cd57b
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Fri Nov 15 11:47:58 2024 +0000

    AVX BF16 and single scale quant optimizations (#10212)
    
    * use 128 bit loads (i've tried 256->128 to death and its slower)
    
    * double accumulator
    
    * avx bf16 vec dot
    
    * +3% q4_0 inference
    
    * +7% tg +5% pp compared to master
    
    * slower f16c version, kep for reference
    
    * 256b version, also slow. i tried :)
    
    * revert f16
    
    * faster with madd
    
    * split to functions
    
    * Q8_0 and IQ4_NL, 5-7% faster
    
    * fix potential overflow (performance reduced)
    
    * 16 bit add for q4_0 only
    
    * merge

commit f0204a0ec70d50ca60e07bc0096ec1d6508ab0c7
Author: R0CKSTAR <xiaodong.ye@mthreads.com>
Date:   Fri Nov 15 19:47:25 2024 +0800

    ci: build test musa with cmake (#10298)
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>

commit 57f8355b29a8c7dfcd1fb6094758ad85644f8535
Author: Romain Biessy <romain.biessy@codeplay.com>
Date:   Fri Nov 15 12:10:45 2024 +0100

    sycl: Update Intel docker images to use DPC++ 2025.0 (#10305)

commit 9901068ac78838745e604fffb4601d315a610456
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Nov 15 05:48:49 2024 -0400

    server : (web UI) add copy button for code block, fix api key (#10242)
    
    * server : (web ui) add copy btn for code blocks
    
    * fix problem with api key
    
    * use settings-modal-short-input component
    
    * always show copy btn for code snippet

commit 231f9360d94446cd083b6b116f63991b1328c484
Author: Chenguang Li <87689256+noemotiovon@users.noreply.github.com>
Date:   Fri Nov 15 15:09:35 2024 +0800

    cann: dockerfile and doc adjustment (#10302)
    
    Co-authored-by: noemotiovon <noemotiovon@gmail.com>

commit 4802ad350b8e19cbc7a77269b4494c896f6e0896
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Nov 15 08:38:43 2024 +0200

    scripts : fix regex in sync [no ci]

commit 5a54af4d4f588f109f31e456483fdf77096399d9
Author: Romain Biessy <romain.biessy@codeplay.com>
Date:   Fri Nov 15 04:09:12 2024 +0100

    sycl: Use syclcompat::dp4a (#10267)
    
    * sycl: Use syclcompat::dp4a
    
    * Using the syclcompat version allow the compiler to optimize the
      operation with native function
    
    * Update news section
    
    * Update CI Windows oneAPI version to 2025.0
    
    * Reword doc
    
    * Call syclcompat::dp4a inside dpct::dp4a
    
    This reverts commit 90cb61d692d61360b46954a1c7f780bd2e569b73.

commit 1607a5e5b08f4e55f118af3d7de325949d8f1835
Author: Charles Xu <charles.xu@arm.com>
Date:   Fri Nov 15 01:28:50 2024 +0100

    backend cpu: add online flow for aarch64 Q4_0 GEMV/GEMM kernels (#9921)
    
    * backend-cpu: add online flow for aarch64 Q4_0 GEMV/GEMM kernels
    
    ---------
    
    Co-authored-by: Diego Devesa <slarengh@gmail.com>

commit ae8de6d50a09d49545e0afab2e50cc4acfb280e2
Author: Diego Devesa <slarengh@gmail.com>
Date:   Thu Nov 14 18:04:35 2024 +0100

    ggml : build backends as libraries (#10256)
    
    * ggml : build backends as libraries
    
    ---------
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: R0CKSTAR <xiaodong.ye@mthreads.com>

commit 4a8ccb37ad9c9027cbcfd5548c19cdffe48d5197
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Nov 14 13:00:15 2024 +0100

    CUDA: no -sm row for very small matrices (#10185)

commit 2a82891a853db908679f7b24b04586e6f6393fe0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 14 11:44:15 2024 +0200

    speculative : fix out-of-bounds access (#10289)

commit af148c9386da825a60c7038549c121c35ca56b50
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Wed Nov 13 23:22:55 2024 -0600

    vulkan: Optimize binary ops (#10270)
    
    Reuse the index calculations across all of src0/src1/dst. Add a shader
    variant for when src0/src1 are the same dimensions and additional modulus
    for src1 aren't needed. Div/mod are slow, so add "fast" div/mod that
    have a fast path when the calculation isn't needed or can be done more
    cheaply.

commit 66798e42fbe636f1cb6236e4bc30939d23ef7c25
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Wed Nov 13 14:59:47 2024 -0600

    vulkan: Use macros to make the mat mul pipeline creation more concise (#10259)
    
    Also add vk_matmul_pipeline2 to hold f16/f32 accumulator versions of a
    pipeline. This isn't really used yet.

commit fb4a0ec0833c71cff5a1a367ba375447ce6106eb
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Wed Nov 13 20:00:35 2024 +0200

    llama : propagate the results of `graph_compute` (#9525)
    
    * llama: propagating the results of `graph_compute` to the user interface
    
    * llama: reverting kv_cache in case of failed compute
    
    * llama: `llama_kv_cache_state` was removed, only the result of `llama_graph_compute` is returned
    
    * llama: restore a kv_cache in case of failed computation
    
    * llama: correct reverting of the entire batch.
    also updates `llama_kv_cache_find_slot`, will correctly count the number of `used` cells for recurrent models
    
    * llama: updated comments
    
    * llama : add comments about KV cache state after error
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 5ea926dad7f62ebccff7b24784bd1e01a06d13ae
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 13 18:11:54 2024 +0200

    sync : ggml

commit 1ee9eea094fe5846c7d8d770aa7caa749d246b23
Author: Small Grass Forest <zixuanxcl@gmail.com>
Date:   Wed Nov 13 19:17:10 2024 +0800

    docs : update bindings list (#10261)
    
    Signed-off-by: tianzixuan <tianzixuan335@hellobike.com>

commit ff7fb670d0a62897c5662536aeb53422c549fbe8
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Wed Nov 13 11:16:30 2024 +0000

    server : add missing docs (#10269)

commit 0e712a5acbbdd1593e5aeb86d4f6b896a11b438c
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Wed Nov 13 19:15:23 2024 +0800

    server : fix incorrect res in validate_model_chat_template (#10272)
    
    * server : fix validate_model_chat_template
    
    * server : fix chat res

commit a0ec17b32ec6077f5ca22fe833ebdc9b86795a4d
Author: Brian <mofosyne@gmail.com>
Date:   Wed Nov 13 21:10:38 2024 +1100

    metadata: Detailed Dataset Authorship Metadata (#8875)
    
    Converter script can now read these two fields as a detailed base model and dataset source.
    This was done so that it will be easier for Hugging Face to integrate detailed metadata as needed.
    
     -  base_model_sources (List[dict], optional)
     -  dataset_sources (List[dict], optional)
    
    Dataset now represented as:
    
     - general.dataset.count
     - general.dataset.{id}.name
     - general.dataset.{id}.author
     - general.dataset.{id}.version
     - general.dataset.{id}.organization
     - general.dataset.{id}.description
     - general.dataset.{id}.url
     - general.dataset.{id}.doi
     - general.dataset.{id}.uuid
     - general.dataset.{id}.repo_url
    
    This also adds to base model these metadata:
    
     - general.base_model.{id}.description

commit 2e82ffa4af29f87e7d3d6dff8060a2a79613b72f
Author: Alberto Cabrera Pérez <alberto.cabrera@codeplay.com>
Date:   Wed Nov 13 09:40:57 2024 +0000

    sycl : Fixes to broken builds and test-backend-ops (#10257)
    
    * Fixes broken build for the SYCL CUDA backend caused by non-explicit gemm call in outprod (merged in with RWKV6 in
    Optimize RWKV6 Operator Naming and Implement Multi-core CPU/ SYCL Acceleration #10133)
    
    * Marks permuted MUL_MAT as unsupported to be able to run test-backend-ops
    
    * Fixes asserts in norm to fix debug builds.

commit 80dd7ff22fd050fed58b552cc8001aaf968b7ebf
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Wed Nov 13 00:58:57 2024 -0600

    vulkan: Optimize contiguous copies (#10254)
    
    * tests: Fix memory bandwidth calculation for perf tests
    
    Add a flops calculation for flash attention.
    
    Add one GGML_OP_CPY perf test.
    
    * vulkan: Optimize contiguous copies
    
    Add a variant of the copy shader for when the tensors are contiguous. Avoid
    the complex addressing calculations, and do four elements per invocation
    to hide some other overhead.
    
    Apply similar changes to the scale shader, since scale is always contiguous.
    
    Add a "progress bar" for shader compiles.

commit 54ef9cfc726a799e6f454ac22c4815d037716eda
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Mon Nov 11 11:13:51 2024 -0600

    vulkan: Throttle the number of shader compiles during the build step. (#10222)
    
    Fixes #9582
    
    Spawning too many concurrent copies of glslc leads to "Failed to create pipes"
    errors on Linux. This change applies the same throttling we use for
    multithreaded pipeline creation.

commit b0cefea58a20020754b7431e3c725625274372a5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Nov 11 08:39:13 2024 +0200

    metal : more precise Q*K in FA vec kernel (#10247)

commit b141e5f6efbab3a00633df88c4f9425bfe8b78ab
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Nov 11 08:38:43 2024 +0200

    server : enable KV cache defrag by default (#10233)
    
    ggml-ci

commit 4b3a9212b602be3d4e2e3ca26efd796cef13c55e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Nov 10 21:45:25 2024 +0200

    flake.lock: Update (#10243)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/807e9154dcb16384b1b765ebe9cd2bba2ac287fd?narHash=sha256-l253w0XMT8nWHGXuXqyiIC/bMvh1VRszGXgdpQlfhvU%3D' (2024-10-29)
      → 'github:NixOS/nixpkgs/4aa36568d413aca0ea84a1684d2d46f55dbabad7?narHash=sha256-Zwl8YgTVJTEum%2BL%2B0zVAWvXAGbWAuXHax3KzuejaDyo%3D' (2024-11-05)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit 505f33274d60676320216b662a97672a76ec600e
Author: MaggotHATE <clay1326@gmail.com>
Date:   Mon Nov 11 00:42:25 2024 +0500

    server : (web UI) Add back sampler settings (#10239)
    
    * Add back samplers to server
    
    * Added tooltips with basic information
    
    * Fixed stretching of input fields.
    
    * use component for settings input, move help msg to tooltips
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <son@huggingface.co>

commit 160687b3ed002eee83a04de83a9cd752f928ced1
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Sun Nov 10 05:37:56 2024 -0600

    vulkan: Fix newly added tests for permuted mul_mat and 1D im2col (#10226)

commit 6423c65aa8be1b98f990cf207422505ac5a441a1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 9 11:53:13 2024 +0200

    metal : reorder write loop in mul mat kernel + style (#10231)
    
    * metal : reorder write loop
    
    * metal : int -> short, style
    
    ggml-ci

commit 39a334a9aaf2000f93a899d9f43d889e460640ee
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 9 11:53:02 2024 +0200

    metal : fix build and some more comments (#10229)

commit bb38cdd8baf37de1fadab3e867c6ba4ae452eff6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 9 11:52:45 2024 +0200

    metal : fix F32 accumulation in FA vec kernel (#10232)

commit f018acba22095b8995bf6c5ef815b16a3ce4cf1b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 9 11:26:34 2024 +0200

    llama : fix Qwen model type strings

commit 46323fa9efd5e6c8aeef8d6eb6c332ee0d95eb13
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 9 11:21:49 2024 +0200

    metal : hide debug messages from normal log

commit 5b359bb1e3585de45bec79fd6c18934897662cdf
Author: SXX <sxx1136965276@gmail.com>
Date:   Sat Nov 9 15:35:46 2024 +0800

    ggml: fix zero division in ‘dne’ calculation in CUDA COUNT_EQUAL operator when ‘ne’ is small (#10213)

commit e89213492d3e01705739789733f0f2d250b4c449
Author: amritahs-ibm <amritahs@linux.vnet.ibm.com>
Date:   Sat Nov 9 12:47:50 2024 +0530

    ggml : optimize llamafile cpu matrix multiplication for ppc64le (#10156)
    
    This change upstreams llamafile's cpu matrix
    multiplication kernels for ppc64le using MMA
    builtins for FP32 datatype.
    
    This change results in a consistent 90%
    improvement in input processing time, and 20%
    to 80% improvement in output processing time,
    across various batch sizes.
    
    The patch is tested with Meta-Lllama-3-8B,
    Mistral-7B, Llama-2-7B-chat-hf models on a
    IBM POWER10 machine.
    
    Signed-off-by: Amrita H S <amritahs@linux.vnet.ibm.com>

commit 8fc393f246c550d2481e53323a47644a94e8d01f
Author: haopeng <657407891@qq.com>
Date:   Sat Nov 9 15:06:54 2024 +0800

    scripts : fix pattern and get n_tokens in one go (#10221)

commit ec450d3bbf9fdb3cd06b27c00c684fd1861cb0cf
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Nov 8 21:59:46 2024 +0200

    metal : opt-in compile flag for BF16 (#10218)
    
    * metal : opt-in compile flag for BF16
    
    ggml-ci
    
    * ci : use BF16
    
    ggml-ci
    
    * swift : switch back to v12
    
    * metal : has_float -> use_float
    
    ggml-ci
    
    * metal : fix BF16 check in MSL
    
    ggml-ci

commit 695ad752b2631af84ba321177656705b30c6e401
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Nov 8 18:37:41 2024 +0200

    metal : improve clarity (minor) (#10171)

commit 841f27abdbbcecc9daac14dc540ba6202e4ffe40
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Nov 8 13:47:22 2024 +0200

    metal : optimize FA kernels (#10171)
    
    * ggml : add ggml_flash_attn_ext_get_prec
    
    * metal : use F16 precision in FA kernels
    
    ggml-ci
    
    * metal : minor clean-up
    
    * metal : compile-guard bf16 FA kernels
    
    ggml-ci
    
    * build : remove obsolete compile flag [no ci]
    
    * metal : prevent int overflows [no ci]
    
    * cuda : disable BF16 FA
    
    ggml-ci
    
    * metal : fix BF16 requirement for FA kernels
    
    ggml-ci
    
    * make : clean-up [no ci]

commit d05b3127bd30515955aa4ee2bacdb68ebafe88f4
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Fri Nov 8 17:34:06 2024 +0800

    swift : exclude ggml-metal-embed.metal (#10211)
    
    * llama.swift : exclude ggml-metal-embed.metal
    
    * swift : exclude build/

commit 76c6e7f10551960e4ec9e14e0535b72081f1c7ad
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Nov 7 18:44:38 2024 -0400

    server : minor UI fix (#10207)

commit a71d81cf8c1afb26b166f897c94ee1581f9fac7d
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Nov 7 17:31:10 2024 -0400

    server : revamp chat UI with vuejs and daisyui (#10175)
    
    * server : simple chat UI with vuejs and daisyui
    
    * move old files to legacy folder
    
    * embed deps into binary
    
    * basic markdown support
    
    * add conversation history, save to localStorage
    
    * fix bg-base classes
    
    * save theme preferences
    
    * fix tests
    
    * regenerate, edit, copy buttons
    
    * small fixes
    
    * docs: how to use legacy ui
    
    * better error handling
    
    * make CORS preflight more explicit
    
    * add GET method for CORS
    
    * fix tests
    
    * clean up a bit
    
    * better auto scroll
    
    * small fixes
    
    * use collapse-arrow
    
    * fix closeAndSaveConfigDialog
    
    * small fix
    
    * remove console.log
    
    * fix style for <pre> element
    
    * lighter bubble color (less distract when reading)

commit eec4d71737b32f312e0082b671629a0368e1a20d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 7 23:11:36 2024 +0200

    scripts : add amx to sync-ggml.sh [no ci]

commit 3b08828674f561c78af182d47fc0636fc3ccd1e9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 7 23:08:24 2024 +0200

    sync : ggml

commit a2c6fd747c77fe183e2f556a4a2f1fb0a0be4c7b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 7 23:07:55 2024 +0200

    scripts : sync update

commit 97404c4a0374cac45c8c34a32d13819de1dd023d
Author: Diego Devesa <slarengh@gmail.com>
Date:   Thu Nov 7 18:16:08 2024 +0100

    ggml : add ggml-cpu.h to the public headers (#10204)

commit 60e17ce23c2740369af6304113a2dfa0454eaf26
Author: Faisal Zaghloul <quic_fzaghlou@quicinc.com>
Date:   Thu Nov 7 11:46:12 2024 -0500

    Remove identical wte/etw logic for jais (#10203)

commit 5107e8cea35be46a27cfc940e6841c0cf81c0525
Author: wwoodsTM <104587230+wwoodsTM@users.noreply.github.com>
Date:   Thu Nov 7 08:20:25 2024 -0700

    DRY: Fixes clone functionality (#10192)

commit 2319126a70b541f8670225a04a38202bbdccbedb
Author: snadampal <87143774+snadampal@users.noreply.github.com>
Date:   Thu Nov 7 02:02:08 2024 -0600

    fix q4_0_8_8 format for corrupted tokens issue (#10198)
    
    Co-authored-by: EC2 Default User <ec2-user@ip-172-31-62-167.us-west-2.compute.internal>

commit 3bcd40b3c593d14261fb2abfabad3c0fb5b9e318
Author: Zhiyuan Li <lizhiyuan@uniartisan.com>
Date:   Thu Nov 7 18:19:10 2024 +1100

    Optimize RWKV6 Operator Naming and Implement Multi-core CPU/ SYCL Acceleration (#10133)
    
    * rwkv6: rename to wkv6
    
    * rwkv6: support avx2 avx512 armv8 armv9
    
    * rwkv6: update cuda file name
    
    * rwkv6: rename params
    
    * wkv on sycl
    
    * sycl: add some ops
    
    * sycl: Enhance OP support judgment
    
    * wkv6: drop armv9 and tranfer to GGML style
    
    ggml-ci
    
    * sync : ggml
    
    * update the function to use appropriate types
    
    * fix define error
    
    * Update ggml/src/ggml-cpu.c
    
    * add appropriate asserts
    
    * move element-wise functions outside
    
    * put the declaration outside the loop
    
    * rewrite to be more inline with the common pattern for distributing threads
    
    * use recommended way GGML_TENSOR_LOCALS
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: Diego Devesa <slarengh@gmail.com>
    Co-authored-by: Plamen Minev <pacominev@gmail.com>
    Co-authored-by: Yuri Khrustalev <ykhrustalev@users.noreply.github.com>
    Co-authored-by: Meng, Hengyu <airdldl@163.com>

commit 5c333e014059122245c318e7ed4ec27d1085573c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 6 19:53:51 2024 +0200

    metal : add BF16 support (#8439)
    
    * ggml : add initial BF16 support
    
    ggml-ci
    
    * metal : add mul_mat_id BF16 support
    
    ggml-ci
    
    * metal : check for bfloat support on the Metal device
    
    ggml-ci
    
    * metal : better var names [no ci]
    
    * metal : do not build bfloat kernels when not supported
    
    ggml-ci
    
    * metal : try to fix BF16 support check
    
    ggml-ci
    
    * metal : this should correctly check bfloat support

commit b11f9ba9b8ce319f04b88afe40d264e6b7f4ba46
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 6 13:29:01 2024 +0200

    server : remove hack for extra parallel slot (#10187)
    
    ggml-ci

commit 94d8cb8be13b7c4d04eeca5a2b956b9148e6f222
Author: Diego Devesa <slarengh@gmail.com>
Date:   Wed Nov 6 12:10:07 2024 +0100

    metal : fix from ptr buffer name (#10189)

commit 1dc04b2deed2d2f2ae3aff9b14ae29674dee1fb8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 6 11:20:10 2024 +0200

    ggml : adjust is_first_call init value (#10193)
    
    ggml-ci

commit a1eaf6a9600bb1608753420ba886a3b0a208ffc0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 6 10:24:23 2024 +0200

    metal : add quantized FA support (#10149)
    
    * metal : add quantized FA (vec) support
    
    ggml-ci
    
    * metal : add quantized FA (non-vec) support
    
    * metal : fix support check
    
    ggml-ci
    
    * metal : clean-up
    
    * metal : clean-up (cont)
    
    * metal : fix shared memory calc + reduce smem + comments
    
    * metal : float-correctness
    
    * metal : minor [no ci]

commit b8deef0ec0af5febac1d2cfd9119ff330ed0b762
Author: Gabe Goodhart <ghart@us.ibm.com>
Date:   Tue Nov 5 05:23:04 2024 -0700

    llama : add <|tool_call|> formatting to Granite template (#10177)
    
    Branch: GraniteToolCallTemplate
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>

commit a9e8a9a0306a8093eef93b0022d9f45510490072
Author: Diego Devesa <slarengh@gmail.com>
Date:   Mon Nov 4 23:17:01 2024 +0100

    ggml : fix arch check in bf16_to_fp32 (#10164)

commit 340736477651095a98a3b10e19b038ec62593a1d
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Mon Nov 4 22:06:31 2024 +0000

    Q6_K AVX improvements (#10118)
    
    * q6_k instruction reordering attempt
    
    * better subtract method
    
    * should be theoretically faster
    
    small improvement with shuffle lut, likely because all loads are already done at that stage
    
    * optimize bit fiddling
    
    * handle -32 offset separately. bsums exists for a reason!
    
    * use shift
    
    * Update ggml-quants.c
    
    * have to update ci macos version to 13 as 12 doesnt work now. 13 is still x86

commit d5a409e57fe8bd24fef597ab8a31110d390a6392
Author: Diego Devesa <slarengh@gmail.com>
Date:   Mon Nov 4 20:06:58 2024 +0100

    ggml : fix gelu tables initialization (#10172)

commit 401558b7ba7a08175c153cd3607230f63c8a528e
Author: Diego Devesa <slarengh@gmail.com>
Date:   Mon Nov 4 17:34:08 2024 +0100

    ggml : fix q4xx mat mul, increase ggml_aligned_malloc alignment (#10167)

commit 9e0ecfb697d297355e43c20559d29bcc71beb0c3
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Nov 4 16:33:29 2024 +0100

    server : clarify /slots endpoint, add is_processing (#10162)
    
    * server : clarify /slots endpoint, add is_processing
    
    * fix tests

commit 6a066b9978533e2ab9890b7f4f8c0262d91798b3
Author: snadampal <87143774+snadampal@users.noreply.github.com>
Date:   Mon Nov 4 09:08:33 2024 -0600

    fix build break on arm64 linux (#10166)
    
    This fixes the build break from the recent changes
    to move the CPU backend to separate files
    https://github.com/ggerganov/llama.cpp/pull/10144

commit ea02c753ebf9342114cb173f10b3ffc2af1e7d04
Author: Diego Devesa <slarengh@gmail.com>
Date:   Mon Nov 4 13:10:23 2024 +0100

    cuda : clear error after changing peer access (#10153)

commit 05697f670b1ea28b80c39854832ea53527f75c55
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Nov 4 13:49:34 2024 +0200

    metal : simplify f16 and f32 dequant kernels (#0)

commit f8e58135cff1c373df2934306f9c9da99673c2ed
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Nov 4 13:43:32 2024 +0200

    metal : move dequantize templates to beginning of MSL source (#0)

commit 329ed914c959c510d076fb06b43eeb3f7b804d6f
Author: leo-pony <nengjunma@outlook.com>
Date:   Mon Nov 4 19:08:22 2024 +0800

    CANN: adjust backend registry refactor. (#10158)
    
    remove buffer->iface.get_name that used in cann as it was removed in backend registry refactor PR.

commit ce027adfb3b131f0d2368294fc276bb0e342b3f6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Nov 4 10:33:37 2024 +0200

    sync : ggml

commit 284e5b0275cc1292096e72e808e41d17e8cdf019
Author: Yuri Khrustalev <ykhrustalev@users.noreply.github.com>
Date:   Sat Nov 2 05:09:12 2024 -0400

    cmake : make it possible linking ggml as external lib (ggml/1003)

commit e2292aaa17cf8530b0d0d899909588c3a095799d
Author: Plamen Minev <pacominev@gmail.com>
Date:   Fri Nov 1 16:55:10 2024 +0200

    metal : fix minor string leaks (ggml/1004)

commit 9f409893519b4a6def46ef80cd6f5d05ac0fb157
Author: Diego Devesa <slarengh@gmail.com>
Date:   Sun Nov 3 19:34:08 2024 +0100

    ggml : move CPU backend to a separate file (#10144)

commit 08828a6d7d0006a487c9655ba8ace0ebe35ecad1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Nov 3 15:18:40 2024 +0200

    metal : minor fixup in FA kernel (#10143)
    
    * metal : minor fixup in FA kernel
    
    ggml-ci
    
    * metal : use the unrolled loop variable
    
    * metal : remove unused var

commit 1839f69130151ceeac4d01c0ef8964e1fb43bba6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Nov 3 15:14:15 2024 +0200

    flake.lock: Update (#10146)

commit 9830b6923b61f1e652a35afeac77aa5f886dad09
Author: Christian Köhnenkamp <cvk5@me.com>
Date:   Sat Nov 2 23:35:31 2024 +0100

    Add apple arm to presets (#10134)
    
    * Add apple arm to presets
    
    * Add final new line

commit 42cadc74bda60afafb45b71b1a39d150ede0ed4d
Author: sasha0552 <admin@sasha0552.org>
Date:   Sat Nov 2 16:34:56 2024 +0000

    server : fix slot selection by lru (#10126)
    
    * server : fix slot selection by lru, migrate lcs to `size_t`
    
    * minor debug log fix

commit 45950415ed985830c59bf42cf9c9216b20cf08ef
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 2 18:34:00 2024 +0200

    server : fix endpoint checks (#10135)
    
    ggml-ci

commit 1926d6e39d6f6358bc1a4c52316a560178be7233
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 2 15:18:56 2024 +0200

    llama : adjust default context size + print warnings (#10136)
    
    * llama : adjust default context size + print warnings
    
    ggml-ci
    
    * ggml-ci : add missing gpu-layers + adjust context sizes

commit b634f8a26fef65210fd9fb2f87e83a2809535e89
Author: Diego Devesa <slarengh@gmail.com>
Date:   Sat Nov 2 13:08:53 2024 +0100

    simple-chat : only add bos on first prompt (#10129)

commit 7554aa4655f44b33a29068f2b18c5976fae45f9d
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat Nov 2 12:53:17 2024 +0100

    convert-lora : make `--base` optional (#10110)
    
    * convert-lora : make `--base` optional
    
    * lint
    
    * handle case where base_model_name_or_path is invalid
    
    * do not include metadata from base model
    
    * clarify unspecified --base
    
    * add small comment [no ci]
    
    * trigger ci

commit a6744e43e80f4be6398fc7733a01642c846dce1d
Author: Diego Devesa <slarengh@gmail.com>
Date:   Fri Nov 1 23:50:59 2024 +0100

    llama : add simple-chat example (#10124)
    
    * llama : add simple-chat example
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>

commit e991e3127ff71a29e61fe1de5dd1cbd2e1df1858
Author: Diego Devesa <slarengh@gmail.com>
Date:   Fri Nov 1 23:48:26 2024 +0100

    llama : use smart pointers for ggml resources (#10117)

commit 418f5eef262cea07c2af4f45ee6a88d882221fcb
Author: Shupei Fan <dymarkfan@outlook.com>
Date:   Sat Nov 2 02:33:14 2024 +0800

    vulkan : improve ggml_vk_create_buffer error handling (#9898)

commit ba6f62eb793d6617892d252f5c04d7685d908a38
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Nov 1 17:31:51 2024 +0200

    readme : update hot topics

commit d865d1478cd4e403f82d793c2afcd0f943412f05
Author: sasha0552 <admin@sasha0552.org>
Date:   Fri Nov 1 13:33:14 2024 +0000

    server : fix smart selection of available slot (#10120)
    
    * Fix smart selection of available slot
    
    * minor fix
    
    * replace vectors of tokens with shorthands

commit 1804adb0cfee4811eaf633741503d683a46e4c77
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Nov 1 12:58:45 2024 +0200

    ggml : remove ggml_scratch (#10121)
    
    ggml-ci

commit 815fe72adcea5ec79d358db6a4c479191f396b3c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Nov 1 10:28:24 2024 +0200

    sync : ggml

commit f221d56220899f38f0126e683b2432bc79d1e3f6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Nov 1 10:23:05 2024 +0200

    ggml : alloc ggml_contexts on the heap (whisper/2525)

commit e597e50794f07ec8dc24b9efb18f94ec6386fda0
Author: Zhenwei Jin <109658203+kylo5aby@users.noreply.github.com>
Date:   Fri Nov 1 11:09:59 2024 +0800

    build: fix build error in Windows env with OneAPI setup (#10107)

commit 85679d37f34f66783cc04664a06c405b28e8e035
Author: Diego Devesa <slarengh@gmail.com>
Date:   Fri Nov 1 00:49:53 2024 +0100

    llama : improve output buffer type selection (#10098)

commit 1e9f94994ef908d964cf81069f03d9d3668beb7d
Author: Diego Devesa <slarengh@gmail.com>
Date:   Fri Nov 1 00:45:34 2024 +0100

    quantize : fix --keep-split (#10114)

commit c02e5ab2a675c8bc1abc8b1e4cb6a93b26bdcce7
Author: Diego Devesa <slarengh@gmail.com>
Date:   Thu Oct 31 22:54:23 2024 +0100

    llama : fix buffer checks for mamba and rwk (#10111)
    
    * llama : fix buffer checks for mamba and rwk
    
    * llama : fix missing worst case flag during reserve
    
    * cuda : fix supports_op for norm
    
    * disable sched SET_CAUSE

commit ab3d71f97f5b2915a229099777af00d3eada1d24
Author: Zhenwei Jin <109658203+kylo5aby@users.noreply.github.com>
Date:   Fri Nov 1 02:50:39 2024 +0800

    loader:  refactor tensor weights storage (#9935)
    
    * loader: refactor tensor weights storage
    
    * use sorted map, sort weights by layer
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 0a683e8088d849626e7471f9e2ed381f7dbdf2e9
Author: Kevin Gibbons <bakkot@gmail.com>
Date:   Thu Oct 31 06:02:35 2024 -0700

    server : include scheme when printing URL (#10106)

commit dea5e86051aadcdf42f7db7a8855a78d8f5ff3d6
Author: Diego Devesa <slarengh@gmail.com>
Date:   Thu Oct 31 11:40:59 2024 +0100

    ggml : check tensor name lengths in gguf files (#10100)

commit 1329c0a75e6a7defc5c380eaf80d8e0f66d7da78
Author: Sergio López <slp@redhat.com>
Date:   Thu Oct 31 10:09:52 2024 +0100

    kompute: add mul_mat_q4_k shader (#10097)
    
    This is a more or less direct translation from the Metal implementation
    to GLSL.
    
    Signed-off-by: Sergio Lopez <slp@redhat.com>

commit 61408e7fad082dc44a11c8a9f1398da4837aad44
Author: Sergio López <slp@redhat.com>
Date:   Wed Oct 30 17:01:52 2024 +0100

    kompute: add backend registry / device interfaces (#10045)
    
    Get in line with the other backends by supporting the newer
    backend/device registry interfaces.
    
    Signed-off-by: Sergio Lopez <slp@redhat.com>

commit b9e02e8184f5e6094a9e87eaf040becd404bfc90
Author: Diego Devesa <slarengh@gmail.com>
Date:   Wed Oct 30 14:51:21 2024 +0100

    ggml : fix memory leaks when loading invalid gguf files (#10094)
    
    * ggml : fix gguf string leak when reading kv pairs fails
    
    * ggml : avoid crashing with GGML_ABORT when the KV has an invalid type
    
    * ggml : avoid crashing on failed memory allocations when loading a gguf file

commit 6763f713bb692910e9b2d9d1a82d6959cee2dcf3
Author: Rich Dougherty <rich@rd.nz>
Date:   Thu Oct 31 01:22:39 2024 +1300

    readme : more lora detail in main example readme (#10064)

commit 79a2bc042dcacaad59306865208a8c8c3149e3ea
Author: Rich Dougherty <rich@rd.nz>
Date:   Thu Oct 31 01:22:21 2024 +1300

    convert : more detailed convert lora usage docs (#10065)

commit fc83a9e58479e4dd70054daa7afe5184c1bbe545
Author: xctan <axunlei@gmail.com>
Date:   Wed Oct 30 15:00:40 2024 +0800

    ggml : add Q4_0_8_8 RISC-V GEMV and GEMM kernels (#10029)
    
    * ggml : RISC-V vector gemv for q4_0_8x8
    
    * ggml : Added WIP rvv q4_0_8x8 gemm
    
    * ggml : Added initial implementation of rvv gemm
    
    * ggml : optimize gemm to avoid register spillover
    
    * ggml : Fix GCC rvv load alignment issue
    
    * ggml : Format gemm rvv code
    
    * ggml : Fix a typo in RVV q4_0_8_8 GEMM

commit c5b0f4b5d90297f3e729fca7f78ddb25fcab5ddc
Author: Diego Devesa <slarengh@gmail.com>
Date:   Wed Oct 30 02:01:23 2024 +0100

    llama : refactor model loader with backend registry (#10026)

commit 8f275a7c4593aa34147595a90282cf950a853690
Author: Changyeon Kim <cyzero.kim@samsung.com>
Date:   Tue Oct 29 17:52:56 2024 +0900

    ggml: Add POOL2D OP for GPU acceleration to the Vulkan backend in the MobileVLM model. (#9763)
    
    * ggml: Add POOL2D OP for GPU ACC to the Vulkan.
    
    - The MobileVLM model now supports inference acceleration through GPU by utilizing the Vulkan backend.
    - A GGML_OP_POOL_2D shader has been added. (Pooling)
    - The encoding performance of the CLIP model improved from 2.8s on the CPU to 0.7s on the GPU.
    
    Signed-off-by: Changyeon Kim <cyzero.kim@samsung.com>
    
    * [fix] Correct the incorrect order of the parameters.
    
    fix casting to int.
    
    Signed-off-by: Changyeon Kim <cyzero.kim@samsung.com>
    
    ---------
    
    Signed-off-by: Changyeon Kim <cyzero.kim@samsung.com>

commit 8d8ff715367480b856ad86ac3888e9742b13a6fa
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 29 10:42:05 2024 +0200

    llama : remove Tail-Free sampling (#10071)
    
    ggml-ci

commit 61715d5cc83a28181df6a641846e4f6a740f3c74
Author: arch-btw <57669023+arch-btw@users.noreply.github.com>
Date:   Mon Oct 28 10:45:33 2024 -0700

    llama : Add IBM granite template (#10013)
    
    * Add granite template to llama.cpp
    
    * Add granite template to test-chat-template.cpp
    
    * Update src/llama.cpp
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    * Update tests/test-chat-template.cpp
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    * Added proper template and expected output
    
    * Small change to \n
    
    Small change to \n
    
    * Add code space &
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    * Fix spacing
    
    * Apply suggestions from code review
    
    * Update src/llama.cpp
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>

commit 07028f9d74d895da2ca4a1956624e3f07e04e620
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Oct 28 17:41:24 2024 +0200

    flake.lock: Update (#10063)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/4c2fcb090b1f3e5b47eaa7bd33913b574a11e0a0?narHash=sha256-/uilDXvCIEs3C9l73JTACm4quuHUsIHcns1c%2BcHUJwA%3D' (2024-10-18)
      → 'github:NixOS/nixpkgs/2768c7d042a37de65bb1b5b3268fc987e534c49d?narHash=sha256-AlcmCXJZPIlO5dmFzV3V2XF6x/OpNWUV8Y/FMPGd8Z4%3D' (2024-10-23)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit 524afeec9dad7d765ce91f5cf30c73703867cb47
Author: R0CKSTAR <xiaodong.ye@mthreads.com>
Date:   Mon Oct 28 17:02:48 2024 +0800

    musa: workaround for Guilty Lockup in cleaning src0 (#10042)
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>

commit 8125e6cbfcf2b3b9066e4d923aca9295526730f5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Oct 28 08:49:32 2024 +0200

    server : don't overfill the batch during infill (#10018)
    
    ggml-ci

commit 8841ce3f439de6e770f70319b7e08b6613197ea7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 27 20:59:58 2024 +0200

    llama : switch KQ multiplication to F32 precision by default (#10015)
    
    ggml-ci

commit cc2983d3753c94a630ca7257723914d4c4f6122b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Oct 26 10:34:08 2024 +0300

    sync : ggml

commit 8c60a8a46261ffb92b6d23a78acfac2fcb6fe233
Author: bssrdf <merlintiger@hotmail.com>
Date:   Wed Oct 23 14:34:00 2024 -0400

    increase cuda_cpy block size (ggml/996)
    
    Co-authored-by: bssrdf <bssrdf@gmail.com>

commit 9e4a2563eadf34e9432d248224d4f43e8495e8fe
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Oct 26 10:33:31 2024 +0300

    scripts : fix amx sync [no ci]

commit 668750357e66bfa3d1504b65699f5a0dfe3cb7cb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Oct 25 22:26:15 2024 +0300

    metal : support permuted matrix multiplicaions (#10033)
    
    * metal : support permuted matrix multiplicaions
    
    ggml-ci
    
    * cont : use nb01 directly for row steps
    
    ggml-ci
    
    * cont : add comments [no ci]
    
    * metal : minor refactor
    
    * metal : minor

commit ff252ea48e90e6552010fd74584334fb41bdd387
Author: wwoodsTM <104587230+wwoodsTM@users.noreply.github.com>
Date:   Fri Oct 25 10:07:34 2024 -0600

    llama : add DRY sampler (#9702)
    
    * sampling : add DRY sampler (post-refactor)
    
    * DRY: Trying to fix coauthors, removed unneeded line
    
    * DRY: Fixed redundant code
    
    * DRY: Fixed crash issue due to DRY being in chain but uninitialized
    
    ---------
    
    Co-authored-by: l3utterfly <gc.pthzfoldr@gmail.com>
    Co-authored-by: pi6am <34464159+pi6am@users.noreply.github.com>

commit d80fb71f8b8bf69ec095ba281f8248d136d21c76
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Fri Oct 25 17:57:54 2024 +0200

    llama: string_split fix (#10022)
    
    * llama: Refactor string_split to use template specialization,  fixes parsing strings with spaces
    
    * llama: Add static_assert in the string_split template to ensure the correct template specialization is used for std::string

commit 2f8bd2b90133cf37ae752015e1bfd738cc6d0112
Author: Srihari-mcw <96763064+Srihari-mcw@users.noreply.github.com>
Date:   Fri Oct 25 12:57:41 2024 +0530

    llamafile : extend sgemm.cpp support for Q5_0 models (#10010)

commit bc5ba007b2c83ac95875e68724dabfc12159fc61
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Oct 25 10:13:46 2024 +0300

    server : check that the prompt fits in the slot's context (#10030)
    
    ggml-ci

commit 958367bf530d943a902afa1ce1c342476098576b
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Oct 24 21:51:22 2024 +0200

    server : refactor slot input data, move tokenizer to HTTP thread (#10023)
    
    * server : refactor slot input data, move tokenizer to HTTP thread
    
    * move prompt_tokens.empty() check
    
    * fix incorrect if branch
    
    * fix infinite generation loop
    
    * bring back infill validation
    
    * add infill test
    
    * try fixing format_infill
    
    * fix test
    
    * remove redundant code
    
    * rename completion to inference
    
    * update docs
    
    * use llama_tokens everywhere

commit 40f2555797f97314de749873cdc29dc102be66e2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Oct 24 21:23:33 2024 +0300

    ci : fix cmake flags for SYCL

commit 167a515651a4b065a16225ffc69564c5674f3d0f
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Oct 24 14:40:23 2024 +0200

    CUDA: fix insufficient buffer clearing for MMQ (#10032)

commit c39665f589091903396a442a6ee56613303e0350
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Oct 24 11:09:36 2024 +0200

    CUDA: fix MMQ for non-contiguous src0, add tests (#10021)
    
    * CUDA: fix MMQ for non-contiguous src0, add tests
    
    * revise test code

commit 0a1c750c80147687df267114c81956757cc14382
Author: wwoodsTM <104587230+wwoodsTM@users.noreply.github.com>
Date:   Wed Oct 23 13:27:51 2024 -0600

    server : samplers accept the prompt correctly (#10019)

commit 190a37d7977eb5bd6a729299bd1e371208c87149
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 23 17:23:55 2024 +0300

    sync : ggml

commit 2d3aba9ee8da9c026d54e8a912a1d64f56809be3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 23 17:16:56 2024 +0300

    llama.vim : bump generation time limit to 3s [no ci]

commit 80273a306d07ed95059d6130389deacb3b2d7196
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Oct 18 09:24:44 2024 +0200

    CUDA: fix 1D im2col, add tests (ggml/993)

commit c19af0acb1fe6d0fdbecadd8483c1fbe5d68d095
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed Oct 16 20:10:01 2024 +0200

    ggml : remove redundant set of contexts used field (ggml/978)
    
    This commit removes the setting of the `used` field of the contexts in
    the global state (g_state) in `ggml_init`.
    
    The motivation for this change is that I believe that this additional
    initialization might not be required after the changes in Commit
    45fc4fed0b9fb5b1af4a8525cbebb95e11208732 ("sync : latest changes from
    whisper.cpp"), which changed the initialization of the contexts field
    from `{ 0 }` to `{ { 0 } }`:
    
    ```console
                 g_state = (struct ggml_state) {
    -                /*.contexts =*/ { 0 },
    +                /*.contexts =*/ { { 0 } },
                 };
    ```
    My understanding is that the `{0}` initialization might not have
    zero-initialized all the nested fields in every array element because of
    compiler differences, and might have been the reason for having the
    explicit setting of the `used` fields to false.

commit ac113a0feee0935b2018312f7bc8d7a646b117ed
Author: Michael Coppola <m18coppola@gmail.com>
Date:   Wed Oct 23 07:09:26 2024 -0400

    llama.vim : add classic vim support (#9995)
    
    * added classic vim support
    
    * fixed ring update, removed blank line
    
    * minor
    
    * minor
    
    * minor doc update
    
    * removed uneeded var
    
    * minor
    
    * minor
    
    * fixed job_start creating new scratch buffers
    
    * fixed job_start creating new scratch buffers
    
    * fixed ghost text indenting when expandtab is on
    
    * removed unused code
    
    * minor
    
    * unified fim_on_exit
    
    * minor
    
    * vim ghost text rendering now uses pos_x and pos_y parameters
    
    * renamed *_hlgroup to hlgroup_*
    
    * renamed *_ghost_text to ghost_text_*, moved nvim/vim detection to llama#init()
    
    * minor
    
    ---------
    
    Co-authored-by: Michael Coppola <info@michaeljcoppola.com>

commit 4c9388fb96ac2415fbb1239b7ba8346616606e2e
Author: Jun Hee Yoo <contact.jhyoo@gmail.com>
Date:   Wed Oct 23 19:33:45 2024 +0900

    metal : add POOL2D and fix IM2COL (#9943)
    
    * add pool_2d
    
    Signed-off-by: Junhee Yoo <junhee.yoo@navercorp.com>
    
    * fix im2col and add unittest for N>=1024
    
    Signed-off-by: Junhee Yoo <junhee.yoo@navercorp.com>
    
    * add tests for N % 1024 != 0
    
    Signed-off-by: Junhee Yoo <junhee.yoo@navercorp.com>
    
    * remove trailing whitespaces
    
    Signed-off-by: Junhee Yoo <junhee.yoo@navercorp.com>
    
    * apply suggestions
    
    Signed-off-by: Junhee Yoo <junhee.yoo@navercorp.com>
    
    * apply more optimization
    
    - original IM2COL kernel + _ext with MIN()
    
    Signed-off-by: Junhee Yoo <junhee.yoo@navercorp.com>
    
    * apply review: change kernel name of pool_2d
    
    Signed-off-by: Junhee Yoo <junhee.yoo@navercorp.com>
    
    * apply review
    
    Signed-off-by: Junhee Yoo <junhee.yoo@navercorp.com>
    
    * fix more formatting and enhance readability
    
    Signed-off-by: Junhee Yoo <junhee.yoo@navercorp.com>
    
    ---------
    
    Signed-off-by: Junhee Yoo <junhee.yoo@navercorp.com>

commit 873279b1592e433c4d9eb5065091cc98473c7bee
Author: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
Date:   Sun Oct 20 00:22:59 2024 +0000

    flake.lock: Update
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/5633bcff0c6162b9e4b5f1264264611e950c8ec7?narHash=sha256-9UTxR8eukdg%2BXZeHgxW5hQA9fIKHsKCdOIUycTryeVw%3D' (2024-10-09)
      → 'github:NixOS/nixpkgs/4c2fcb090b1f3e5b47eaa7bd33913b574a11e0a0?narHash=sha256-/uilDXvCIEs3C9l73JTACm4quuHUsIHcns1c%2BcHUJwA%3D' (2024-10-18)

commit c8c07d658a6cefc5a50cfdf6be7d726503612303
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Oct 22 16:59:02 2024 +0200

    llama : fix empty batch causing llama_batch_allocr to crash (#9966)
    
    * llama : fix empty batch cause llama_batch_allocr to crash
    
    * move batch_allocr inside decode/encode_internal
    
    * fix build
    
    * add GGML_ASSERT
    
    * Apply suggestions from code review
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 19d900a7565b8f6b0a708836a57d26966cb9efe2
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Oct 22 15:31:06 2024 +0200

    llama : rename batch to ubatch (#9950)
    
    This commit renames the member field batch in llm_build_context to
    ubatch, and also the parameter batch in llama_build_graph, and
    llama_set_inputs to ubatch.
    
    The motivation for this change is to make the code more readable
    (considering there are the structs llama_batch and llama_sbatch), and
    consistent with other parts of the code base where parameters/fields of
    type llama_ubatch are named ubatch.

commit 11d47057a51f3d9b9231e6b57d0ca36020c0ee99
Author: Molly Sophia <mollysophia379@gmail.com>
Date:   Tue Oct 22 21:22:26 2024 +0800

    Rwkv chat template fix (#10001)
    
    * llama: remove useless template matching for rwkv-world
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * converter: Add comment about the hack for rwkv models
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * Update src/llama.cpp
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    ---------
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>

commit c421ac072d46172ab18924e1e8be53680b54ed3b
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Oct 22 13:08:41 2024 +0200

    lora : warn user if new token is added in the adapter (#9948)

commit 4ff7fe1fb36b04ddd158b2de881c348c5f0ff5e4
Author: Molly Sophia <mollysophia379@gmail.com>
Date:   Tue Oct 22 18:33:37 2024 +0800

    llama : add chat template for RWKV-World + fix EOT (#9968)
    
    * Add chat template for RWKV-World
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * RWKV: Fix the chat template not being used
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * RWKV v6: Set EOT token to ``\n\n``
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * readme: add rwkv into supported model list
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    ---------
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>

commit 6b8447352df3d662b56280c8fc38d7f092885787
Author: leo-pony <nengjunma@outlook.com>
Date:   Tue Oct 22 16:16:01 2024 +0800

    [CANN] Adapt to dynamically loadable backends mechanism (#9970)
    
    * [CANN] Adapt to dynamically loadable backends mechanism
    
    * Fix the Bug: inference running result is garbled in debug running model for LM models who's type is Q4_0 class
    
    * Handle the review comments of this pull request

commit 674804a99617b4f90292b4080ecab450ea3d30ba
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Oct 22 09:40:02 2024 +0200

    arg : fix typo in embeddings argument help [no ci] (#9994)
    
    This commit fixes two typos in the help text for the `--embd-normalize`
    and `--embd-separator` arguments. It also updates common.h which contain
    the same typo in two comments.

commit e94a138d644a9b34da61805f7aeb8af595c61b53
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 22 00:35:25 2024 +0300

    llama.vim : fix info text display [no ci] (#9787)

commit e01c67affe450638162a1a457e2e57859ef6ebf0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Oct 21 22:52:22 2024 +0300

    llama.vim : move info to the right of screen [no ci] (#9787)
    
    'eol' messes up the rendering with nvim v0.10.2 for some reason

commit 994cfb1acb9144bc95be0ab319175f30737cc92b
Author: Asghar Ghorbani <a-ghorbani@users.noreply.github.com>
Date:   Mon Oct 21 20:20:59 2024 +0200

    readme : update UI list (#9972)
    
    add PocketPal AI app

commit 94008cc76075fb4a29ee371e7ac255378d1bce6c
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Mon Oct 21 20:12:52 2024 +0200

    arg : fix attention non-causal arg value hint (#9985)
    
    This commit updates the argument value hint for the `--attention`
    argument to `non-causal`.
    
    The motivation for this change is that the only values for this argument
    are `causal` and `non-causal`.

commit dbd5f2f5736aec6ff8fd63df3b351dae23c43e2f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Oct 21 20:25:02 2024 +0300

    llama.vim : plugin for Neovim (#9787)

commit f594bc80baf683818f29d8f5d6fb52daab99e572
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Oct 21 16:20:46 2024 +0300

    ggml : add asserts for type conversion in fattn kernels (#9971)
    
    ggml-ci

commit d5ebd79c76abd4887f0283cd6f6f9689122094d0
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Mon Oct 21 13:35:40 2024 +0300

    rpc : pack only RPC structs (#9959)

commit 55e47786e373c90fc7803e718e3e1dd6d53c3db6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Oct 21 09:46:40 2024 +0300

    llama : default sampling changes + greedy update (#9897)
    
    * llama : deprecate softmax sampler + fix dist sampler
    
    ggml-ci
    
    * tests : replace macros with functions
    
    ggml-ci
    
    * sampling : change temperature sampler logic
    
    For t <= 0.0f, keep the max logit intact and set the rest to -inf
    
    * cont : no need for special "greedy" logic
    
    top-k == 1 is the same
    
    * tests : init prob correctly
    
    * llama : handle temp <= 0.0 in the temp_ext sampler too
    
    ggml-ci
    
    * cont : avoid extra loop in temperature sampler for sub-zero temp
    
    ggml-ci

commit bc219750845a59166d79f0d4ee3da1993b369b8a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Oct 21 09:37:12 2024 +0300

    speculative : fix handling of some input params (#9963)
    
    * speculative : fix batch sizes at initialization
    
    ggml-ci
    
    * speculative : handle params.n_predict == -1
    
    * speculative : limit batch size to llama_n_batch

commit 1db8c84fc62857e1e45c1c7ea93bcd5344cb3d31
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Mon Oct 21 14:26:09 2024 +0800

    fix mul_mat_vec_q and *_vec_q error (#9939)
    
    Co-authored-by: arthw <14088817+arthw@users.noreply.github.com>

commit 45f097645efb11b6d09a5b4adbbfd7c312ac0126
Author: Loïc Carrère <loic.carrere@gmail.com>
Date:   Sun Oct 20 18:25:41 2024 +0200

    readme : update bindings list (#9951)
    
    Update the binding list by adding LM-Kit.NET (C# & VB.NET)

commit 7cab2083c768dd92c20b105556c4165b59cd8a41
Author: icppWorld <124377669+icppWorld@users.noreply.github.com>
Date:   Sun Oct 20 12:01:34 2024 -0400

    readme : update infra list (#9942)
    
    llama_cpp_canister allows you to run llama.cpp as a Smart Contract on the Internet Computer. The smart contract runs as WebAssembly in a so-called 'canister'.

commit cda0e4b648dde8fac162b3430b14a99597d3d74f
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Oct 18 23:18:01 2024 +0200

    llama : remove all_pos_0, all_pos_1, all_seq_id from llama_batch (#9745)
    
    * refactor llama_batch_get_one
    
    * adapt all examples
    
    * fix simple.cpp
    
    * fix llama_bench
    
    * fix
    
    * fix context shifting
    
    * free batch before return
    
    * use common_batch_add, reuse llama_batch in loop
    
    * null terminated seq_id list
    
    * fix save-load-state example
    
    * fix perplexity
    
    * correct token pos in llama_batch_allocr

commit afd9909a6481402844aecefa8a8908afdd7f52f1
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Fri Oct 18 14:33:58 2024 +0300

    rpc : backend refactoring (#9912)
    
    * rpc : refactor backend
    
    Use structs for RPC request/response messages
    
    * rpc : refactor server

commit 87421a23e8c60e00a7b227d501e8aab2a1aff7ce
Author: Ouadie EL FAROUKI <ouadie.elfarouki@codeplay.com>
Date:   Fri Oct 18 06:46:16 2024 +0100

    [SYCL] Add SYCL Backend registry, device and Event Interfaces (#9705)
    
    * implemented missing SYCL event APIs
    
    * sycl : Added device and backend reg interfaces
    
    * Restructured ggml-sycl.cpp

commit 60ce97c9d809f4b040e90b597468b839df5728d0
Author: Ma Mingfei <mingfei.ma@intel.com>
Date:   Fri Oct 18 13:34:36 2024 +0800

    add amx kernel for gemm (#8998)
    
    add intel amx isa detection
    
    add vnni kernel for gemv cases
    
    add vnni and amx kernel support for block_q8_0
    
    code cleanup
    
    fix packing B issue
    
    enable openmp
    
    fine tune amx kernel
    
    switch to aten parallel pattern
    
    add error message for nested parallelism
    
    code cleanup
    
    add f16 support in ggml-amx
    
    add amx kernels for QK_K quant formats: Q4_K, Q5_K, Q6_K and IQ4_XS
    
    update CMakeList
    
    update README
    
    fix some compilation warning
    
    fix compiler warning when amx is not enabled
    
    minor change
    
    ggml-ci
    
    move ggml_amx_init from ggml.c to ggml-amx/mmq.cpp
    
    ggml-ci
    
    update CMakeLists with -mamx-tile, -mamx-int8 and -mamx-bf16
    
    ggml-ci
    
    add amx as an ggml-backend
    
    update header file, the old path for immintrin.h has changed to ggml-cpu-impl.h
    
    minor change
    
    update CMakeLists.txt
    
    minor change
    
    apply weight prepacking in set_tensor method in ggml-backend
    
    fix compile error
    
    ggml-ci
    
    minor change
    
    ggml-ci
    
    update CMakeLists.txt
    
    ggml-ci
    
    add march dependency
    
    minor change
    
    ggml-ci
    
    change ggml_backend_buffer_is_host to return false for amx backend
    
    ggml-ci
    
    fix supports_op
    
    use device reg for AMX backend
    
    ggml-ci
    
    minor change
    
    ggml-ci
    
    minor change
    
    fix rebase
    
    set .buffer_from_host_ptr to be false for AMX backend

commit 8901755ba328643c9ab071c20e1939ea52951a0e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Oct 18 07:32:19 2024 +0300

    server : add n_indent parameter for line indentation requirement (#9929)
    
    ggml-ci

commit 6f55bccbb8835d42147add4ee48807450f5ff535
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Oct 18 01:41:51 2024 +0200

    llama : rename batch_all to batch (#8881)
    
    This commit addresses the TODO in the code to rename the `batch_all`
    parameter to `batch` in `llama_decode_internal`.

commit 17bb9280807cfbb6611b853aa1ef05114bd9efe9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Oct 17 23:43:05 2024 +0300

    readme : remove --memory-f32 references (#9925)

commit 9f45fc1e9950a496febc575cdd196cd5cad000cc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Oct 17 23:26:32 2024 +0300

    llama : change warning to debug log

commit 99bd4ac28c32cd17c0e337ff5601393b033dc5fc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Oct 17 22:32:47 2024 +0300

    llama : infill sampling handle very long tokens (#9924)
    
    * llama : infill sampling handle very long tokens
    
    ggml-ci
    
    * cont : better indices
    
    ggml-ci

commit 3752217ed5a6a11864682fbf009bcb36afffd6bc
Author: Tim Wang <overocean@gmail.com>
Date:   Thu Oct 17 17:57:14 2024 +1100

    readme : update bindings list (#9918)
    
    Co-authored-by: Tim Wang <tim.wang@ing.com>

commit f010b77a372ffcfaf4338c670d6d3ecd89aa4eb6
Author: Diego Devesa <slarengh@gmail.com>
Date:   Thu Oct 17 02:46:58 2024 +0200

    vulkan : add backend registry / device interfaces (#9721)
    
    * vulkan : add backend registry / device interfaces
    
    * llama : print devices used on model load

commit 21942002780352b4a54f4bd3e5eefa3bc7f14fe6
Author: Gilad S. <7817232+giladgd@users.noreply.github.com>
Date:   Thu Oct 17 02:34:22 2024 +0300

    fix: allocating CPU buffer with size `0` (#9917)

commit 73afe681aa76e818733fc1f30de082c1d6910bcd
Author: Gilad S. <7817232+giladgd@users.noreply.github.com>
Date:   Thu Oct 17 01:36:51 2024 +0300

    fix: use `vm_allocate` to allocate CPU backend buffer on macOS (#9875)
    
    * fix: use `vm_allocate` to allocate CPU backend buffer on macOS
    
    * fix: switch to `posix_memalign` to keep existing `free()` usages work
    
    * feat: move `GGML_ALIGNED_MALLOC` to `ggml-backend-impl.h`, add support for `vm_allocate` on macOS
    
    * style: formatting
    
    * fix: move const outside of `#ifndef`
    
    * style: formatting
    
    * fix: unused var
    
    * fix: transform `GGML_ALIGNED_MALLOC` and `GGML_ALIGNED_FREE` into functions and add them to `ggml-impl.h`
    
    * fix: unused var
    
    * fix: page align to `GGUF_DEFAULT_ALIGNMENT`
    
    * fix: page align to `TENSOR_ALIGNMENT`
    
    * fix: convert `TENSOR_ALIGNMENT` to a macro
    
    * fix: increase page size to `32` on iOS
    
    * fix: iOS page size
    
    * fix: `hbw_posix_memalign` alignment

commit 9e041024481f6b249ab8918e18b9477f873b5a5e
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed Oct 16 19:34:28 2024 +0200

    llama : suppress conversion from 'size_t' to 'int' (#9046)
    
    * llama : suppress conversion from 'size_t' to 'int'
    
    This commit updates llm_tokenizer_spm.tokenize to suppress/remove the
    following warnings that are generated on Windows when using MSVC:
    
    ```console
    src\llama-vocab.cpp(211,1): warning C4267: 'argument':
        conversion from 'size_t' to 'int', possible loss of data
    src\llama-vocab.cpp(517,1): warning C4267: 'argument':
        conversion from 'size_t' to 'int', possible loss of data
    ```
    
    This is done by adding a cast for the size_t returned from
    symbols.size(). I believe this is safe as it seems unlikely that
    symbols, which stores an entry for each UTF8 character, would become
    larger than INT_MAX.
    
    The motivation for this change is to reduce the number of warnings that
    are currently generated when building on Windows.
    
    * squash! llama : suppress conversion from 'size_t' to 'int'
    
    Move cast into for loop.

commit dbf18e4de9e7aa496871f1555f9f0c8d84567108
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed Oct 16 19:24:05 2024 +0200

    llava : fix typo in error message [no ci] (#9884)

commit 66c2c93082289325199ae1f773f3b0ab2e399a47
Author: Joe Eli McIlvain <joe.eli.mac@gmail.com>
Date:   Wed Oct 16 09:03:24 2024 -0700

    grammar : fix JSON Schema for string regex with top-level alt. (#9903)
    
    Prior to this commit, using a JSON Schema containing a string
    with `pattern` regular expression that uses top-level alternation
    (e.g. `"pattern": "^A|B|C|D$"`) would result in invalid JSON
    output from the constrained sampling grammar, because it
    ended up creating a grammar rule like this for the string:
    
    ```
    thing ::= "\"" "A" | "B" | "C" | "D" "\"" space
    ```
    
    Note that this rule will only match a starting quote for the "A" case,
    and will only match an ending quote for the "D" case,
    so this rule will always produce invalid JSON when used for sampling
    (that is, the JSON will always be lacking the starting quote,
    the ending quote, or both).
    
    This was fixed in a simple way by adding parentheses to the
    generated rule (for all string pattern rules, to keep it simple),
    such that the new generated rule looks like this (correct):
    
    ```
    thing ::= "\"" ("A" | "B" | "C" | "D") "\"" space
    ```

commit 10433e8b457c4cfd759cbb41fc55fc398db4a5da
Author: Molly Sophia <mollysophia379@gmail.com>
Date:   Wed Oct 16 18:10:21 2024 +0800

    llama : add tensor name for "result_norm" (#9907)
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>

commit 1f66b699c48cb5ab3265ed72c48e8549b1674291
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Wed Oct 16 08:35:53 2024 +0000

    server : fix the disappearance of the end of the text (#9867)
    
    * server: fix the disappearance of the end of the text when streaming with stop strings
    
    * simplify "send text" checks

commit 0e41b300ed28f7fe185d938b2e3d56a0bf7411ed
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 16 11:28:14 2024 +0300

    sync : ggml

commit cd60b88bf7ad7785fb6ac9864e360cf10e42faad
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed Oct 9 16:40:35 2024 +0200

    ggml-alloc : remove buffer_id from leaf_alloc (ggml/987)
    
    This commit removes the buffer_id field from the leaf_alloc struct.
    
    The motivation for is that this field is only written to and never
    read/used as far as I can tell. Each tensor_alloc has a buffer_id field
    and this is what caused me to look into this more closely, to
    understand what the buffer_id in leaf_alloc was used for.

commit becfd387f6919d99ec34b76c2522f90ac250c489
Author: leo-pony <nengjunma@outlook.com>
Date:   Wed Oct 16 08:51:46 2024 +0800

    [CANN] Fix cann compilation error (#9891)
    
    Fix cann compilation error after merging llama.cpp supports dynamically loadable backends.

commit 755a9b2bf00fbae988e03a47e852b66eaddd113a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 15 16:35:33 2024 +0300

    llama : add infill sampler (#9896)
    
    ggml-ci

commit 223c25a72fcc3f65cdfd7f5d57edd5b44b550e18
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 15 16:28:55 2024 +0300

    server : improve infill context reuse (#9894)
    
    ggml-ci

commit fbc98b748e7b075e327bcf13237057f647678049
Author: MaggotHATE <clay1326@gmail.com>
Date:   Tue Oct 15 15:54:55 2024 +0500

    sampling : add XTC sampler (#9742)
    
    * Initial XTC commit
    
    Adds XTC sampler, not activated by default, but recommended settings by default.
    
    * Cleanup
    
    * Simplified chances calculation
    
    To be more inline with the original implementation, chance is calculated once at the beginning.
    
    * First fixes by comments
    
    Still need to look into sorting
    
    * Fixed trailing backspaces
    
    * Fixed RNG to be reproduceable
    
    Thanks to @slaren for directions
    
    * Fixed forgotten header
    
    * Moved `min_keep`
    
    Moved from conditions to a simple check at the end.
    
    * Fixed broken randomization
    
    Thanks to @slaren for explanation
    
    * Swapped sorting for a custom algorithm
    
    Shifts tokens to remove the penalized ones, then puts the penalized at the back. Should make `min_keep` still viable.
    
    * Algorithm rework
    
    1. Scan token from top till the first non-penalizable
    2. Remove the last captured token (the least probable above threshold)
    3. Shift all tokens to override the remaining penalizable
    4. Penalize and put them at the the bottom.
    
    * Added XTC to `test-sampling`
    
    * Simplified algorithm and more tests
    
    * Updated info in common and args
    
    * Merged back lost commits in common and arg
    
    * Update dump info in common
    
    * Fixed incorrect min_keep check
    
    * Added XTC to README
    
    * Renamed parameters, fixed info and defaults
    
    * probability is at 0 by default, but XTC is included in sampling queue
    * threshold higher than 0.5 switches XTC off
    
    * Initial server support
    
    * Added XTC to server UIs
    
    * Fixed labels in old server UI
    
    * Made algorithm safer and more readable
    
    * Removed xtc_threshold_max
    
    * Fixed arg after update
    
    * Quick fixes by comments
    
    * Simplified algorithm since threshold_max is removed
    
    * Renamed random distribution
    
    * Fixed tests and outdated README
    
    * Small fixes

commit dcdd535302fc9702a4709be25f56540d65163a44
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 15 12:48:44 2024 +0300

    server : update preact (#9895)

commit 4c42f93b22146c83b763d8cbee5fafc512746649
Author: Michał Tuszyński <srgtuszy@gmail.com>
Date:   Tue Oct 15 10:20:34 2024 +0200

    readme : update bindings list (#9889)

commit a89f75e1b7b90cb2d4d4c52ca53ef9e9b466aa45
Author: VoidIsVoid <343750470@qq.com>
Date:   Mon Oct 14 15:04:36 2024 +0800

    server : handle "logprobs" field with false value (#9871)
    
    Co-authored-by: Gimling <huangjl@ruyi.ai>

commit 13dca2a54a394757d56fdd652b9f0df08f44ea22
Author: agray3 <agray3@users.noreply.github.com>
Date:   Mon Oct 14 01:49:08 2024 +0100

    Vectorize load instructions in dmmv f16 CUDA kernel (#9816)
    
    * Vectorize load instructions in dmmv f16 CUDA kernel
    
    Replaces scalar with vector load instructions, which substantially
    improves performance on NVIDIA HBM GPUs, e.g. gives a 1.27X overall
    speedup for Meta-Llama-3-8B-Instruct-F16 BS1 inference evaluation on
    H100 SXM 80GB HBM3. On GDDR GPUs, there is a slight (1.01X) speedup.
    
    * addressed comment
    
    * Update ggml/src/ggml-cuda/dmmv.cu
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

commit d4c19c0f5cdb1e512573e8c86c79e8d0238c73c4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 13 21:31:35 2024 +0300

    server : accept extra_context for the infill endpoint (#9874)
    
    * server : accept extra_context for the infill endpoint
    
    ggml-ci
    
    * server : update readme [no ci]
    
    * server : use repo-level FIM pattern if possible
    
    ggml-ci

commit c7181bd294757dd80a7904e3dd0fea2d0be914e7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 13 18:52:48 2024 +0300

    server : reuse cached context chunks (#9866)
    
    ggml-ci

commit 92be9f12164f18ce845a5bab60cefa5f7fec6836
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 13 06:11:26 2024 +0300

    flake.lock: Update (#9870)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/bc947f541ae55e999ffdb4013441347d83b00feb?narHash=sha256-NOiTvBbRLIOe5F6RbHaAh6%2B%2BBNjsb149fGZd1T4%2BKBg%3D' (2024-10-04)
      → 'github:NixOS/nixpkgs/5633bcff0c6162b9e4b5f1264264611e950c8ec7?narHash=sha256-9UTxR8eukdg%2BXZeHgxW5hQA9fIKHsKCdOIUycTryeVw%3D' (2024-10-09)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit edc265661cd707327297b6ec4d83423c43cb50a5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Oct 12 16:14:27 2024 +0300

    server : add option to time limit the generation phase (#9865)
    
    ggml-ci

commit 1bde94dd024b632f98428f4bf2ce483295130779
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Oct 12 16:06:31 2024 +0300

    server : remove self-extend features (#9860)
    
    * server : remove self-extend
    
    ggml-ci
    
    * server : fix context limit check to use slot.n_past
    
    ggml-ci

commit 95c76e8e92ecc93f784b185eafae36a0e7ad2fa3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Oct 12 14:51:54 2024 +0300

    server : remove legacy system_prompt feature (#9857)
    
    * server : remove legacy system_prompt feature
    
    ggml-ci
    
    * readme : update [no ci]
    
    * server : fix non-transformer logic + remove response from /props

commit 11ac9800aff532715a5bc7991062c68ba3472e6e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Oct 12 08:21:51 2024 +0300

    llama : improve infill support and special token detection (#9798)
    
    * llama : improve infill support
    
    ggml-ci
    
    * llama : add more FIM token strings
    
    ggml-ci
    
    * server : update prompt on slot restore (#9800)
    
    * gguf : deprecate old FIM token KVs

commit 943d20b4111c746bcd9dbc7e4771de313b08b50c
Author: R0CKSTAR <xiaodong.ye@mthreads.com>
Date:   Sat Oct 12 13:09:53 2024 +0800

    musa : update doc (#9856)
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>

commit 96776405a17034dcfd53d3ddf5d142d34bdbb657
Author: Diego Devesa <slarengh@gmail.com>
Date:   Fri Oct 11 15:34:45 2024 +0200

    ggml : move more prints to the ggml log system (#9839)
    
    * ggml : move more prints to the ggml log system
    
    * show BLAS OpenMP warnings in all builds using debug print

commit 7eee341bee09957139789c2d828995953f0fc7ff
Author: Diego Devesa <slarengh@gmail.com>
Date:   Thu Oct 10 22:57:42 2024 +0200

    common : use common_ prefix for common library functions (#9805)
    
    * common : use common_ prefix for common library functions
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 0e9f760eb12546704ef8fa72577bc1a3ffe1bc04
Author: Diego Devesa <slarengh@gmail.com>
Date:   Thu Oct 10 20:14:55 2024 +0200

    rpc : add backend registry / device interfaces (#9812)
    
    * rpc : add backend registry / device interfaces
    
    * llama : add llama_supports_rpc API
    
    * ggml_backend_rpc_start_rpc_server -> ggml_backend_rpc_start_server

commit cf8e0a3bb9c0e93e371773b282054cdbbb231038
Author: R0CKSTAR <xiaodong.ye@mthreads.com>
Date:   Fri Oct 11 02:10:37 2024 +0800

    musa: add docker image support (#9685)
    
    * mtgpu: add docker image support
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>
    
    * mtgpu: enable docker workflow
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>
    
    ---------
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>

commit c7499c557cc1efafaf0a6bc12963c39826299703
Author: Diego Devesa <slarengh@gmail.com>
Date:   Thu Oct 10 19:50:49 2024 +0200

    examples : do not use common library in simple example (#9803)
    
    * examples : do not use common library in simple example
    
    * add command line parser, simplify code

commit c81f3bbb051f8b736e117dfc78c99d7c4e0450f6
Author: Diego Devesa <slarengh@gmail.com>
Date:   Wed Oct 9 18:49:52 2024 +0200

    cmake : do not build common library by default when standalone (#9804)

commit e7022064ab637ccb5f37867196f1802c4a453c91
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 9 17:00:18 2024 +0300

    perplexity : fix integer overflow (#9783)
    
    * perplexity : fix integer overflow
    
    ggml-ci
    
    * perplexity : keep n_vocab as int and make appropriate casts
    
    ggml-ci

commit 3dc48fe75ad48f8856118520a267c96f74df8e90
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 9 10:55:42 2024 +0300

    examples : remove llama.vim
    
    An updated version will be added in #9787

commit dca1d4b58a7f1acf1bd253be84e50d6367f492fd
Author: Diego Devesa <slarengh@gmail.com>
Date:   Tue Oct 8 14:21:43 2024 +0200

    ggml : fix BLAS with unsupported types (#9775)
    
    * ggml : do not use BLAS with types without to_float
    
    * ggml : return pointer from ggml_internal_get_type_traits to avoid unnecessary copies
    
    * ggml : rename ggml_internal_get_type_traits -> ggml_get_type_traits
    
    it's not really internal if everybody uses it

commit 458367a90606448a9c0262b276947c9e536086e0
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Oct 8 13:27:04 2024 +0200

    server : better security control for public deployments (#9776)
    
    * server : more explicit endpoint access settings
    
    * protect /props endpoint
    
    * fix tests
    
    * update server docs
    
    * fix typo
    
    * fix tests

commit fa42aa6d8902cc4eaf31866b3b3b7b61b69da930
Author: standby24x7 <standby24x7@gmail.com>
Date:   Tue Oct 8 15:19:53 2024 +0900

    scripts : fix spelling typo in messages and comments (#9782)
    
    Signed-off-by: Masanari Iida <standby24x7@gmail.com>

commit 6374743747b14db4eb73ce82ae449a2978bc3b47
Author: Diego Devesa <slarengh@gmail.com>
Date:   Mon Oct 7 21:55:08 2024 +0200

    ggml : add backend registry / device interfaces to BLAS backend (#9752)
    
    * ggml : add backend registry / device interfaces to BLAS backend
    
    * fix mmap usage when using host buffers

commit f1af42fa8c925096407c61ff0a3d5d5d669cc535
Author: Andrew Minh Nguyen <40281306+amqdn@users.noreply.github.com>
Date:   Mon Oct 7 09:37:31 2024 -0700

    Update building for Android (#9672)
    
    * docs : clarify building Android on Termux
    
    * docs : update building Android on Termux
    
    * docs : add cross-compiling for Android
    
    * cmake : link dl explicitly for Android

commit 6279dac039ddeb6d5ebd125a6274fd3c37a77ba8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Oct 7 19:35:42 2024 +0300

    flake.lock: Update (#9753)
    
    Flake lock file updates:
    
    • Updated input 'flake-parts':
        'github:hercules-ci/flake-parts/bcef6817a8b2aa20a5a6dbb19b43e63c5bf8619a?narHash=sha256-HO4zgY0ekfwO5bX0QH/3kJ/h4KvUDFZg8YpkNwIbg1U%3D' (2024-09-12)
      → 'github:hercules-ci/flake-parts/3d04084d54bedc3d6b8b736c70ef449225c361b1?narHash=sha256-K5ZLCyfO/Zj9mPFldf3iwS6oZStJcU4tSpiXTMYaaL0%3D' (2024-10-01)
    • Updated input 'flake-parts/nixpkgs-lib':
        'https://github.com/NixOS/nixpkgs/archive/356624c12086a18f2ea2825fed34523d60ccc4e3.tar.gz?narHash=sha256-Ss8QWLXdr2JCBPcYChJhz4xJm%2Bh/xjl4G0c0XlP6a74%3D' (2024-09-01)
      → 'https://github.com/NixOS/nixpkgs/archive/fb192fec7cc7a4c26d51779e9bab07ce6fa5597a.tar.gz?narHash=sha256-0xHYkMkeLVQAMa7gvkddbPqpxph%2BhDzdu1XdGPJR%2BOs%3D' (2024-10-01)
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/1925c603f17fc89f4c8f6bf6f631a802ad85d784?narHash=sha256-J%2BPeFKSDV%2BpHL7ukkfpVzCOO7mBSrrpJ3svwBFABbhI%3D' (2024-09-26)
      → 'github:NixOS/nixpkgs/bc947f541ae55e999ffdb4013441347d83b00feb?narHash=sha256-NOiTvBbRLIOe5F6RbHaAh6%2B%2BBNjsb149fGZd1T4%2BKBg%3D' (2024-10-04)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit d5ac8cf2f2e30459489e6721a17d15b92a0c42a6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Oct 7 18:27:51 2024 +0300

    ggml : add metal backend registry / device (#9713)
    
    * ggml : add metal backend registry / device
    
    ggml-ci
    
    * metal : fix names [no ci]
    
    * metal : global registry and device instances
    
    ggml-ci
    
    * cont : alternative initialization of global objects
    
    ggml-ci
    
    * llama : adapt to backend changes
    
    ggml-ci
    
    * fixes
    
    * metal : fix indent
    
    * metal : fix build when MTLGPUFamilyApple3 is not available
    
    ggml-ci
    
    * fix merge
    
    * metal : avoid unnecessary singleton accesses
    
    ggml-ci
    
    * metal : minor fix [no ci]
    
    * metal : g_state -> g_ggml_ctx_dev_main [no ci]
    
    * metal : avoid reference of device context in the backend context
    
    ggml-ci
    
    * metal : minor [no ci]
    
    * metal : fix maxTransferRate check
    
    * metal : remove transfer rate stuff
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 96b69121033d2b6b951d1b6b1b43f8b4f97dac99
Author: Paul Tsochantaris <ptsochantaris@icloud.com>
Date:   Mon Oct 7 13:26:31 2024 +0100

    metal : single allocation of encode_async block (#9747)
    
    * Single allocation of encode_async block with non-ARC capture in ggml-metal.m
    
    * Moving Block_release to the deallocation code
    
    * Release encode block when re-setting encoding buffer count if needed
    
    * Update ggml/src/ggml-metal.m
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit d5cb86844f26f600c48bf3643738ea68138f961d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 6 14:15:27 2024 +0300

    contrib : simplify + minor edits [no ci]

commit f4b2dcdf4992ef11a854abc9b662624490e37b4c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 6 13:49:41 2024 +0300

    readme : fix typo [no ci]

commit b6d6c5289f1c9c677657c380591201ddb210b649
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 6 12:53:28 2024 +0300

    sync : llama.cpp

commit b0915d5b51cbaa982ce9bbb9ce302bb9abdca0eb
Author: SRHMorris <69468379+SRHMorris@users.noreply.github.com>
Date:   Sun Oct 6 08:34:20 2024 +0100

    vulkan : retry allocation with fallback flags (whisper/2451)
    
    Co-authored-by: Samuel Morris <samuel.morris@artlist.io>

commit 8c475b97b8ba7d678d4c9904b1161bd8811a9b44
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Oct 5 15:55:04 2024 +0300

    rerank : use [SEP] token instead of [BOS] (#9737)
    
    * rerank : use [SEP] token instead of [BOS]
    
    ggml-ci
    
    * common : sanity check for non-NULL tokens
    
    ggml-ci
    
    * ci : adjust rank score interval
    
    ggml-ci
    
    * ci : add shebang to run.sh
    
    ggml-ci

commit 58b16695e146628481c6b9b8a3b101c0c9bac00f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Oct 5 15:53:49 2024 +0300

    sync : ggml

commit 905f5485b279518d30b402565c23fb153f822c0d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Oct 5 14:33:54 2024 +0300

    metal : zero-init buffer contexts (whisper/0)

commit 71967c2a6d30da9f61580d3e2d4cb00e0223b6fa
Author: Viet-Anh NGUYEN (Andrew) <vietanh.dev@gmail.com>
Date:   Sat Oct 5 01:29:35 2024 +0700

    Add Llama Assistant (#9744)

commit 17880771ad7dca16cdc969062f2a56f779662835
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Oct 4 18:50:25 2024 +0300

    sync : ggml

commit 55951c018d7c107b6ef0a4c8561a6e68183d19d9
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Oct 4 15:46:18 2024 +0200

    ggml : fix typo in example usage ggml_gallocr_new (ggml/984)

commit ff565769f289c6adcc91ed1b8fdabaf9a0d4f6ee
Author: Diego Devesa <slarengh@gmail.com>
Date:   Fri Oct 4 08:41:40 2024 +0200

    ggml : fixes after sync (ggml/983)
    
    ggml : remove test-backend-buffer
    
    ggml : fix CUDA build warnings

commit f3fdcfaa79afa12047def3a8793d4a566e0532d4
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Oct 4 11:47:19 2024 +0200

    ci : fine-grant permission (#9710)

commit 133c7b46b3482f7c126c0c4ba14265f684138306
Author: Daniel Kleine <53251018+d-kleine@users.noreply.github.com>
Date:   Fri Oct 4 10:54:44 2024 +0200

    Fixed RNG seed docs (#9723)
    
    * Update README.md
    
    fixed RNG seed info
    
    * changed print format to unsigned

commit d5ed2b929d85bbd7dbeecb690880f07d9d7a6077
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Oct 3 21:18:19 2024 +0300

    metal : remove abort (skip) (ggml/0)

commit 1bb8a64ebfcbe599dacb4fc8069731b6cba0b5d6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Oct 3 21:17:49 2024 +0300

    sync : ggml

commit fabdc3bda396307565c4f3f4ecbc3a751a2eb6d3
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Oct 3 17:29:59 2024 +0200

    ggml/ex: calculate accuracy in graph, adapt MNIST (ggml/980)

commit eee39bdc96065b69242877fe8f1be05c885fc2aa
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Oct 2 15:32:39 2024 +0200

    ggml: refactor cross entropy loss CPU impl. (ggml/976)

commit 5d5ab1e5cca0d6d63701a1cb85dbe26cb57d2c4e
Author: Jack Mousseau <jack@software.inc>
Date:   Thu Oct 3 11:01:46 2024 -0700

    metal : fix compute pass descriptor autorelease crash (#9718)

commit a7ad553513a5d70b4ceacd36f64705cf3654dc97
Author: Diego Devesa <slarengh@gmail.com>
Date:   Thu Oct 3 17:39:18 2024 +0200

    ggml-backend : add device description to CPU backend (#9720)

commit d6fe7abf04e8ec5240dead6e2773ed1b7e7495d3
Author: bandoti <141645996+bandoti@users.noreply.github.com>
Date:   Thu Oct 3 12:39:03 2024 -0300

    ggml: unify backend logging mechanism (#9709)
    
    * Add scaffolding for ggml logging macros
    
    * Metal backend now uses GGML logging
    
    * Cuda backend now uses GGML logging
    
    * Cann backend now uses GGML logging
    
    * Add enum tag to parameters
    
    * Use C memory allocation funcs
    
    * Fix compile error
    
    * Use GGML_LOG instead of GGML_PRINT
    
    * Rename llama_state to llama_logger_state
    
    * Prevent null format string
    
    * Fix whitespace
    
    * Remove log callbacks from ggml backends
    
    * Remove cuda log statement

commit e3c355ba654d4164c1c09e5d0dcacecb8b214af8
Author: compilade <git@compilade.net>
Date:   Thu Oct 3 10:22:15 2024 -0400

    convert : handle tokenizer merges format from transformers 4.45 (#9696)

commit 841713e1e487bdb82fd106a52ad998c5f87b59e9
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Thu Oct 3 13:00:52 2024 +0300

    rpc : enable vulkan (#9714)
    
    closes #8536

commit 5639971466ed74386a1811938022f0c333007b55
Author: Ouadie EL FAROUKI <ouadie.elfarouki@codeplay.com>
Date:   Thu Oct 3 07:50:44 2024 +0100

    Fixed dequant precision issues in Q4_1 and Q5_1 (#9711)

commit c83ad6d01e7b89ec71080d97c7e5db7ac1f4fda6
Author: Diego Devesa <slarengh@gmail.com>
Date:   Thu Oct 3 01:49:47 2024 +0200

    ggml-backend : add device and backend reg interfaces (#9707)
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

commit a39ab216aa624308fda7fa84439c6b61dc98b87a
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed Oct 2 15:49:55 2024 +0200

    llama : reduce compile time and binary size (#9712)
    
    * llama : speed up compile time
    
    * fix build
    
    * fix build (2)

commit f536f4c4391bec74c432a924625c04e8c484d3ee
Author: Alberto Cabrera Pérez <alberto.cabrera@codeplay.com>
Date:   Wed Oct 2 13:57:18 2024 +0100

    [SYCL] Initial cmake support of SYCL for AMD GPUs (#9658)
    
    sycl: initial cmake support of SYCL for AMD GPUs

commit 00b7317e636ffdc7dae59cb51dbd1cda357a3168
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Wed Oct 2 13:49:16 2024 +0300

    vulkan : do not use tensor->extra (#9407)
    
    * vulkan : do not use tensor->extra
    
    This patch allows using the Vulkan backend with the RPC backend as
    tensor->extra is no longer used.
    
    Ref: #8536
    
    * Adapt GGML_VULKAN_CHECK_RESULTS to extra removal (#2)
    
    ---------
    
    Co-authored-by: 0cc4m <picard12@live.de>

commit 76b37d1541a880b9645557c8715a343fd074cc5c
Author: Zhenwei Jin <109658203+kylo5aby@users.noreply.github.com>
Date:   Wed Oct 2 15:21:57 2024 +0800

    gguf-split : improve --split and --merge logic (#9619)
    
    * make sure params --split and --merge are not specified at same time
    
    * update gguf-split params parse logic
    
    * Update examples/gguf-split/gguf-split.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>

commit 148844fe97fff4c1563a3111bf238ba4dd22ef56
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 2 10:14:44 2024 +0300

    examples : remove benchmark (#9704)
    
    ggml-ci

commit 3f1ae2e32cde00c39b96be6d01c2997c29bae555
Author: Paweł Wodnicki <151604+32bitmicro@users.noreply.github.com>
Date:   Tue Oct 1 12:18:46 2024 -0500

    Update README.md (#9591)
    
    Add Bielik model.

commit f1b8c4271125c2bfb9cfebd72a8b9e9f99061a30
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 1 16:09:42 2024 +0300

    sync : ggml

commit e98c1c188ebbeb55d0cd6389ad03f0cbf995b451
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Sep 30 09:55:23 2024 +0200

    test: fix OPT_STEP_ADAMW for test-backend-ops (ggml/974)

commit cb00020504416606601f8cb35f55ee07b710b4b7
Author: Salvatore Mesoraca <s.mesoraca16@gmail.com>
Date:   Mon Sep 30 09:14:09 2024 +0200

    vulkan : mul_mat: fix UB with small warps (ggml/952)
    
    When the device's warp size is less than 16,
    it is possible for loadstride_a (mul_mm.comp:114)
    and loadstride_b (mul_mm.comp:115) to be set to 0.
    Because they are calculated as: the workgroup size,
    multiplied by LOAD_VEC_* (which can be 1) and divided by 16.
    And the workgroup size is set to be the same as the
    warp/subgroup size.
    
    The loadstride_* variables are used as increments in the
    loops that populate the buffers used for the multiplication.
    
    When they are 0 they cause an infinite loop.
    But infinite loops without side-effects are UB and the
    values of loadstride_* are known at compile time.
    So, the compiler quietly optimizes all the loops away.
    As a consequence, the buffers are not populated and
    the multiplication result is just a matrix with all elements
    set to 0.
    
    We prevent the UB by making sure that the workgroup size
    will never be less than 16, even if our device has a
    smaller warp size (e.g. 8).
    
    Signed-off-by: Salvatore Mesoraca <s.mesoraca16@gmail.com>

commit 6c5322481a75f1be54e58a000c3f78484d07f948
Author: Borislav Stanimirov <b.stanimirov@abv.bg>
Date:   Mon Sep 30 10:11:41 2024 +0300

    ggml : fix ggml_cast (ggml/973)

commit 7254cdf7e8d6312840509ca8d766358c687c6266
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Sep 29 23:18:02 2024 +0200

    ggml: fix gradient allocation logic (ggml/966)
    
    * ggml: fix gradient allocation logic
    
    * gradient allocation in ggml_build_backward_expand
    
    * fixup
    
    * fix test-backend-ops grad
    
    * suggestions by slaren
    
    * fix test1.c
    
    * fix legacy opt API
    
    * fix test-grad0
    
    * remove keep arg

commit cad341d88924788b940997c55e7bef000c99e784
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 1 16:00:25 2024 +0300

    metal : reduce command encoding overhead (#9698)
    
    * metal : reduce command encoding overhead
    
    ggml-ci
    
    * metal : add comments

commit a90484c6d9db699bf739d0f33daf1c50cbdd45c9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 1 11:42:01 2024 +0300

    llama : print correct model type for Llama 3.2 1B and 3B

commit 1927378bcce20ba72b6c89d5977b854a4bcaeb5d
Author: compilade <git@compilade.net>
Date:   Tue Oct 1 02:31:36 2024 -0400

    convert : refactor rope_freqs generation (#9396)
    
    * convert : refactor rope_freqs generation
    
    This should also fix vocab-only conversion for Phi-3.
    
    * convert : adapt MiniCPM3 to separate rope_freqs insertion
    
    MiniCPM3's tokenizer is treated as a SentencePiece tokenizer to avoid
    having to run its custom Python code which mixes tokenization
    in the same file as tool calls.
    
    gguf-py : add long and short RoPE factors to tensor mappings
    
    Empty, but the key names are used to populate the mappings.

commit 6f1d9d71f4c568778a7637ff6582e6f6ba5fb9d3
Author: serhii-nakon <57632032+serhii-nakon@users.noreply.github.com>
Date:   Mon Sep 30 21:57:12 2024 +0300

    Fix Docker ROCM builds, use AMDGPU_TARGETS instead of GPU_TARGETS (#9641)
    
    * Fix Docker ROCM builds, use AMDGPU_TARGETS instead of GPU_TARGETS
    
    * Set ROCM_DOCKER_ARCH as string due it incorrectly build and cause OOM exit code

commit 511636df0c90826b4dd1fc21ff260c19d69a3b5d
Author: compilade <git@compilade.net>
Date:   Mon Sep 30 14:13:16 2024 -0400

    ci : reduce severity of unused Pyright ignore comments (#9697)

commit 08a43d05b6ba74de97610ae519450ad9996475e0
Author: vb <vaibhavs10@gmail.com>
Date:   Mon Sep 30 17:03:47 2024 +0200

    py : update transfomers version (#9694)
    
    * update transfomers version.
    
    * update hfh version.

commit ace4f4be37abed4801fbd54a94cf38a7ae462416
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Sep 30 17:48:49 2024 +0300

    flake.lock: Update (#9680)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/c04d5652cfa9742b1d519688f65d1bbccea9eb7e?narHash=sha256-PmUr/2GQGvFTIJ6/Tvsins7Q43KTMvMFhvG6oaYK%2BWk%3D' (2024-09-19)
      → 'github:NixOS/nixpkgs/1925c603f17fc89f4c8f6bf6f631a802ad85d784?narHash=sha256-J%2BPeFKSDV%2BpHL7ukkfpVzCOO7mBSrrpJ3svwBFABbhI%3D' (2024-09-26)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit 8277a817f18967581b02b2248989d773e8e99998
Author: Ruchira Hasaranga <ruchira66@gmail.com>
Date:   Mon Sep 30 13:53:42 2024 +0530

    console : utf-8 fix for windows stdin (#9690)
    
    * utf-8 fix for windows stdin
    
    * Update common/console.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit c919d5db39c8a7fcb64737f008e4b105ee0acd20
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Sep 29 21:18:23 2024 +0300

    ggml : define missing HWCAP flags (#9684)
    
    ggml-ci
    
    Co-authored-by: Willy Tarreau <w@1wt.eu>

commit d0b1d663e430354ab35853a6e1bce51cc8819376
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Sep 29 21:16:07 2024 +0300

    sync : ggml

commit aaa40999251f5d18309b3fddc5a7d576f5fdb4e1
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Sep 29 19:56:17 2024 +0200

    CUDA: remove bad assert (ggml/972)

commit 641002fba8d2a0c0269027e23d2ef58e90546028
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Sun Sep 29 11:50:17 2024 -0500

    vulkan : multithread pipeline creation (ggml/963)

commit 0de8b203f1d31cf5ee0d2a3560a0ad78d44f4d4c
Author: Jeff Bolz <jbolz@nvidia.com>
Date:   Fri Sep 27 02:58:01 2024 -0500

    vulkan : fix build for GGML_VULKAN_RUN_TESTS, add TFLOPS to log (ggml/961)

commit 544f409b4bd8fc98a3e87820f0ac934e00402de7
Author: Salvatore Mesoraca <s.mesoraca16@gmail.com>
Date:   Thu Sep 26 08:59:42 2024 +0200

    vulkan : argsort barriers must be under uniform control flow (ggml/951)
    
    a return before a barrier (that happens only in some threads in
    a workgroup) leads to UB.
    While the old code actually works on some devices,
    it fails on some others (i.e. "smaller" GPUs).
    
    BTW, I think it would be better to set specialization constants
    when the graph is built, in that way the local workgroup
    could be sized appropriately.
    But it would take a lot of work.
    
    Signed-off-by: Salvatore Mesoraca <s.mesoraca16@gmail.com>

commit 6084bfb261b03f812de2255b05b6b5bb8d1c7171
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Sep 24 13:23:59 2024 +0300

    ggml : fix GGML_MAX_N_THREADS + improve formatting (ggml/969)

commit faac0bae265449fd988c57bf894018edc36fbe1e
Author: matiaslin <45382001+matiaslin@users.noreply.github.com>
Date:   Sun Sep 29 05:25:00 2024 -0700

    common : ensure llama_batch size does not exceed max size (#9668)
    
    A crash was observed when the number of tokens added to a batch exceeds
    llama_batch size. An assertion in llama_batch_add was added to protect
    against llama_batch size overflow.

commit f99d3f8367174f7aba73c07fd87de687d4a0ece1
Author: nopperl <54780682+nopperl@users.noreply.github.com>
Date:   Sun Sep 29 12:02:06 2024 +0000

    py : add model class for Chameleon conversion (#9683)

commit 589b48d41efb0e95133b77c335f4fb9779af9bfb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Sep 29 14:38:18 2024 +0300

    contrib : add Resources section (#9675)

commit f4d2b8846a6b34419ff9e9491aee6cd95e444bfc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Sep 28 17:42:03 2024 +0300

    llama : add reranking support (#9510)
    
    * py : add XLMRobertaForSequenceClassification [no ci]
    
    * py : fix scalar-tensor conversion [no ci]
    
    * py : fix position embeddings chop [no ci]
    
    * llama : read new cls tensors [no ci]
    
    * llama : add classigication head (wip) [no ci]
    
    * llama : add "rank" pooling type
    
    ggml-ci
    
    * server : add rerank endpoint
    
    ggml-ci
    
    * llama : aboud ggml_repeat during classification
    
    * rerank : cleanup + comments
    
    * server : accept /rerank endpoint in addition to /v1/rerank [no ci]
    
    * embedding : parse special tokens
    
    * jina : support v1 reranker
    
    * vocab : minor style
    
    ggml-ci
    
    * server : initiate tests for later
    
    ggml-ci
    
    * server : add docs
    
    * llama : add comment [no ci]
    
    * llama : fix uninitialized tensors
    
    * ci : add rerank tests
    
    ggml-ci
    
    * add reranking test
    
    * change test data
    
    * Update examples/server/server.cpp
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    * add `--reranking` argument
    
    * update server docs
    
    * llama : fix comment [no ci]
    
    ggml-ci
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <son@huggingface.co>
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>

commit 1b2f992cd2cff0b69e5abe78bb8888d51ed19d67
Author: slaren <slarengh@gmail.com>
Date:   Sat Sep 28 14:32:46 2024 +0200

    test-backend-ops : use flops for some performance tests (#9657)
    
    * test-backend-ops : use flops for some performance tests
    
    - parallelize tensor quantization
    
    - use a different set of cases for performance and correctness tests
    
    - run each test for at least one second

commit 739842703e32cd43443c45e0b4f6647cc4e6b3d6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Sep 28 15:13:21 2024 +0300

    llama : add comment about thread-safety [no ci] (#9449)

commit 6102037bbb55521880ae78a6ee6c2a0c00c901df
Author: Zhenwei Jin <109658203+kylo5aby@users.noreply.github.com>
Date:   Sat Sep 28 20:10:58 2024 +0800

    vocab : refactor tokenizer to reduce init overhead (#9449)
    
    * refactor tokenizer
    
    * llama : make llm_tokenizer more private
    
    ggml-ci
    
    * refactor tokenizer
    
    * refactor tokenizer
    
    * llama : make llm_tokenizer more private
    
    ggml-ci
    
    * remove unused files
    
    * remove unused fileds to avoid unused filed build error
    
    * avoid symbol link error
    
    * Update src/llama.cpp
    
    * Update src/llama.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 9a913110cf471a8287ac06c43cbe307d3cf6df99
Author: nopperl <54780682+nopperl@users.noreply.github.com>
Date:   Sat Sep 28 12:08:43 2024 +0000

    llama : add support for Chameleon (#8543)
    
    * convert chameleon hf to gguf
    
    * add chameleon tokenizer tests
    
    * fix lint
    
    * implement chameleon graph
    
    * add swin norm param
    
    * return qk norm weights and biases to original format
    
    * implement swin norm
    
    * suppress image token output
    
    * rem tabs
    
    * add comment to conversion
    
    * fix ci
    
    * check for k norm separately
    
    * adapt to new lora implementation
    
    * fix layer input for swin norm
    
    * move swin_norm in gguf writer
    
    * add comment regarding special token regex in chameleon pre-tokenizer
    
    * Update src/llama.cpp
    
    Co-authored-by: compilade <git@compilade.net>
    
    * fix punctuation regex in chameleon pre-tokenizer (@compilade)
    
    Co-authored-by: compilade <git@compilade.net>
    
    * fix lint
    
    * trigger ci
    
    ---------
    
    Co-authored-by: compilade <git@compilade.net>

commit 43bcdd9703ec19af7d2a519640b5ed6f4aac3d53
Author: Aarni Koskela <akx@iki.fi>
Date:   Sat Sep 28 15:07:14 2024 +0300

    readme : add tool (#9655)

commit 6a0f7794847244fb3b99a983e03137d7e832b585
Author: Dan Johansson <164997844+eddnjjn@users.noreply.github.com>
Date:   Sat Sep 28 14:06:16 2024 +0200

    ggml : add run-time detection of neon, i8mm and sve (#9331)
    
    * ggml: Added run-time detection of neon, i8mm and sve
    
    Adds run-time detection of the Arm instructions set features
    neon, i8mm and sve for Linux and Apple build targets.
    
    * ggml: Extend feature detection to include non aarch64 Arm arch
    
    * ggml: Move definition of ggml_arm_arch_features to the global data section

commit 89f9944981010d195e411a9fbfbb19959412f710
Author: Markus Tavenrath <mtavenrath@users.noreply.github.com>
Date:   Sat Sep 28 12:05:05 2024 +0200

    Enable use to the rebar feature to upload buffers to the device. (#9251)

commit b5de3b74a595cbfefab7eeb5a567425c6a9690cf
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 27 20:57:51 2024 +0300

    readme : update hot topics

commit 44f59b4301c51f071daa2e951301bb17c14acc9b
Author: Borislav Stanimirov <b.stanimirov@abv.bg>
Date:   Fri Sep 27 10:42:06 2024 +0300

    cmake : add option for common library (#9661)

commit 95bc82fbc0df6d48cf66c857a4dda3d044f45ca2
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Thu Sep 26 17:38:31 2024 +0800

    [SYCL] add missed dll file in package (#9577)
    
    * update oneapi to 2024.2
    
    * use 2024.1
    
    ---------
    
    Co-authored-by: arthw <14088817+arthw@users.noreply.github.com>

commit 7691654c68aa5316108f881136c59f815ccb6809
Author: R0CKSTAR <xiaodong.ye@mthreads.com>
Date:   Thu Sep 26 09:27:40 2024 +0800

    mtgpu: enable VMM (#9597)
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>

commit ea9c32be71b91b42ecc538bd902e93cbb5fb36cb
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed Sep 25 17:26:01 2024 +0200

    ci : fix docker build number and tag name (#9638)
    
    * ci : fix docker build number and tag name
    
    * fine-grant permissions

commit 1e436302188a704ac9ea044af03193648806f19c
Author: Charles Xu <63788048+chaxu01@users.noreply.github.com>
Date:   Wed Sep 25 15:12:20 2024 +0200

    ggml : remove assert for AArch64 GEMV and GEMM Q4 kernels (#9217)
    
    * ggml : remove assert for AArch64 GEMV and GEMM Q4 kernels
    
    * added fallback mechanism when the offline re-quantized model is not
    optimized for the underlying target.
    
    * fix for build errors
    
    * remove prints from the low-level code
    
    * Rebase to the latest upstream

commit afbbfaa537a96f562c34df4542930fa951b40d9e
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed Sep 25 14:05:13 2024 +0200

    server : add more env vars, improve gen-docs (#9635)
    
    * server : add more env vars, improve gen-docs
    
    * update server docs
    
    * LLAMA_ARG_NO_CONTEXT_SHIFT

commit 3d6bf6919f7b10726421779cd344f2da05421c68
Author: Gabe Goodhart <ghart@us.ibm.com>
Date:   Wed Sep 25 01:06:52 2024 -0600

    llama : add IBM Granite MoE architecture (#9438)
    
    * feat(gguf-py): Add granitemoe architecture
    
    This includes the addition of new tensor names for the new moe layers.
    These may not be correct at this point due to the need for the hack in
    gguf_writer.py to double-check the length of the shape for these layers.
    
    Branch: GraniteMoE
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * feat(convert_hf_to_gguf): Add GraniteMoeModel
    
    GraniteMoe has the same configuration deltas as Granite
    
    Branch: GraniteMoE
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * fix(granitemoe convert): Split the double-sized input layer into gate and up
    
    After a lot of staring and squinting, it's clear that the standard mixtral
    expert implementation is equivalent to the vectorized parallel experts in
    granite. The difference is that in granite, the w1 and w3 are concatenated
    into a single tensor "input_linear." Rather than reimplementing all of the
    math on the llama.cpp side, the much simpler route is to just split this
    tensor during conversion and follow the standard mixtral route.
    
    Branch: GraniteMoE
    
    Co-Authored-By: alex.brooks@ibm.com
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * feat(granitemoe): Implement granitemoe
    
    GraniteMoE follows the mixtral architecture (once the input_linear layers
    are split into gate_exps/up_exps). The main delta is the addition of the
    same four multipliers used in Granite.
    
    Branch: GraniteMoE
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * Typo fix in docstring
    
    Co-Authored-By: ggerganov@gmail.com
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * fix(conversion): Simplify tensor name mapping in conversion
    
    Branch: GraniteMoE
    
    Co-Authored-By: git@compilade.net
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * fix(convert): Remove unused tensor name mappings
    
    Branch: GraniteMoE
    
    Co-Authored-By: git@compilade.net
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * fix(convert): Sanity check on merged FFN tensor sizes
    
    Branch: GraniteMoE
    
    Co-Authored-By: git@compilade.net
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * fix: Allow "output" layer in granite moe architecture (convert and cpp)
    
    Branch: GraniteMoE
    
    Co-Authored-By: git@compilade.net
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * fix(granite): Add missing 'output' tensor for Granite
    
    This is a fix for the previous `granite` architecture PR. Recent snapshots
    have included this (`lm_head.weights`) as part of the architecture
    
    Branch: GraniteMoE
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    ---------
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 904837e0cb2f5f01bf5d5901b7aa57a026860ae4
Author: Dou Xinpeng <15529241576@163.com>
Date:   Wed Sep 25 11:30:38 2024 +0800

    cann: fix crash when llama-bench is running on multiple cann devices (#9627)

commit 70392f1f81470607ba3afef04aa56c9f65587664
Author: Eric Zhang <34133756+EZForever@users.noreply.github.com>
Date:   Tue Sep 24 16:03:21 2024 +0800

    ggml : add AVX512DQ requirement for AVX512 builds (#9622)

commit bb5f8199754b5f055cf436711d14c58e7be28e12
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Sep 24 11:01:18 2024 +0300

    sync : ggml

commit c038931615d2525d732943fa8661e29aa361b192
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 20 21:50:16 2024 +0300

    examples : adapt to ggml.h changes (ggml/0)
    
    ggml-ci

commit 31ac5834fe75c296476658a124c06c84772aa641
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Sep 24 10:16:06 2024 +0300

    llama : keep track of all EOG tokens in the vocab (#9609)
    
    ggml-ci

commit cea1486ecf34a1c7e014a9e290eb458f5a11f876
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Sep 24 10:15:35 2024 +0300

    log : add CONT level for continuing previous log entry (#9610)

commit 0aa15011e315659640504731d1d05663837130fa
Author: StrangeBytesDev <141275258+StrangeBytesDev@users.noreply.github.com>
Date:   Mon Sep 23 23:04:39 2024 -0700

    server : add newline after chat example (#9616)

commit b0f27361f3539a81d983a8b045f3c61e682d9fc0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Sep 24 09:03:17 2024 +0300

    sampling : avoid expensive softmax during greedy sampling (#9605)
    
    * sampling : avoid expensive softmax during greedy sampling
    
    ggml-ci
    
    * speculative : fix default RNG seed + set sparams.n_probs
    
    * Update tests/test-sampling.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * sampling : add clarifying comment [no ci]
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit c087b6f11d3385f4293b6841ebfb755063479490
Author: Max Krasnyansky <quic_maxk@quicinc.com>
Date:   Mon Sep 23 21:18:48 2024 -0700

    threads: fix msvc build without openmp (#9615)
    
    We're missing atomic_thread_fence() in MSVC builds when openmp is disabled.

commit 116efee0eef09d8c3c4c60b52fa01b56ddeb432c
Author: Ivan <nekotekina@gmail.com>
Date:   Tue Sep 24 03:14:24 2024 +0300

    cuda: add q8_0->f32 cpy operation (#9571)
    
    llama: enable K-shift for quantized KV cache
    It will fail on unsupported backends or quant types.

commit 0b3bf966f47bf2ba88e5d4e3ed429602008c7e63
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Sep 23 22:23:54 2024 +0200

    server : add --no-context-shift option (#9607)
    
    * server : add --no-context-shift option
    
    * small fix
    
    * Update examples/server/tests/features/embeddings.feature
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * tests : minor fix
    
    * revert usage of GGML_ASSERT
    
    * update server documentation
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit f0c7b5edf82aa200656fd88c11ae3a805d7130bf
Author: Max Krasnyansky <quic_maxk@quicinc.com>
Date:   Mon Sep 23 11:42:43 2024 -0700

    threads: improve ggml_barrier scaling with large number of threads (#9598)
    
    Make sure n_barrier and n_barrier_passed do not share the cache line to avoid cache line bouncing.
    This optimization shows performance improvements even for n_threads <= 8 cases.
    
    Resurect TSAN (Thread Sanitizer) check so that we can avoid doing expensive read-modify-write
    in the normal case and just use thread-fence as originally intended.
    
    ---
    Here is the original description and suggestions from Willy Tarreau :
    
    There's currently some false sharing between n_barrier and
    n_barrier_passed that is amplified in ggml_barrier() by the fact that
    all threads need to increment n_barrier when entering, while all
    previous threads continue to read n_barrier_passed, waiting for the last
    one to release them all. The side effect is that all these readers are
    slowing down all new threads by making the cache line bounce back and
    forth between readers and writers.
    
    Just placing them in two distinct cache lines is sufficient to boost
    the performance by 21% on a 80-core ARM server compared to the
    no-openmp version, and by 3% compared to the openmp version.
    
    Note that the variables could have been spread apart in the structure
    as well, but it doesn't seem that the size of this threadpool struct is
    critical so here we're simply aligning them.
    
    Finally, the same issue was present when leaving the barrier since all
    threads had to update the n_barrier_passed counter, though only one
    would add a non-zero value. This alone is responsible for half of the
    cost due to undesired serialization.
    
    It might be possible that using a small array of n_barrier counters
    could make things even faster on many-core systems, but it would likely
    complicate the logic needed to detect the last thread.
    
    Co-authored-by: Willy Tarreau <w@1wt.eu>

commit 1d48e98e4f3316bd2f6b187d288c7b6cb88d5cb3
Author: Riceball LEE <snowyu.lee@gmail.com>
Date:   Mon Sep 23 23:58:17 2024 +0800

    readme : add programmable prompt engine language CLI (#9599)

commit f3979df762b75a2b1c0f622a2cd15d1bc60f037f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Sep 23 18:43:40 2024 +0300

    flake.lock: Update (#9586)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/4f807e8940284ad7925ebd0a0993d2a1791acb2f?narHash=sha256-IiA3jfbR7K/B5%2B9byVi9BZGWTD4VSbWe8VLpp9B/iYk%3D' (2024-09-11)
      → 'github:NixOS/nixpkgs/c04d5652cfa9742b1d519688f65d1bbccea9eb7e?narHash=sha256-PmUr/2GQGvFTIJ6/Tvsins7Q43KTMvMFhvG6oaYK%2BWk%3D' (2024-09-19)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit 1e7b9299c6ccb5bbc55d3db7cfa9b51f3ab09b59
Author: Srihari-mcw <96763064+Srihari-mcw@users.noreply.github.com>
Date:   Mon Sep 23 19:36:38 2024 +0530

    ggml : AVX512 gemm for Q4_0_8_8 (#9532)
    
    * AVX512 version of ggml_gemm_q4_0_8x8_q8_0
    
    * Remove zero vector parameter passing
    
    * Rename functions and rearrange order of macros
    
    * Edit commments
    
    * style : minor adjustments
    
    * Update x to start from 0
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 37f8c7b4c97784496cfd91040d55fa22f50b1d57
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Sep 23 11:28:02 2024 +0300

    perplexity : remove extra new lines after chunks (#9596)

commit bf9c1013ac40e5f1bd8e60b6d8bf16e0e8401445
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Sep 23 11:27:47 2024 +0300

    metal : use F32 prec for K*Q in vec FA (#9595)
    
    ggml-ci

commit e62e9789cda3bf5573a747e55ec2a7ee32908f56
Author: Akarshan Biswas <akarshanbiswas@fedoraproject.org>
Date:   Mon Sep 23 08:58:06 2024 +0530

    Revert "[SYCL] fallback mmvq (#9088)" (#9579)
    
    This reverts commit 50addec9a532a6518146ab837a85504850627316.

commit c35e586ea57221844442c65a1172498c54971cb0
Author: R0CKSTAR <xiaodong.ye@mthreads.com>
Date:   Sun Sep 22 22:55:49 2024 +0800

    musa: enable building fat binaries, enable unified memory, and disable Flash Attention on QY1 (MTT S80) (#9526)
    
    * mtgpu: add mp_21 support
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>
    
    * mtgpu: disable flash attention on qy1 (MTT S80); disable q3_k and mul_mat_batched_cublas
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>
    
    * mtgpu: enable unified memory
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>
    
    * mtgpu: map cublasOperation_t to mublasOperation_t (sync code to latest)
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>
    
    ---------
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>

commit 912c331d3dba3a079815844208dc36164baa8cc7
Author: Molly Sophia <mollysophia379@gmail.com>
Date:   Sun Sep 22 21:26:50 2024 +0800

    Fix merge error in #9454 (#9589)
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>

commit a5b57b08ce1998f7046df75324e86b9e2561c7af
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Sep 22 09:34:52 2024 +0200

    CUDA: enable Gemma FA for HIP/Pascal (#9581)

commit ecd5d6b65be08927e62de1587d5fd22778cdc250
Author: Shankar <gshankar.87@gmail.com>
Date:   Sat Sep 21 19:30:34 2024 -0700

    llama: remove redundant loop when constructing ubatch (#9574)

commit 2a63caaa69fad6c2afddc47bae052cf2afb01529
Author: Molly Sophia <mollysophia379@gmail.com>
Date:   Sun Sep 22 10:29:12 2024 +0800

    RWKV v6: RWKV_WKV op CUDA implementation (#9454)
    
    * ggml: CUDA unary op EXP
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * ggml: rwkv_wkv op CUDA impl
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    ---------
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>

commit d09770cae71b416c032ec143dda530f7413c4038
Author: slaren <slarengh@gmail.com>
Date:   Sat Sep 21 14:24:23 2024 +0200

    ggml-alloc : fix list of allocated tensors with GGML_ALLOCATOR_DEBUG (#9573)

commit 41f477879fd5ccc31211634292e8e293ec700e85
Author: agray3 <agray3@users.noreply.github.com>
Date:   Sat Sep 21 01:41:07 2024 +0100

    Update CUDA graph on scale change plus clear nodes/params  (#9550)
    
    * Avoid using saved CUDA graph if scale changes and reset nodes/params on update
    
    Fixes https://github.com/ggerganov/llama.cpp/issues/9451
    
    * clear before resize

commit e948a7da7af7f2dbfdcd695b3ba903ab12575f78
Author: Huang Qi <huangqi3@xiaomi.com>
Date:   Sat Sep 21 08:39:41 2024 +0800

    CI: Provide prebuilt windows binary for hip (#9467)

commit 63351143b2ea5efe9f8b9c61f553af8a51f1deff
Author: slaren <slarengh@gmail.com>
Date:   Fri Sep 20 20:55:36 2024 +0200

    quantize : improve type name parsing (#9570)
    
    quantize : do not ignore invalid types in arg parsing
    
    quantize : ignore case of type and ftype arguments

commit d13edb17ed1ce3b961016cbdb616b1c8d161c026
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 20 20:12:52 2024 +0300

    ggml : fix builds (#0)
    
    ggml-ci

commit 27609c49b94766a136764ce7919c8a082bf71548
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 20 19:13:02 2024 +0300

    ggml : fix trailing whitespace (#0)
    
    ggml-ci

commit 43015353262de835215103475055b74045cb9f49
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 20 19:06:59 2024 +0300

    sync : ggml
    
    ggml-ci

commit 424c5d00a9b97dd5559635872db9b57f87c23b02
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Sep 20 19:04:44 2024 +0300

    ggml/examples: add backend support for numerical optimization (ggml/949)
    
    * CUDA eval works
    
    * stochastic gradient descent op
    
    * Adam except decay
    
    * CUDA CROSS_ENTROPY_LOSS_BACK
    
    * CUDA mnist-fc training works
    
    * backend CLI arg
    
    * refactor gguf load
    
    * remove sched from opt_step_adam
    
    * implement l1 regularization (weight decay)
    
    * extra call to add optimizer
    
    * initialize gradients with ggml_graph_reset
    
    * gradient accumulation
    
    * increment iter per eval instead of epoch
    
    * adjust backend interfaces
    
    * fix ggml_graph_reset without backend
    
    * fix ggml graph export/import
    
    * fixup
    
    * rename
    
    * revert ggml_opt changes
    
    * more general CUDA repeat_back
    
    * update documentation, fix CNN
    
    * validation split
    
    * add clarifying comment
    
    * optimize PyTorch training
    
    * adjust buffer size, thread count
    
    * fix 0.0f validation split
    
    * Update examples/mnist/mnist-common.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * fix gradient accumulation
    
    * tensor flag for accumulators -> tensor hash set
    
    * Update include/ggml.h
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Update tests/test-backend-ops.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Update tests/test-backend-ops.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * fix test prints
    
    * Update src/ggml-backend.c
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * better CUDA support for noncontiguous out_prod
    
    * add comment
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>

commit a6809c6a2e8163cba296d4cc0c5cd4bbc8073638
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Sep 8 11:10:43 2024 +0300

    examples : add null threadpool args where needed (ggml/0)
    
    ggml-ci

commit 5cb12f68395a5ec00b357e408883ce124a11dcdb
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Sep 20 18:35:35 2024 +0200

    CUDA: fix sum.cu compilation for CUDA < 11.7 (#9562)

commit d39e26741f9f02340651dbc640c9776e1a1128ef
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 20 11:46:56 2024 +0300

    examples : flush log upon ctrl+c (#9559)

commit 722ec1eb51ed53be2ab1ef31c6a1da8261803c71
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Fri Sep 20 08:38:10 2024 +0200

    perplexity : do not escape input data by default (#9548)

commit 6026da52d6942b253df835070619775d849d0258
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Sep 19 12:44:53 2024 +0300

    server : clean-up completed tasks from waiting list (#9531)
    
    ggml-ci

commit eca0fab44eb449ed34e17a9b651ae7398b494117
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Thu Sep 19 09:58:14 2024 +0200

    imatrix : disable prompt escape by default (#9543)

commit 64c6af3195c3cd4aa3328a1282d29cd2635c34c9
Author: slaren <slarengh@gmail.com>
Date:   Wed Sep 18 19:13:08 2024 +0200

    ggml : fix n_threads_cur initialization with one thread (#9538)
    
    * ggml : fix n_threads_cur initialization with one thread
    
    * Update ggml/src/ggml.c
    
    ---------
    
    Co-authored-by: Max Krasnyansky <quic_maxk@quicinc.com>

commit 0d2f22e45c3c3b6f8222acb6284d0c8c93443ba1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Sep 18 18:34:32 2024 +0300

    scripts : verify py deps at the start of compare (#9520)

commit 6443ddd98576a9da904ef9f07df4e4398bb6a01a
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed Sep 18 13:42:36 2024 +0200

    llama : use reserve/emplace_back in sampler_sample (#9534)
    
    This commit updates the llama_sampler_sample function to use reserve and
    emplace_back for the vector of llama_token_data structs.
    
    The motivation for this change is to avoid the creation of n_vocab
    default-constructed llama_token_data structs which are then
    immediately overwritten.

commit 8a308354f6520df6bea851b435bd8054ee5617b4
Author: Vinesh Janarthanan <36610342+VJHack@users.noreply.github.com>
Date:   Wed Sep 18 01:50:34 2024 -0500

    server : match OAI structured output response (#9527)

commit f799155ab8279cfa668086ce584aa52ecc05f5e5
Author: Eric Zhang <34133756+EZForever@users.noreply.github.com>
Date:   Wed Sep 18 14:28:20 2024 +0800

    server : fix OpenSSL build (remove obsolete `LOG_INFO`) (#9529)

commit faf67b3de4688f47c3b1019c89df255df2fd59b4
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Wed Sep 18 08:30:31 2024 +0800

    [SYCL]set context default value to avoid memory issue, update guide (#9476)
    
    * set context default to avoid memory issue, update guide
    
    * Update docs/backend/SYCL.md
    
    Co-authored-by: Meng, Hengyu <hengyu.meng@intel.com>
    
    ---------
    
    Co-authored-by: arthw <14088817+arthw@users.noreply.github.com>
    Co-authored-by: Meng, Hengyu <hengyu.meng@intel.com>

commit 7be099fa817e9c53ffb4c3ed7d063e1cffcd675a
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Tue Sep 17 22:41:38 2024 +0200

    llama-bench: correct argument parsing error message (#9524)

commit 8b836ae731bbb2c5640bc47df5b0a78ffcb129cb
Author: Bert Wagner <github@bertwagner.com>
Date:   Tue Sep 17 09:35:38 2024 -0400

    arg : add env variable for parallel (#9513)
    
    * add env variable for parallel
    
    * Update README.md with env:  LLAMA_ARG_N_PARALLEL

commit 8344ef58f8cdb8ebd6faf4463ca32ae91c374c81
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Tue Sep 17 12:18:22 2024 +0200

    llama : fix n_vocab init for 'no_vocab' case (#9511)
    
    * llama: fixed n_vocab for `no_vocab` models
    
    * llama: updated error output for `llama_decode_internal` and `llama_encode_internal`
    
    * llama: log warning if there's no vocab_size in metadata
    
    * llama: correct vocab size for logging
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 0226613853133c081b55bb892a41bb5eacc0bc94
Author: Max Krasnyansky <quic_maxk@quicinc.com>
Date:   Tue Sep 17 01:19:46 2024 -0700

    threadpool : skip polling for unused threads (#9461)
    
    * threadpool: skip polling for unused threads
    
    Currently all threads do N polling rounds even if only 1 thread is active (n_threads_cur == 1).
    This commit adds a check to skip the polling for unused threads (ith >= n_threads_cur).
    
    n_threads_cur is now an atomic_int to explicitly tell thread sanitizer that it is written
    from one thread and read from other threads (not a race conditions).
    
    * threadpool: further simplify and improve ggml_barrier
    
    Avoid using strict memory order while polling, yet make sure that all threads go through
    full memory barrier (memory fence) on ggml_barrier entrace and exit.
    
    * threads: add simple barrier test
    
    This test does lots of small, parallel matmul ops where the barriers in between dominate the overhead.
    
    * threadpool: improve thread sync for new-graphs
    
    Using the same tricks as ggml_barrier. All the polling is done with relaxed memory order
    to keep it efficient, once the new graph is detected we do full fence using read-modify-write
    with strict memory order.
    
    * threadpool: improve abort handling
    
    Do not use threadpool->ec (exit code) to decide whether to exit the compute loop.
    threadpool->ec is not atomic which makes thread-sanitizer rightfully unhappy about it.
    
    Instead introduce atomic threadpool->abort flag used for this. This is consistent with
    how we handle threadpool->stop or pause.
    
    While at it add an explicit atomic_load for n_threads_cur for consistency.
    
    * test-barrier: release threadpool before releasing the context
    
    fixes use-after-free detected by gcc thread-sanitizer on x86-64
    for some reason llvm sanitizer is not detecting this issue.

commit 503147a9f9d195d6a14e7c998df23b6eb61f2bae
Author: Yuri Khrustalev <ykhrustalev@users.noreply.github.com>
Date:   Tue Sep 17 02:51:15 2024 -0400

    unicode : add <algorithm> (#9508)

commit 0d2ec438330271d201c2e9224aca23d0d5c908bf
Author: Gabe Goodhart <ghart@us.ibm.com>
Date:   Tue Sep 17 00:44:58 2024 -0600

    llama : support IBM Granite architecture (#9412)
    
    * feat(gguf-py): Add Granite model and params to gguf-py
    
    Branch: GraniteLM
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * feat(convert_hf_to_gguf): Add registration and param setup for Granite
    
    Branch: GraniteLM
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * feat(llama.cpp): Add config parsing for Granite multiplier params
    
    Branch: GraniteLM
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * feat(llama.cpp): First pass at full port of granite deviations from llama
    
    Something is still not working right since the results are mostly terrible,
    but on occasion it's producing relevant results at this point, so
    _something_ is working.
    
    Branch: GraniteLM
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * fix(llama.cpp): Determine granite language 3b instruct by vocab size
    
    Branch: GraniteLM
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * fix(convert_hf_to_gguf): Use LlamaModel as base for GraniteModel
    
    The defaults in LlamaModel are needed for Granite as well
    
    Branch: GraniteLM
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * fix(llama.cpp): Switch Granite param names to use _scale for consistency
    
    Other scalar multipliers are called *_scale, so this provides a more
    consistent naming convention.
    
    Branch: GraniteLM
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * fix(convert_hf_to_gguf/gguf-py): _multiplier -> _scale
    
    The transformers names with _multiplier will now be converted to the _scale
    equivalent during conversion.
    
    Branch: GraniteLM
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * fix(llama.cpp): Use separate switch clause for granite in llm_load_hparams
    
    Branch: GraniteLM
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    ---------
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>

commit 37f3a3810e545be6ac835c726f97956c1403ea02
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Tue Sep 17 08:23:30 2024 +0200

    llama : add llama_n_head() (#9512)

commit 23e0d70bacaaca1429d365a44aa9e7434f17823b
Author: slaren <slarengh@gmail.com>
Date:   Mon Sep 16 16:22:07 2024 +0200

    ggml : move common CPU backend impl to new header (#9509)

commit acb2c32c336ce60d765bb189563cc216e57e9fc2
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Mon Sep 16 13:07:13 2024 +0200

    llama : rename n_embed to n_embd in rwkv6_time_mix (#9504)
    
    This commit renames n_embed to n_embd in llm_build_rwkv6_time_mix.
    
    The motivation for this change is consistency with the other rwkv6
    functions like build_rwkv6 (and other parts of the code base).

commit a6a3a5c531c73aef85750a847d21e7d4671e723d
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Mon Sep 16 13:06:50 2024 +0200

    ggml : link MATH_LIBRARY not by its full path (#9339)

commit d54c21df7e2669c6cd7492713479d1aeb5846883
Author: compilade <git@compilade.net>
Date:   Mon Sep 16 03:30:22 2024 -0400

    convert : identify missing model files (#9397)

commit 19514d632e5274bc0c27c2269e8f2ad88b526d62
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Sep 16 10:27:50 2024 +0300

    cmake : do not hide GGML options + rename option (#9465)
    
    * cmake : do not hide GGML options
    
    ggml-ci
    
    * build : rename flag GGML_CUDA_USE_GRAPHS -> GGML_CUDA_GRAPHS
    
    for consistency
    
    ggml-ci

commit 5c3d0f1824714e9a97fc9b06e046eefcb6ecc721
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Mon Sep 16 06:48:24 2024 +0000

    ggml : IQ4_NL sgemm + Q4_0 AVX optimization (#9422)
    
    * squashed
    
    readd my iq4_nl sgemm PR https://github.com/ggerganov/llama.cpp/pull/8049
    
    have ggml_vec_dot_q4_0 do two blocks per loop for avx
    
    try out f16c ggml_vec_dot_iq4_nl, but it's not really faster. as per https://github.com/ggerganov/llama.cpp/pull/8549 we can calculate several blocks at a time with no issue
    
    * shuffle
    
    * remove f16c iq4_nl as i cant make it faster than before

commit 0aadac10c7dd704f8285ddf5a63d6f764cb340aa
Author: Shane A <shanea@allenai.org>
Date:   Sun Sep 15 23:47:37 2024 -0700

    llama : support OLMoE (#9462)

commit 95ca85168b5b089b80a811b59528ce0a2f1bd1dd
Author: CarryFun <76023481+CarryFun@users.noreply.github.com>
Date:   Mon Sep 16 14:45:20 2024 +0800

    llama : support MiniCPM3 (#9322)
    
    Co-authored-by: 范睿凯 <fanruikai@modelbest.cn>

commit 441b72b91f818fe69497e5816f87969e90c73c43
Author: Vinesh Janarthanan <36610342+VJHack@users.noreply.github.com>
Date:   Mon Sep 16 01:20:01 2024 -0500

    main : option to disable context shift (#9484)
    
    * added cli arg to disable context shift
    
    * reverted precommit
    
    * updated README.md for main
    
    * white space
    
    * allow disabling context shift in the server
    
    * Update common/arg.cpp
    
    no-context-shift only works for main example
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * added server example to --no-context-shift args
    
    * removed server changes
    
    * white space
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit c4965a64f72ac9434c21cf0e1c3421d13e889155
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Sep 16 09:05:56 2024 +0300

    metal : handle zero-sized allocs (#9466)

commit 90a2fff0e7f80c6fea3fc6cf9a7b482744f3f164
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Sep 16 05:14:23 2024 +0300

    flake.lock: Update (#9488)

commit 6262d13e0b2da91f230129a93a996609a2f5a2f2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Sep 15 20:46:12 2024 +0300

    common : reimplement logging (#9418)
    
    https://github.com/ggerganov/llama.cpp/pull/9418

commit e6deac31f7e62db43b6afbc3be814f764fd5a187
Author: slaren <slarengh@gmail.com>
Date:   Sun Sep 15 19:02:27 2024 +0200

    gguf-split : add basic checks (#9499)
    
    * gguf-split : do not overwrite existing files when merging
    
    * gguf-split : error when too many arguments are passed

commit 6988da94a261444859f78595899212eeedc5ff9d
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Sun Sep 15 18:55:52 2024 +0200

    cmake : correct order of sycl flags (#9497)

commit 3c7989fd29a2db2b75e28fd708cc441febe99a82
Author: Csaba Kecskemeti <csaba.kecskemeti@gmail.com>
Date:   Sun Sep 15 00:48:25 2024 -0700

    py : add "LLaMAForCausalLM" conversion support (#9485)
    
    Co-authored-by: Csaba Kecskemeti <csabakecskemeti@Csabas-Mac-Pro.local>

commit d6b37c881f056bd32b681dcd7658a37ea6ec3a1e
Author: OSecret <135510162+OLSecret@users.noreply.github.com>
Date:   Sun Sep 15 10:36:53 2024 +0300

    readme : update tools list (#9475)
    
    * Added link to proprietary wrapper for Unity3d into README.md
    
    Wrapper has prebuild library and was tested on iOS, Android, WebGL, PC, Mac platforms, has online demos like [this](https://d23myu0xfn2ttc.cloudfront.net/rich/index.html) and [that](https://d23myu0xfn2ttc.cloudfront.net/).
    
    * Update README.md
    
    Fixes upon review

commit 7596487bebd58eade3cd0133d42a9008aaaf9d09
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Sun Sep 15 09:06:38 2024 +0200

    cmake : try to fix sycl+intel build (#9487)

commit 822b6322dea704110797a5671fc80ae39ee6ac97
Author: Yuri Khrustalev <ykhrustalev@users.noreply.github.com>
Date:   Sat Sep 14 05:54:37 2024 -0400

    ggml : ggml_type_name return "NONE" for invalid values (#9458)
    
    When running on Windows, the quantization utility attempts to print the types that are not set which leads to a crash.

commit dcdcee3a744f39714503ee2b19c49b7c7b6209c9
Author: VoidIsVoid <343750470@qq.com>
Date:   Sat Sep 14 17:36:44 2024 +0800

    server: add data: [DONE] to /chat/completions stream response (#9459)

commit 1f4111e540bacec8d00ca9fd96417bf4c1339394
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Sep 14 10:55:05 2024 +0300

    cmake : use list(APPEND ...) instead of set() + dedup linker (#9463)
    
    * cmake : use list(APPEND ...) instead of set() + dedup linker
    
    ggml-ci
    
    * cmake : try fix sycl
    
    * cmake : try to fix sycl 2
    
    * cmake : fix sycl build (#9469)
    
    * try fix sycl build
    
    * use CMAKE_CXX_FLAGS as a string variable
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * one more CMAKE_CXX_FLAGS fix (#9471)
    
    ---------
    
    Co-authored-by: Michael Podvitskiy <podvitskiymichael@gmail.com>

commit befaf1197fa447f61714de041828852a270659d2
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Sat Sep 14 09:50:12 2024 +0200

    llama : make cell_id const in inp_s_mask block (#9470)
    
    This commit makes the cell_id variable const in the inp_s_mask block.
    
    The motivation for this change is consistency with the code in the
    inp_s_copy block.

commit feff4aa8461da7c432d144c11da4802e41fef3cf
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Sep 13 14:23:11 2024 +0200

    server : add loading html page while model is loading (#9468)
    
    * Adding loading page for '/' server requests
    
    * set content when model is loading
    
    * removed loading html file
    
    * updated cmakelist
    
    * updated makefile
    
    * cleaned up whitespace
    
    * cleanup for PR removed error
    
    * updated server test to handle 503 HTML
    
    * updated server test to handle 503 HTML
    
    * ca†ch 503 before parsing json
    
    * revert test
    
    * account for both api and web browser requests
    
    * precommit corrections
    
    * eol fix
    
    * revert changes to pre-commit
    
    * removed print statement
    
    * made loading message more descriptive
    
    * also support .html files
    
    ---------
    
    Co-authored-by: VJHack <flymyplane21@gmail.com>
    Co-authored-by: Vinesh Janarthanan <36610342+VJHack@users.noreply.github.com>

commit 0abc6a2c25272d5cf01384dda8ee8bfec4ba8745
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 13 09:53:38 2024 +0300

    llama : llama_perf + option to disable timings during decode (#9355)
    
    * llama : llama_perf + option to disable timings during decode
    
    ggml-ci
    
    * common : add llama_arg
    
    * Update src/llama.cpp
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    * perf : separate functions in the API
    
    ggml-ci
    
    * perf : safer pointer handling + naming update
    
    ggml-ci
    
    * minor : better local var name
    
    * perf : abort on invalid sampler pointer
    
    ggml-ci
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>

commit bd35cb0ae357185c173345f10dc89a4ff925fc25
Author: Gilad S. <7817232+giladgd@users.noreply.github.com>
Date:   Fri Sep 13 04:54:49 2024 +0300

    feat: remove a sampler from a chain (#9445)
    
    * feat: remove a sampler from a chain
    
    * fix: return removed sampler
    
    * fix: safer casting

commit 78203641fee3b1f82abaff0c7f667e1b4a286390
Author: Mathijs Henquet <mathijs.henquet@gmail.com>
Date:   Thu Sep 12 22:30:11 2024 +0200

    server : Add option to return token pieces in /tokenize endpoint (#9108)
    
    * server : added with_pieces functionality to /tokenize endpoint
    
    * server : Add tokenize with pieces tests to server.feature
    
    * Handle case if tokenizer splits along utf8 continuation bytes
    
    * Add example of token splitting
    
    * Remove trailing ws
    
    * Fix trailing ws
    
    * Maybe fix ci
    
    * maybe this fix windows ci?
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <son@huggingface.co>

commit e6b7801bd189d102d901d3e72035611a25456ef1
Author: Dou Xinpeng <81913537+Dou-Git@users.noreply.github.com>
Date:   Thu Sep 12 19:46:43 2024 +0800

    cann: Add host buffer type for Ascend NPU (#9406)
    
    * feat: Add host buffer type for Ascend NPU(CANN backend)
    
    * fix some checking errors
    
    * Add a few comments

commit e665744317c77fc3483fc5224fe6d586b5166b33
Author: fengerhu1 <2748250768@qq.com>
Date:   Thu Sep 12 19:34:22 2024 +0800

    llava : fix the script error in MobileVLM README (#9054)
    
    Signed-off-by: Erhu Feng <2748250768@qq.com>

commit d4c3c10fad1bd6adec72d2f1f236761a8d6a07f8
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Sep 12 13:33:57 2024 +0200

    lora : raise error if lm_head is ignored (#9103)
    
    * lora : raise error if lm_head is ignored
    
    * fix style
    
    * clarify comment

commit 2a825116b6f7f3a9b1726e5e0c3eb22f7768bd33
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Thu Sep 12 13:30:01 2024 +0200

    cmake : fix for builds without `GGML_CDEF_PUBLIC` (#9338)
    
    * `GGML_TARGET_DEFINES-NOTFOUND` fix for builds without `GGML_CDEF_PUBLIC`
    
    * Update CMakeLists.txt, spaces fix

commit 4dc4f5f14ae522494649d82ad06b031cf9501038
Author: Huang Qi <huangqi3@xiaomi.com>
Date:   Thu Sep 12 19:28:43 2024 +0800

    ci : update HIP SDK to 24.Q3 (ROCm 6.1) (#9329)

commit c837981bba7cf6839b69d32b25552ce685936b14
Author: daminho <37615795+daminho@users.noreply.github.com>
Date:   Thu Sep 12 20:28:20 2024 +0900

    py : add Phi-1.5/Phi-2 tokenizer (#9361)
    
    * add phi2 tokenizer
    
    * add phi name to convert_hf_to_gguf_update.py
    
    * make tokenizer_pre consistent; llama.cpp work

commit 3c26a1644dacfa6b5d58af550210524efd7b93fc
Author: Trivikram Kamat <16024985+trivikr@users.noreply.github.com>
Date:   Thu Sep 12 04:27:45 2024 -0700

    ci : bump actions/checkout to v4 (#9377)

commit ff76e18516dbe269b35ba1bb500524ed5e39225c
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Thu Sep 12 13:27:14 2024 +0200

    cmake : fixed the order of linking libraries for llama-quantize (#9450)

commit 39f852f44039b058fdd0611ee127c6efa7ba4a04
Author: Molly Sophia <mollysophia379@gmail.com>
Date:   Thu Sep 12 19:25:16 2024 +0800

    py : add special tokens in hf_converter for RWKV v6 (#9428)
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>

commit 2b00fa799773cc75d53b841c03d21d7468a1e3a1
Author: Ahmad Tameem <113388789+Tameem-10xE@users.noreply.github.com>
Date:   Thu Sep 12 16:24:31 2024 +0500

    riscv : modify Makefile and add a RISCV_VECT to print log info (#9442)
    
    - Added ggml_cpu_has_riscv_v() in GGML to print system info in log
    - Modified Makefile to only use flag when cross compiling for RISC-V

commit d6a04f872dea8ade92527bb1488d4b0b90cc49f0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Sep 12 14:23:49 2024 +0300

    ggml : hide ggml_object, ggml_cgraph, ggml_hash_set (#9408)
    
    * ggml : hide ggml_object, ggml_cgraph, ggml_hash_set
    
    ggml-ci
    
    * ggml : add ggml-impl.h to backends
    
    * ggml : fix compiler warnings
    
    ggml-ci
    
    * ggml : add assert upon adding nodes

commit c9c8575a1a8a170329afca4c4df4c005806efb1d
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Thu Sep 12 17:44:17 2024 +0800

    enhance run script to be easy to change the parameters (#9448)
    
    Co-authored-by: arthw <14088817+arthw@users.noreply.github.com>

commit df4b7945aeccae2a71348e5a9c1eab5241e3e0ef
Author: Xinpeng Dou <81913537+Dou-Git@users.noreply.github.com>
Date:   Thu Sep 12 09:02:35 2024 +0800

    cann: Fix error when running a non-exist op (#9424)

commit 449ccfb6f5f1bbd70e04f75a330d9d7c1af82187
Author: Faisal Zaghloul <quic_fzaghlou@quicinc.com>
Date:   Wed Sep 11 20:29:53 2024 -0400

    Add Jais to list of supported models (#9439)
    
    Co-authored-by: fmz <quic_fzaghlou@quic.com>

commit 1b28061400eb9832603c9f1dfbec4d339a8490a2
Author: slaren <slarengh@gmail.com>
Date:   Wed Sep 11 17:52:13 2024 +0200

    llama : skip token bounds check when evaluating embeddings (#9437)

commit 8db003a19d7055b5bd248ce2afff9324e5b8da95
Author: Pavel Zloi <github.com@drteam.rocks>
Date:   Wed Sep 11 15:29:51 2024 +0300

    py : support converting local models (#7547)
    
    * Support of converting local models added to convert-hf-to-gguf-update.py
    
    * Description fixed
    
    * shutil added to imports

commit 0996c5597f680effacc046832bb807c14900e22d
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed Sep 11 12:59:13 2024 +0200

    llava : correct args for minicpmv-cli (#9429)

commit 5bb2c5dbd26b246d334f0087b3cbd800f2e65c54
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed Sep 11 12:02:09 2024 +0200

    files : remove accidentally added `lora_test` submodule (#9430)

commit 67155ab7f5e47c01b62aa989eab30f517bf6dc67
Author: Farbod Bijary <110523279+farbodbj@users.noreply.github.com>
Date:   Wed Sep 11 12:52:37 2024 +0330

    feat: Implements retrying logic for downloading models using --model-url flag (#9255)
    
    * feat: Implements retrying logic for downloading models using --model-url flag
    
    * Update common/common.cpp
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    * Update common/common.cpp
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    * apply comments
    
    * implements a retry function to avoid duplication
    
    * fix editorconfig
    
    * change function name
    
    ---------
    
    Co-authored-by: farbod <farbod.bjary82@gmail.com>
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: Xuan Son Nguyen <son@huggingface.co>

commit 5af118efdaf1098798a06b24fd8a557760e99631
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Sep 11 10:22:40 2024 +0200

    CUDA: fix --split-mode row race condition (#9413)

commit d2b496bff4f353a6429f8e833448f071bd237ba7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Sep 11 10:03:54 2024 +0300

    batched-bench : remove unused code (#9305)

commit b34e02348064c2f0cef1f89b44d9bee4eb15b9e7
Author: R0CKSTAR <xiaodong.ye@mthreads.com>
Date:   Wed Sep 11 09:46:55 2024 +0800

    musa: remove Clang builtins mapping (#9421)
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>

commit 51b603863627c4074e77b7e556e18ece86bdf9a3
Author: Alberto Cabrera Pérez <alberto.cabrera@codeplay.com>
Date:   Wed Sep 11 01:53:42 2024 +0100

    sycl : update support conditions  (#9394)
    
    * sycl : update support condition to im2col
    
    Signed-off-by: Alberto Cabrera <alberto.cabrera@codeplay.com>
    
    * Added TODO to remind supporting FP32 im2col
    
    ---------
    
    Signed-off-by: Alberto Cabrera <alberto.cabrera@codeplay.com>

commit cb9c933eb2a0d2b514556bdcb934b56dfe5d6771
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Sep 11 01:46:59 2024 +0300

    flake.lock: Update (#9360)
    
    Flake lock file updates:
    
    • Updated input 'flake-parts':
        'github:hercules-ci/flake-parts/af510d4a62d071ea13925ce41c95e3dec816c01d?narHash=sha256-ODYRm8zHfLTH3soTFWE452ydPYz2iTvr9T8ftDMUQ3E%3D' (2024-08-30)
      → 'github:hercules-ci/flake-parts/567b938d64d4b4112ee253b9274472dc3a346eb6?narHash=sha256-%2Bebgonl3NbiKD2UD0x4BszCZQ6sTfL4xioaM49o5B3Y%3D' (2024-09-01)
    • Updated input 'flake-parts/nixpkgs-lib':
        'https://github.com/NixOS/nixpkgs/archive/a5d394176e64ab29c852d03346c1fc9b0b7d33eb.tar.gz?narHash=sha256-uFf2QeW7eAHlYXuDktm9c25OxOyCoUOQmh5SZ9amE5Q%3D' (2024-08-01)
      → 'https://github.com/NixOS/nixpkgs/archive/356624c12086a18f2ea2825fed34523d60ccc4e3.tar.gz?narHash=sha256-Ss8QWLXdr2JCBPcYChJhz4xJm%2Bh/xjl4G0c0XlP6a74%3D' (2024-09-01)
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/71e91c409d1e654808b2621f28a327acfdad8dc2?narHash=sha256-GnR7/ibgIH1vhoy8cYdmXE6iyZqKqFxQSVkFgosBh6w%3D' (2024-08-28)
      → 'github:NixOS/nixpkgs/574d1eac1c200690e27b8eb4e24887f8df7ac27c?narHash=sha256-v3rIhsJBOMLR8e/RNWxr828tB%2BWywYIoajrZKFM%2B0Gg%3D' (2024-09-06)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit 6cd4e034442f71718563e600070c2b6fc389e100
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Sep 10 22:41:29 2024 +0200

    arg : bring back missing ifdef (#9411)
    
    * arg : bring back missing ifdef
    
    * replace with llama_supports_gpu_offload

commit 8d300bd35fbe23b35a4e1ece0cf0fe8f43331029
Author: matteo <matteogeniaccio@yahoo.it>
Date:   Tue Sep 10 22:40:59 2024 +0200

    enable --special arg for llama-server (#9419)
    
    Co-authored-by: matteo serva <matteo.serva@gmail.com>

commit 49006c67b4c6cc2e7c75a875b4d6e161ebae287c
Author: slaren <slarengh@gmail.com>
Date:   Tue Sep 10 18:04:25 2024 +0200

    llama : move random seed generation to the samplers (#9398)
    
    * llama_sampler_penalties : clamp penalty_last_n to zero

commit 00ba2ff78100e187ae17987bacd1c916211718b2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Sep 10 10:17:03 2024 +0300

    metal : fix compile warning with GGML_METAL_NDEBUG (#0)

commit 83008b7cfe90ad89d0c0ed2c2424fd75edc25ac1
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Sep 10 09:03:21 2024 +0200

    llama : update llm_build_copy_mask_state comment [no ci] (#9385)
    
    This commit updates the comment, which seems to contain a typo or be an
    outdated comment, in the copy_mask_state function changing the variable
    n_rs to n_kv.
    
    I believe this change is correct and what the comment wants to
    convey is to copy the states that are not going to be used in the
    upcoming processing, which are the tokens states from n_seqs up to
    the number of possible token states n_kv.

commit 0b4ac75772b744bb0a0d674927587621d1057884
Author: Molly Sophia <mollysophia379@gmail.com>
Date:   Tue Sep 10 15:02:30 2024 +0800

    RWKV v6: Add time_mix_decay_w1/w2 in quant exclusion list (#9387)
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>

commit fb3f2498156b3140e2050ec9c7bf61372f63ff56
Author: slaren <slarengh@gmail.com>
Date:   Tue Sep 10 08:23:33 2024 +0200

    make : do not run llama-gen-docs when building (#9399)

commit bfe76d4a17228bfd1565761f203123bc4914771b
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Sep 9 23:36:09 2024 +0200

    common : move arg parser code to `arg.cpp` (#9388)
    
    * common : move arg parser to arg.cpp
    
    * better categorize args
    
    * add cmake
    
    * missing climits
    
    * missing cstdarg
    
    * common : more explicit includes
    
    * fix build
    
    * refactor gpt_params_parse
    
    * update server readme
    
    * fix test
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 293bebe0773c907c0c866213856eeba41b035df1
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Mon Sep 9 18:40:10 2024 +0300

    rpc : fix segfault with nkvo (#9389)
    
    * rpc : fix nkvo
    
    * rpc : buf_size must not be static
    
    ref: #9337
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 5fac4d57643b1de8e9ab746f14d2fc4e319ae0c2
Author: Prashant Vithule <119530321+Vithulep@users.noreply.github.com>
Date:   Mon Sep 9 21:07:18 2024 +0530

    ggml : vector length agnostic SVE support (#9290)
    
    * Implemented vector length agnostic SVE using switch case for 512-bit, 256-bit, 128-bit vector lengths
    
    * Implemented vector length agnostic SVE using switch case for 512-bit, 256-bit, 128-bit vector lengths
    
    * Removed WhiteSpaces
    
    * ggml : style changes + fix 512-bit nb loop check
    
    - fix local scope in switch cases
    - consistent predicate names
    - empty lines when necessary
    - opening braces, spaces
    - const-correctness
    - add asserts
    
    * Update ggml/src/ggml-quants.c
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 5fb5e24811cb01d48b482c15a974bfbd9f433e1d
Author: slaren <slarengh@gmail.com>
Date:   Mon Sep 9 17:10:46 2024 +0200

    llama : minor sampling refactor (2) (#9386)

commit 38ca6f644bd48301e9caa80f9913c22e70a8fd1b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Sep 9 15:51:37 2024 +0300

    readme : update hot topics

commit 8e6e2fbe1458ac91387266241262294a964d6b95
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Sep 9 14:22:53 2024 +0200

    CUDA: fix variable name conflict for Windows build (#9382)

commit 5ed087573e1f326cfa70e29c1895d074a7a1a00c
Author: Antonis Makropoulos <benuix@gmail.com>
Date:   Mon Sep 9 14:21:38 2024 +0300

    readme : add LLMUnity to UI projects (#9381)
    
    * add LLMUnity to UI projects
    
    * add newline to examples/rpc/README.md to fix editorconfig-checker unit test

commit 54f376d0b92c6ff6feb1fa2ef8ed2022348100ba
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Mon Sep 9 11:04:39 2024 +0300

    rpc : update README [no ci] (#9320)
    
    Update README with instructions how to offload model layers to both
    local and remote devices

commit b2e89a327457179a34eae4d7de0d412ed945679c
Author: Dan Johansson <164997844+eddnjjn@users.noreply.github.com>
Date:   Mon Sep 9 09:02:45 2024 +0200

    Arm AArch64: Documentation updates (#9321)
    
    * Arm AArch64: Documentation updates
    
    * Update docs/build.md to include information on how to enable the Arm optimized gemm/gemv kernels
    
    * Update examples/quantize/README.md with information on the Q4_0_4_4, Q4_0_4_8 and Q4_0_8_8 formats
    
    * Add newline to the end of docs/build.md

commit daa9623ab051a8162ae750b150b9522571b55f21
Author: Markus Tavenrath <mtavenrath@users.noreply.github.com>
Date:   Sun Sep 8 21:43:48 2024 +0200

    Overlap cmdbuffer creation and cmdbuffer execution in Vulkan backend by submitting smaller cmdbuffers early. (#9118)
    
    * Overlap cmdbuffer creation and cmdbuffer execution in Vulkan backend by submitting smaller cmdbuffers early.
    
    * fix compile issues
    
    * Fix issues where the last submit wasn't executed or handled properly.
    
    * remove trailing whitespace
    
    * Repair GGML_VULKAN_CHECK_RESULTS
    
    * Increase submit counter only if actual work has been submitted and increase submit count to 100.
    
    * Fix some nodes are not checked with GGML_VULKAN_CHECK_RESULTS enabled.

commit e079bffb6678e1b5ded21c719600bedd7175e726
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Sep 8 22:01:02 2024 +0300

    cuda : fix FA Q src index (1 -> 0) (#9374)

commit 3f7ccfd649abc83d059b5462221ac14de4ede6b7
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sun Sep 8 18:08:55 2024 +0200

    common : bring back missing args, add env var duplication check (#9375)
    
    * common : bring back missing args
    
    * move duplication check to test-arg-parser
    
    * add check for duplicated env var
    
    * correct default values

commit a249843d89bd6373772eede26d7f2aa708b26600
Author: slaren <slarengh@gmail.com>
Date:   Sun Sep 8 16:44:42 2024 +0200

    common : restore --n-gpu-layers (#9371)

commit 19f4a7b296efda7c13a6b21d428b2286b5d1aa06
Author: slaren <slarengh@gmail.com>
Date:   Sun Sep 8 15:52:07 2024 +0200

    llama : refactor samplers internal implementation (#9370)

commit 2a358fb0c4b6e917ac852aa17444cc94dd28a2a6
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Sun Sep 8 19:05:29 2024 +0800

    [SYCL] add check malloc result on device (#9346)
    
    * add check malloc result on device
    
    * update for review comments, check all malloc_device() result
    
    ---------
    
    Co-authored-by: arthw <14088817+arthw@users.noreply.github.com>

commit eae597182cb61bbde0b26e7cec5999d28b9327fe
Author: slaren <slarengh@gmail.com>
Date:   Sun Sep 8 12:41:51 2024 +0200

    llama : sanitize tokens in the upper bound (#9359)

commit 00b02bb249406ec0123757a67a5b714c3f33a699
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sun Sep 8 12:12:17 2024 +0200

    imatrix : fix arg parser for imatrix (#9366)
    
    * imatrix : fix arg parser
    
    * beautify printing first arg

commit a8768614558262b6762fc0feeddcc6f7ce4bc5fc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Sep 8 09:57:57 2024 +0300

    metal : update support condition for im2col + fix warning (#0)

commit 385decbd632f70db9f1fbd93dbb2b908853e135c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Sep 8 09:38:56 2024 +0300

    sync : ggml

commit 60a3107ccd791d858e12a87e6024ce918d3bf595
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Sep 8 09:38:42 2024 +0300

    scripts : option to increase git patch context

commit 406c1a32a18807b9b5711aa2fa1859527106bc1d
Author: Salvatore Mesoraca <s.mesoraca16@gmail.com>
Date:   Fri Sep 6 14:34:25 2024 +0200

    vulkan: add dryrun support to sin and cos ops (ggml/947)
    
    sin and cos failed test-backend-ops because they
    tried to dereference a context pointer that is null
    on dry runs.
    
    This commit prevents that segfault.
    
    Signed-off-by: Salvatore Mesoraca <s.mesoraca16@gmail.com>

commit 9cb9260861e464e40d38764cfb021856786c7202
Author: Salvatore Mesoraca <s.mesoraca16@gmail.com>
Date:   Fri Sep 6 14:34:07 2024 +0200

    vulkan: correctly report support for OP_CONT (ggml/946)
    
    test-backend-ops fails because ggml_cont aborts
    when invoked passing an unsupported type.
    
    This commit makes ggml_cont tests pass
    
    Signed-off-by: Salvatore Mesoraca <s.mesoraca16@gmail.com>

commit 202084d31d4247764fc6d6d40d2e2bda0c89a73a
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Sep 3 17:21:46 2024 +0200

    tests: add gradient tests for all backends (ggml/932)
    
    * tests: add gradient checking to test-backend-ops
    
    * remove old comment
    
    * reorder includes
    
    * adjust SIN/COS parameters
    
    * add documentation, use supports_op if possible

commit dbbebcab339c7d63d3806e8f32574bb9aad9a694
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Aug 31 14:35:42 2024 +0200

    ggml: fix ggml_graph_cpy undefined behavior (ggml/943)

commit ba1cf846edeb94fc567a9ab0f7ef705282e6d7d3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Aug 28 18:45:01 2024 +0300

    cann : fix doxy (ggml/0)

commit d2d3200b385970cb2a4658fd9342a3b1212bdb46
Author: Mengqing Cao <cmq0113@163.com>
Date:   Fri Aug 9 20:21:56 2024 +0800

    cann : add Ascend NPU support (whisper/2336)
    
    * enable Ascend NPU in src/whisper.cpp
      * sync test-backend-ops with llama.cpp

commit 51d964a4efdb0e8d43f5b85a775951185f6b641e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Aug 28 17:08:03 2024 +0300

    cuda : mark BF16 CONT as unsupported

commit efe6a83e3046a94cda38c8be5a7801eadc21d78d
Author: Salvatore Mesoraca <s.mesoraca16@gmail.com>
Date:   Wed Aug 28 10:23:02 2024 +0200

    ggml : fix cont with transposed tensors when one dimension is 1 (ggml/934)
    
    * ggml_cont: fix issue with transposed tensors when one dimension is 1
    
    when using multiple threads, it is not enough
    to check for the tensors to be contiguous for
    ggml_compute_forward_dup_same_cont to work correctly.
    The tensors strides also need to match.
    
    Signed-off-by: Salvatore Mesoraca <s.mesoraca16@gmail.com>
    
    * Add ggml_cont tests
    
    Signed-off-by: Salvatore Mesoraca <s.mesoraca16@gmail.com>
    
    * Remove dead code
    
    it isn't possible to reach this code because
    all these functions are invoked by ggml_compute_forward_dup
    if and only if src0->type != dst->type
    
    Signed-off-by: Salvatore Mesoraca <s.mesoraca16@gmail.com>
    
    * Make ggml_compute_forward_dup_same_cont work with contiguous tensors
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Signed-off-by: Salvatore Mesoraca <s.mesoraca16@gmail.com>
    
    ---------
    
    Signed-off-by: Salvatore Mesoraca <s.mesoraca16@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit fbb7fcffbcfc50009c275e75982b1603a6cb6b80
Author: Kevin Gibbons <bakkot@gmail.com>
Date:   Sat Sep 7 22:51:00 2024 -0700

    llama : set attrs of mislabelled EOT/EOM tokens (#9348)

commit a5b5d9a1014248f385939e6739e9db9de7147e55
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Sep 8 00:33:50 2024 +0300

    llama.android : fix build (#9350)

commit f12295b8a9336962bf02a359beb1be0d322b4be7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Sep 8 00:33:33 2024 +0300

    llama : fix empty ring buffer push (#9358)

commit faf69d4237c9ae4d7f572b4674d1002463e8acd3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Sep 8 00:33:13 2024 +0300

    llama : sanitize invalid tokens (#9357)
    
    * common : do not add null tokens during warmup
    
    ggml-ci
    
    * llama : check that the input tokens are valid
    
    ggml-ci
    
    * tests : fix batch size of bert model
    
    ggml-ci

commit e536426ded3fb4a8cd13626e53508cd92928d6c2
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Sat Sep 7 19:02:26 2024 +0000

    llamafile : disable sgemm for batch-size 1 (#9330)

commit 1b9ae5189cd279c6b45e36d43e4f9ccae628d02f
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat Sep 7 20:43:51 2024 +0200

    common : refactor arg parser (#9308)
    
    * (wip) argparser v3
    
    * migrated
    
    * add test
    
    * handle env
    
    * fix linux build
    
    * add export-docs example
    
    * fix build (2)
    
    * skip build test-arg-parser on windows
    
    * update server docs
    
    * bring back missing --alias
    
    * bring back --n-predict
    
    * clarify test-arg-parser
    
    * small correction
    
    * add comments
    
    * fix args with 2 values
    
    * refine example-specific args
    
    * no more lamba capture
    
    Co-authored-by: slaren@users.noreply.github.com
    
    * params.sparams
    
    * optimize more
    
    * export-docs --> gen-docs

commit e32d0816edfd247784f6780b0bb9ae0bceef5e47
Author: slaren <slarengh@gmail.com>
Date:   Sat Sep 7 20:23:07 2024 +0200

    ggml : always check bounds on get_rows operations (#9354)

commit df270ef74596da8f1178f08991f4c51f18c9ee82
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Sep 7 15:16:19 2024 +0300

    llama : refactor sampling v2 (#9294)
    
    - Add `struct llama_sampler` and `struct llama_sampler_i`
    - Add `llama_sampler_` API
    - Add `llama_sampler_chain_` API for chaining multiple samplers
    - Remove `LLAMA_API_INTERNAL`
    - Add `llama_perf_` API and remove old `llama_print_timings` and `llama_reset_timings`

commit 947538acb8617756a092042ff7e58db18dde05ec
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat Sep 7 12:01:34 2024 +0200

    ggml : fix missing `cpu_set_t` on emscripten (#9336)
    
    * ggml : fix missing cpu_set_t on emscripten
    
    * better version
    
    * bring back android part

commit 6c89eb0b4749184f4d19e96ada5dcb8847129b9c
Author: slaren <slarengh@gmail.com>
Date:   Sat Sep 7 09:48:54 2024 +0200

    ci : disable rocm image creation (#9340)

commit 9b2c24c0993487d3b34a873980e292da571481f3
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Sep 6 23:21:29 2024 +0200

    server : simplify state machine for slot (#9283)
    
    * server : simplify state machine for slot
    
    * add SLOT_STATE_DONE_PROMPT
    
    * pop_deferred_task
    
    * add missing notify_one
    
    * fix passkey test
    
    * metrics : add n_busy_slots_per_decode
    
    * fix test step
    
    * add test
    
    * maybe fix AddressSanitizer?
    
    * fix deque ?
    
    * missing lock
    
    * pop_deferred_task: also notify
    
    * Update examples/server/server.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 134bc38ecf3e2c5460581badce289a1ffa680453
Author: Aarni Koskela <akx@iki.fi>
Date:   Sat Sep 7 00:03:01 2024 +0300

    llama-bench : log benchmark progress (#9287)
    
    * llama-bench : add optional progress messages

commit 815b1fb20a53e439882171757825bacb1350de04
Author: Aarni Koskela <akx@iki.fi>
Date:   Fri Sep 6 18:59:58 2024 +0300

    batched-bench : add `--output-format jsonl` option (#9293)
    
    `--output-format` is modeled after `llama-bench`'s options

commit 409dc4f8bb5185786087f52259ee4626be93f54d
Author: Changyeon Kim <cyzero.kim@samsung.com>
Date:   Fri Sep 6 21:54:50 2024 +0900

    ggml : fix build break for the vulkan-debug (#9265)
    
    - windows build : Ok.
    - linux build : Ok.
    
    Signed-off-by: Changyeon Kim <cyzero.kim@samsung.com>

commit 4a1411b4f17b6569dc0a067e5914dcc0f738b370
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Sep 6 14:06:04 2024 +0200

    server : fix missing lock (#9334)

commit 8ebe8ddebd68526757c631cd019de009697c63c2
Author: Markus Tavenrath <mtavenrath@users.noreply.github.com>
Date:   Fri Sep 6 08:56:17 2024 +0200

    Improve Vulkan shader build system (#9239)
    
    * Improve Vulkan shader builds system
    
    - Add dependency to vulkan-shaders-gen to rebuild shaders when changing the shader compilation utility.
    - Add option to generate debug info for Vulkan shaders to provide shader source to Vulkan shader profiling tools
    
    * remove not required self dependency

commit 9bc6db28d011d47a5f318dc4aebbe7927fac4629
Author: compilade <git@compilade.net>
Date:   Thu Sep 5 21:48:47 2024 -0400

    ggml-quants : ternary packing for TriLMs and BitNet b1.58 (#8151)
    
    * ggml-quants : 1.625 bpw ternary packing for BitNet 1.58b
    
    * ggml-quants : faster 1.625 bpw AVX2 vec_dot
    
    Not using a lookup table anymore makes it match q4_0 speed.
    
    * gguf-py : fix formatting
    
    * llama : remove spaces on empty line
    
    * ggml-quants : subtract 1 when back in epi8
    
    This makes the 1.625 bpw type go faster than q4_0. Still not the fastest.
    
    * ggml-quants : Q2_2 now faster than Q4_K on with AVX2
    
    * ggml-quants : cleanup Q1_3 code formatting
    
    * ggml-quants : ARM NEON vec_dot for q2_2 and q1_3
    
    * ggml-quants : use ceiling division when quantizing q1_3
    
    * convert-hf : simplify BitNet pre-quantization
    
    This still results in the exact same tensor weights and scales,
    but it reveals some weirdness in the current algorithm.
    
    * convert-hf : allow converting the weird BitNet 1.3B
    
    Its FFN size is 5460 which is not convenient.
    The offending tensors are kept in F16,
    which makes the final model 5.01 bpw.
    
    * bitnet : replace 1.58b with b1.58, as in the paper
    
    * ggml-quants : fix build failure on Windows
    
    * ggml-quants : attempt to fix Arm 32-bit support
    
    * ggml : add some informative comments in q1_3 vec_dot
    
    * ggml : add TQ1_0 and TQ2_0 ternary quantization types
    
    * ggml : even faster TQ2_0
    
    * ggml : also faster TQ1_0
    
    Same optimization as for TQ2_0 by offsetting the sum instead of the weights.
    This makes TQ1_0 almost as fast as Q8_0 on AVX2.
    
    * ggml : fix build issues in certain environments
    
    * ggml : add NEON vec_dot implementation for TQ1_0 and TQ2_0
    
    * ggml : avoid directly using vmlal_high_s8, for 32-bit ARM compat
    
    The compiler seems smart enough to use the same instruction
    even when using vget_high_s8 instead.
    
    * ggml : remove q1_3 and q2_2
    
    No more 1.625 bpw and 2.000 bpw,
    now instead using 1.6875 bpw and 2.0625 bpw
    with TQ1_0 and TQ2_0, respectively.
    
    * llama : remove the separate scale tensors of BitNet b1.58
    
    They won't be needed, since the remaining ternary quant types have
    built-in scales.
    
    * ggml-quants : rename fields of TQ1_0 and TQ2_0 structs for consistency
    
    * ggml-quants : allow using vdotq_s32 in TQ2_0 vec_dot
    
    Not yet tested on hardware which supports it,
    might not work or might not even compile. But also it might.
    It should make the performance better on recent ARM CPUs.
    
    * ggml-quants : remove comment about possible format change of TQ2_0
    
    Making it slightly more convenient for AVX512
    but less convenient for everything else is not worth the trouble.
    
    * gguf-py : Numpy (de)quantization for TQ1_0 and TQ2_0
    
    * ggml-quants : use roundf instead of nearest_int for TQ1_0 and TQ2_0
    
    This does not change anything for ternary models,
    since their values should never end up being in halfway cases anyway.
    
    * convert : allow direct conversion to TQ1_0 and TQ2_0
    
    The token embeddings and output tensors are kept in F16
    to allow quantizing them to Q4_K and Q6_K with llama-quantize.
    
    * llama : handle fallback for TQ1_0 and TQ2_0 with Q4_0
    
    Q4_0 is not completely symmetric (so not lossless for ternary models),
    but it should be good enough.
    
    * ggml-quants : allow using ARM dot product instructions for TQ1_0
    
    * ggml-quants : deduplicate TQ1_0 and TQ2_0 __ARM_FEATURE_DOTPROD support
    
    * ggml : remove unused ggml_mul special case
    
    It would otherwise conflict with the more general
    optimization coming with Mamba-2.
    
    * ggml : handle TQ1_0 and TQ2_0 in dequantization-based operators
    
    * test-backend-ops : add TQ1_0 and TQ2_0 comments for later
    
    Not yet adding uncommented, because some backends like SYCL and Metal
    do not properly handle unknown types in supports_op for GGML_OP_MUL_MAT.
    (and Metal also doesn't handle it with GGML_OP_GET_ROWS)
    Support for TQ1_0 and TQ2_0 for other backends than CPU
    will be added in follow-up pull requests.

commit 32b2ec88bc44b086f3807c739daf28a1613abde1
Author: awatuna <23447591+awatuna@users.noreply.github.com>
Date:   Fri Sep 6 06:34:36 2024 +0800

    Update build.yml (#9184)
    
    build rpc-server for windows cuda

commit 1031771faaba28d07b4ec6a812cb21532efc8c31
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Fri Sep 6 00:14:12 2024 +0200

    CMake fix: host for msvc compiler can only be x86 or x64 (#8624)

commit 4db04784f96757d74f74c8c110c2a00d55e33514
Author: slaren <slarengh@gmail.com>
Date:   Thu Sep 5 11:13:11 2024 +0200

    cuda : fix defrag with quantized KV (#9319)

commit bdf314f38a2c90e18285f7d7067e8d736a14000a
Author: slaren <slarengh@gmail.com>
Date:   Thu Sep 5 02:19:39 2024 +0200

    llama-bench : fix NUL terminators in CPU name (#9313)

commit 581c305186a0ff93f360346c57e21fe16e967bb7
Author: Srihari-mcw <96763064+Srihari-mcw@users.noreply.github.com>
Date:   Wed Sep 4 22:21:22 2024 +0530

    ggml : AVX2 support for Q4_0_8_8 (#8713)
    
    * Add AVX2 based implementations for quantize_q8_0_4x8, ggml_gemv_q4_0_8x8_q8_0 and ggml_gemm_q4_0_8x8_q8_0 functions
    
    * Update code to fix issues occuring due to non alignment of elements to be processed as multiple of 16 in MSVC
    
    * Update comments and indentation
    
    * Make updates to reduce number of load instructions

commit 5910ea942772ab6cbc21d0ad2d1208750ba39e1d
Author: Ouadie EL FAROUKI <ouadie.elfarouki@codeplay.com>
Date:   Wed Sep 4 16:26:33 2024 +0100

    [SYCL] Fix DMMV dequantization (#9279)
    
    Fixed dmmv dequant for ncols== GGML_SYCL_DMMV_X

commit c8671ae28214b29920b86c66a5d36fd6dd0db870
Author: 杨朱 · Kiki <baofa.fan@daocloud.io>
Date:   Wed Sep 4 19:45:28 2024 +0800

    Fix broken links in docker.md (#9306)

commit 82e3b03c11826d20a24ab66d60f4de58f48ddcdb
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Wed Sep 4 11:08:32 2024 +0300

    rpc : make RPC servers come first in the device list (#9296)
    
    * rpc : make RPC servers come first in the device list
    
    * rpc : disable options for non-RPC builds
    
    * rpc : rpc_count always zero for non-RPC builds

commit 9379d3cc1718e9ec6ab5ffcb60a06bbe048a61ee
Author: Pascal Patry <ppatry@mtacitlabs.com>
Date:   Wed Sep 4 02:45:40 2024 -0400

    readme : rename result_format to response_format (#9300)

commit 7605ae7daf31c02211bcfec2f46635ef6ec4b98a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Sep 4 02:36:43 2024 +0300

    flake.lock: Update (#9261)
    
    Flake lock file updates:
    
    • Updated input 'flake-parts':
        'github:hercules-ci/flake-parts/8471fe90ad337a8074e957b69ca4d0089218391d?narHash=sha256-XOQkdLafnb/p9ij77byFQjDf5m5QYl9b2REiVClC%2Bx4%3D' (2024-08-01)
      → 'github:hercules-ci/flake-parts/af510d4a62d071ea13925ce41c95e3dec816c01d?narHash=sha256-ODYRm8zHfLTH3soTFWE452ydPYz2iTvr9T8ftDMUQ3E%3D' (2024-08-30)
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/c374d94f1536013ca8e92341b540eba4c22f9c62?narHash=sha256-Z/ELQhrSd7bMzTO8r7NZgi9g5emh%2BaRKoCdaAv5fiO0%3D' (2024-08-21)
      → 'github:NixOS/nixpkgs/71e91c409d1e654808b2621f28a327acfdad8dc2?narHash=sha256-GnR7/ibgIH1vhoy8cYdmXE6iyZqKqFxQSVkFgosBh6w%3D' (2024-08-28)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit 8962422b1c6f9b8b15f5aeaea42600bcc2d44177
Author: Aarni Koskela <akx@iki.fi>
Date:   Tue Sep 3 20:58:54 2024 +0300

    llama-bench : add JSONL (NDJSON) output mode (#9288)
    
    * llama-bench : add JSONL (NDJSON) output mode
    
    * llama-bench : update usage docs

commit b69a480af4db8ffd6cbb2efe7ed8132bc7d39073
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Sep 3 10:00:36 2024 +0300

    readme : refactor API section + remove old hot topics

commit 48baa61eccdca9205daf8d620ba28055c2347b64
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Sep 2 22:08:38 2024 +0200

    server : test script : add timeout for all requests (#9282)

commit f1485161e58d562099dd050f8ac3a9ea9f4cd765
Author: Zhenwei Jin <109658203+kylo5aby@users.noreply.github.com>
Date:   Tue Sep 3 01:53:23 2024 +0800

    src: make tail invalid when kv cell is intersection for mamba (#9249)

commit 048de848ee4baad4531fcd6239438f5d55be365c
Author: slaren <slarengh@gmail.com>
Date:   Mon Sep 2 18:11:13 2024 +0200

    docker : fix missing binaries in full-cuda image (#9278)

commit f771d064a95d212ddadea452ad0cc1e42b2c3db3
Author: yuri@FreeBSD <yurivict@users.noreply.github.com>
Date:   Mon Sep 2 08:25:30 2024 -0700

    ggml : add pthread includes on FreeBSD (#9258)

commit 6e7d133a5f9409dd257fad90d7f320721b07a1b2
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Sep 2 17:11:51 2024 +0200

    server : refactor multitask handling (#9274)
    
    * server : remove multitask from server_task
    
    * refactor completions handler
    
    * fix embeddings
    
    * use res_ok everywhere
    
    * small change for handle_slots_action
    
    * use unordered_set everywhere
    
    * (try) fix test
    
    * no more "mutable" lambda
    
    * Apply suggestions from code review
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * use deque
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit b60074f1c26d8354053a952844ef3f7ebefd8803
Author: Guoliang Hua <32868157+nbcsm@users.noreply.github.com>
Date:   Mon Sep 2 20:36:43 2024 +0800

    llama-cli : remove duplicated log message (#9275)

commit 9c1ba557335c3cab46807e6478eb7d17a63361f1
Author: Tushar <ditsuke@protonmail.com>
Date:   Mon Sep 2 16:51:01 2024 +0530

    build(nix): Package gguf-py (#5664)
    
    * style: format with nixfmt/rfc101-style
    
    * build(nix): Package gguf-py
    
    * build(nix): Refactor to new scope for gguf-py
    
    * build(nix): Exclude gguf-py from devShells
    
    * build(nix): Refactor gguf-py derivation to take in exact deps
    
    * build(nix): Enable pytestCheckHook and pythonImportsCheck for gguf-py
    
    * build(python): Package python scripts with pyproject.toml
    
    * chore: Cleanup
    
    * dev(nix): Break up python/C devShells
    
    * build(python): Relax pytorch version constraint
    
    Nix has an older version
    
    * chore: Move cmake to nativeBuildInputs for devShell
    
    * fmt: Reconcile formatting with rebase
    
    * style: nix fmt
    
    * cleanup: Remove unncessary __init__.py
    
    * chore: Suggestions from review
    
    - Filter out non-source files from llama-scripts flake derivation
    - Clean up unused closure
    - Remove scripts devShell
    
    * revert: Bad changes
    
    * dev: Simplify devShells, restore the -extra devShell
    
    * build(nix): Add pyyaml for gguf-py
    
    * chore: Remove some unused bindings
    
    * dev: Add tiktoken to -extra devShells

commit c6d4cb46559b359d2682cf2a002e7fe01bb7a767
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Sep 2 11:52:04 2024 +0300

    llama : minor style

commit 8f1d81a0b6f50b9bad72db0b6fcd299ad9ecd48c
Author: Molly Sophia <mollysophia379@gmail.com>
Date:   Sun Sep 1 22:38:17 2024 +0800

    llama : support RWKV v6 models (#8980)
    
    * convert_hf_to_gguf: Add support for RWKV v6
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * Add RWKV tokenization
    
    * Fix build
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * Do not use special tokens when matching in RWKV tokenizer
    
    * Fix model loading
    
    * Add (broken) placeholder graph builder for RWKV
    
    * Add workaround for kv cache
    
    * Add logits conversion to rwkv5
    
    * Add rwkv5 layer norms
    
    * Add time mix KVRG & correct merge mistake
    
    * Add remaining time mix parameters
    
    * Add time mix output loading
    
    * Add placeholder llm_build_time_mix
    
    * Fix build
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * Load more tensors for rwkv v6
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * Fix rwkv tokenizer
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * ggml: Add unary operator Exp
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * RWKV v6 graph building
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * Add ``rescale_every_n_layers`` parameter
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * Add ``wkv.head_size`` key for RWKV
    
    so it doesn't reuse Mamba ssm parameters
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * Fix offloading layers to CUDA
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * Fix parallel inferencing for RWKV
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * Remove trailing whitespaces
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * build_rwkv: Avoid using inplace operations
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * convert_hf_to_gguf: rwkv: Avoid using ``eval``
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * convert_hf_to_gguf: rwkv tokenizer: Don't escape sequences manually
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * Update convert_hf_to_gguf.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * ggml: Add backward computation for unary op ``exp``
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * Update convert_hf_to_gguf.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Update convert_hf_to_gguf.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Use MODEL_ARCH.RWKV6 instead of MODEL_ARCH.RWKV
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * build_rwkv6: Simplify graph
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * llama: rwkv6: Detect model.type
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * llama: rwkv6: Fix tensor loading for 7B/14B models
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * llama: rwkv6: Fix group_norm assertion failure with Metal
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * llama: rwkv6: Clean up
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * llama: rwkv6: Add quantization tensor exclusion
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * llama: rwkv6: Use the new advanced batch splits
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * Update src/llama.cpp
    
    Co-authored-by: compilade <git@compilade.net>
    
    * llama: rwkv6: Use ``ggml_norm`` instead of ``ggml_group_norm``
    
    Co-authored-by: compilade <git@compilade.net>
    
    * llama: rwkv6: Apply code style and misc changes
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * converter: Use class name ``Rwkv6Model``
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * llama: rwkv6: Make use of key ``feed_forward_length``
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * llama: rwkv6: Add kv ``time_mix_extra_dim`` and ``time_decay_extra_dim``
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * converter: Match ``new_name`` instead of ``name`` for float32 explicit tensors
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * llama: rwkv6: Keep ``time_mix_w1/w2`` as F32
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * llama: rwkv6: Remove unused nodes
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * llama: rwkv6: Apply code format changes
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * llama: rwkv6: Add lora for some supported tensors
    
    Currently att.key/receptance/value/gate/output, ffn.receptance/key/value, as well as head.weight
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    * rwkv : speed-up tokenization using trie
    
    * minor : style + indentation
    
    * llama: rwkv6: Avoid division by zero
    
    Co-authored-by: compilade <git@compilade.net>
    
    * ggml: rwkv_wkv: Avoid copying the state
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    
    ---------
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>
    Co-authored-by: Layl Bongers <3094382+LaylBongers@users.noreply.github.com>
    Co-authored-by: compilade <git@compilade.net>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit a47667cff41f5a198eb791974e0afcc1cddd3229
Author: Echo Nolan <echo@echonolan.net>
Date:   Thu Aug 22 17:19:14 2024 -0400

    nix: fix CUDA build - replace deprecated autoAddOpenGLRunpathHook
    
    The CUDA nix build broke when we updated nixpkgs in
    8cd1bcfd3fc9f2b5cbafd7fb7581b3278acec25f. As far as I can tell all
    that happened is cudaPackages.autoAddOpenGLRunpathHook got moved to
    pkgs.autoAddDriverRunpath. This commit fixes it.

commit ea5d7478b1edcfc83f8ea8f9f0934585cc0b92e3
Author: Srihari-mcw <96763064+Srihari-mcw@users.noreply.github.com>
Date:   Sat Aug 31 13:50:35 2024 +0530

    sgemm : improved Q4_0 and Q8_0 performance via 4xN and Mx4 gemm (#8908)

commit 49271efbaf3f7a4ae52c4ca299b6d4e82598a97d
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Sat Aug 31 09:50:22 2024 +0200

    llama : fix typo in xcda_array_view comment [no ci] (#9132)

commit 0ab30f8d82fc7156b750c194d64a887e80cbfb82
Author: Sutou Kouhei <kou@cozmixng.org>
Date:   Sat Aug 31 03:08:10 2024 +0900

    llama : fix llama_split_mode enum values in main_gpu document (#9057)
    
    LLAMA_SPLIT_* were renamed to LLAMA_SPLIT_MODE_* in #5697.

commit cddae4884c853b1a7ab420458236d666e2e34423
Author: 蕭澧邦 <45505768+shou692199@users.noreply.github.com>
Date:   Fri Aug 30 20:10:01 2024 +0800

    Correct typo run_llama2.sh > run-llama2.sh (#9149)

commit 7ea8d80d53364f2cdcd7bbc0f031106f529b1fe3
Author: tc-mb <157115220+tc-mb@users.noreply.github.com>
Date:   Fri Aug 30 13:21:57 2024 +0800

    llava : the function "clip" should be int (#9237)

commit 42c76d1358021ccdbe8ba89109c143dd7ae166df
Author: Faisal Zaghloul <quic_fzaghlou@quicinc.com>
Date:   Thu Aug 29 19:20:53 2024 -0400

    Threadpool: take 2 (#8672)
    
    * Introduce ggml_compute_threadpool
    
    - OpenMP functional: check
    - Vanilla ggml functional: Check
    - ggml w/threadpool functional: Check
    - OpenMP no regression: No glaring problems
    - Vanilla ggml no regression: No glaring problems
    - ggml w/threadpool no regression: No glaring problems
    
    * Minor fixes
    
    * fixed use after release bug
    
    * fixed a harmless race condition
    
    * Fix Android bulid issue
    
    * fix more race conditions
    
    * fix deadlock for cases where cgraph.n_nodes == 1
    
    and fix --poll case
    
    * threadpool: use cpu_get_num_math to set the default number of threadpool threads
    
    This way we avoid using E-Cores and Hyperthreaded siblings.
    
    * bench: create fresh threadpool for each test
    
    For benchmarking it's better to start a fresh pool for each test with the exact number of threads
    needed for that test. Having larger pools is suboptimal (causes more load, etc).
    
    * atomics: always use stdatomics with clang and use relaxed memory order when polling in ggml_barrier
    
    This also removes sched_yield() calls from ggml_barrier() to match OpenMP behavior.
    
    * threadpool: make polling the default to match openmp behavior
    
    All command line args now allow for setting poll to 0 (false).
    
    * threadpool: do not wakeup threads in already paused threadpool
    
    * fix potential race condition in check_for_work
    
    * threadpool: do not create two threadpools if their params are identical
    
    * threadpool: reduce pause/resume/wakeup overhead in common cases
    
    We now start threadpool in paused state only if we have two.
    The resume is now implicit (ie new work) which allows for reduced locking and context-switch overhead.
    
    * threadpool: add support for hybrid polling
    
    poll params (--poll, ...) now specify "polling level", i.e. how aggresively we poll before waiting on cond.var.
    poll=0 means no polling, 1 means poll for 128K rounds then wait, 2 for 256K rounds, ...
    
    The default value of 50 (ie 50x128K rounds) seems like a decent default across modern platforms.
    We can tune this further as things evolve.
    
    * threadpool: reduce the number of barrier required
    
    New work is now indicated with an atomic counter that is incremented for
    each new graph that needs to be computed.
    This removes the need for extra barrier for clearing the "new_work" and
    removes the special case for trivial graphs.
    
    * threadpool: remove special-casing for disposable threadpools
    
    With the efficient hybrid polling there is no need to make disposable pools any different.
    This simplifies the overall logic and reduces branching.
    
    Include n_threads in debug print for disposable threadpool.
    
    Declare pause and stop flags as atomic_bool
    This doesn't actually generate any memory barriers and simply informs
    the thread sanitizer that these flags can be written & read by different
    threads without locking.
    
    * threadpool: do not clear barrier counters between graphs computes (fixes race with small graphs)
    
    This fixes the race condition with very small graphs where the main thread happens to
    start a new graph while the workers are just about to exit from barriers.
    
    * threadpool: use relaxed order for chunk sync
    
    Full memory barrier is an overkill for this since each thread works on different chunk
    
    * threadpool: remove abort_callback from threadpool state
    
    * threadpool: better naming for thread/cpumask releated functions
    
    * threadpool: consistent use of int type for n_threads params
    
    * threadpool: add support for ggml_threadpool_params_default/init
    
    Also removes the need for explicit mask_specified param.
    all-zero cpumask means use default (usually inherited) cpu affinity mask.
    
    * threadpool: move typedef into ggml.h
    
    * threadpool: fix apply_priority() function name
    
    * threadpool: fix swift wrapper errors due to n_threads int type cleanup
    
    * threadpool: enable --cpu-mask and other threadpool related options only if threadpool is enabled
    
    * threadpool: replace checks for compute_thread ret code with proper status check
    
    * threadpool: simplify threadpool init logic and fix main thread affinity application
    
    Most of the init code is now exactly the same between threadpool and openmp.
    
    * threadpool: update threadpool resume/pause function names
    
    * threadpool: enable openmp by default for now
    
    * threadpool: don't forget to free workers state when omp is enabled
    
    * threadpool: avoid updating process priority on the platforms that do not require it
    
    On Windows we need to change overall process priority class in order to set thread priorities,
    but on Linux, Mac, etc we do not need to touch the overall process settings.
    
    * threadpool: update calling thread prio and affinity only at start/resume
    
    This avoids extra syscalls for each graph_compute()
    
    * llama-bench: turn threadpool params into vectors, add output headers, etc
    
    * llama-bench: add support for cool off between tests --delay
    
    This helps for long running tests on platforms that are thermally limited (phones, laptops, etc).
    --delay (disabled by default) introduces the sleep for N seconds before starting each test.
    
    * threadpool: move process priority setting into the apps (bench and cli)
    
    This avoids changing the overall process priority on Windows for the apps
    that use ggml/llama.cpp directy.
    
    * threadpool: move all pause/resume logic into ggml
    
    * threadpool: futher api cleanup and prep for future refactoring
    
    All threadpool related functions and structs use ggml_threadpool prefix.
    
    * threadpool: minor indent fixes
    
    * threadpool: improve setprioty error message
    
    * Update examples/llama-bench/llama-bench.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * threadpool: fix indent in set_threadpool call
    
    * use int32_t for n_thread type in public llama.cpp API
    
    * threadpool: use _new and _free instead of _create and _release
    
    * fix two more public APIs to use int32_t for n_threads
    
    * build: set _GNU_SOURCE for Adroid
    
    ---------
    
    Co-authored-by: Max Krasnyansky <quic_maxk@quicinc.com>
    Co-authored-by: fmz <quic_fzaghlou@quic.com>
    Co-authored-by: Max Krasnyansky <max.krasnyansky@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>

commit 9f7d4bcf5c27d37b0c7da82eeaf9c1499510554b
Author: Jan Boon <jan.boon@kaetemi.be>
Date:   Tue Aug 27 18:28:06 2024 +0800

    server : fix crash when error handler dumps invalid utf-8 json (#9195)

commit 1d1ccce67613674c75c9c7e3fa4c1e24e428ba48
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Aug 29 07:28:14 2024 +0300

    flake.lock: Update (#9162)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/c3aa7b8938b17aebd2deecf7be0636000d62a2b9?narHash=sha256-med8%2B5DSWa2UnOqtdICndjDAEjxr5D7zaIiK4pn0Q7c%3D' (2024-08-14)
      → 'github:NixOS/nixpkgs/c374d94f1536013ca8e92341b540eba4c22f9c62?narHash=sha256-Z/ELQhrSd7bMzTO8r7NZgi9g5emh%2BaRKoCdaAv5fiO0%3D' (2024-08-21)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit 9fe94ccac92693d4ae1bc283ff0574e8b3f4e765
Author: slaren <slarengh@gmail.com>
Date:   Wed Aug 28 17:28:00 2024 +0200

    docker : build images only once (#9225)

commit 66b039a5011522b6a61495eea2a9862601e169f7
Author: slaren <slarengh@gmail.com>
Date:   Wed Aug 28 13:20:36 2024 +0200

    docker : update CUDA images (#9213)

commit 20f1789dfb4e535d64ba2f523c64929e7891f428
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Aug 27 22:10:58 2024 +0300

    vulkan : fix build (#0)
    
    ggml-ci

commit 231cff5f6f1c050bcb448a8ac5857533b4c05dc7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Aug 27 22:01:45 2024 +0300

    sync : ggml

commit 3246fe84d78c8ccccd4291132809236ef477e9ea
Author: Xie Yanbo <xieyanbo@gmail.com>
Date:   Tue Aug 27 20:33:08 2024 +0800

    Fix minicpm example directory (#9111)

commit 78eb487bb0038eae95506d3d832b94c979185b09
Author: compilade <git@compilade.net>
Date:   Tue Aug 27 06:09:23 2024 -0400

    llama : fix qs.n_attention_wv for DeepSeek-V2 (#9156)

commit a77feb5d71831c61e455541e8a655b9f0337ea8c
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Aug 27 11:07:01 2024 +0200

    server : add some missing env variables (#9116)
    
    * server : add some missing env variables
    
    * add LLAMA_ARG_HOST to server dockerfile
    
    * also add LLAMA_ARG_CONT_BATCHING

commit 2e59d61c1b321431c597c1f12249273427d5640d
Author: CausalLM <148736309+CausalLM@users.noreply.github.com>
Date:   Tue Aug 27 14:58:22 2024 +0800

    llama : fix ChatGLM4 wrong shape (#9194)
    
    This should fix THUDM/glm-4-9b-chat-1m and CausalLM/miniG

commit 75e1dbbaaba5d762223370f3dab9db24a7f53465
Author: Carsten Kragelund Jørgensen <carsten@kragelund.me>
Date:   Tue Aug 27 08:53:40 2024 +0200

    llama : fix llama3.1 rope_freqs not respecting custom head_dim (#9141)
    
    * fix: llama3.1 rope_freqs not respecting custom head_dim
    
    * fix: use potential head_dim for Exaone

commit ad76569f8e78ab6ca921bda25cef25a157361719
Author: arch-btw <57669023+arch-btw@users.noreply.github.com>
Date:   Mon Aug 26 22:58:50 2024 -0700

    common : Update stb_image.h to latest version (#9161)
    
    * Update stb_image.h to latest version
    
    Fixes https://github.com/ggerganov/llama.cpp/issues/7431
    
    * Update .ecrc

commit 7d787ed96c32be18603c158ab0276992cf0dc346
Author: slaren <slarengh@gmail.com>
Date:   Mon Aug 26 19:44:43 2024 +0200

    ggml : do not crash when quantizing q4_x_x with an imatrix (#9192)

commit 06658ad7c37f440502de2b9486ce43c47b4ec710
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 26 18:31:02 2024 +0300

    metal : separate scale and mask from QKT in FA kernel (#9189)
    
    * metal : separate scale and mask from QKT in FA kernel
    
    * metal : ne01 check no longer necessary
    
    * metal : keep data in local memory

commit fc18425b6a8ad03847383ce2b69d52edfd49b0ff
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 26 17:55:36 2024 +0300

    ggml : add SSM Metal kernels (#8546)
    
    * ggml : add ggml_ssm_conv metal impl
    
    * ggml : add ssm_scan metal impl
    
    ggml-ci

commit 879275ac984235373dd44ed780d5e3a3883cb558
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 26 16:30:25 2024 +0300

    tests : fix compile warnings for unreachable code (#9185)
    
    ggml-ci

commit 7a3df798fc43e1b366146b1ad99137a9e95c87dd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 26 12:19:39 2024 +0300

    ci : add VULKAN support to ggml-ci (#9055)

commit e5edb210cd361689da41f032f1b36c45ff1bf9be
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 26 12:16:57 2024 +0300

    server : update deps (#9183)

commit 0c41e03ceba1aafaf469476a56347419d5785376
Author: slaren <slarengh@gmail.com>
Date:   Mon Aug 26 11:08:59 2024 +0200

    metal : gemma2 flash attention support (#9159)

commit f12ceaca0c7b59def30b7a832a6904df7ed3f4f7
Author: slaren <slarengh@gmail.com>
Date:   Mon Aug 26 11:03:30 2024 +0200

    ggml-ci : try to improve build time (#9160)

commit 436787f170329b3f549e6c2c46593d2af8482e7c
Author: Justine Tunney <jtunney@mozilla.com>
Date:   Sun Aug 25 23:09:53 2024 -0700

    llama : fix time complexity of string replacement (#9163)
    
    This change fixes a bug where replacing text in a very long string could
    cause llama.cpp to hang indefinitely. This is because the algorithm used
    was quadratic, due to memmove() when s.replace() is called in a loop. It
    seems most search results and LLM responses actually provide the O(n**2)
    algorithm, which is a great tragedy. Using a builder string fixes things

commit 93bc3839f980ff14be86efe408b4cd7e89b26835
Author: Herman Semenov <GermanAizek@yandex.ru>
Date:   Sun Aug 25 22:54:37 2024 +0000

    common: fixed not working find argument --n-gpu-layers-draft (#9175)

commit f91fc5639be1e272194303ea26e1d4c226ec981b
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Aug 25 22:11:48 2024 +0200

    CUDA: fix Gemma 2 numerical issues for FA (#9166)

commit e11bd856d538e44d24d8cad4b0381fba0984d162
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Aug 24 21:34:59 2024 +0200

    CPU/CUDA: Gemma 2 FlashAttention support (#8542)
    
    * CPU/CUDA: Gemma 2 FlashAttention support
    
    * apply logit_softcap to scale in kernel
    
    * disable logit softcapping tests on Metal
    
    * remove metal check

commit 8f824ffe8ee1feadd14428f1dda1283fa3b933be
Author: João Dinis Ferreira <hello@joaof.eu>
Date:   Sat Aug 24 07:22:45 2024 +0100

    quantize : fix typo in usage help of `quantize.cpp` (#9145)

commit 3ba780e2a8f0ffe13f571b27f0bbf2ca5a199efc
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Aug 23 12:58:53 2024 +0200

    lora : fix llama conversion script with ROPE_FREQS (#9117)

commit a07c32ea54850c989f0ef6989da5b955b77b7172
Author: piDack <104877312+piDack@users.noreply.github.com>
Date:   Fri Aug 23 15:27:17 2024 +0800

    llama : use F32 precision in GLM4 attention and no FA (#9130)

commit 11b84eb4578864827afcf956db5b571003f18180
Author: Akarshan Biswas <akarshan.biswas@gmail.com>
Date:   Thu Aug 22 19:39:47 2024 +0530

    [SYCL] Add a space to supress a cmake warning (#9133)

commit 1731d4238f9e4f925a750810e7f5480827c66dcf
Author: luoyu-intel <yu.luo@intel.com>
Date:   Thu Aug 22 12:50:10 2024 +0800

    [SYCL] Add oneDNN primitive support (#9091)
    
    * add onednn
    
    * add sycl_f16
    
    * add dnnl stream
    
    * add engine map
    
    * use dnnl for intel only
    
    * use fp16fp16fp16
    
    * update doc

commit a1631e53f6763e17da522ba219b030d8932900bd
Author: compilade <git@compilade.net>
Date:   Wed Aug 21 17:58:11 2024 -0400

    llama : simplify Mamba with advanced batch splits (#8526)
    
    * llama : advanced batch splits
    
    This includes equal-sequence-length batch splits which are useful
    to simplify recurrent model operators.
    
    * llama : always make recurrent state slots contiguous
    
    * ggml : simplify mamba operators
    
    * llama : fix integer signedness mixing
    
    * llama : logits_all has priority over batch->logits
    
    Otherwise, the server embeddings tests failed.
    This was likely an existing problem but was only detected here
    because of an additional assertion.
    
    * llama : apply suggestions
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * llama : fix t5 segfault
    
    * llama : fix Mamba session save and restore
    
    * llama : minor cosmetic changes
    
    * llama : rename llama_reorder_outputs to llama_output_reorder
    
    Also move it closer to llama_output_reserve.
    
    * llama : fix pooled embeddings when using batches with equal_seqs
    
    * minor : add struct members for clarity
    
    ggml-ci
    
    * llama : fix T5 segfault again
    
    * llama : fix Mamba pooled embeddings with multiple sequences
    
    Until the pooled embeddings are refactored to allow splitting
    across ubatches for causal embeddings,
    recurrent models can only process a single sequence per ubatch
    when calculating pooled embeddings.
    
    * llama : add llama_model_is_recurrent to simplify figuring that out
    
    This will make it easier to more cleanly support RWKV-v6 and Mamba-2.
    
    * llama : fix simple splits when the batch contains embeddings
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit fc54ef0d1c138133a01933296d50a36a1ab64735
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed Aug 21 11:04:34 2024 +0200

    server : support reading arguments from environment variables (#9105)
    
    * server : support reading arguments from environment variables
    
    * add -fa and -dt
    
    * readme : specify non-arg env var

commit b40eb84895bf723c7b327a1e3bf6e0e2c41877f8
Author: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>
Date:   Wed Aug 21 12:06:36 2024 +0400

    llama : support for `falcon-mamba` architecture (#9074)
    
    * feat: initial support for llama.cpp
    
    * fix: lint
    
    * refactor: better refactor
    
    * Update src/llama.cpp
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Update src/llama.cpp
    
    Co-authored-by: compilade <git@compilade.net>
    
    * fix: address comments
    
    * Update convert_hf_to_gguf.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * fix: add more cleanup and harmonization
    
    * fix: lint
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * fix: change name
    
    * Apply suggestions from code review
    
    Co-authored-by: compilade <git@compilade.net>
    
    * add in operator
    
    * fix: add `dt_b_c_rms` in `llm_load_print_meta`
    
    * fix: correct printf format for bool
    
    * fix: correct print format
    
    * Update src/llama.cpp
    
    Co-authored-by: compilade <git@compilade.net>
    
    * llama : quantize more Mamba tensors
    
    * llama : use f16 as the fallback of fallback quant types
    
    ---------
    
    Co-authored-by: compilade <git@compilade.net>

commit f63f603c879c2232eaeded8c0aeba4244471d720
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Wed Aug 21 09:45:49 2024 +0200

    llava : zero-initialize clip_ctx structure fields with aggregate initialization 908)
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

commit 8455340b874aa90d5f70104c99ed2778d9e31c36
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed Aug 21 09:32:58 2024 +0200

    llama : std::move llm_bigram_bpe from work_queue (#9062)
    
    * llama : std::move llm_bigram_bpe from work_queue
    
    This commit updates the retrieval of llm_bigram_bpe objects from
    work_queue.top() by using std::move.
    
    The motivation for this is to avoid the copying of the std::string
    `text` member of the llm_bigram_bpe struct.
    
    * squash! llama : std::move llm_bigram_bpe from work_queue
    
    Introduced a MovablePriorityQueue class to allow moving elements
    out of the priority queue for llm_bigram_bpe.
    
    * squash! llama : std::move llm_bigram_bpe from work_queue
    
    Rename MovablePriorityQueue to lama_priority_queue.
    
    * squash! llama : std::move llm_bigram_bpe from work_queue
    
    Rename lama_priority_queue -> llama_priority_queue.

commit 2f3c1466ff46a2413b0e363a5005c46538186ee6
Author: Changyeon Kim <cyzero.kim@samsung.com>
Date:   Wed Aug 21 04:00:00 2024 +0900

    llava: Add ACC OP for GPU acceleration to the Vulkan backend in the LLAVA CLIP model. (#8984)
    
    * llava: Add ACC OP for GPU acceleration to the Vulkan backend in the LLAVA CLIP model.
    
    - The CLIP model now prioritizes the Vulkan backend over the CPU when vulkan available.
    - A GGML_OP_ACC shader has been added.
    - The encoding performance of the CLIP model improved from 4.2s on the CPU to 0.9s on the GPU.
    
    Signed-off-by: Changyeon Kim <cyzero.kim@samsung.com>
    
    * fix-up coding style.
    
    Signed-off-by: Changyeon Kim <cyzero.kim@samsung.com>
    
    * Fix-up the missing initial parameter to resolve the compilation warning.
    
    Signed-off-by: Changyeon Kim <cyzero.kim@samsung.com>
    
    * [fix] Add missing parameters.
    
    Signed-off-by: Changyeon Kim <cyzero.kim@samsung.com>
    
    * [fix] Use nb1 and nb2 for dst.
    
    Signed-off-by: Changyeon Kim <cyzero.kim@samsung.com>
    
    * Fix check results ggml_acc call
    
    ---------
    
    Signed-off-by: Changyeon Kim <cyzero.kim@samsung.com>
    Co-authored-by: 0cc4m <picard12@live.de>

commit 50addec9a532a6518146ab837a85504850627316
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Tue Aug 20 23:50:17 2024 +0800

    [SYCL] fallback mmvq (#9088)
    
    * fallback mmvq to mul_mat
    
    * mmvq in cuda path
    
    * Update ggml/src/ggml-sycl.cpp
    
    Co-authored-by: Alberto Cabrera Pérez <alberto.cabrera@codeplay.com>
    
    ---------
    
    Co-authored-by: Alberto Cabrera Pérez <alberto.cabrera@codeplay.com>

commit 4f8d19ff17faa601f456ee1db7d8b8a15fa3f90b
Author: zhentaoyu <zhentao.yu@intel.com>
Date:   Tue Aug 20 23:06:51 2024 +0800

    [SYCL] Fix SYCL `im2col` and `convert` Overflow with Large Dims (#9052)
    
    * sycl: fix im2col overflow and sync with cuda
    
    Signed-off-by: zhentaoyu <zhentao.yu@intel.com>
    
    * sycl: fix convert overflow
    
    Signed-off-by: zhentaoyu <zhentao.yu@intel.com>
    
    * sycl: fix convert and dequantize
    
    Signed-off-by: zhentaoyu <zhentao.yu@intel.com>
    
    * sycl: fix ib in dmmv
    
    Signed-off-by: zhentaoyu <zhentao.yu@intel.com>
    
    * sycl:refine convert
    
    Signed-off-by: zhentaoyu <zhentao.yu@intel.com>
    
    * sycl: move downsample global_range into common
    
    Signed-off-by: zhentaoyu <zhentao.yu@intel.com>
    
    * test: add im2col and convert test cases
    
    Signed-off-by: zhentaoyu <zhentao.yu@intel.com>
    
    * test: make new cases only in sycl
    
    Signed-off-by: zhentaoyu <zhentao.yu@intel.com>
    
    * test: comment new test_cases for only local testing
    
    Signed-off-by: zhentaoyu <zhentao.yu@intel.com>
    
    ---------
    
    Signed-off-by: zhentaoyu <zhentao.yu@intel.com>

commit 90db8146d56d83a605f2c475eca39bcda29cf16d
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Tue Aug 20 11:09:55 2024 +0200

    tests : add missing comma in grammar integration tests (#9099)
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

commit cfac111e2b3953cdb6b0126e67a2487687646971
Author: wangshuai09 <391746016@qq.com>
Date:   Mon Aug 19 16:46:38 2024 +0800

    cann: add doc for cann backend (#8867)
    
    Co-authored-by: xuedinge233 <damow890@gmail.com>
    Co-authored-by: hipudding <huafengchun@gmail.com>

commit 1b6ff90ff8301d9fe2027be2bb9fea26177d775e
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Mon Aug 19 10:11:45 2024 +0300

    rpc : print error message when failed to connect endpoint (#9042)

commit 18eaf29f4c5f7c8e89412f3a9a0392a9e4e01d75
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Mon Aug 19 10:10:21 2024 +0300

    rpc : prevent crashes on invalid input (#9040)
    
    Add more checks which prevent RPC server from crashing if invalid input
    is received from client

commit 554b049068de24201d19dde2fa83e35389d4585d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Aug 18 17:43:32 2024 +0300

    flake.lock: Update (#9068)

commit 2339a0be1c8e31fcf4531427183b94f2ef019e56
Author: ltoniazzi <61414566+ltoniazzi@users.noreply.github.com>
Date:   Sun Aug 18 10:58:04 2024 +0100

    tests : add integration test for lora adapters (#8957)
    
    * Add printing to check weights match torch version
    
    * minor code style changes
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <son@huggingface.co>

commit 2fb9267887d24a431892ce4dccc75c7095b0d54d
Author: Yoshi Suhara <ysuhara@nvidia.com>
Date:   Sat Aug 17 06:34:21 2024 -0700

    Fix incorrect use of ctx_split for bias tensors (#9063)

commit 8b3befc0e2ed8fb18b903735831496b8b0c80949
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Aug 16 17:19:05 2024 +0200

    server : refactor middleware and /health endpoint (#9056)
    
    * server : refactor middleware and /health endpoint
    
    * move "fail_on_no_slot" to /slots
    
    * Update examples/server/server.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * fix server tests
    
    * fix CI
    
    * update server docs
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit d565bb2fd5a2a58b9924a7a34e77a87c78c52137
Author: tc-mb <157115220+tc-mb@users.noreply.github.com>
Date:   Fri Aug 16 21:34:41 2024 +0800

    llava : support MiniCPM-V-2.6 (#8967)
    
    * init
    
    * rename
    
    * add run android for termux in readme
    
    * add android readme
    
    * add instructions in readme
    
    * change name in readme
    
    * Update README.md
    
    * fixed line
    
    * add result in readme
    
    * random pos_embed
    
    * add positions index
    
    * change for ollama
    
    * change for ollama
    
    * better pos_embed in clip
    
    * support ollama
    
    * updata cmakelist
    
    * updata cmakelist
    
    * rename wrapper
    
    * clear code
    
    * replace and organize code
    
    * add link
    
    * sync master
    
    * fix warnings
    
    * fix warnings
    
    * fix bug in bicubic resize when need resize iamge smaller
    
    * receive review comments and modify
    
    * receive review comments and modify
    
    * put all code into llava dir
    
    * fix quality problem in pr code
    
    * change n_layer
    
    * add space in "-1"
    
    * imitate reshape bug of python code
    
    * fix bug in clip
    
    * fix issues for merging
    
    * fix llama-minicpmv-cli in cmake file
    
    * change pr readme
    
    * fix code review
    
    * remove in line 33 directory in the /cmakelists.txt (not in example, in the main dir
    
    * fix cmakefile
    
    * add warn
    
    * fix KEY_HAS_MINICPMV_PROJ
    
    * remove load_image_size into clip_ctx
    
    * remove the extern "C", MINICPMV_API
    
    * fix uhd code for review comment
    
    * delete minicpmv-wrapper in pr
    
    * remove uhd_image_embed
    
    * Modify 2 notes
    
    * support minicpmv2.6
    
    * modify convert script of minicpmv
    
    * modify convert
    
    * modify convert
    
    * add readme
    
    * add resampler of v2.6
    
    * modify clip
    
    * modify readme
    
    * fix type-check
    
    * fix type-check
    
    * fix type-check
    
    * fix type-check
    
    * modify convert script and readme
    
    * fix convert script and readme
    
    * fix convert
    
    * fix num in convert
    
    * fix type-check
    
    ---------
    
    Co-authored-by: Hongji Zhu <fireyoucan@gmail.com>
    Co-authored-by: harvestingmoon <leewenyeong@gmail.com>

commit ee2984bdaf10c14d440ad873a049bcc09b786d9b
Author: Farbod Bijary <110523279+farbodbj@users.noreply.github.com>
Date:   Fri Aug 16 14:06:30 2024 +0330

    py : fix wrong input type for raw_dtype in ggml to gguf scripts (#8928)
    
    Co-authored-by: farbod <farbod.bjary82@gmail.com>

commit c8ddce85606d9fb6e30745b6e4fe103eecadc73f
Author: Aisuko <urakiny@gmail.com>
Date:   Fri Aug 16 19:08:59 2024 +1000

    Fix inference example lacks required parameters (#9035)
    
    Signed-off-by: Aisuko <urakiny@gmail.com>

commit 23fd4535445e3e859ded2f02d6142b6e93b43890
Author: compilade <git@compilade.net>
Date:   Fri Aug 16 02:36:11 2024 -0400

    gguf-py : bump version from 0.9.1 to 0.10.0 (#9051)

commit c679e0cb5c378bfb63643c44a453345ba8b90126
Author: Minsoo Cheong <icycle0409@snu.ac.kr>
Date:   Fri Aug 16 15:35:18 2024 +0900

    llama : add EXAONE model support (#9025)
    
    * add exaone model support
    
    * add chat template
    
    * fix whitespace
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * add ftype
    
    * add exaone pre-tokenizer in `llama-vocab.cpp`
    
    Co-Authored-By: compilade <113953597+compilade@users.noreply.github.com>
    
    * fix lint
    
    Co-Authored-By: compilade <113953597+compilade@users.noreply.github.com>
    
    * add `EXAONE` to supported models in `README.md`
    
    * fix space
    
    Co-authored-by: compilade <git@compilade.net>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: compilade <113953597+compilade@users.noreply.github.com>
    Co-authored-by: compilade <git@compilade.net>

commit fb487bb5671b37267f35ff43fe8849ddccd925cd
Author: Liu Jia <jia3.liu@intel.com>
Date:   Fri Aug 16 14:23:12 2024 +0800

    common : add support for cpu_get_num_physical_cores() on Windows (#8771)
    
    * Add support for cpu_get_num_phsical_cores() on Windows
    
    * fix build bug on msys2-clang64 and ucrt64
    
    * avoid adding new function
    
    * add new macros to avoid windows+mingw64
    
    * Add error checking to return default value

commit 2a24c8caa6d10a7263ca317fa7cb64f0edc72aae
Author: Yoshi Suhara <y.suhara@gmail.com>
Date:   Thu Aug 15 19:23:33 2024 -0700

    Add Nemotron/Minitron GGUF Conversion & Inference Support (#8922)
    
    * Add nemotron GGUF conversion & inference support
    
    * Fix formatting issues
    
    * Remove unnecessary write_tensors()
    
    * Update convert_hf_to_gguf.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Update src/llama.cpp
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Address comments by @compilade
    
    * Replace ggml_mul_mat()->llm_build_lora_mm()
    
    * Remove mutable variable
    
    * Use  for bias tensors
    
    * Cover corner case for role_scaling not in config.json
    
    ---------
    
    Co-authored-by: compilade <git@compilade.net>

commit e3f6fd56b1ab3c426596217d786910d641ae6ce0
Author: Nico Bosshard <nico@bosshome.ch>
Date:   Fri Aug 16 04:22:55 2024 +0200

    ggml : dynamic ggml_sched_max_splits based on graph_size (#9047)
    
    * ggml : Dynamic ggml_sched_max_splits based on graph_size
    
    * Fixed and readded debug code for causes

commit 4b9afbbe9037f8a2d659097c0c7d9fce32c6494c
Author: gtygo <gtydoit@gmail.com>
Date:   Thu Aug 15 15:40:12 2024 +0800

    retrieval : fix memory leak in retrieval query handling (#8955)
    
    * retrieval
    
    * Reuse querybatch to reduce frequent memory allocation
    
    * delete unused white space

commit 37501d9c79ed5897db4b73ea7502211d25b3f763
Author: Riceball LEE <snowyu.lee@gmail.com>
Date:   Thu Aug 15 15:28:05 2024 +0800

    server : fix duplicated n_predict key in the generation_settings (#8994)

commit 4af8420afba39de4968adf2695ce12fd42422a13
Author: Zhenwei Jin <109658203+kylo5aby@users.noreply.github.com>
Date:   Thu Aug 15 15:23:23 2024 +0800

    common : remove duplicate function llama_should_add_bos_token (#8778)

commit 6bda7ce6c3a9284fcbb70c1ace4107db8eb63e5c
Author: Esko Toivonen <eskot98@gmail.com>
Date:   Thu Aug 15 10:17:12 2024 +0300

    llama : add pre-tokenizer regexes for BLOOM and gpt3-finnish (#8850)

commit d5492f0525fa533817a67e93a4bde9d71d81cf58
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Aug 15 10:11:11 2024 +0300

    ci : disable bench workflow (#9010)

commit 234b30676a97ce227b604c38beb9dcaca406dea9
Author: Jiří Podivín <66251151+jpodivin@users.noreply.github.com>
Date:   Thu Aug 15 08:21:57 2024 +0200

    server : init stop and error fields of the result struct (#9026)
    
    Signed-off-by: Jiri Podivin <jpodivin@redhat.com>

commit 5fd89a70ead34d1a17015ddecad05aaa2490ca46
Author: 0cc4m <picard12@live.de>
Date:   Wed Aug 14 18:32:53 2024 +0200

    Vulkan Optimizations and Fixes (#8959)
    
    * Optimize Vulkan REPEAT performance
    
    * Use Vulkan GLSL fused multiply-add instruction where possible
    
    * Add GGML_VULKAN_PERF option to output performance data per operator
    
    * Rework and fix Vulkan descriptor set and descriptor pool handling
    
    * Fix float32 concat f16 shader validation error
    
    * Add Vulkan GROUP_NORM eps parameter
    
    * Fix validation error with transfer queue memory barrier flags
    
    * Remove trailing whitespaces

commit 98a532d474c73d3494a5353024cb6a4fbbabbb35
Author: compilade <git@compilade.net>
Date:   Wed Aug 14 02:51:02 2024 -0400

    server : fix segfault on long system prompt (#8987)
    
    * server : fix segfault on long system prompt
    
    * server : fix parallel generation with very small batch sizes
    
    * server : fix typo in comment

commit 43bdd3ce188cd247bda7c1bd1ad01fa64e566690
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Aug 14 09:14:49 2024 +0300

    cmake : remove unused option GGML_CURL (#9011)

commit 06943a69f678fb32829ff06d9c18367b17d4b361
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Aug 13 21:13:15 2024 +0200

    ggml : move rope type enum to ggml.h (#8949)
    
    * ggml : move rope type enum to ggml.h
    
    This commit moves the `llama_rope_type` enum from `llama.h` to
    `ggml.h` and changes its name to `ggml_rope_type`.
    
    The motivation for this change is to address the TODO in `llama.h` and
    use the enum in ggml.
    
    Note: This commit does not change the `mode` parameter to be of type
    `enum ggml_rope_type`. The name `mode` and its usage suggest that it
    might be more generic and possibly used as a bit field for multiple
    flags. Further investigation/discussion may be needed to determine
    if `mode` should be restricted to RoPE types.
    
    * squash! ggml : move rope type enum to ggml.h
    
    This commit removes GGML_ROPE_TYPE_NONE and GGML_ROPE_TYPE_GLM from
    ggml.h, and back the llama_rope_type enum.
    
    I've kept the assert for GGML_ROPE_TYPE_GLM as I'm not sure if it is
    safe to remove it yet.
    
    * squash! ggml : move rope type enum to ggml.h
    
    This commit removes the enum ggml_rope_type from ggml.h and replaces it
    with a define (GGML_ROPE_TYPE_NEOX). This define is used in the code to
    check if the mode is set to GPT-NeoX. Also the enum llama_rope_type has
    been updated to reflect this change.
    
    * squash! ggml : move rope type enum to ggml.h
    
    This commit contains a suggestion enable the GGML_ROPE_TYPE_NEOX
    macro/define to be passed to the shader compiler.
    
    * squash! ggml : move rope type enum to ggml.h
    
    This commit fixes the editorconfig-checker warnings.
    
    * squash! ggml : move rope type enum to ggml.h
    
    Update comment for ggml_rope function.
    
    * Revert "squash! ggml : move rope type enum to ggml.h"
    
    This reverts commit 6261222bd0dc0efd51f0fb0435ad3f16a5b52fd6.
    
    * squash! ggml : move rope type enum to ggml.h
    
    Add GGML_ROPE_TYPE_NEOX to rope_common.comp.
    
    * remove extra line
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 828d6ff7d796f48b2c345f6be2805a3c531a089c
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Aug 13 11:41:14 2024 +0200

    export-lora : throw error if lora is quantized (#9002)

commit fc4ca27b25464a11b3b86c9dbb5b6ed6065965c2
Author: Diogo Teles Sant'Anna <diogoteles@google.com>
Date:   Mon Aug 12 13:28:23 2024 -0300

    ci : fix github workflow vulnerable to script injection (#9008)
    
    Signed-off-by: Diogo Teles Sant'Anna <diogoteles@google.com>

commit 1f67436c5ee6f4c99e71a8518bdfc214c27ce934
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Mon Aug 12 19:17:03 2024 +0300

    ci : enable RPC in all of the released builds (#9006)
    
    ref: #8912

commit 0fd93cdef5e583aa980b3c0d693c0d207f0787a7
Author: Nico Bosshard <nico@bosshome.ch>
Date:   Mon Aug 12 17:13:59 2024 +0200

    llama : model-based max number of graph nodes calculation (#8970)
    
    * llama : model-based max number of graph nodes calculation
    
    * Update src/llama.cpp
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 84eb2f4fad28ceadd415a4e775320c983f4d9a7d
Author: Frank Mai <thxcode0824@gmail.com>
Date:   Mon Aug 12 20:45:50 2024 +0800

    docs: introduce gpustack and gguf-parser (#8873)
    
    * readme: introduce gpustack
    
    GPUStack is an open-source GPU cluster manager for running large
    language models, which uses llama.cpp as the backend.
    
    Signed-off-by: thxCode <thxcode0824@gmail.com>
    
    * readme: introduce gguf-parser
    
    GGUF Parser is a tool to review/check the GGUF file and estimate the
    memory usage without downloading the whole model.
    
    Signed-off-by: thxCode <thxcode0824@gmail.com>
    
    ---------
    
    Signed-off-by: thxCode <thxcode0824@gmail.com>

commit 1262e7ed13ac197c944f15e1ddb083cb4f36cf65
Author: DavidKorczynski <david@adalogics.com>
Date:   Mon Aug 12 13:36:41 2024 +0100

    grammar-parser : fix possible null-deref (#9004)
    
    Fixes: https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=70680
    
    Signed-off-by: David Korczynski <david@adalogics.com>

commit df5478fbea7e652cfad4ee7974ac3b624fd6c7f6
Author: DavidKorczynski <david@adalogics.com>
Date:   Mon Aug 12 13:21:41 2024 +0100

    ggml: fix div-by-zero (#9003)
    
    Fixes: https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=70724
    
    In order to access the above bug you need to login using one of the
    emails in
    https://github.com/google/oss-fuzz/blob/master/projects/llamacpp/project.yaml#L3-L5
    
    Signed-off-by: David Korczynski <david@adalogics.com>

commit 2589292cde038ba876c041bcd7b3f0c81f3f11fe
Author: Liu Jia <jia3.liu@intel.com>
Date:   Mon Aug 12 17:46:03 2024 +0800

    Fix a spelling mistake (#9001)

commit d3ae0ee8d75033921a076131d4d0fa1c6ec579a7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 12 11:02:01 2024 +0300

    py : fix requirements check '==' -> '~=' (#8982)
    
    * py : fix requirements check '==' -> '~='
    
    * cont : fix the fix
    
    * ci : run on all requirements.txt

commit 5ef07e25ac39e62297a67208c5bcced50835a2dd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 12 10:21:50 2024 +0300

    server : handle models with missing EOS token (#8997)
    
    ggml-ci

commit 4134999e01f31256b15342b41c4de9e2477c4a6c
Author: compilade <git@compilade.net>
Date:   Sun Aug 11 14:45:41 2024 -0400

    gguf-py : Numpy dequantization for most types (#8939)
    
    * gguf-py : Numpy dequantization for most types
    
    * gguf-py : Numpy dequantization for grid-based i-quants

commit 8cd1bcfd3fc9f2b5cbafd7fb7581b3278acec25f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Aug 11 16:58:58 2024 +0300

    flake.lock: Update (#8979)

commit a21c6fd45032a20180e026773582d21294c85619
Author: Neo Zhang <zhang.jianyu@outlook.com>
Date:   Sun Aug 11 16:37:43 2024 +0800

    update guide (#8909)
    
    Co-authored-by: Neo Zhang <>

commit 33309f661a93c9c0ab65a79e5e7e30fa6162992e
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Sun Aug 11 10:35:26 2024 +0200

    llama : check all graph nodes when searching for result_embd_pooled (#8956)
    
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

commit 7c5bfd57f83fd3630934cfa70892aa4022d3faf7
Author: Markus Tavenrath <mtavenrath@users.noreply.github.com>
Date:   Sun Aug 11 10:09:09 2024 +0200

    Optimize Vulkan backend for better CPU performance and less GPU synchronization overhead. (#8943)
    
    * Optimize Vulkan backend for better CPU performance and less GPU synchronization overhead.
    
    - Allocation overhead for the temporary std::vectors was easily detectable with a sampling profiler and simple to remove.
    - ggml_vk_sync_buffer introduce a full pipeline sync which has a significant cost on the GPU side, sometimes larger than the actual kernel execution. Adding only barriers for shader read/writes and transfers seems to be sufficient looking at the code which either launches compute kernels or copies tensors.
    
    * Fix small typo
    
    ---------
    
    Co-authored-by: 0cc4m <picard12@live.de>

commit 6e02327e8b7837358e0406bf90a4632e18e27846
Author: slaren <slarengh@gmail.com>
Date:   Sat Aug 10 15:42:10 2024 +0200

    metal : fix uninitialized abort_callback (#8968)

commit 7eb23840ed0f388e10c4bbc4d65802fdfb977b40
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat Aug 10 13:04:40 2024 +0200

    llama : default n_swa for phi-3 (#8931)
    
    * default n_swa for phi-3
    
    * fix
    
    * double check swa

commit 7c3f55c10051c634546247387c5c359c9d499360
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Sat Aug 10 11:43:26 2024 +0200

    Add support for encoder-only T5 models (#8900)
    
    * gguf-py : add T5ENCODER model architecture
    
    * common : call llama_decode() during warmup only if the model has decoder
    
    * convert-hf : add T5EncoderModel
    
    * llama : add llama_model_has_decoder() API function
    
    * llama : split build_t5() into build_t5_encoder() and build_t5_decoder()
    
    * llama : add support for LLM_ARCH_T5ENCODER
    
    * llama-embedding : add support for LLAMA_POOLING_TYPE_NONE
    
    * llama-embedding : add support for encoder-only models
    
    ---------
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

commit 911b437f228e75aa3d235acec21bfddd23ecce2f
Author: Matteo Mortari <matteo.mortari@gmail.com>
Date:   Sat Aug 10 07:58:49 2024 +0200

    gguf-py : fix double call to add_architecture() (#8952)
    
    Signed-off-by: tarilabs <matteo.mortari@gmail.com>

commit b72942fac998672a79a1ae3c03b340f7e629980b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Aug 9 23:03:21 2024 +0300

    Merge commit from fork

commit 6afd1a99dc9792096d4567ab9fa1ad530c81c6cd
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Fri Aug 9 18:53:09 2024 +0200

    llama : add support for lora adapters in T5 model (#8938)
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

commit 272e3bd95e620d285b2fb9faaa7b6b1b8edbbf3a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Aug 9 18:24:30 2024 +0300

    make : fix llava obj file race (#8946)
    
    ggml-ci

commit 45a55b91aa87958a75d691be64b070266d4fbb94
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Aug 9 18:23:52 2024 +0300

    llama : better replace_all (cont) (#8926)
    
    * llama : better replace_all (cont)
    
    ggml-ci
    
    * code : deduplicate replace_all
    
    ggml-ci

commit 3071c0a5f218f107dabd13b73f6090af683ef5ec
Author: tc-mb <157115220+tc-mb@users.noreply.github.com>
Date:   Fri Aug 9 18:33:53 2024 +0800

    llava : support MiniCPM-V-2.5 (#7599)
    
    * init
    
    * rename
    
    * add run android for termux in readme
    
    * add android readme
    
    * add instructions in readme
    
    * change name in readme
    
    * Update README.md
    
    * fixed line
    
    * add result in readme
    
    * random pos_embed
    
    * add positions index
    
    * change for ollama
    
    * change for ollama
    
    * better pos_embed in clip
    
    * support ollama
    
    * updata cmakelist
    
    * updata cmakelist
    
    * rename wrapper
    
    * clear code
    
    * replace and organize code
    
    * add link
    
    * sync master
    
    * fix warnings
    
    * fix warnings
    
    * fix bug in bicubic resize when need resize iamge smaller
    
    * receive review comments and modify
    
    * receive review comments and modify
    
    * put all code into llava dir
    
    * fix quality problem in pr code
    
    * change n_layer
    
    * add space in "-1"
    
    * imitate reshape bug of python code
    
    * fix bug in clip
    
    * fix issues for merging
    
    * fix llama-minicpmv-cli in cmake file
    
    * change pr readme
    
    * fix code review
    
    * remove in line 33 directory in the /cmakelists.txt (not in example, in the main dir
    
    * fix cmakefile
    
    * add warn
    
    * fix KEY_HAS_MINICPMV_PROJ
    
    * remove load_image_size into clip_ctx
    
    * remove the extern "C", MINICPMV_API
    
    * fix uhd code for review comment
    
    * delete minicpmv-wrapper in pr
    
    * remove uhd_image_embed
    
    * Modify 2 notes
    
    * clip : style changes
    
    * del common.h in clip
    
    * fix Type-Check error
    
    * fix Type-Check error
    
    * fix Type-Check error
    
    * fix Type-Check error
    
    * fix makefile error
    
    * fix ubuntu-make error
    
    * try fix clip
    
    * try fix 1
    
    ---------
    
    Co-authored-by: Hongji Zhu <fireyoucan@gmail.com>
    Co-authored-by: harvestingmoon <leewenyeong@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 4305b57c80eff4f0df5f6acb60b292f03b8f0dd0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Aug 9 10:03:48 2024 +0300

    sync : ggml

commit 70c0ea35609a2ab87a358e25f4ffad1aad408992
Author: Matt Stephenson <mstephenson6@users.noreply.github.com>
Date:   Tue Jul 16 03:21:09 2024 -0400

    whisper : use vulkan as gpu backend when available (whisper/2302)
    
    * ggml: use vulkan as gpu backend when available
    
    Signed-off-by: Matt Stephenson <mstephenson6@users.noreply.github.com>
    
    * whisper: enable using vk as default buffer type
    
    Signed-off-by: Matt Stephenson <mstephenson6@users.noreply.github.com>
    
    ---------
    
    Signed-off-by: Matt Stephenson <mstephenson6@users.noreply.github.com>

commit 5b2c04f4925e152c362b824f4b41eb2a081fa623
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Aug 9 08:33:30 2024 +0200

    embedding : add --pooling option to README.md [no ci] (#8934)
    
    This commit adds the `--pooling` option to the README.md file in the
    `examples/embedding` directory.
    
    The motivation for adding this options is that currently if the model
    used does not specify a pooling type the embedding example will fail
    with the following error message:
    ```console
    main: error: pooling type NONE not supported
    ```
    
    This commit also updates the name of the executable in the examples
    section.

commit 6f6496bb0999d1bce5daff0cfc55ceb0dd13c888
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Aug 9 08:32:23 2024 +0200

    llama : fix typo in llama_tensor_get_type comment [no ci] (#8937)

commit daef3ab233fc02e4c53afd9b07366379876f00d1
Author: Mathieu Geli <mathieu.geli@gmail.com>
Date:   Fri Aug 9 08:32:02 2024 +0200

    server : add one level list nesting for embeddings (#8936)

commit 345a686d8271a24db20d31789c0d4b9ed51dcb0c
Author: compilade <git@compilade.net>
Date:   Thu Aug 8 23:54:00 2024 -0400

    llama : reduce useless copies when saving session (#8916)
    
    * llama : avoid useless copies in dummy session writer
    
    * llama : avoid double tensor copy when saving session to buffer

commit 3a14e00366399040a139c67dd5951177a8cb5695
Author: compilade <git@compilade.net>
Date:   Thu Aug 8 13:33:09 2024 -0400

    gguf-py : simplify support for quant types (#8838)
    
    * gguf-py : use classes for quants
    
    * convert_hf : simplify internal quantization type selection
    
    * gguf-py : fix flake8 lint
    
    * gguf-py : fix BF16 numpy view type
    
    * gguf-py : remove LlamaFileTypeMap
    
    Too specific to 'llama.cpp', and would be a maintenance burden
    to keep up to date.
    
    * gguf-py : add generic quantize and dequantize functions
    
    The quant classes no longer need to be known,
    only the target or the source type,
    for 'quantize' and 'dequantize', respectively.

commit afd27f01fe832ece3d07ef03b7d34a9e80c4a895
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Aug 8 14:56:52 2024 +0300

    scripts : sync cann files (#0)

commit 366d486c163ea883442313cff1a3b154ab93e168
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Aug 8 14:40:12 2024 +0300

    scripts : fix sync filenames (#0)

commit e44a561ab090b11d3a6fe539b768404663e4c3d2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Aug 8 13:19:47 2024 +0300

    sync : ggml

commit f93d49ab1e2d1d698af8f17ead6d635405d4b289
Author: Borislav Stanimirov <b.stanimirov@abv.bg>
Date:   Wed Aug 7 10:00:56 2024 +0300

    ggml : ignore more msvc warnings (ggml/906)

commit 5b33ea1ee72fadfa4f96d86dc614631739758cce
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Aug 7 09:57:00 2024 +0300

    metal : fix struct name (ggml/912)
    
    ggml-ci

commit 85fca8deb6273b956bc9184bc9d70a07e1d50d07
Author: Conrad Kramer <conrad@conradkramer.com>
Date:   Wed Aug 7 02:55:49 2024 -0400

    metal : add abort callback (ggml/905)

commit ebd541a5705b6f7a4ce67824d1c2d4fc790f1770
Author: Pablo Duboue <pablo.duboue@gmail.com>
Date:   Thu Aug 8 04:44:51 2024 -0400

    make : clean llamafile objects (#8923)
    
    `ggml/src/llamafile/sgemm.o` was not deleted on `make clean`

commit 15fa07a5c564d3ed7e7eb64b73272cedb27e73ec
Author: slaren <slarengh@gmail.com>
Date:   Wed Aug 7 18:24:05 2024 +0200

    make : use C compiler to build metal embed object (#8899)
    
    * make : use C compiler to build metal embed object
    
    * use rm + rmdir to avoid -r flag in rm

commit be55695eff44784a141a863f273661a6bce63dfc
Author: slaren <slarengh@gmail.com>
Date:   Wed Aug 7 13:29:02 2024 +0200

    ggml-backend : fix async copy from CPU (#8897)
    
    * ggml-backend : fix async copy from CPU
    
    * cuda : more reliable async copy, fix stream used when the devices are the same

commit 0478174d5959b66096ae6609fcb0df14cab66b51
Author: Ouadie EL FAROUKI <ouadie.elfarouki@codeplay.com>
Date:   Wed Aug 7 11:25:36 2024 +0100

    [SYCL] Updated SYCL device filtering  (#8901)
    
    * Updated device filter to depend on default_selector (fixes non-intel device issues)
    * Small related update to example/sycl Readme

commit a8dbc6f753f296108121d50f78f67463403dd6fc
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Aug 7 09:07:52 2024 +0200

    CUDA/HIP: fix tests/test-backend-ops (#8896)

commit 506122d854c5d05b4a3d45a294f14bd4c02d9868
Author: Zhenwei Jin <109658203+kylo5aby@users.noreply.github.com>
Date:   Wed Aug 7 09:01:06 2024 +0800

    llama-bench : add support for getting cpu info on Windows (#8824)
    
    * Add support for getting cpu info on Windows for llama_bench
    
    * refactor
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 725e3d94379d5b619c027347308bccf2e0ead89f
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed Aug 7 01:43:00 2024 +0200

    quantize : update usage comment in quantize.cpp (#8889)
    
    This commit updates the usage comment in quantize.cpp to reflect the
    new name of the executable, which is llama-quantize.

commit 31958546c3e4695a8a24bb7ba3b79a8f76d05afe
Author: Nexes the Old <124105151+Nexesenex@users.noreply.github.com>
Date:   Wed Aug 7 01:41:54 2024 +0200

    typo correction (#8891)

commit 1e6f6554aa11fa10160a5fda689e736c3c34169f
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Aug 6 17:33:39 2024 +0200

    server : add lora hotswap endpoint (WIP) (#8857)
    
    * server : add lora hotswap endpoint
    
    * handle lora_no_apply
    
    * fix build
    
    * updae docs
    
    * clean up struct def
    
    * fix build
    
    * add LoRA test
    
    * fix style

commit 641f5dd2a6422941faa9f32ab5cb50fc1b24c1f5
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Aug 6 17:13:55 2024 +0200

    CUDA: fix padding logic for FP16/FP32 (#8884)

commit 5f4dcb1e60bbfe936b45778dad177a5b9a09b066
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Aug 6 16:44:35 2024 +0200

    simple : update name of executable to llama-simple (#8885)
    
    This commit updates the name of the executable in README.md from
    `simple` to `llama-simple`.

commit db20f50cf4710c46e3a996919a26858cae8c80ed
Author: Jaeden Amero <jaeden@patater.com>
Date:   Tue Aug 6 17:21:47 2024 +0400

    cmake : Link vulkan-shaders-gen with pthreads (#8835)
    
    When using CMake to build with Vulkan support, compiling
    vulkan-shaders-gen fails due to missing a CMakeLists.txt specification
    to link vulkan-shaders-gen with the threading library, resulting in the
    following error.
    
        [5/172] Linking CXX executable bin/vulkan-shaders-gen
        FAILED: bin/vulkan-shaders-gen
        : && /usr/bin/c++ ggml/src/vulkan-shaders/CMakeFiles/vulkan-shaders-gen.dir/vulkan-shaders-gen.cpp.o -o bin/vulkan-shaders-gen   && :
        ld: error: undefined symbol: pthread_create
        >>> referenced by vulkan-shaders-gen.cpp
        >>>               ggml/src/vulkan-shaders/CMakeFiles/vulkan-shaders-gen.dir/vulkan-shaders-gen.cpp.o:(std::__1::__libcpp_thread_create[abi:se180100](pthread**,
        >>>               void* (*)(void*), void*))
        c++: error: linker command failed with exit code 1 (use -v to see invocation)
        [6/172] Generating build details from Git
        -- Found Git: /usr/local/bin/git (found version "2.45.2")
        ninja: build stopped: subcommand failed.
    
    Add the CMakeLists.txt specification to link vulkan-shaders-gen with the
    threading library and fix the above error.
    
    Fixes #8834

commit efda90c93a62274ec0b0bfa80c4eee4bdb6966d0
Author: MaggotHATE <clay1326@gmail.com>
Date:   Tue Aug 6 16:32:03 2024 +0500

    [Vulkan] Fix compilation of `vulkan-shaders-gen` on w64devkit after `e31a4f6` (#8880)
    
    * Fix compilation issue in `vulkan-shaders-gen`
    
    https://github.com/ggerganov/llama.cpp/commit/e31a4f679779220312c165b0f5994c680a610e38 broke compilation on w64devkit. Including `algorithm` seems to fix that.
    
    * Guard it under `#ifdef _WIN32`

commit 0bf16de07b0692e7df26b9a633e232bbd66e0360
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Aug 6 11:48:01 2024 +0300

    contributing : add note about write access

commit 2d5dd7bb3fa382806cd3e0bfc7a1d92349bc0ccf
Author: Molly Sophia <mollysophia379@gmail.com>
Date:   Tue Aug 6 15:26:46 2024 +0800

    ggml : add epsilon as a parameter for group_norm (#8818)
    
    Signed-off-by: Molly Sophia <mollysophia379@gmail.com>

commit cdd1889de62a7140c8c016405ec917464bde8bd3
Author: Douglas Hanley <thesecretaryofwar@gmail.com>
Date:   Tue Aug 6 02:20:54 2024 -0500

    convert : add support for XLMRoberta embedding models (#8658)
    
    * add conversion for bge-m3; small fix in unigram tokenizer
    
    * clean up and simplify XLMRoberta conversion

commit c21a896405de4cdf4207eb8130555ceaac0ab110
Author: Mengqing Cao <cmq0113@163.com>
Date:   Tue Aug 6 12:42:42 2024 +0800

    [CANN]: Fix ggml_backend_cann_buffer_get_tensor (#8871)
    
    * cann: fix ggml_backend_cann_buffer_get_tensor
    
     1. fix data ptr offset
     2. enable the acquisition of incomplete tensors
    
    * fix backend cann set_tensor

commit d4ff847153e9cf7220d1b39aa21172069e6e8cea
Author: Neo Zhang <zhang.jianyu@outlook.com>
Date:   Tue Aug 6 09:09:12 2024 +0800

    [SYCL] correct cmd name (#8877)

commit 0a4ce786814b123096d18aadca89cd352b9e590b
Author: Liu Jia <109258120+Septa2112@users.noreply.github.com>
Date:   Tue Aug 6 00:14:10 2024 +0800

    common : Changed tuple to struct (TODO fix) (#8823)
    
    * common : Changed tuple to struct (TODO fix)
    
    Use struct `llama_init_result` to replace the previous
    std::tuple<struct llama_model *, struct llama_context *>
    
    * delete llama_init_default_params()
    
    * delete the extra whitespace

commit bc0f887e159c0d78c28121e2c8b5c58094170875
Author: wangshuai09 <391746016@qq.com>
Date:   Mon Aug 5 21:10:37 2024 +0800

    cann: fix buffer_num and runtime speed slowly error (#8865)

commit b42978e7e4d56eaaa93588414e804d9fbbc3cae2
Author: Eric Curtin <ericcurtin17@gmail.com>
Date:   Mon Aug 5 13:45:01 2024 +0100

    readme : add ramalama to the availables UI (#8811)
    
    ramalama is a repo agnostic boring CLI tool that supports pulling from
    ollama, huggingface and oci registries.
    
    Signed-off-by: Eric Curtin <ecurtin@redhat.com>

commit b9dfc25ca385a83bde9e9456c4d4fae15377bc7b
Author: Justine Tunney <jtunney@mozilla.com>
Date:   Mon Aug 5 05:43:40 2024 -0700

    ggml : fix overflows in elu function (#8866)
    
    It's helpful to use expm1f(x), because expf(x)-1 will result in overflow
    for 25% of single-precision floating point numbers.

commit 1ef14b30075da594cb24f0ab858a14bf1d8d1797
Author: Brian <mofosyne@gmail.com>
Date:   Mon Aug 5 21:15:28 2024 +1000

    py: Add more authorship metadata from model card (#8810)
    
    * py: add more authorship metadata from model card
    
    * fixup! py: add more authorship metadata from model card

commit d3f0c7166adfa952237e0f437a5344362d8256d4
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Mon Aug 5 09:38:01 2024 +0200

    Stop the generation when <|eom_id|> token is encountered - needed for Llama 3.1 tool call support (#8858)
    
    * gguf-py, llama : add constants and methods related to Llama-3.1 <|eom_id|> token
    
    * llama : find Llama-3.1 <|eom_id|> token id during vocab loading
    
    * llama-vocab : add Llama-3.1 <|eom_id|> token to the set of tokens stopping the generation
    
    ---------
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

commit e31a4f679779220312c165b0f5994c680a610e38
Author: stduhpf <stephduh@live.fr>
Date:   Mon Aug 5 08:18:27 2024 +0200

    cmake: fix paths for vulkan shaders compilation on Windows (#8573)
    
    * Vulkan-shaders: attempt fix compilation on windows
    
    * fix miss-matched parenthesis

commit 400ae6f65f0b55babd48d1e3ec7fd663a97fc8d0
Author: BarfingLemurs <128182951+BarfingLemurs@users.noreply.github.com>
Date:   Mon Aug 5 01:54:10 2024 -0400

    readme : update model list (#8851)

commit f1ea5146d741a0c9be6d8fbfab9323fea6c4a3f0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 5 08:53:39 2024 +0300

    llama : better replace_all (#8852)

commit 064cdc265fb63590c7c8f04a609d36ef200d55a7
Author: 0cc4m <picard12@live.de>
Date:   Mon Aug 5 07:52:55 2024 +0200

    vulkan : fix Qantized Mat-Vec Mul on AMD GPUs for ncols < 64 (#8855)
    
    * Fix Vulkan mul mat vec invalid results when ncols < warp size
    
    * Only run backend ops mul mat vec block size test if block size not already covered

commit 5587e57a76630651752031223cc7024cb32cf308
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Aug 4 19:13:25 2024 +0300

    sync : ggml
    
    ggml-ci

commit a3738b2fa7c60ef2c4592435d1aa7fb8f1f69c3e
Author: 0cc4m <picard12@live.de>
Date:   Sun Aug 4 17:28:08 2024 +0200

    vulkan : implement Stable Diffusion operators (ggml/904)
    
    * Fix Vulkan repeat op
    
    * Implement Vulkan concat op
    
    * Delete old Vulkan shader generator
    
    * Implement Vulkan im2col op
    
    * Implement Vulkan unary gelu_quick op
    
    * Implement Vulkan group_norm op
    
    * Implement Vulkan timestep_embedding op
    
    * Implement Vulkan upscale op
    
    * Fix Vulkan vk_context tensor extra index issue
    
    * Fix Vulkan matmul shader parameter bug
    
    * Properly fix Vulkan matmul shader parameter bug
    
    * Add Vulkan ADD f16 + f32 -> f16 operator support
    
    * Implement Vulkan tanh op
    
    * Fix Vulkan group count too large Validation error on non-Nvidia GPUs
    
    * Throw error when too much memory is requested
    
    * Fix another Vulkan group count too large Validation error on non-Nvidia GPUs
    
    * Fix matmul MMQ condition
    
    * Implement Vulkan pad op
    
    * Fix Vulkan crash when tensor is used multiple times in a compute graph
    
    * Add Vulkan CONCAT f16 + f16 -> f16 op
    
    * Add Vulkan LEAKY_RELU op

commit 655858ace0cf2720e56eb01f84ad05e0c94ada3c
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Mon Jul 29 15:06:06 2024 +0200

    ggml : move c parameter comment to ggml_rope_ext (ggml/901)
    
    This commit moves the comment for the c parameter from ggml_rope to
    ggml_rope_ext. The comment is currently incorrect as ggml_rope does not
    have a c parameter (freq_factors tensor).
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit c02b0a8a4dee489b29073f25a27ed6e5628e86e1
Author: wangshuai09 <391746016@qq.com>
Date:   Mon Aug 5 12:22:30 2024 +0800

    cann: support q4_0 model (#8822)

commit 0d6fb52be0c1b7e77eb855f3adc4952771c8ce4c
Author: Brandon Squizzato <35474886+bsquizz@users.noreply.github.com>
Date:   Sun Aug 4 14:17:16 2024 -0400

    Install curl in runtime layer (#8693)

commit 978ba3d83d17b10fdf9807006048432b5b3769fc
Author: ardfork <134447697+ardfork@users.noreply.github.com>
Date:   Sun Aug 4 18:16:23 2024 +0000

    Server: Don't ignore llama.cpp params (#8754)
    
    * Don't ignore llama.cpp params
    
    * Add fallback for max_tokens

commit ecf6b7f23e664afd7ff856ec39034240ce438daa
Author: Brian Cunnie <brian.cunnie@gmail.com>
Date:   Sun Aug 4 03:55:03 2024 -0700

    batched-bench : handle empty `-npl` (#8839)
    
    * [example] batched-bench "segmentation fault"
    
    When `llama-batched-bench` is invoked _without_ setting `-npl`, "number
    of parallel prompts", it segfaults.
    
    The segfault is caused by invoking `max_element()` on a zero-length
    vector, `n_pl`
    
    This commit addresses that by first checking to see if the number of
    parallel prompts is zero, and if so sets the maximum sequence size to 1;
    otherwise, sets it to the original, the result of `max_element()`.
    
    Fixes, when running `lldb build/bin/llama-batched-bench -- -m models/Meta-Llama-3-8B.gguf`
    
    ```
    * thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x0)
        frame #0: 0x000000010000366c llama-batched-bench`main(argc=3, argv=0x000000016fdff268) at batched-bench.cpp:72:28
       69       llama_context_params ctx_params = llama_context_params_from_gpt_params(params);
       70
       71       // ensure enough sequences are available
    -> 72       ctx_params.n_seq_max = *std::max_element(n_pl.begin(), n_pl.end());
    ```
    
    * Update examples/batched-bench/batched-bench.cpp
    
    Co-authored-by: compilade <git@compilade.net>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: compilade <git@compilade.net>

commit 01aae2b4975b57a265ce8194928fd87f2d71027e
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Sat Aug 3 15:07:47 2024 +0200

    baby-llama : remove duplicate vector include

commit 4b77ea95f56a4c49bc995f08eac62a6416875ccc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Aug 4 05:53:20 2024 +0300

    flake.lock: Update (#8847)

commit 76614f352e94d25659306d9e97321f204e5de0d3
Author: jdomke <28772296+jdomke@users.noreply.github.com>
Date:   Sun Aug 4 01:34:41 2024 +0900

    ggml : reading the runtime sve config of the cpu (#8709)
    
    * ggml : reading the runtime sve config of the cpu
    
    * change to one time init to prevent performance drop
    
    * prefix variable to avoid possible conflicts
    
    * revert xxhash fix and add brackets
    
    ---------
    
    Co-authored-by: domke <673751-domke@users.noreply.gitlab.com>

commit b72c20b85c1029d135022d39e9a20d4807c11893
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Fri Aug 2 21:11:39 2024 +0200

    Fix conversion of unnormalized BF16->BF16 weights (#7843)
    
    * add truncate_bf16
    
    * truncate intermediate fp32 if converting bf16 to bf16
    
    * fix masking in __compute_fp32_to_bf16
    
    * np.int16 no longer used
    
    * missing cast and additional numpy 2.x fix
    
    * ggml-impl : do not flush bf16 subnormals to zero
    
    * ggml : add reference fp32 to bf16 conversion
    
    The fast version is no longer equivalent for all platforms
    because of the handling of subnormal values.
    
    * gguf-py : remove flush to zero for bf16 subnormals
    
    * gguf-py : remove float32 truncation to bf16
    
    Rounding achieves the same thing in the cases where this was used.
    
    * missed prototype update in merge
    
    * merge cleanup
    
    ---------
    
    Co-authored-by: Francis Couture-Harpin <git@compilade.net>

commit e09a800f9a9b19c73aa78e03b4c4be8ed988f3e6
Author: Mengqing Cao <cmq0113@163.com>
Date:   Fri Aug 2 16:50:53 2024 +0800

    cann: Fix ggml_cann_im2col for 1D im2col (#8819)
    
    * fix ggml_cann_im2col for 1D im2col
    
    * fix build warning

commit 0fbbd884589d585c3b43cae8c16938ffffb863b9
Author: Ouadie EL FAROUKI <ouadie.elfarouki@codeplay.com>
Date:   Fri Aug 2 01:55:17 2024 +0100

    [SYCL] Fixing wrong VDR iq4nl value (#8812)

commit afbb4c1322a747d2a7b4bf67c868148f8afcc6c8
Author: matteo <matteogeniaccio@yahoo.it>
Date:   Thu Aug 1 23:28:28 2024 +0200

    ggml-cuda: Adding support for unified memory (#8035)
    
    * Adding support for unified memory
    
    * adding again the documentation about unified memory
    
    * refactoring: Moved the unified memory code in the correct location.
    
    * Fixed compilation error when using hipblas
    
    * cleaning up the documentation
    
    * Updating the documentation
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    * adding one more case where the PR should not be enabled
    
    ---------
    
    Co-authored-by: matteo serva <matteo.serva@gmail.com>
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

commit b7a08fd5e0e7c898c68d1743066ea495202d9608
Author: Alex O'Connell <35843486+acon96@users.noreply.github.com>
Date:   Thu Aug 1 12:53:46 2024 -0400

    Build: Only include execinfo.h on linux systems that support it (#8783)
    
    * Only enable backtrace on GLIBC linux systems
    
    * fix missing file from copy
    
    * use glibc macro instead of defining a custom one

commit 7a11eb3a260915aee16101808f291a244e2facc7
Author: slaren <slarengh@gmail.com>
Date:   Thu Aug 1 15:26:22 2024 +0200

    cuda : fix dmmv cols requirement to 2*GGML_CUDA_DMMV_X (#8800)
    
    * cuda : fix dmmv cols requirement to 2*GGML_CUDA_DMMV_X
    
    * update asserts
    
    * only use dmmv for supported types
    
    * add test

commit c8a0090922bad576623de4aae227717085249262
Author: wangshuai09 <391746016@qq.com>
Date:   Thu Aug 1 10:39:05 2024 +0800

    cann: support q8_0 for Ascend backend (#8805)

commit afbbcf3c04e3c6420cad3d72571478cd62ac176c
Author: Igor Okulist <okigan@gmail.com>
Date:   Wed Jul 31 18:59:09 2024 -0500

    server : update llama-server embedding flag documentation (#8779)
    
    Fixes #8763

commit ed9d2854c9de4ae1f448334294e61167b04bec2a
Author: Clint Herron <hanclinto@gmail.com>
Date:   Wed Jul 31 15:51:06 2024 -0400

    Build: Fix potential race condition (#8781)
    
    * Fix potential race condition as pointed out by @fairydreaming in #8776
    
    * Reference the .o rather than rebuilding every time.
    
    * Adding in CXXFLAGS and LDFLAGS
    
    * Removing unnecessary linker flags.

commit 398ede5efeb07b9adf9fbda7ea63f630d476a792
Author: pculliton <phillipculliton@gmail.com>
Date:   Wed Jul 31 11:12:10 2024 -0400

    Adding Gemma 2 2B configs (#8784)
    
    * Adding Gemma 2 2B configs
    
    Updates to Q scaling and Gemma 2 model sizes to match v2 2B model.
    
    * Update src/llama.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 44d28ddd5caaa5e9de573bdaaa5b5b2448a29ace
Author: Borislav Stanimirov <b.stanimirov@abv.bg>
Date:   Wed Jul 31 16:40:08 2024 +0300

    cmake : fix use of external ggml (#8787)

commit 268c5660062270a2c19a36fc655168aa287aaec2
Author: Someone <sergei.kozlukov@aalto.fi>
Date:   Tue Jul 30 23:35:30 2024 +0300

    nix: cuda: rely on propagatedBuildInputs (#8772)
    
    Listing individual outputs no longer necessary to reduce the runtime closure size after https://github.com/NixOS/nixpkgs/pull/323056.

commit 7e72aa74fd676a093eb9970e761085ec22734c71
Author: Brian <mofosyne@gmail.com>
Date:   Wed Jul 31 00:57:03 2024 +1000

    py: add_array() will not add to kv store if value is an empty array (#8774)
    
    * gguf_writer.py: add_array() should not add to kv store if empty
    
    * Apply suggestions from code review
    
    I was wondering if there was a specific reason for `if val` but good to hear we can safely use `len(val == 0`
    
    Co-authored-by: compilade <git@compilade.net>
    
    ---------
    
    Co-authored-by: compilade <git@compilade.net>

commit 7c27a19b2eb91bb0f43c7f7aec0386cec2dddc33
Author: l3utterfly <gc.pthzfoldr@gmail.com>
Date:   Tue Jul 30 23:40:18 2024 +0900

    added android implementation of ggml_print_backtrace_symbols (#8751)
    
    * added android implementation of ggml_print_backtrace_symbols
    
    * Update ggml/src/ggml.c
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Update ggml/src/ggml.c
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Update ggml/src/ggml.c
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Update ggml/src/ggml.c
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Update ggml/src/ggml.c
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 140074bb8647df41840d6f32f4409fa8959bcf9f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jul 30 15:58:57 2024 +0300

    flake.lock: Update (#8729)

commit 6e2b6000e5fe808954a7dcef8225b5b7f2c1b9e9
Author: wangshuai09 <391746016@qq.com>
Date:   Tue Jul 30 18:37:35 2024 +0800

    cann: update cmake (#8765)

commit c887d8b01726b11ea03dbcaa9d44fa74422d0076
Author: zhentaoyu <zhentao.yu@intel.com>
Date:   Tue Jul 30 14:56:51 2024 +0800

    [SYCL] Add `TIMESTEP_EMBEDDING` OP (#8707)
    
    Signed-off-by: zhentaoyu <zhentao.yu@intel.com>

commit 75af08c475e285888f66556d0f459c533b7deb95
Author: CarterLi999 <664681047@qq.com>
Date:   Tue Jul 30 00:38:34 2024 +0800

    ggml: bugfix: fix the inactive elements is agnostic for risc-v vector (#8748)
    
    In these codes, we want to retain the value that they previously held
    when mask[i] is false. So we should use undisturbed. With the default
    agnostic policy of rvv intrinsic, these values can be held or be
    written with 1s.
    
    Co-authored-by: carter.li <carter.li@starfivetech.com>

commit 439b3fc75a8deb42899ac47a8f52aae75e0339fe
Author: R0CKSTAR <xiaodong.ye@mthreads.com>
Date:   Mon Jul 29 20:56:12 2024 +0800

    cuda : organize vendor-specific headers into vendors directory (#8746)
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>

commit 0832de723695ab400316a6c49b9f712380e3a731
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Mon Jul 29 10:50:27 2024 +0800

    [SYCL] add conv support (#8688)

commit 6eeaeba126ff701f3e8f79f246805b7023709972
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Jul 28 22:32:44 2024 +0200

    cmake: use 1 more thread for non-ggml in CI (#8740)

commit 4730faca618ff9cee0780580145e3cbe86f24876
Author: Austin <77757836+teleprint-me@users.noreply.github.com>
Date:   Sun Jul 28 03:52:42 2024 -0400

    chore : Fix vulkan related compiler warnings, add help text, improve CLI options (#8477)
    
    * chore: Fix compiler warnings, add help text, improve CLI options
    
    * Add prototypes for function definitions
    * Invert logic of --no-clean option to be more intuitive
    * Provide a new help prompt with clear instructions
    
    * chore : Add ignore rule for vulkan shader generator
    
    Signed-off-by: teleprint-me <77757836+teleprint-me@users.noreply.github.com>
    
    * Update ggml/src/vulkan-shaders/vulkan-shaders-gen.cpp
    
    Co-authored-by: 0cc4m <picard12@live.de>
    
    * chore : Remove void and apply C++ style empty parameters
    
    * chore : Remove void and apply C++ style empty parameters
    
    ---------
    
    Signed-off-by: teleprint-me <77757836+teleprint-me@users.noreply.github.com>
    Co-authored-by: 0cc4m <picard12@live.de>

commit 4c676c85e59ef8f771f3a129e6eb217552139231
Author: compilade <git@compilade.net>
Date:   Sun Jul 28 00:42:05 2024 -0400

    llama : refactor session file management (#8699)
    
    * llama : refactor session file management
    
    * llama : saving and restoring state checks for overflow
    
    The size of the buffers should now be given to the functions working
    with them, otherwise a truncated file could cause out of bound reads.
    
    * llama : stream from session file instead of copying into a big buffer
    
    Loading session files should no longer cause a memory usage spike.
    
    * llama : llama_state_get_size returns the actual size instead of max
    
    This is a breaking change, but makes that function *much* easier
    to keep up to date, and it also makes it reflect the behavior
    of llama_state_seq_get_size.
    
    * llama : share code between whole and seq_id-specific state saving
    
    Both session file types now use a more similar format.
    
    * llama : no longer store all hparams in session files
    
    Instead, the model arch name is stored.
    The layer count and the embedding dimensions of the KV cache
    are still verified when loading.
    Storing all the hparams is not necessary.
    
    * llama : fix uint64_t format type
    
    * llama : various integer type cast and format string fixes
    
    Some platforms use "%lu" and others "%llu" for uint64_t.
    Not sure how to handle that, so casting to size_t when displaying errors.
    
    * llama : remove _context suffix for llama_data_context
    
    * llama : fix session file loading
    
    llama_state_get_size cannot be used to get the max size anymore.
    
    * llama : more graceful error handling of invalid session files
    
    * llama : remove LLAMA_MAX_RNG_STATE
    
    It's no longer necessary to limit the size of the RNG state,
    because the max size of session files is not estimated anymore.
    
    * llama : cast seq_id in comparison with unsigned n_seq_max

commit e54c35e4fb5777c76316a50671640e6e144c9538
Author: R0CKSTAR <yeahdongcn@gmail.com>
Date:   Sun Jul 28 07:41:25 2024 +0800

    feat: Support Moore Threads GPU  (#8383)
    
    * Update doc for MUSA
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>
    
    * Add GGML_MUSA in Makefile
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>
    
    * Add GGML_MUSA in CMake
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>
    
    * CUDA => MUSA
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>
    
    * MUSA adds support for __vsubss4
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>
    
    * Fix CI build failure
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>
    
    ---------
    
    Signed-off-by: Xiaodong Ye <xiaodong.ye@mthreads.com>

commit 5e2727fe0321c38d1664d26173c654fa1801dc5f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jul 27 18:08:31 2024 +0300

    scripts : sync vulkan-shaders (#0)

commit 56f20aa25d5f97248a204b473c99f4040900f0e5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jul 27 17:19:35 2024 +0300

    scripts : sync ggml-aarch64 sources

commit 345c8c0c87a97c1595f9c8b14833d531c8c7d8df
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jul 27 15:57:09 2024 +0300

    ggml : add missing semicolon (#0)
    
    ggml-ci

commit ae7985cd7beca3b849328d169a8d592469cd021f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jul 27 15:53:48 2024 +0300

    sync : ggml
    
    ggml-ci

commit a05ca9369716a8319014cd1fc365980d43f8aae9
Author: Mahesh Madhav <67384846+heshpdx@users.noreply.github.com>
Date:   Thu Jul 25 00:54:08 2024 -0700

    ggml : loop tiling optimizations for scalar path (ggml/898)
    
    Apply a loop tiling technique to the generic path, which provides
    performance upside for ISAs with enough registers to take advantage
    of it. Also helps the compiler optimize this path.

commit 9f77d899b7b0d56496f679e54b797da6199fed8e
Author: Ivan Filipov <159561759+vanaka11@users.noreply.github.com>
Date:   Mon Jul 22 14:32:02 2024 +0300

    ggml: add support for float16 input tensors in pooling operations (ggml/895)
    
    * Add support for float16 tensors in 1d pooling operations
    
    * Add support for float16 input tensors in 2d pooling operations
    
    * code cleanup
    
    remove unnecessary casting during srow ptr initialization
    
    ---------
    
    Co-authored-by: vanaka11 <vanaka1189@gmail.com>

commit 203b7f1531303a060730ec1d1e01920e70302398
Author: Tony Wasserka <4840017+neobrain@users.noreply.github.com>
Date:   Sat Jul 20 20:49:44 2024 +0200

    vulkan : initialize vk_buffer_struct members to VK_NULL_HANDLE (ggml/893)
    
    This prevents invalid frees when destroying a partially initialized
    vk_buffer_struct. For example, this could happen in ggml_vk_create_buffer
    when running out of device memory.
    
    Co-authored-by: Tony Wasserka <neobrain@users.noreply.github.com>

commit d2b851bfa131478665315bc5c7c707506c14d703
Author: Borislav Stanimirov <b.stanimirov@abv.bg>
Date:   Fri Jul 12 17:24:20 2024 +0300

    cmake : only enable GGML_NATIVE and x86 flags if not crosscompiling (ggml/885)

commit c12b6e8ee7d905e0f299caf311689189fb1b4ac5
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Mon Jul 8 12:03:42 2024 +0200

    ggml : remove unnecessary UNUSED macro call (ggml/880)
    
    This commit removes an UNUSED macro call that is not needed as the
    variable n0 is used in the code and will not produce a warning.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit b5e95468b1676e1e5c9d80d1eeeb26f542a38f42
Author: Jeffrey Morgan <jmorganca@gmail.com>
Date:   Sat Jul 27 05:03:45 2024 -0700

    llama : add support for llama 3.1 rope scaling factors (#8676)
    
    * Add llama 3.1 rope scaling factors to llama conversion and inference
    
    This commit generates the rope factors on conversion and adds them to the resulting model as a tensor. At inference time, these factors are passed to the `ggml_rope_ext` rope oepration, improving results for context windows above 8192
    
    * Update convert_hf_to_gguf.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * address comments
    
    * address comments
    
    * Update src/llama.cpp
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Update convert_hf_to_gguf.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    ---------
    
    Co-authored-by: compilade <git@compilade.net>

commit 92090eca212650727e38b335c1d4accfbcc9b79c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jul 27 14:59:29 2024 +0300

    llama : add function for model-based max number of graph nodes (#8622)
    
    * llama : model-based max number of graph nodes
    
    ggml-ci
    
    * llama : disable 405B max_nodes path due to lack of complaints
    
    ggml-ci

commit 9d03d085dd6cb275c078690bb64073b9b043e95f
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Sat Jul 27 12:45:02 2024 +0200

    common : add --no-warmup option for main/llama-cli (#8712)
    
    This commit adds a --no-warmup option for llama-cli.
    
    The motivation for this is that it can be convenient to skip the
    warmup llama_decode call when debugging.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit bfb4c74981f0a40d757b450b596a9fe4ca983d26
Author: wangshuai09 <391746016@qq.com>
Date:   Sat Jul 27 16:36:44 2024 +0800

    cann: Fix Multi-NPU execution error (#8710)
    
    * cann: fix multi-npu exec error
    
    * cann: update comment  for ggml_backend_cann_supports_buft

commit 2b1f616b208a4a21c4ee7a7eb85d822ff1d787af
Author: slaren <slarengh@gmail.com>
Date:   Sat Jul 27 04:41:55 2024 +0200

    ggml : reduce hash table reset cost (#8698)
    
    * ggml : reduce hash table reset cost
    
    * fix unreachable code warnings after GGML_ASSERT(false)
    
    * GGML_ASSERT(false) -> GGML_ABORT("fatal error")
    
    * GGML_ABORT use format string

commit 01245f5b1629075543bc4478418c7d72a0b4b3c7
Author: Judd <foldl@users.noreply.github.com>
Date:   Fri Jul 26 16:38:12 2024 +0800

    llama : fix order of parameters (#8706)
    
    usage of `aclrtGetMemInfo` is correct:
    
    https://www.hiascend.com/doc_center/source/zh/canncommercial/63RC2/inferapplicationdev/aclcppdevg/aclcppdevg_03_0103.html
    
    Co-authored-by: Judd <foldl@boxvest.com>

commit 01aec4a6310ab0160483196db0e726d78d4c94b6
Author: Yaiko <elyaiko@hotmail.com>
Date:   Thu Jul 25 18:10:16 2024 -0400

    server : add Speech Recognition & Synthesis to UI (#8679)
    
    * server : add Speech Recognition & Synthesis to UI
    
    * server : add Speech Recognition & Synthesis to UI (fixes)

commit 41cd47caab88c442edc50e90c8d8d0ac3e82768d
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Jul 25 23:49:39 2024 +0200

    examples : export-lora : fix issue with quantized base models (#8687)

commit 49ce0ab6d45402e8bb622bf86f86529f2b0ba552
Author: DavidKorczynski <david@adalogics.com>
Date:   Thu Jul 25 22:23:05 2024 +0100

    ggml: handle ggml_init failure to fix NULL pointer deref (#8692)
    
    `ggml_init` can fail if no unused context is found. In that case, a NULL-pointer deref will happen later in the code during a call to `ggml_set_on_alloc`.
    
    This fixes it by bailing out if no context is found.

commit 4226a8d10e3904db3a1297919fe6c7f06beba6c0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jul 25 19:57:31 2024 +0300

    llama : fix build + fix fabs compile warnings (#8683)
    
    ggml-ci

commit bf5a81df375f1c71e41462e1f48d57db359c9e80
Author: Andreas (Andi) Kunar <andreask@msn.com>
Date:   Thu Jul 25 18:01:00 2024 +0200

    ggml : fix build on Windows with Snapdragon X (#8531)
    
    * Improvements for Windows with Snapdragon X
    
    * Revert "Improvements for Windows with Snapdragon X"
    
    This reverts commit bf21397ae5ea7c73d3494db3b91505599909227d.
    
    * Improvements for Windows with Snapdragon X
    
    * WOA build clarifications
    
    * WIndows on ARM build clarifications
    
    * cmake build for Windows clarifications
    
    * Update docs/build.md
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: AndreasKunar <andreaskmsn.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 88954f7fbd31aeb8c75140edee03e7a8ad5e2d9c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jul 25 18:57:44 2024 +0300

    tests : fix printfs (#8068)

commit ed67bcb24f2d6ac0072cae72620b2bd971741b98
Author: Chen Xi <xi2.chen@intel.com>
Date:   Thu Jul 25 11:45:18 2024 +0000

    [SYCL] fix multi-gpu issue on sycl (#8554)
    
    
    
    ---------
    
    Signed-off-by: Chen Xi <xi2chen@intel.com>
    Co-authored-by: Meng, Hengyu <hengyu.meng@intel.com>

commit eddcb5238b2e09a37798b87cde1244017a194bcc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jul 25 12:37:42 2024 +0300

    ggml : add and use ggml_cpu_has_llamafile() (#8664)

commit be6d7c079173d941b4f784500f9148f46cec2724
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Jul 25 10:39:04 2024 +0200

    examples : remove `finetune` and `train-text-from-scratch` (#8669)
    
    * examples : remove finetune and train-text-from-scratch
    
    * fix build
    
    * update help message
    
    * fix small typo for export-lora

commit 4b0eff3df58d8d86e47348fb73d54da3194d416d
Author: Ujjawal Panchal <31011628+Ujjawal-K-Panchal@users.noreply.github.com>
Date:   Thu Jul 25 13:43:27 2024 +0530

    docs : Quantum -> Quantized (#8666)
    
    * docfix: imatrix readme, quantum models -> quantized models.
    
    * docfix: server readme: quantum models -> quantized models.

commit 8a4bad50a8ed24ed1e9df003521468dcc37320e8
Author: Fan Shupei <dymarkfan@outlook.com>
Date:   Thu Jul 25 15:21:09 2024 +0800

    llama: use sliding window for phi3 (#8627)
    
    * use sliding window for phi3
    
    * fix typo, "data_swa" -> "data"
    
    * [conver_hf_to_gguf.py] add phi3 sliding window

commit 68504f0970db5a3602d176953690f503059906b1
Author: MorganRO8 <47795945+MorganRO8@users.noreply.github.com>
Date:   Wed Jul 24 12:48:00 2024 -0400

    readme : update games list (#8673)
    
    Added link to game I made that depends on llama

commit f19bf99c015d3d745143e8bb4f056e0ea015ad40
Author: Joe Todd <joe.todd@codeplay.com>
Date:   Wed Jul 24 14:36:00 2024 +0100

    Build Llama SYCL Intel with static libs (#8668)
    
    Ensure SYCL CI builds both static & dynamic libs for testing purposes
    
    Signed-off-by: Joe Todd <joe.todd@codeplay.com>

commit 3a7ac5300a7e8ebbe4a3eb5aff9dba11ed76ea61
Author: Thorsten Sommer <SommerEngineering@users.noreply.github.com>
Date:   Wed Jul 24 14:52:30 2024 +0200

    readme : update UI list [no ci] (#8505)

commit 96952e7181929c6001b2bc69a33f240de731cc3a
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed Jul 24 13:48:46 2024 +0200

    llama : fix `llama_chat_format_single` for mistral (#8657)
    
    * fix `llama_chat_format_single` for mistral
    
    * fix typo
    
    * use printf

commit 79167d9e49aef9caa98e13ee7ca067ec9f88b4b5
Author: Joe Todd <joe.todd@codeplay.com>
Date:   Wed Jul 24 11:55:26 2024 +0100

    Re-add erroneously removed -fsycl from GGML_EXTRA_LIBS (#8667)

commit b115105f05e3372bc75b2a486c1930c365fd2846
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed Jul 24 11:25:19 2024 +0200

    add llama_lora_adapter_clear (#8653)

commit de280085e7917dbb7f5753de5842ff4455f82a81
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Jul 23 23:48:37 2024 +0200

    examples : Fix `llama-export-lora` example (#8607)
    
    * fix export-lora example
    
    * add more logging
    
    * reject merging subset
    
    * better check
    
    * typo

commit b841d0740855c5af1344a81f261139a45a2b39ee
Author: Vali Malinoiu <0x4139@gmail.com>
Date:   Tue Jul 23 17:37:42 2024 +0300

    server : fix URL.parse in the UI (#8646)

commit 64cf50a0ed62d41e4f6c13e08a9b6b0816f46c6e
Author: Joe Todd <joe.todd@codeplay.com>
Date:   Tue Jul 23 14:58:37 2024 +0100

    sycl : Add support for non-release DPC++ & oneMKL (#8644)
    
    * Update cmake to support nvidia hardware & open-source compiler
    ---------
    Signed-off-by: Joe Todd <joe.todd@codeplay.com>

commit 938943cdbf4dd79005a394d732bc226f9e34e0ff
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jul 23 13:10:17 2024 +0300

    llama : move vocab, grammar and sampling into separate files (#8508)
    
    * llama : move sampling code into llama-sampling
    
    ggml-ci
    
    * llama : move grammar code into llama-grammar
    
    ggml-ci
    
    * cont
    
    ggml-ci
    
    * cont : pre-fetch rules
    
    * cont
    
    ggml-ci
    
    * llama : deprecate llama_sample_grammar
    
    * llama : move tokenizers into llama-vocab
    
    ggml-ci
    
    * make : update llama.cpp deps [no ci]
    
    * llama : redirect external API to internal APIs
    
    ggml-ci
    
    * llama : suffix the internal APIs with "_impl"
    
    ggml-ci
    
    * llama : clean-up

commit 751fcfc6c33ea5f43cadd4d976f8fb176871df5e
Author: 0cc4m <picard12@live.de>
Date:   Tue Jul 23 10:56:49 2024 +0200

    Vulkan IQ4_NL Support (#8613)
    
    * Fix Vulkan matmul tests compile errors
    
    * Add Vulkan IQ4_NL support
    
    * Fix Vulkan DeepSeek-Coder-V2-Lite MoE support

commit 46e47417aa4f18c08738afd4d9a3e838e97ca03f
Author: Jeroen Mostert <jeroen.mostert@cm.com>
Date:   Tue Jul 23 10:50:40 2024 +0200

    Allow all RDNA2 archs to use sdot4 intrinsic (#8629)
    
    The check gating the use of `__builtin_amdgc_sdot4` specifically checks for gfx1030. This causes a severe perf regression for anything gfx103? that's not gfx1030 and not using `HSA_OVERRIDE_GFX_VERSION` (if you've built ROCm to support it). We already have a generic RDNA2 define, let's use it.

commit e7e6487ba06634edf58dfdf9673bad9df41b445a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jul 23 11:28:38 2024 +0300

    contrib : clarify PR squashing + module names (#8630)
    
    * contrib : clarify PR squashing
    
    * contrib : fix typo + add list of modules

commit 063d99ad11f1295046610ce5b97e105849a4b573
Author: luoyu-intel <yu.luo@intel.com>
Date:   Tue Jul 23 07:43:28 2024 +0000

    [SYCL] fix scratch size of softmax (#8642)

commit 081fe431aa8fb6307145c4feb3eed4f48cab19f8
Author: Keke Han <hankeke303@163.com>
Date:   Tue Jul 23 00:43:43 2024 +0800

    llama : fix codeshell support (#8599)
    
    * llama : fix codeshell support
    
    * llama : move codeshell after smollm below to respect the enum order

commit d94c6e0ccbd29ee1ba4f44e9caa8682ad94df9fa
Author: Jason Stillerman <jason.t.stillerman@gmail.com>
Date:   Mon Jul 22 10:43:01 2024 -0400

    llama : add support for SmolLm pre-tokenizer (#8609)
    
    * Adding SmolLM Pre Tokenizer
    
    * Update convert_hf_to_gguf_update.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Update src/llama.cpp
    
    Co-authored-by: compilade <git@compilade.net>
    
    * handle regex
    
    * removed .inp and out .out ggufs
    
    ---------
    
    Co-authored-by: compilade <git@compilade.net>

commit 566daa5a5b38018b2727950bbd280239adb981b6
Author: Jiří Podivín <66251151+jpodivin@users.noreply.github.com>
Date:   Mon Jul 22 15:44:53 2024 +0200

    *.py: Stylistic adjustments for python (#8233)
    
    * Superflous parens in conditionals were removed.
    * Unused args in function were removed.
    * Replaced unused `idx` var with `_`
    * Initializing file_format and format_version attributes
    * Renaming constant to capitals
    * Preventing redefinition of the `f` var
    
    Signed-off-by: Jiri Podivin <jpodivin@redhat.com>

commit 6f11a83e4e7700fdf353ed4a29599cb662c792f6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jul 22 13:33:22 2024 +0300

    llama : allow overrides for tokenizer flags (#8614)
    
    ggml-ci

commit e093dd238282a980d248db0786063f8174400f70
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jul 22 13:32:49 2024 +0300

    tests : re-enable tokenizer tests (#8611)
    
    * models : remove duplicated gpt-2 vocab
    
    * models : remove old stablelm vocab
    
    * tests : re-enable MPT tokenizer tests
    
    * tests : re-enable DeepSeek tokenizer tests
    
    * cmake : sort
    
    ggml-ci

commit 50e05353e88d50b644688caa91f5955e8bdb9eb9
Author: Douglas Hanley <thesecretaryofwar@gmail.com>
Date:   Mon Jul 22 03:06:17 2024 -0500

    llama : add Mistral Nemo inference support (#8604)

commit 628154492a0b2dcc954a3af99e5c267097568eae
Author: Jan Boon <jan.boon@kaetemi.be>
Date:   Mon Jul 22 16:02:09 2024 +0800

    server : update doc to clarify n_keep when there is bos token (#8619)

commit 04bab6b7da0ed2b0a57174a57784d602aabb6c10
Author: Mark Zhuang <zhuangqiubin@gmail.com>
Date:   Mon Jul 22 15:56:45 2024 +0800

    ggml: fix compile error for RISC-V (#8623)

commit b7c11d36e605b35206901d0e21905f1b99508e33
Author: devojony <61173062+devojony@users.noreply.github.com>
Date:   Mon Jul 22 14:54:42 2024 +0800

    examples: fix android example cannot be generated continuously (#8621)
    
    When generation ends `completion_loop()` should return a NULL, not the empty string

commit 45f2c19cc57286eead7b232ce8028273a817aa4d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jul 21 16:45:10 2024 +0300

    flake.lock: Update (#8610)

commit 22f281aa16f44d8f6ec2c180a0685ff27e04e714
Author: M-A <maruel@gmail.com>
Date:   Sat Jul 20 22:09:17 2024 -0400

    examples : Rewrite pydantic_models_to_grammar_examples.py (#8493)
    
    Changes:
    
    - Move each example into its own function. This makes the code much
      easier to read and understand.
    - Make the program easy to only run one test by commenting out function
      calls in main().
    - Make the output easy to parse by indenting the output for each example.
    - Add shebang and +x bit to make it clear it's an executable.
    - Make the host configurable via --host with a default 127.0.0.1:8080.
    - Make the code look in the tools list to call the registered tool,
      instead of hardcoding the returned values. This makes the code more
      copy-pastable.
    - Add error checking, so that the program exits 1 if the LLM didn't
      returned expected values. It's super useful to check for correctness.
    
    Testing:
    
    - Tested with Mistral-7B-Instruct-v0.3 in F16 and Q5_K_M and
      Meta-Llama-3-8B-Instruct in F16 and Q5_K_M.
      - I did not observe a failure even once in Mistral-7B-Instruct-v0.3.
      - Llama-3 failed about a third of the time in example_concurrent: it
        only returned one call instead of 3. Even for F16.
    
    Potential follow ups:
    
    - Do not fix the prompt encoding yet. Surprisingly it mostly works even
      if the prompt encoding is not model optimized.
    - Add chained answer and response.
    
    Test only change.

commit 328884f4219c0228673cf1870ac63987fb4f9fd0
Author: compilade <git@compilade.net>
Date:   Sat Jul 20 21:58:49 2024 -0400

    gguf-py : fix some metadata name extraction edge cases (#8591)
    
    * gguf-py : fix some metadata name extraction edge cases
    
    * convert_lora : use the lora dir for the model card path
    
    * gguf-py : more metadata edge cases fixes
    
    Multiple finetune versions are now joined together,
    and the removal of the basename annotation on trailing versions
    is more robust.
    
    * gguf-py : add more name metadata extraction tests
    
    * convert_lora : fix default filename
    
    The default filename was previously hardcoded.
    
    * convert_hf : Model.fname_out can no longer be None
    
    * gguf-py : do not use title case for naming convention
    
    Some models use acronyms in lowercase,
    which can't be title-cased like other words,
    so it's best to simply use the same case
    as in the original model name.
    
    Note that the size label still has an uppercased suffix
    to make it distinguishable from the context size of a finetune.

commit c69c63039cd75f8f33f253ab7485d82a8b4cd403
Author: compilade <git@compilade.net>
Date:   Sat Jul 20 21:53:01 2024 -0400

    convert_hf : fix Gemma v1 conversion (#8597)
    
    * convert_hf : fix Gemma v1 conversion
    
    * convert_hf : allow renaming tokens, but with a warning
    
    * convert_hf : fix Gemma v1 not setting BOS and EOS tokens

commit 69c487f4ed57bb4d4514a1b7ff12608d5a8e7ef0
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jul 20 22:25:26 2024 +0200

    CUDA: MMQ code deduplication + iquant support (#8495)
    
    * CUDA: MMQ code deduplication + iquant support
    
    * 1 less parallel job for CI build

commit 07283b1a90e1320aae4762c7e03c879043910252
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jul 20 17:15:42 2024 +0300

    gguf : handle null name during init (#8587)

commit 940362224d20e35f13aa5fd34a0d937ae57bdf7d
Author: Michael Coppola <m18coppola@gmail.com>
Date:   Sat Jul 20 09:43:51 2024 -0400

    llama : add support for Tekken pre-tokenizer (#8579)
    
    * llama : Added support for Tekken pre-tokenizer (#8577)
    
    Removed uneeded `vocab.tokenizer_clean_spaces` assignment
    
    * llama : fix order of pre-tokenizers
    
    * * Tekken pre-tokenizer no longer uses clean_up_tokenization_spaces
    * Updated chkhsh for Tekken tokenizer
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 69b9945b44c3057ec17cb556994cd36060455d44
Author: Huifeng Ou <79071290+ho2103@users.noreply.github.com>
Date:   Sat Jul 20 09:09:37 2024 -0400

    llama.swiftui: fix end of generation bug (#8268)
    
    * fix continuing generating blank lines after getting EOT token or EOS token from LLM
    
    * change variable name to is_done (variable name suggested by ggerganov)
    
    * minor : fix trailing whitespace
    
    * minor : add space
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit c3776cacabce2ee35f172fb72be7a519752125fa
Author: Brian <mofosyne@gmail.com>
Date:   Sat Jul 20 17:35:25 2024 +1000

    gguf_dump.py: fix markddown kv array print (#8588)
    
    * gguf_dump.py: fix markddown kv array print
    
    * Update gguf-py/scripts/gguf_dump.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * gguf_dump.py: refactor kv array string handling
    
    * gguf_dump.py: escape backticks inside of strings
    
    * gguf_dump.py: inline code markdown escape handler added
    
    >>> escape_markdown_inline_code("hello world")
    '`hello world`'
    >>> escape_markdown_inline_code("hello ` world")
    '``hello ` world``'
    
    * gguf_dump.py: handle edge case about backticks on start or end of a string
    
    ---------
    
    Co-authored-by: compilade <git@compilade.net>

commit 87e397d00bdcedd5cbf6dfda06a7b0f302462728
Author: slaren <slarengh@gmail.com>
Date:   Fri Jul 19 17:17:27 2024 +0200

    ggml : fix quant dot product with odd number of blocks (#8549)
    
    * ggml : fix iq4_nl dot product with odd number of blocks
    
    * ggml : fix odd blocks for ARM_NEON (#8556)
    
    * ggml : fix iq4_nl dot product with odd number of blocks
    
    * ggml : fix q4_1
    
    * ggml : fix q5_0
    
    * ggml : fix q5_1
    
    * ggml : fix iq4_nl metal
    
    ggml-ci
    
    * ggml : fix q4_0
    
    * ggml : fix q8_0
    
    ggml-ci
    
    * ggml : remove special Q4_0 code for first 2 blocks
    
    * ggml : fix sumf redefinition
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 57b1d4f9eb6f2c139b31ea79626d954b261e1051
Author: Brian <mofosyne@gmail.com>
Date:   Sat Jul 20 00:04:38 2024 +1000

    convert-*.py: remove add_name from ChatGLMModel class (#8590)

commit d19754553029296c46d40b2f7bd4e2c2f531a8c5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 19 16:50:47 2024 +0300

    llama : bump max layers from 256 to 512 (#8530)
    
    * llama : bump max layers from 256 to 512
    
    * llama : replace asserts with exceptions

commit be0cfb41752551a4680ee7dfd29f2a05b50db442
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 19 14:34:55 2024 +0300

    readme : fix server badge

commit b57eb9ca4fb79f3163c6a69e154ff76157a4f716
Author: Clint Herron <hanclinto@gmail.com>
Date:   Fri Jul 19 07:05:45 2024 -0400

    ggml : add friendlier error message to fopen errors (#8575)
    
    * Add additional error information when model files fail to load.
    
    * Adding additional error information to most instances of fopen.

commit f299aa98ecc19cbc574e9d698e03999e89de3d3d
Author: Frank Mai <thxcode0824@gmail.com>
Date:   Fri Jul 19 17:44:41 2024 +0800

    fix: typo of chatglm4 chat tmpl (#8586)
    
    Signed-off-by: thxCode <thxcode0824@gmail.com>

commit 3d0e4367d99087892e355ddbeebd232a0b2f40de
Author: Brian <mofosyne@gmail.com>
Date:   Fri Jul 19 17:51:51 2024 +1000

    convert-*.py: add general.name kv override (#8571)

commit a15ef8f8a08e12f9c5162c221e67779e71182073
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Jul 18 23:48:47 2024 +0200

    CUDA: fix partial offloading for ne0 % 256 != 0 (#8572)

commit 705b7ecf60e667ced57c15d67aa86865e3cc7aa7
Author: 65a <10104049+65a@users.noreply.github.com>
Date:   Thu Jul 18 07:47:12 2024 -0700

    cmake : install all ggml public headers (#8480)
    
    Co-authored-by: 65a <65a@65a.invalid>

commit 0d2c7321e9678e91b760ebe57f0d063856bb018b
Author: Eric Zhang <34133756+EZForever@users.noreply.github.com>
Date:   Thu Jul 18 18:43:49 2024 +0800

    server: use relative routes for static files in new UI (#8552)
    
    * server: public: fix api_url on non-index pages
    
    * server: public: use relative routes for static files in new UI

commit 672a6f101839a150180fc28891ebed379c8f2353
Author: Brian <mofosyne@gmail.com>
Date:   Thu Jul 18 20:40:15 2024 +1000

    convert-*.py: GGUF Naming Convention Refactor and Metadata Override Refactor (#7499)
    
    Main thing is that the default output filename will take this form
    
    {name}{parameters}{finetune}{version}{encoding}{kind}
    
    In addition this add and remove some entries in the KV store and adds a metadata class with automatic heuristics capability to derive some values based on model card content
    
    * No Change:
      - Internal GGUF Spec
        - `general.architecture`
        - `general.quantization_version`
        - `general.alignment`
        - `general.file_type`
      - General Model Details
        - `general.name`
        - `general.author`
        - `general.version`
        - `general.description`
      - Licensing details
        - `general.license`
      - Typically represents the converted GGUF repo (Unless made from scratch)
        - `general.url`
      - Model Source during conversion
        - `general.source.url`
    
    * Removed:
      - Model Source during conversion
        - `general.source.huggingface.repository`
    
    * Added:
      - General Model Details
        - `general.organization`
        - `general.finetune`
        - `general.basename`
        - `general.quantized_by`
        - `general.size_label`
      - Licensing details
        - `general.license.name`
        - `general.license.link`
      - Typically represents the converted GGUF repo (Unless made from scratch)
        - `general.doi`
        - `general.uuid`
        - `general.repo_url`
      - Model Source during conversion
        - `general.source.doi`
        - `general.source.uuid`
        - `general.source.repo_url`
      - Base Model Source
        - `general.base_model.count`
        - `general.base_model.{id}.name`
        - `general.base_model.{id}.author`
        - `general.base_model.{id}.version`
        - `general.base_model.{id}.organization`
        - `general.base_model.{id}.url` (Model Website/Paper)
        - `general.base_model.{id}.doi`
        - `general.base_model.{id}.uuid`
        - `general.base_model.{id}.repo_url` (Model Source Repository (git/svn/etc...))
      - Array based KV stores
        - `general.tags`
        - `general.languages`
        - `general.datasets`
    
    ---------
    
    Co-authored-by: compilade <git@compilade.net>
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>

commit 3807c3de04cde853418033c95e96642876545f3e
Author: RunningLeon <mnsheng@yeah.net>
Date:   Thu Jul 18 16:06:22 2024 +0800

    server : respect `--special` cli arg (#8553)

commit e02b597be3702174e7b47b44cd03e1da1553284b
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jul 17 23:35:44 2024 +0200

    lookup: fibonacci hashing, fix crashes (#8548)

commit b3283448ce9a5098226afe1d8648ccc578511fe4
Author: Al Mochkin <14274697+amochkin@users.noreply.github.com>
Date:   Wed Jul 17 20:21:55 2024 +0200

    build : Fix docker build warnings (#8535) (#8537)

commit 30f80ca0bcee58669ada7a94244eeccc8c4807cc
Author: Brian <mofosyne@gmail.com>
Date:   Thu Jul 18 00:57:06 2024 +1000

    CONTRIBUTING.md : remove mention of noci (#8541)

commit 1bdd8ae19f50bc6f108fa247e90688e5c60559fc
Author: hipudding <huafengchun@gmail.com>
Date:   Wed Jul 17 19:23:50 2024 +0800

    [CANN] Add Ascend NPU backend (#6035)
    
    * [CANN] Add Ascend NPU backend
    
    Ascend is a full-stack AI computing infrastructure for industry
    applications and services based on Huawei Ascend processors and
    software.
    
    CANN (Compute Architecture of Neural Networks), developped by
    Huawei, is a heterogeneous computing architecture for AI.
    
    Co-authored-by: wangshuai09 <391746016@qq.com>
    
    * delete trailing whitespaces
    
    * Modify the code based on review comment
    
    * Rename LLAMA_CANN to GGML_CANN
    
    * Make ggml-common.h private
    
    * add ggml_cann prefix for acl funcs
    
    * Add logging for CANN backend
    
    * Delete Trailing whitespace
    
    ---------
    
    Co-authored-by: wangshuai09 <391746016@qq.com>

commit da3913d8f9475b0d4bcbeb4936c724af4eade092
Author: Masaya, Kato <62578291+msy-kato@users.noreply.github.com>
Date:   Wed Jul 17 16:34:28 2024 +0900

    batched: fix n_predict parameter (#8527)

commit d65a8361fed8be97389776fb94217e937ec6b03c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jul 17 10:32:59 2024 +0300

    llama : disable context-shift for DeepSeek v2 (#8501)

commit 5e116e8dd51775f8f1c090570be148d5d7eea6c3
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Jul 16 21:20:59 2024 +0200

    make/cmake: add missing force MMQ/cuBLAS for HIP (#8515)

commit 1666f92dcda14e002cbb08e1028f9fff341d73ad
Author: Brian <mofosyne@gmail.com>
Date:   Tue Jul 16 17:14:16 2024 +1000

    gguf-hash : update clib.json to point to original xxhash repo (#8491)
    
    * Update clib.json to point to Cyan4973 original xxhash
    
    Convinced Cyan4973 to add clib.json directly to his repo, so can now point the clib package directly to him now. Previously pointed to my fork with the clib.json package metadata
    
    https://github.com/Cyan4973/xxHash/pull/954
    
    * gguf-hash: readme update to point to Cyan4973 xxHash repo [no ci]

commit 37b12f92ab696d70f9a65d7447ce721b094fb32e
Author: Steve Bonds <sbonds@gmail.com>
Date:   Tue Jul 16 00:04:45 2024 -0700

    export-lora : handle help argument (#8497)
    
    The --help option on export-lora isn't accepted as valid. The help still gets displayed by default, but the script exits with an error message and nonzero status.

commit 0efec57787cb8d8d76b17cd3765e6521e22c1f19
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jul 16 10:00:30 2024 +0300

    llama : valign + remove unused ftype (#8502)

commit 7acfd4e8d55082c1b597dfc3ffe04fb5d530c6dc
Author: compilade <git@compilade.net>
Date:   Mon Jul 15 23:13:10 2024 -0400

    convert_hf : faster lazy safetensors (#8482)
    
    * convert_hf : faster lazy safetensors
    
    This makes '--dry-run' much, much faster.
    
    * convert_hf : fix memory leak in lazy MoE conversion
    
    The '_lazy' queue was sometimes self-referential,
    which caused reference cycles of objects old enough
    to avoid garbage collection until potential memory exhaustion.

commit 97bdd26eee11fe109dec00de75690ceef61c03f2
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Jul 15 20:50:47 2024 +0200

    Refactor lora adapter support (#8332)
    
    * lora: load to devide buft
    
    * add patch tensor function
    
    * correct tensor patch
    
    * llama_lora_adapter_apply
    
    * correct ggml_backend_tensor_copy
    
    * add llm_build_mm
    
    * fix auto merge
    
    * update based on review comments
    
    * add convert script
    
    * no more transpose A
    
    * add f16 convert
    
    * add metadata check
    
    * add sanity check
    
    * fix ftype
    
    * add requirements
    
    * fix requirements
    
    * fix outfile
    
    * conversion: only allow selected models
    
    * fix types
    
    * cuda : do not use dmmv if the tensor does not have enough cols
    
    * llama : lora fixes
    
    * do not disable mmap with lora
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * llm_build_lora_mm_id
    
    * convert_lora : MoE LoRA conversion support
    
    * convert_lora : prefer safetensors, similarly to convert_hf
    
    * convert_hf : simplify modify_tensors for InternLM2
    
    * convert_lora : lazy conversion
    
    * llama : load and use alpha from LoRA adapters
    
    * llama : use llm_build_lora_mm in most model graphs
    
    * auto scale
    
    * Revert "auto scale"
    
    This reverts commit 42415a4874e0f963e4aca6796ea5dfb97cd17464.
    
    * remove redundant params
    
    * Apply suggestions from code review
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * change kv metadata
    
    * move add_type to __init__
    
    * convert_hf : move add_type to main()
    
    * convert_lora : use the GGUFWriter from Model instead of overwriting it
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: Francis Couture-Harpin <git@compilade.net>

commit 4db8f60fe79a391e82b0464013ada123baced96a
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Jul 15 19:23:10 2024 +0200

    fix ci (#8494)

commit 8fac431b0692e88cdc55250f29f8d4386be82c5d
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Mon Jul 15 14:48:17 2024 +0200

    ggml : suppress unknown pragma 'GCC' on windows (#8460)
    
    This commit adds a macro guard to pragma GCC to avoid the following
    warning on windows:
    
    ```console
    C:\llama.cpp\ggml\src\ggml-aarch64.c(17,9): warning C4068:
    unknown pragma 'GCC' [C:\lama.cpp\build\ggml\src\ggml.vcxproj]
    ```

commit f17f39ff9cb71fa7b0eda0c649bb0ce9b2d8a6b8
Author: M-A <maruel@gmail.com>
Date:   Mon Jul 15 08:04:56 2024 -0400

    server: update README.md with llama-server --help output [no ci] (#8472)
    
    The README.md had a stale information. In particular, the --ctx-size
    "defaults to 512" confused me and I had to check the code to confirm
    this was false. This the server is evolving rapidly, it's probably
    better to keep the source of truth at a single place (in the source) and
    generate the README.md based on that.
    
    Did:
    
        make llama-server
        ./llama-server --help > t.txt
        vimdiff t.txt examples/server/README.md
    
    I copied the content inside a backquote block. I would have preferred
    proper text but it would require a fair amount of surgery to make the
    current output compatible with markdown. A follow up could be to
    automate this process with a script.
    
    No functional change.

commit 9104bc20edf47f74dc49379b7d61d0c6e85a4882
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jul 15 14:54:58 2024 +0300

    common : add --no-cont-batching arg (#6358)

commit fc690b018e459012cd82c37f1343e9bb658987d1
Author: NikolaiLyssogor <59844691+NikolaiLyssogor@users.noreply.github.com>
Date:   Mon Jul 15 04:46:39 2024 -0700

    docs: fix links in development docs [no ci] (#8481)
    
    Fixes a few links to within the repo that were broken in the reorganization of the
    documentation in #8325.

commit 16bdfa42acb09175e88cf97e9d9e4e48f616d120
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Mon Jul 15 19:32:15 2024 +0800

    [SYCL] add concat through dim 1/2 (#8483)
    
    * add concat through dim 1/2

commit 3dfda05956befb350745c5c2f7134d06adfe8724
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jul 15 14:10:39 2024 +0300

    llama : de-duplicate deepseek2 norm

commit bda62d7999caa8c222b6c354ac1e7c7442508539
Author: 0cc4m <picard12@live.de>
Date:   Mon Jul 15 09:38:52 2024 +0200

    Vulkan MMQ Fix (#8479)
    
    * Fix incoherence by adding missing LOAD_VEC_A parameter
    
    * Fix Vulkan op result checker build error

commit 090fca7a073efe9ad3dd2b9ac12491a4f20ac957
Author: compilade <git@compilade.net>
Date:   Sun Jul 14 19:51:21 2024 -0400

    pydantic : replace uses of __annotations__ with get_type_hints (#8474)
    
    * pydantic : replace uses of __annotations__ with get_type_hints
    
    * pydantic : fix Python 3.9 and 3.10 support

commit aaab2419eaa17ab3aa38f4ba49c7eea406999e99
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jul 14 18:54:02 2024 +0300

    flake.lock: Update (#8475)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/9f4128e00b0ae8ec65918efeba59db998750ead6?narHash=sha256-rwz8NJZV%2B387rnWpTYcXaRNvzUSnnF9aHONoJIYmiUQ%3D' (2024-07-03)
      → 'github:NixOS/nixpkgs/7e7c39ea35c5cdd002cd4588b03a3fb9ece6fad9?narHash=sha256-EYekUHJE2gxeo2pM/zM9Wlqw1Uw2XTJXOSAO79ksc4Y%3D' (2024-07-12)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit 73cf442e7bc9baca7b3e213b261551812f1676c9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jul 14 14:05:09 2024 +0300

    llama : fix Gemma-2 Query scaling factors (#8473)
    
    * 9B - query_pre_attn_scalar = 256 not 224
    
    See https://github.com/google/gemma_pytorch/commit/03e657582d17cb5a8617ebf333c1c16f3694670e
    
    Gemma 9b should use 256 and not 224 (self.config.hidden_size // self.config.num_attention_heads)
    
    * llama : fix Gemma-2 Query scaling factor
    
    ggml-ci
    
    ---------
    
    Co-authored-by: Daniel Han <danielhanchen@gmail.com>

commit e236528e7628a0e59751eee9addf21fc3c33d376
Author: Brian <mofosyne@gmail.com>
Date:   Sun Jul 14 16:47:14 2024 +1000

    gguf_hash.py: Add sha256 (#8470)
    
    * gguf_hash.py: Add sha256
    
    * gguf_hash.py: rename string UUIDv5 --> uuid
    
    * Apply suggestions from code review
    
    Co-authored-by: compilade <git@compilade.net>
    
    ---------
    
    Co-authored-by: compilade <git@compilade.net>

commit fa79495bb4897953a75607addd9d2cdd2ec63222
Author: compilade <git@compilade.net>
Date:   Sat Jul 13 23:35:10 2024 -0400

    llama : fix pre-tokenization of non-special added tokens (#8228)
    
    * llama : fix mpt and olmo pre-tokenizer
    
    * llama : pre-tokenize non-special user-defined tokens first
    
    * llama : fix detection of control-like user-defined tokens
    
    * convert_hf : identify which user-defined tokens are control tokens
    
    Only used in _set_vocab_gpt2() for now.
    
    * convert_hf : identify more added control tokens for SPM tokenziers
    
    This makes Gemma and Gemma-2 tokenize pretty much EVERYTHING correctly,
    including HTML tags and consecutive spaces,
    but it unfortunately requires model re-conversion.
    
    There seems to be a weird behavior of the HF tokenizer for Gemma,
    which prefers to use the 16-space token over more lengthy space tokens,
    while using the SentencePiece tokenizer does not do this.
    (the implementation in llama.cpp has the same behavior as SentencePiece)
    
    * llama : fix wrong pre-tokenization of byte tokens
    
    * llama : fix Viking pre-tokenizer regex
    
    The order was previously wrong, which caused errors in some tests.
    
    * llama : fix command-r detokenization
    
    * convert_hf : reduce usages of the UNKNOWN token type
    
    * llama : add UNKNOWN tokens in the special tokens cache
    
    * convert_hf : reduce usages of UNKNOWN for InternLM2
    
    This makes the changes from #8321 more consistent
    with the other changes made here.
    
    * test-tokenizer-random : reduce potential confilcts with #8379
    
    * test-tokenizer-random : add a failing edge case for falcon

commit 17eb6aa8a992cda37ee65cf848d9289bd6cad860
Author: bandoti <141645996+bandoti@users.noreply.github.com>
Date:   Sat Jul 13 13:12:39 2024 -0300

    vulkan : cmake integration (#8119)
    
    * Add Vulkan to CMake pkg
    
    * Add Sycl to CMake pkg
    
    * Add OpenMP to CMake pkg
    
    * Split generated shader file into separate translation unit
    
    * Add CMake target for Vulkan shaders
    
    * Update README.md
    
    * Add make target for Vulkan shaders
    
    * Use pkg-config to locate vulkan library
    
    * Add vulkan SDK dep to ubuntu-22-cmake-vulkan workflow
    
    * Clean up tabs
    
    * Move sudo to apt-key invocation
    
    * Forward GGML_EXTRA_LIBS to CMake config pkg
    
    * Update vulkan obj file paths
    
    * Add shaderc to nix pkg
    
    * Add python3 to Vulkan nix build
    
    * Link against ggml in cmake pkg
    
    * Remove Python dependency from Vulkan build
    
    * code review changes
    
    * Remove trailing newline
    
    * Add cflags from pkg-config to fix w64devkit build
    
    * Update README.md
    
    * Remove trailing whitespace
    
    * Update README.md
    
    * Remove trailing whitespace
    
    * Fix doc heading
    
    * Make glslc required Vulkan component
    
    * remove clblast from nix pkg

commit c917b67f06c42d8ca8391b9bc73f5fe62c83bf70
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jul 13 18:32:33 2024 +0300

    metal : template-ify some of the kernels (#8447)
    
    ggml-ci

commit 4e24cffd8cccd653634e24ee461c252bd77b1426
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 12 14:48:15 2024 +0300

    server : handle content array in chat API (#8449)
    
    * server : handle content array in chat API
    
    * Update examples/server/utils.hpp
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>

commit 6af51c0d96e6268769fc05c98d5b1a5e832c0017
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 12 14:48:04 2024 +0300

    main : print error on empty input (#8456)

commit f53226245f421bd01b47cce43a47e791de82c636
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Jul 12 11:05:21 2024 +0200

    llama : suppress unary minus operator warning (#8448)
    
    This commit updates the _try_copy lambda and moves the unary minus
    operator to after the cast to int32_t.
    
    The motivation for this that currently the following warning is
    generated on windows:
    
    ```console
    llama.cpp\src\llama.cpp(21147,30): warning C4146: unary minus operator
    applied to unsigned type, result still unsigned
    ```

commit c3ebcfa148e867a68e78fd5c4f0c23e8f84c788b
Author: Douglas Hanley <thesecretaryofwar@gmail.com>
Date:   Fri Jul 12 03:14:12 2024 -0500

    server : ensure batches are either all embed or all completion (#8420)
    
    * make sure batches are all embed or all non-embed
    
    * non-embedding batch for sampled tokens; fix unused params warning

commit 8a4441ea1a2564578134404f31158c318e9c0bf3
Author: Armen Kaleshian <kriation@users.noreply.github.com>
Date:   Fri Jul 12 04:08:19 2024 -0400

    docker : fix filename for convert-hf-to-gguf.py in tools.sh (#8441)
    
    Commit b0a4699 changed the name of this script from convert-hf-to-gguf.py to
    convert_hf_to_gguf.py breaking how convert is called from within a Docker
    container.

commit 5aefbce27a66473d4b1263ba3f7bdd3d14245975
Author: Jiří Podivín <66251151+jpodivin@users.noreply.github.com>
Date:   Fri Jul 12 10:06:33 2024 +0200

    convert : remove fsep token from GPTRefactForCausalLM (#8237)
    
    The <filename> token used by Refact doesn't serve
    the same purpose as the <file_separator> from CodeGemma.
    
    Signed-off-by: Jiri Podivin <jpodivin@redhat.com>

commit 71c1121d11f1437be9421fd0cbaa011b9ef49098
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 12 10:46:14 2024 +0300

    examples : sprintf -> snprintf (#8434)
    
    * examples : sprintf -> snprintf
    
    ggml-ci
    
    * examples : use sizeof() instead of hardcoded constants

commit 370b1f7e7a7514d9a63daf15f7b0f00319b7f908
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 12 10:46:02 2024 +0300

    ggml : minor naming changes (#8433)
    
    * ggml : minor naming changes
    
    ggml-ci
    
    * ggml : use PRId64 [no ci]
    
    * ggml : revert FA K/Q names

commit b549a1bbefb2f1fbb8b558bac1f2ae7967e60964
Author: Chen Xi <xixichen08@foxmail.com>
Date:   Fri Jul 12 00:52:04 2024 +0000

    [SYCL] fix the mul_mat_id ut issues (#8427)
    
    * fix part of mul_mat_id
    
    * skip the bfloat 16 sycl ut
    
    Signed-off-by: Chen Xi <xi2chen@intel.com>
    
    ---------
    
    Signed-off-by: Chen Xi <xi2chen@intel.com>
    Co-authored-by: Meng, Hengyu <hengyu.meng@intel.com>
    Co-authored-by: Chen Xi <xi2chen@intel.com>

commit 368645698ab648e390dcd7c00a2bf60efa654f57
Author: Nicholai Tukanov <nicholaitukanov@gmail.com>
Date:   Thu Jul 11 11:49:15 2024 -0500

    ggml : add NVPL BLAS support (#8329) (#8425)
    
    * ggml : add NVPL BLAS support
    
    * ggml : replace `<BLASLIB>_ENABLE_CBLAS` with `GGML_BLAS_USE_<BLASLIB>`
    
    ---------
    
    Co-authored-by: ntukanov <ntukanov@nvidia.com>

commit b078c619aa4e97fe726d61b0b5499a2e19a418a4
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Jul 11 17:53:42 2024 +0200

    cuda : suppress 'noreturn' warn in no_device_code (#8414)
    
    * cuda : suppress 'noreturn' warn in no_device_code
    
    This commit adds a while(true) loop to the no_device_code function in
    common.cuh. This is done to suppress the warning:
    
    ```console
    /ggml/src/ggml-cuda/template-instances/../common.cuh:346:1: warning:
    function declared 'noreturn' should not return [-Winvalid-noreturn]
      346 | }
          | ^
    ```
    
    The motivation for this is to reduce the number of warnings when
    compilng with GGML_HIPBLAS=ON.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! cuda : suppress 'noreturn' warn in no_device_code
    
    Update __trap macro instead of using a while loop to suppress the
    warning.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 808aba39161e5d7ca2ff24110b5aa14d2e536988
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Jul 11 16:47:47 2024 +0200

    CUDA: optimize and refactor MMQ (#8416)
    
    * CUDA: optimize and refactor MMQ
    
    * explicit q8_1 memory layouts, add documentation

commit a977c115448e40856fb9cbe3ceb6d8ce802553b0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jul 11 11:20:40 2024 +0300

    gitignore : deprecated binaries

commit 9a55ffe6fbf7f19c865f8f277ca7cd585cc0b094
Author: compilade <git@compilade.net>
Date:   Thu Jul 11 03:41:48 2024 -0400

    tokenize : add --no-parse-special option (#8423)
    
    This should allow more easily explaining
    how parse_special affects tokenization.

commit 7a221b672e49dfae459b1af27210ba3f2b5419b6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jul 11 10:21:30 2024 +0300

    llama : use F32 precision in Qwen2 attention and no FA (#8412)

commit 278d0e18469aacf505be18ce790a63c7cc31be26
Author: Clint Herron <hanclinto@gmail.com>
Date:   Wed Jul 10 20:08:17 2024 -0400

    Initialize default slot sampling parameters from the global context. (#8418)

commit dd07a123b79f9bd9e8a4ba0447427b3083e9347a
Author: Clint Herron <hanclinto@gmail.com>
Date:   Wed Jul 10 12:35:18 2024 -0400

    Name Migration: Build the deprecation-warning 'main' binary every time (#8404)
    
    * Modify the deprecation-warning 'main' binary to build every time, instead of only when a legacy binary is present. This is to help users of tutorials and other instruction sets from knowing what to do when the 'main' binary is missing and they are trying to follow instructions.
    
    * Adjusting 'server' name-deprecation binary to build all the time, similar to the 'main' legacy name binary.

commit f4444d992c16b6b9442f4770c7c3a10b19a08343
Author: AidanBeltonS <aidan.belton@codeplay.com>
Date:   Wed Jul 10 16:10:49 2024 +0100

    [SYCL] Use multi_ptr to clean up deprecated warnings (#8256)

commit 6b2a849d1f43d46b82d2f9c08c3275137b528784
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jul 10 15:23:29 2024 +0300

    ggml : move sgemm sources to llamafile subfolder (#8394)
    
    ggml-ci

commit 0f1a39f3439825acf7e3a1663566d410be152170
Author: Dibakar Gope <dibakar.gope@arm.com>
Date:   Wed Jul 10 07:14:51 2024 -0500

    ggml : add AArch64 optimized GEMV and GEMM Q4 kernels (#5780)
    
    * Arm AArch64: optimized GEMV and GEMM kernels for q4_0_q8_0, and q8_0_q8_0 quantization
    
    * Arm AArch64: add optimized GEMV and GEMM asm kernels for q4_0_q8_0 quantization and refactor code to address llama.cpp pr#5780 suggestions
    
    * Arm AArch64: add optimized GEMV and GEMM asm kernels for q4_0_q8_0 quantization and refactor code to address llama.cpp pr#5780 suggestions
    
    * Arm AArch64: add optimized GEMV and GEMM asm kernels for q4_0_q8_0 quantization and refactor code to address llama.cpp pr#5780 suggestions
    
    * Arm AArch64: add optimized GEMV and GEMM asm kernels for q4_0_q8_0 quantization and refactor code to address llama.cpp pr#5780 suggestions
    
    * Arm AArch64: add copyright claim only to ggml-aarch64.cpp and ggml-aarch64.h files
    
    * Arm AArch64: minor code refactoring for rebase
    
    * Arm AArch64: minor code refactoring for resolving a build issue with cmake
    
    * Arm AArch64: minor code refactoring to split the Q4_0_AARC64 type into three separate types: Q4_0_4_4, Q4_0_4_8, and Q4_0_8_8
    
    * Arm AArch64: minor code change for resolving a build issue with server-windows
    
    * retrigger checks
    
    * Arm AArch64: minor code changes for rebase
    
    * Arm AArch64: minor changes to skip the pr#7433 vec_dot code for arm cpus with SVE VL not equal to 256 bits
    
    * Arm AArch64: remove stale LLAMA_QKK_64 from CMakeLists.txt and delete build.zig
    
    * Arm AArch64: add reference scalar gemm and gemv, and avoid dynamic memory allocations during quantization for Q4_0_4_4, Q4_0_4_8, and Q4_0_8_8
    
    * Arm AArch64: add multithreaded quantization support for the new types: Q4_0_4_4, Q4_0_4_8, and Q4_0_8_8
    
    * Arm AArch64: minor code refactoring
    
    * Arm AArch64: simplify logic for calling gemm and gemv functions in ggml_compute_forward_mul_mat
    
    * Arm AArch64: minimize changes in ggml_compute_forward_mul_mat
    
    * Arm AArch64: minor code refactoring, and add reference scalar code to quantize routines for new quant types
    
    * Arm AArch64: minor code refactoring
    
    * Arm AArch64: minor code refactoring
    
    * Arm AArch64: minor code refactoring
    
    * rebase on the latest master commit 3fd62a6 and adapt to the new directory structure
    
    * Arm AArch64: remove a redundant comment
    
    * Arm AArch64: add pragma in ggml-aarch64.c to turn -Woverlength-strings warning off
    
    * Arm AArch64: use __aarch64__ check to guard 64-bit neon kernels
    
    * Arm AArch64: update docs/build.md README to include compile time flags for buiilding the Q4_0_4_4 quant type

commit 83321c6958acf20747d288031174cc1bf8816e33
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Wed Jul 10 15:12:35 2024 +0300

    gguf-py rel pipeline (#8410)
    
    * Upd gguf-py/readme
    
    * Bump patch version for release

commit cc61948b1f97165b46990f4c2d9699bf9bb64443
Author: Borislav Stanimirov <b.stanimirov@abv.bg>
Date:   Wed Jul 10 14:45:44 2024 +0300

    llama : C++20 compatibility for u8 strings (#8408)

commit 7a80710d93ee0f4e0d335d65984ae747e62c0337
Author: Borislav Stanimirov <b.stanimirov@abv.bg>
Date:   Wed Jul 10 14:40:53 2024 +0300

    msvc : silence codecvt c++17 deprecation warnings (#8395)

commit a8be1e6f5995bec3b1d8474d48ce3a139553a8e1
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Wed Jul 10 13:38:58 2024 +0200

    llama : add assert about missing llama_encode() call (#8400)
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

commit e4dd31ff892627665d3c53c5d1a03c0d282d9d45
Author: RunningLeon <maningsheng@sensetime.com>
Date:   Wed Jul 10 19:26:40 2024 +0800

    py : fix converter for internlm2 (#8321)
    
    * update internlm2
    
    * remove unused file
    
    * fix lint

commit 8f0fad42b9589f9b11362c74ea5338c51e4c7e61
Author: laik <laik.lj@me.com>
Date:   Wed Jul 10 19:19:10 2024 +0800

    py : fix extra space in convert_hf_to_gguf.py (#8407)

commit a59f8fdc85e1119d470d8766e29617962549d993
Author: Clint Herron <hanclinto@gmail.com>
Date:   Tue Jul 9 18:26:40 2024 -0400

    Server: Enable setting default sampling parameters via command-line (#8402)
    
    * Load server sampling parameters from the server context by default.
    
    * Wordsmithing comment

commit fd560fe680c72fd0a0af2bc8881add20ad919071
Author: Andy Salerno <andysalerno@gmail.com>
Date:   Tue Jul 9 11:58:44 2024 -0700

    Update README.md to fix broken link to docs (#8399)
    
    Update the "Performance troubleshooting" doc link to be correct - the file was moved into a dir called 'development'

commit e500d6135ac42c4a23baf6a9a1a934dc023e5c7d
Author: Clint Herron <hanclinto@gmail.com>
Date:   Tue Jul 9 11:54:43 2024 -0400

    Deprecation warning to assist with migration to new binary names (#8283)
    
    * Adding a simple program to provide a deprecation warning that can exist to help people notice the binary name change from #7809 and migrate to the new filenames.
    
    * Build legacy replacement binaries only if they already exist. Check for their existence every time so that they are not ignored.

commit a03e8dd99d3954e7547137936c5a2a3b348bdb7f
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Jul 9 17:11:07 2024 +0200

    make/cmake: LLAMA_NO_CCACHE -> GGML_NO_CCACHE (#8392)

commit 5b0b8d8cfb5ddf2118f686ba6c30fab3f71b384b
Author: Alberto Cabrera Pérez <alberto.cabrera@codeplay.com>
Date:   Tue Jul 9 15:03:15 2024 +0100

    sycl : Reenabled mmvq path for the SYCL Nvidia Backend (#8372)
    
    * SYCL : Reenabled mmvq path for the SYCL Nvidia Backend
    
    * Reduced verbosity of comment

commit 9925ca4087a34ab973b07bf06c0b770cb586830b
Author: Borislav Stanimirov <b.stanimirov@abv.bg>
Date:   Tue Jul 9 11:38:00 2024 +0300

    cmake : allow external ggml (#8370)

commit 9beb2dda038e2107a39a95def94a4cf971e048ea
Author: daghanerdonmez <44506702+daghanerdonmez@users.noreply.github.com>
Date:   Tue Jul 9 09:16:00 2024 +0300

    readme : fix typo [no ci] (#8389)
    
    Bakus-Naur --> Backus-Naur

commit 7d0e23d72ef4540d0d4409cb63ae682c17d53926
Author: compilade <git@compilade.net>
Date:   Tue Jul 9 01:04:49 2024 -0400

    gguf-py : do not use internal numpy types (#7472)

commit 7fdb6f73e35605c8dbc39e9f19cd9ed84dbc87f2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jul 9 01:36:38 2024 +0300

    flake.lock: Update (#8342)
    
    Flake lock file updates:
    
    • Updated input 'flake-parts':
        'github:hercules-ci/flake-parts/2a55567fcf15b1b1c7ed712a2c6fadaec7412ea8?narHash=sha256-iKzJcpdXih14qYVcZ9QC9XuZYnPc6T8YImb6dX166kw%3D' (2024-06-01)
      → 'github:hercules-ci/flake-parts/9227223f6d922fee3c7b190b2cc238a99527bbb7?narHash=sha256-pQMhCCHyQGRzdfAkdJ4cIWiw%2BJNuWsTX7f0ZYSyz0VY%3D' (2024-07-03)
    • Updated input 'flake-parts/nixpkgs-lib':
        'https://github.com/NixOS/nixpkgs/archive/eb9ceca17df2ea50a250b6b27f7bf6ab0186f198.tar.gz?narHash=sha256-lIbdfCsf8LMFloheeE6N31%2BBMIeixqyQWbSr2vk79EQ%3D' (2024-06-01)
      → 'https://github.com/NixOS/nixpkgs/archive/5daf0514482af3f97abaefc78a6606365c9108e2.tar.gz?narHash=sha256-Fm2rDDs86sHy0/1jxTOKB1118Q0O3Uc7EC0iXvXKpbI%3D' (2024-07-01)
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/b2852eb9365c6de48ffb0dc2c9562591f652242a?narHash=sha256-C8e9S7RzshSdHB7L%2Bv9I51af1gDM5unhJ2xO1ywxNH8%3D' (2024-06-27)
      → 'github:NixOS/nixpkgs/9f4128e00b0ae8ec65918efeba59db998750ead6?narHash=sha256-rwz8NJZV%2B387rnWpTYcXaRNvzUSnnF9aHONoJIYmiUQ%3D' (2024-07-03)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit a130eccef42b75a84da270411cefeed45c153e30
Author: Alberto Cabrera Pérez <alberto.cabrera@codeplay.com>
Date:   Mon Jul 8 21:35:17 2024 +0100

    labeler : updated sycl to match docs and code refactor (#8373)

commit c4dd11d1d3903e1922c06242e189f6310fc4d8c3
Author: b4b4o <zwbao@foxmail.com>
Date:   Mon Jul 8 22:19:24 2024 +0800

    readme : fix web link error [no ci] (#8347)

commit 2ec846d558f6385ea647f7b8e665eb249c1ebce7
Author: Alberto Cabrera Pérez <alberto.cabrera@intel.com>
Date:   Mon Jul 8 14:22:41 2024 +0100

    sycl : fix powf call in device code (#8368)

commit 3f2d538b817112ad8429341c7e8657dcd660f4d3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jul 8 13:51:31 2024 +0300

    scripts : fix sync for sycl

commit 2ee44c9a1865a928ccbbc16a2d7841d7513f31c1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jul 8 10:39:50 2024 +0300

    sync : ggml
    
    ggml-ci

commit 6847d54c4f72f8bf6767b6feae7a95394654335e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jul 8 10:39:36 2024 +0300

    tests : fix whitespace (#0)

commit fde13b3bb9f569f07fd8af74696ee48a43d05131
Author: John Balis <phobossystems@gmail.com>
Date:   Tue Jul 2 11:09:52 2024 -0500

    feat: cuda implementation for `ggml_conv_transpose_1d` (ggml/854)
    
    * conv transpose 1d passing test for 1d input and kernel
    
    * working for different input and output channel counts, added test for variable stride
    
    * initial draft appears to work with stride other than 1
    
    * working with all old and new conv1d  tests
    
    * added a test for large tensors
    
    * removed use cuda hardcoding
    
    * restored test-conv-transpose.c
    
    * removed unused arugments, and fixed bug where test failure would cause subsequent tests to fail
    
    * fixed accumulator bug
    
    * added test to test-backend-ops
    
    * fixed mistake
    
    * addressed review
    
    * fixed includes
    
    * removed blank lines
    
    * style and warning fixes
    
    * return failure when test fails
    
    * fix supports_op
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 470939d483d1c89b7292f78bac1fd27c42c171ce
Author: Kevin Wang <kevmo314@gmail.com>
Date:   Mon Jul 8 03:26:53 2024 -0400

    common : preallocate sampling token data vector (#8363)
    
    `emplace_back` repeatedly-called is slower than preallocating the vector to the vocab size and directly inserting the data. Some rudimentary profiling with `chrono` improves the performance of this block of code from ~500us/op to ~40us/op.
    
    Overall, this slightly improves the sampling performance which has a more substantial impact for the `examples/lookahead` implementation -- I am able to see a ~10% performance boost in lookahead inference.

commit 6f0dbf6ab087bcd286fb78560099ca0458316735
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jul 8 09:34:35 2024 +0300

    infill : assert prefix/suffix tokens + remove old space logic (#8351)

commit ffd00797d81ef7db1528b9e10adbdc333ade6495
Author: Kevin Wang <kevmo314@gmail.com>
Date:   Mon Jul 8 02:31:55 2024 -0400

    common : avoid unnecessary logits fetch (#8358)

commit 04ce3a8b19256a155aea4d14eaa87edf274c93c3
Author: toyer <2042519524@qq.com>
Date:   Mon Jul 8 13:57:19 2024 +0800

    readme : add supported glm models (#8360)

commit 3fd62a6b1c9ca7b7c0093e984cc9c133c6f2726d
Author: compilade <git@compilade.net>
Date:   Sun Jul 7 15:04:39 2024 -0400

    py : type-check all Python scripts with Pyright (#8341)
    
    * py : type-check all Python scripts with Pyright
    
    * server-tests : use trailing slash in openai base_url
    
    * server-tests : add more type annotations
    
    * server-tests : strip "chat" from base_url in oai_chat_completions
    
    * server-tests : model metadata is a dict
    
    * ci : disable pip cache in type-check workflow
    
    The cache is not shared between branches, and it's 250MB in size,
    so it would become quite a big part of the 10GB cache limit of the repo.
    
    * py : fix new type errors from master branch
    
    * tests : fix test-tokenizer-random.py
    
    Apparently, gcc applies optimisations even when pre-processing,
    which confuses pycparser.
    
    * ci : only show warnings and errors in python type-check
    
    The "information" level otherwise has entries
    from 'examples/pydantic_models_to_grammar.py',
    which could be confusing for someone trying to figure out what failed,
    considering that these messages can safely be ignored
    even though they look like errors.

commit a8db2a9ce64cd4417f6a312ab61858f17f0f8584
Author: Denis Spasyuk <34203011+dspasyuk@users.noreply.github.com>
Date:   Sun Jul 7 09:08:28 2024 -0600

    Update llama-cli documentation (#8315)
    
    * Update README.md
    
    * Update README.md
    
    * Update README.md
    
    fixed llama-cli/main, templates on some cmds added chat template sections and fixed typos in some areas
    
    * Update README.md
    
    * Update README.md
    
    * Update README.md

commit 4090ea5501c702cd858095c394ba068919c56cc8
Author: Alex Tuddenham <61622354+AlexsCode@users.noreply.github.com>
Date:   Sun Jul 7 15:59:14 2024 +0100

    ci : add checks for cmake,make and ctest in ci/run.sh (#8200)
    
    * Added checks for cmake,make and ctest
    
    * Removed erroneous whitespace

commit f1948f1e108157d5e7eb60fc8f24a10412e5d4ff
Author: Andy Tai <andy-tai@users.noreply.github.com>
Date:   Sun Jul 7 06:21:37 2024 -0700

    readme : update bindings list (#8222)
    
    * adding guile_llama_cpp  to binding list
    
    * fix formatting
    
    * fix formatting

commit f7cab35ef9e66c57b4a416d09eb6c814e0ba4e4c
Author: Brian <mofosyne@gmail.com>
Date:   Sun Jul 7 22:58:43 2024 +1000

    gguf-hash: model wide and per tensor hashing using xxhash and sha1 (#8048)
    
    CLI to hash GGUF files to detect difference on a per model and per tensor level
    
    The hash type we support is:
    
    - `--xxh64`: use xhash 64bit hash mode (default)
    - `--sha1`: use sha1
    - `--uuid`: use uuid
    - `--sha256`: use sha256
    
    While most POSIX systems already have hash checking programs like sha256sum, it
    is designed to check entire files. This is not ideal for our purpose if we want
    to check for consistency of the tensor data even if the metadata content of the
    gguf KV store has been updated.
    
    This program is designed to hash a gguf tensor payload on a 'per tensor layer'
    in addition to a 'entire tensor model' hash. The intent is that the entire
    tensor layer can be checked first but if there is any detected inconsistencies,
    then the per tensor hash can be used to narrow down the specific tensor layer
    that has inconsistencies.
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 905942abdba5ba0b28a1b0805e51e4f818c54bc9
Author: toyer <2042519524@qq.com>
Date:   Sun Jul 7 20:52:10 2024 +0800

    llama : support glm3 and glm4 (#8031)
    
    * add chatglm3-6b model support huggingface model:
     https://hf-mirror.com/THUDM/chatglm3-6b
    
    Signed-off-by: XingXing Qiao <qiaoxx@dingdao.com>
    
    * remove .rotary_pos_emb.inv_freq and unuse code for chatglm3 model
    
    Signed-off-by: XingXing Qiao <qiaoxx@dingdao.com>
    
    * fix lint error
    
    Signed-off-by: XingXing Qiao <qiaoxx@dingdao.com>
    
    * optimize convert-hf-to-gguf.py for chatglm model
    
    Signed-off-by: XingXing Qiao <qiaoxx@dingdao.com>
    
    * support glm-4-9b-chat
    
    Signed-off-by: XingXing Qiao <qiaoxx@dingdao.com>
    
    * fix eos tokens to glm4
    
    * remove unused log
    
    * add preprocess to chatglm3 and chatglm4
    
    * add eos_id_list to llama.cpp
    
    * fix code style
    
    * fix code style
    
    * fix conflicts
    
    * fix conflicts
    
    * Revert "add eos_id_list to llama.cpp"
    
    This reverts commit 3a4d5790bfdc205c5b658204239f168fc21cc1a8.
    
    * set <|endoftext|> as eos and <|user|> as eot
    
    * fix chat template bug
    
    * add comment to glm prefix and suffix
    
    * fix conflicts and add rope_ratio & ChatGLMForConditionalGeneration
    
    * fix chat template bug
    
    * fix codestyle
    
    * fix conflicts
    
    * modified the general name of glm model
    
    * fix conflicts
    
    * remove prefix and suffix
    
    * use normal glm4 chattempalte & use LLM_FFN_SWIGLU in phi3
    
    * fix: resolve Flake8 errors in `convert-hf-to-gguf.py`
    
    - Fix E302 by adding two blank lines before top-level function definitions
    - Replace print statements to fix NP100
    - Fix E303 by ensuring only one blank line between lines of code
    
    * fix rope ratio to solve incorrect answers
    
    * fix by comments
    
    ---------
    
    Signed-off-by: XingXing Qiao <qiaoxx@dingdao.com>
    Co-authored-by: XingXing Qiao <qiaoxx@dingdao.com>
    Co-authored-by: Umpire2018 <138990495+Umpire2018@users.noreply.github.com>

commit b5040086d436e7345e4fa33a5b9558060c75603f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jul 7 14:59:02 2024 +0300

    llama : fix n_rot default (#8348)
    
    ggml-ci

commit d39130a3985ce0747d7229a6b7ebebb6376b5abf
Author: compilade <git@compilade.net>
Date:   Sun Jul 7 07:23:38 2024 -0400

    py : use cpu-only torch in requirements.txt (#8335)

commit b81ba1f96b06c691020429a2d2dfcbada899ca86
Author: standby24x7 <standby24x7@gmail.com>
Date:   Sun Jul 7 19:38:02 2024 +0900

    finetune: Rename command name in README.md (#8343)
    
    Rename an old command name "finetune" to "llama-finetune"
    in README.md
    
    Signed-off-by: Masanari Iida <standby24x7@gmail.com>

commit 210eb9ed0ac3979a93e37568d96447ec3e95ad05
Author: standby24x7 <standby24x7@gmail.com>
Date:   Sun Jul 7 19:37:47 2024 +0900

    finetune: Rename an old command name in finetune.sh (#8344)
    
    This patch replaces an old commad "main" with "llama-cli"
    in finetune.sh.
    The part that I fixed is comment, so it doesn't change
    the script.
    
    Signed-off-by: Masanari Iida <standby24x7@gmail.com>

commit cb4d86c4d723af87d3d7e3177e9485f200391384
Author: Bjarke Viksøe <164612031+bviksoe@users.noreply.github.com>
Date:   Sun Jul 7 11:10:38 2024 +0200

    server: Retrieve prompt template in /props (#8337)
    
    * server: Retrieve prompt template in /props
    
    This PR adds the following:
    - Expose the model's Jinja2 prompt template from the model in the /props endpoint.
    - Change log-level from Error to Warning for warning about template mismatch.
    
    The front-end stands a better chance of actually executing the Jinja template format correctly. Server is currently just guessing it.
    
    Ideally this should have been inside a JSON block that expose the same key/value pairs as listed during startup in "llm_load_print_meta" function.
    
    * Make string buffer dynamic
    
    * Add doc and better string handling
    
    * Using chat_template naming convention
    
    * Use intermediate vector for string assignment

commit 86e7299ef5dff0f388922dc6fcbce009e99d8005
Author: Derrick T. Woolworth <dwoolworth@gmail.com>
Date:   Sat Jul 6 15:32:04 2024 -0500

    added support for Authorization Bearer tokens when downloading model (#8307)
    
    * added support for Authorization Bearer tokens
    
    * removed auth_token, removed set_ function, other small fixes
    
    * Update common/common.cpp
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>

commit 60d83a0149849e9217e4b8ae26e277a41aea906e
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat Jul 6 19:01:23 2024 +0200

    update main readme (#8333)

commit 87e25a1d1bd26eb06d1cab9e2ee4e14a7a0be33c
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Sat Jul 6 09:22:16 2024 +0200

    llama : add early return for empty range (#8327)
    
    * llama : add early return for empty range
    
    This commit adds an early return to the llama_kv_cache_seq_add and
    llama_kv_cache_seq_div functions.
    
    The motivation for adding this is to avoid looping over the cache
    when the range is empty. I ran into this when using the self-extend
    feature in main.cpp.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * llama : add static_cast to fix CI warning/error
    
    This commit attempts to fix the following warning/error:
    
    ```console
    src/llama.cpp:7271:31: error:
    comparison of integer expressions of different signedness:
    ‘int’ and ‘uint32_t’ {aka ‘unsigned int’} [-Werror=sign-compare]
     7271 |                         if (i < hparams.n_layer_dense_lead) {
          |                             ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~
    ```
    This can be reproduced locally by setting -Wsign-compare in the
    Makefile.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! llama : add early return for empty range
    
    Remove the setting of cache.head to 0 when the range is empty.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * Update src/llama.cpp
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 213701b51a17175d0d326b566efc03f30ec7fbe6
Author: jaime-m-p <167997752+jaime-m-p@users.noreply.github.com>
Date:   Fri Jul 5 19:01:35 2024 +0200

    Detokenizer fixes (#8039)
    
    * Add llama_detokenize():
      - Update header files location
      - UNKNOWN and CONTROL are 'special pieces'
      - Remove space after UNKNOWN and CONTROL
      - Refactor llama_token_to_piece()
      - Add flag: clean_up_tokenization_spaces
      - Symmetric params for llama_tokenize() and llama_detokenize()
    
    * Update and fix tokenizer tests:
      - Using llama_detokenize()
      - Unexpected vocab type as test fail instead of error
        - Useful when automating tests:
        - If you don't know in advance the vocab type
        - Differenciate other loading errors
      - Skip unicode surrogaes and undefined
      - Gracefully exit threads
        - Using exit() is throwing random exceptions
      - Clean old known problematic codepoints
      - Minor: confusing hexadecimal codepoint
    
    * Update bruteforce random tests
      - Add detokenizer checks
      - New generator: ascii_lr_strip
      - New generator: apostrophe
      - Add more vocabs files
      - Detokenize special tokens.
      - Replace errors with '\uFFFD' when detokenizing to 'utf-8'
      - More edge cases
      - Better detokenization results check
    
    * Fix add_space_prefix, set false by default
    * Better leading space removal
    * Do not remove space when decoding special tokens
    * Bugfix: custom regexs splits undefined unicode codepoints
    * 'viking' detokenizer clean spaces

commit be20e7f49d9e5c6d9e8d9b4871eeba3df7a1639d
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Jul 5 18:08:32 2024 +0200

    Reorganize documentation pages (#8325)
    
    * re-organize docs
    
    * add link among docs
    
    * add link to build docs
    
    * fix style
    
    * de-duplicate sections

commit 7ed03b8974269b6c48e55c4245d12fb3264a6cf5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 5 17:32:09 2024 +0300

    llama : fix compile warning (#8304)

commit 1d894a790e62b10d066adcdd9c9feb559455b7d2
Author: Natsu <chino@hotococoa.moe>
Date:   Fri Jul 5 22:29:35 2024 +0800

    cmake : add GGML_BUILD and GGML_SHARED macro definitions (#8281)

commit 1f3e1b66e21310ed78b964f72f19766549633f0e
Author: Ouadie EL FAROUKI <ouadie.elfarouki@codeplay.com>
Date:   Fri Jul 5 13:23:25 2024 +0100

    Enabled more data types for oneMKL gemm_batch (#8236)

commit 148ec970b62c3c5ae0a8bfdaad2fc237aaae350d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 5 10:15:36 2024 +0300

    convert : remove AWQ remnants (#8320)

commit 2cccbaa0086c62801f95405131a5b2faf3ba1de8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 5 10:15:24 2024 +0300

    llama : minor indentation during tensor loading (#8304)
    
    * llama : minor indentation during tensor loading
    
    ggml-ci
    
    * llama : use int for layer iterators [no ci]

commit 8e558309dc149dc1f9fd159185b0b9071527ffb5
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Jul 5 09:06:31 2024 +0200

    CUDA: MMQ support for iq4_nl, iq4_xs (#8278)

commit 0a423800ffe4e5da3d83527ef3473da88cd78146
Author: Daniele <57776841+daniandtheweb@users.noreply.github.com>
Date:   Fri Jul 5 07:06:09 2024 +0000

    CUDA: revert part of the RDNA1 optimizations (#8309)
    
    The change on the launch_bounds was causing a small performance drop in perplexity of 25 t/s

commit d12f781074b92589a72a36ffabb583933f7b9dc0
Author: Douglas Hanley <thesecretaryofwar@gmail.com>
Date:   Fri Jul 5 02:05:56 2024 -0500

    llama : streamline embeddings from "non-embedding" models (#8087)

commit bcefa03bc01a41aace2e200ee8e77827d6d39b4f
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Jul 5 09:05:34 2024 +0200

    CUDA: fix MMQ stream-k rounding if ne00 % 128 != 0 (#8311)

commit 5a7447c5692c9e3dde1161e8e69edb76d3d34714
Author: Pieter Ouwerkerk <pieter.ouwerkerk@gmail.com>
Date:   Fri Jul 5 02:58:41 2024 -0400

    readme : fix minor typos [no ci] (#8314)

commit 61ecafa3905a02a299b7ae889b5578cfeb8d79df
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Jul 5 08:14:24 2024 +0200

    passkey : add short intro to README.md [no-ci] (#8317)
    
    * passkey : add short intro to README.md [no-ci]
    
    This commit adds a short introduction to the README.md file in the
    examples/passkey directory.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * Update examples/passkey/README.md
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit aa5898dc53afcf77f16222cd719e10b29049f95a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 5 09:10:03 2024 +0300

    llama : prefer n_ over num_ prefix (#8308)

commit 6c05752c507f51b88348f16d9e2c8df49f66c028
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 5 09:09:47 2024 +0300

    contributing : update guidelines (#8316)

commit a9554e20b66546b0549aebe2e1034bc8afe9d809
Author: luoyu-intel <yu.luo@intel.com>
Date:   Fri Jul 5 05:06:13 2024 +0000

    [SYCL] Fix WARP_SIZE=16 bug of Intel GPU (#8266)
    
    * fix group_norm ut
    
    * split softmax
    
    * fix softmax
    
    * add concat support condition
    
    * revert debug code
    
    * move QK_WARP_SIZE to presets.hpp

commit e235b267a2539d043734ff340eff74107722eb57
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 5 07:53:33 2024 +0300

    py : switch to snake_case (#8305)
    
    * py : switch to snake_case
    
    ggml-ci
    
    * cont
    
    ggml-ci
    
    * cont
    
    ggml-ci
    
    * cont : fix link
    
    * gguf-py : use snake_case in scripts entrypoint export
    
    * py : rename requirements for convert_legacy_llama.py
    
    Needed for scripts/check-requirements.sh
    
    ---------
    
    Co-authored-by: Francis Couture-Harpin <git@compilade.net>

commit f09b7cb609d80b8031803f89255991dc8b35db69
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Fri Jul 5 10:32:29 2024 +0800

    rm get_work_group_size() by local cache for performance (#8286)
    
    Co-authored-by: arthw <14088817+arthw@users.noreply.github.com>

commit a38b884c6c4b0c256583acfaaabdf556c62fabea
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Jul 4 20:55:03 2024 +0200

    cli: add EOT when user hit Ctrl+C (#8296)
    
    * main: add need_insert_eot
    
    * do not format system prompt if it is empty

commit d7fd29fff16456ce9c3a23fd2d09a66256b05aff
Author: Icecream95 <the.real.icecream95@gmail.com>
Date:   Fri Jul 5 05:14:21 2024 +1200

    llama : add OpenELM support (#7359)
    
    * Initial OpenELM support (270M only so far)
    
    * Fill out missing entries in llama_model_type_name
    
    * fixup! Initial OpenELM support (270M only so far)
    
    Fix formatting
    
    * llama : support all OpenELM models
    
    * llama : add variable GQA and variable FFN sizes
    
    Some metadata keys can now also be arrays to support setting
    their value per-layer for models like OpenELM.
    
    * llama : minor spacing changes
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * llama : use std::array for per-layer hparams
    
    * llama : fix save/load state
    
    * llama : do not print hparams for vocab-only models
    
    * llama : handle n_head == 0
    
    * llama : use const ref for print_f and fix division by zero
    
    * llama : fix t5 uses of n_head and n_ff
    
    * llama : minor comment
    
    ---------
    
    Co-authored-by: Francis Couture-Harpin <git@compilade.net>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 6f63d646c1a06a6e09f721009a2676864ae04e31
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Jul 4 18:38:58 2024 +0200

    tokenize : add --show-count (token) option (#8299)
    
    This commit adds a new option to the tokenize example, --show-count.
    When this is set the total number of tokens are printed to stdout.
    
    This was added as an option as I was concerned that there might be
    scripts that use the output from this program and it might be better to
    not print this information by default.
    
    The motivation for this is that can be useful to find out how many
    tokens a file contains, for example when trying to determine prompt
    input file sizes for testing.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 51d2ebadbbf365b894f3888361df42dbacb12b7a
Author: ditsuke <ditsuke@protonmail.com>
Date:   Thu Jul 4 20:54:35 2024 +0530

    build: Export hf-to-gguf as snakecase

commit 1e920018d38aee270a044cc48eaefe7c64cb8750
Author: ditsuke <ditsuke@protonmail.com>
Date:   Wed Jul 3 01:02:56 2024 +0530

    doc: Add context for why we add an explicit pytorch source

commit 01a5f06550a866bd63717635e5e7e1ff4203b873
Author: ditsuke <ditsuke@protonmail.com>
Date:   Tue Jul 2 15:48:13 2024 +0530

    chore: Remove rebase artifacts

commit 07786a61a2097306ee496f565f78796f1aa205e6
Author: ditsuke <ditsuke@protonmail.com>
Date:   Tue Jul 2 15:35:43 2024 +0530

    chore: Fixup requirements and build

commit de14e2ea2b542386dcb800226eb360be76f433fd
Author: ditsuke <ditsuke@protonmail.com>
Date:   Tue Jul 2 15:18:13 2024 +0530

    chore: ignore all __pychache__

commit 821922916f95cc3853fc7f58ea2f1e15a0ffa669
Author: ditsuke <ditsuke@protonmail.com>
Date:   Sun Mar 10 23:21:46 2024 +0530

    fix: Update script paths in CI scripts

commit b1c3f26e5e882fa46d7a6704d893b31a025e9000
Author: ditsuke <ditsuke@protonmail.com>
Date:   Thu Feb 29 01:47:15 2024 +0530

    fix: Actually include scripts in build
    
    Not namespaced though :(

commit b0a46993dfbf8b8127598f319d4dcfdd83824ba8
Author: ditsuke <ditsuke@protonmail.com>
Date:   Tue Feb 27 12:01:02 2024 +0530

    build(python): Package scripts with pip-0517 compliance

commit 807b0c49ff7071094f97ebc3a0a8e2b9e274f503
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Thu Jul 4 15:46:11 2024 +0200

    Inference support for T5 and FLAN-T5 model families (#5763)
    
    * llama : add inference support and model types for T5 and FLAN-T5 model families
    
    * llama : add new API functions to support encoder-decoder models: llama_encode(), llama_model_has_encoder(), llama_model_decoder_start_token()
    
    * common, llama-cli, llama-batched : add support for encoder-decoder models
    
    * convert-hf : handle shared token embeddings tensors in T5Model
    
    * convert-hf : add support for SentencePiece BPE tokenizer in T5Model (for Pile-T5 models)
    
    * convert-hf : add MT5ForConditionalGeneration and UMT5ForConditionalGeneration to architectures supported by T5Model
    
    * convert : add t5 tokenizer tests, use "slow" HF tokenizer for t5
    
    ---------
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit f8c4c0738d72d2162736edd72dd5db8b269adca1
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Jul 4 12:53:42 2024 +0200

    tests : add _CRT_SECURE_NO_WARNINGS for WIN32 (#8231)
    
    This commit adds the compile definition `_CRT_SECURE_NO_WARNINGS`
    to the root cmake subproject.
    
    The motivation for this is that currently the following warnings are
    displayed when compiling the tests and common cmake subprojects:
    ```console
    test-llama-grammar.cpp
    C:\llama.cpp\src\.\llama.cpp(1406,77): warning C4996: 'strerror':
    This function or variable may be unsafe. Consider using strerror_s
    instead. To disable deprecation, use _CRT_SECURE_NO_WARNINGS. See
    online help for details.
    [C:\llama.cpp\build\tests\test-llama-grammar.vcxproj]
    ...
    ```
    
    This compile definition is currently set for the `src` subproject
    and this change moves into the root cmake project so that it is applied
    to all cmake subprojects.

commit 402d6feffa0572d1c7a957901b6d1702bd188484
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Jul 4 12:50:57 2024 +0200

    llama : suppress unref var in Windows MSVC (#8150)
    
    * llama : suppress unref var in Windows MSVC
    
    This commit suppresses two warnings that are currently generated for
    src/llama.cpp when building on Windows MSVC
    
    ```console
    C:\llama.cpp\src\llama.cpp(14349,45): warning C4101: 'ex':
    unreferenced local variable [C:\llama.cpp\build\src\llama.vcxproj]
    C:\llama.cpp\src\llama.cpp(19285,44): warning C4101: 'e':
    unreferenced local variable [C:\llama.cpp\build\src\llama.vcxproj]
    ```
    
    * Update src/llama.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 20fc3804bfb727074bc270b6eacb60af8d0bf7d4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jul 4 10:41:03 2024 +0300

    convert : fix gemma v1 tokenizer convert (#8248)
    
    ggml-ci

commit f619024764e72261f14d7c31d892b8fb976603b4
Author: AidanBeltonS <aidan.belton@codeplay.com>
Date:   Thu Jul 4 02:07:19 2024 +0100

    [SYCL] Remove unneeded semicolons (#8280)

commit d23287f122c34ebef368742116d53a0ccb2041ee
Author: Daniele <57776841+daniandtheweb@users.noreply.github.com>
Date:   Wed Jul 3 23:02:58 2024 +0000

    Define and optimize  RDNA1 (#8085)

commit 5f2d4e60e202aabee10051e6615bb821e51787be
Author: slaren <slarengh@gmail.com>
Date:   Wed Jul 3 19:33:31 2024 +0200

    ppl : fix n_seq_max for perplexity (#8277)
    
    * ppl : fix n_seq_max for perplexity
    
    * use 1 seq for kl_divergence

commit 916248af1f3c16abd7408de848e025da095c621c
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed Jul 3 16:01:54 2024 +0200

    fix phi 3 conversion (#8262)

commit f8d6a23804f3798ff2869da68c1223b618df09ec
Author: Judd <foldl@users.noreply.github.com>
Date:   Wed Jul 3 20:40:16 2024 +0800

    fix typo (#8267)
    
    Co-authored-by: Judd <foldl@boxvest.com>

commit fadde6713506d9e6c124f5680ab8c7abebe31837
Author: AidanBeltonS <aidan.belton@codeplay.com>
Date:   Wed Jul 3 02:55:34 2024 +0100

    Dequant improvements rebase (#8255)
    
    * Single load for half2
    
    * Store scales in local mem
    
    * Vec load quantized values

commit a27152b602b369e76f85b7cb7b872a321b7218f7
Author: MistApproach <98988043+MistApproach@users.noreply.github.com>
Date:   Tue Jul 2 22:56:46 2024 +0200

    fix: add missing short command line argument -mli for multiline-input (#8261)

commit 3e2618bc7bf9e9fbf58c32cc3c8dd7d5df1de27e
Author: Clint Herron <hanclinto@gmail.com>
Date:   Tue Jul 2 13:19:56 2024 -0400

    Adding step to `clean` target to remove legacy binary names to reduce upgrade / migration confusion arising from #7809. (#8257)

commit 07a3fc0608a68c0c93a5fbfa9c58f4c9ec64cb81
Author: Clint Herron <hanclinto@gmail.com>
Date:   Tue Jul 2 12:18:10 2024 -0400

    Removes multiple newlines at the end of files that is breaking the editorconfig step of CI. (#8258)

commit 968967376dc2c018d29f897c4883d335bbf384fb
Author: Faisal Zaghloul <faisal.zaghloul@gmail.com>
Date:   Tue Jul 2 10:36:00 2024 -0400

    Add `JAIS` model(s) (#8118)
    
    * Add `JAIS` model(s)
    
    * cleanup
    
    * address review comments
    
    * remove hack
    
    * un-hardcode max-alibi-bias
    
    * minor tweaks
    
    ---------
    
    Co-authored-by: fmz <quic_fzaghlou@quic.com>

commit 023b8807e10bc3ade24a255f01c1ad2a01bb4228
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Jul 2 08:40:49 2024 +0200

    convert-hf : print output file name when completed (#8181)
    
    * convert-hf : print output file name when completed
    
    This commit adds the output file name to the log message when the
    conversion is completed.
    
    The motivation for this change is that when `--outfile` option is not
    specified it migth not be obvious where the output file is written.
    
    With this change the output of running the script will be something like
    the following:
    ```console
    INFO:hf-to-gguf:Model successfully exported to models/gemma-2-9b-it.gguf.
    ```
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! convert-hf : print output file name when completed
    
    Updates the output of to support printing the directory if the output is
    split into multiple files. Also the output file name is now retrieved
    from the model_instance object.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! convert-hf : print output file name when completed
    
    Use parent attribute of Path object and string interpolation.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! convert-hf : print output file name when completed
    
    Use os.sep instead of hardcoding the path separator.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 0e0590adab9f367b15ae2bf090a6d24f9df47ff1
Author: slaren <slarengh@gmail.com>
Date:   Tue Jul 2 08:39:38 2024 +0200

    cuda : update supports_op for matrix multiplication (#8245)

commit a9f3b102157ba992cfe058909b7f6e1906d2d647
Author: luoyu-intel <yu.luo@intel.com>
Date:   Tue Jul 2 04:50:07 2024 +0000

    [SYCL] Fix win build conflict of math library (#8230)
    
    * fix win build conflict of math library
    
    * fix the condition: !(win32 & SYCL)
    
    * revert warp_size=16

commit d08c20eddedb24515a3212e2de66bdff41a26b8c
Author: luoyu-intel <yu.luo@intel.com>
Date:   Tue Jul 2 02:16:00 2024 +0000

    [SYCL] Fix the sub group size of Intel (#8106)
    
    * use warp_size macro for all sycl kernels
    
    * fix mask of permute_sub_group_by_xor
    
    * fix rms_norm with correct warp number
    
    * fix rms_norm_f32/group_norm_f32
    
    * move norm to norm.cpp file
    
    * fix quantize bug
    
    * fix mmvq's batch size

commit 5fac350b9cc49d0446fc291b9c4ad53666c77591
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Jul 2 01:07:23 2024 +0200

    Fix gemma2 tokenizer convert (#8244)
    
    * fix gemma2 tokenizer convert
    
    * remove scores
    
    * improve code, fix new line issue

commit cb5fad4c6c2cbef92e9b8b63449e1cb7664e4846
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jul 1 20:39:06 2024 +0200

    CUDA: refactor and optimize IQ MMVQ (#8215)
    
    * CUDA: refactor and optimize IQ MMVQ
    
    * uint -> uint32_t
    
    * __dp4a -> ggml_cuda_dp4a
    
    * remove MIN_CC_DP4A checks
    
    * change default
    
    * try CI fix

commit dae57a1ebc1c9bd5693ab999e19d77c5506ae559
Author: Mateusz Charytoniuk <mateusz.charytoniuk@protonmail.com>
Date:   Mon Jul 1 19:13:22 2024 +0200

    readme: add Paddler to the list of projects (#8239)

commit 49122a873f54615626d1b49a2a39013ed4be98d5
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Jul 1 18:48:34 2024 +0200

    gemma2: add sliding window mask (#8227)
    
    * gemma2: add sliding window mask
    
    * fix data_swa uninitialized
    
    * better naming
    
    * add co-author
    
    Co-authored-by: Arlo Phoenix <arlo-phoenix@users.noreply.github.com>
    
    * replace list with single tensor
    
    * update
    
    * llama : minor styling
    
    * convert : add sanity check for query_pre_attn_scalar
    
    * fix small typo in README
    
    ---------
    
    Co-authored-by: Arlo Phoenix <arlo-phoenix@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 0ddeff10230b88f1fa9866bbe5fe0d71ba2323a0
Author: Roni <sulpher@gmx.net>
Date:   Mon Jul 1 14:48:16 2024 +0200

    readme : update tool list (#8209)
    
    * Added gppm to Tool list in README
    
    * Update README.md
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 3840b6f593751a0ba636bfda73b630cd6c29d7b5
Author: Michael Francis <edude03@gmail.com>
Date:   Mon Jul 1 07:47:04 2024 -0400

    nix : enable curl (#8043)
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 257f8e41e24b5bbfc27d9e907189a3e0cdb650d4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jul 1 14:46:18 2024 +0300

    nix : remove OpenCL remnants (#8235)
    
    * nix : remove OpenCL remnants
    
    * minor : remove parentheses

commit 694c59cb42d1ebd6a7d912ca65d3d7363e0f14c9
Author: iacore <74560659+iacore@users.noreply.github.com>
Date:   Mon Jul 1 11:40:58 2024 +0000

    Document BERT support. (#8205)
    
    * Update README.md
    
    document BERT support
    
    * Update README.md

commit 197fe6c1d7bec6718ce901f0141b2725240f298c
Author: zhentaoyu <zhentao.yu@intel.com>
Date:   Mon Jul 1 19:39:06 2024 +0800

    [SYCL] Update SYCL-Rope op and Refactor (#8157)
    
    * align with rope.cu and move sycl-op to a single file

commit d0a7145ba99ed3a8bc3145aa785b5c86ffe65020
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jul 1 02:09:34 2024 +0300

    flake.lock: Update (#8218)

commit 9ef07800622e4c371605f9419864d15667c3558f
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sun Jun 30 20:27:13 2024 +0200

    Fix new line issue with chat template, disable template when in-prefix/suffix is set (#8203)
    
    * preserve new line llama_chat_format_single
    
    * disable chat template if in-prefix/suffix is set
    
    * remove redundant change

commit 1c5eba6f8e628fb0a98afb27d8aaeb3b0e136451
Author: Andrei <abetlen@gmail.com>
Date:   Sat Jun 29 20:44:08 2024 -0700

    llama: Add attention and final logit soft-capping, update scaling factor to Gemma2 (#8197)
    
    * Add attention and final logit softcapping.
    
    * fix
    
    * Add custom add_ functions
    
    * Disable flash attention for Gemma2
    
    * Update src/llama.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Add default value for attention and final logit softcap value
    
    * Add custom kq scaling from Gemma2Attention
    
    * Remove custom pre attention scaling and use computed value instead.
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 72272b83a3878e91251218c981b4c6ec16c33912
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat Jun 29 00:14:20 2024 +0200

    fix code typo in llama-cli (#8198)

commit 8748d8ac6f172b99826ab18f01d9a3a165987d54
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Fri Jun 28 18:02:05 2024 +0100

    json: attempt to skip slow tests when running under emulator (#8189)

commit 26a39bbd6b0bbd66118bb68569f0276d7fe7df6c
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Jun 28 15:11:44 2024 +0200

    Add MiniCPM, Deepseek V2 chat template + clean up `llama_chat_apply_template_internal` (#8172)
    
    * tmp_contains
    
    * minicpm chat template
    
    * add DeepSeek Lite template
    
    * change deepseek-lite to deepseek2
    
    * correct code comment
    
    * correct code from master branch

commit 38373cfbab5397cc2ab5c3694a3dee12a9e58f45
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Fri Jun 28 12:53:43 2024 +0200

    Add SPM infill support (#8016)
    
    * add --spm-infill option
    
    * support --spm-infill
    
    * support --spm-infill

commit b851b3fba0a1b06a1129189bac300e07dd1648c8
Author: slaren <slarengh@gmail.com>
Date:   Fri Jun 28 12:37:45 2024 +0200

    cmake : allow user to override default options (#8178)

commit 139cc621e90b4f61830515c3c124cf35b3d7a6dc
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Fri Jun 28 09:26:45 2024 +0100

    `json`: restore default additionalProperties to false, fix some pattern escapes (#8180)
    
    * json: expand ESCAPED_IN_REGEXPS_BUT_NOT_IN_LITERALS charset
    
    * json: revert default of additionalProperties to false
    
    * Update README.md

commit e57dc62057d41211ac018056c19c02cd544694df
Author: pculliton <phillipculliton@gmail.com>
Date:   Fri Jun 28 00:00:43 2024 -0400

    llama: Add support for Gemma2ForCausalLM (#8156)
    
    * Inference support for Gemma 2 model family
    
    * Update convert-hf-to-gguf.py, constants, and tensor mappings
    
    * cleanup
    
    * format fix
    
    * Fix special token vocab bug
    
    * Don't add space prefix
    
    * fix deleted lines
    
    * Update src/llama.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Add model type names
    
    * Add control vector
    
    * Fix model type identification
    
    ---------
    
    Co-authored-by: Andrei Betlen <abetlen@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>

commit a27aa50ab7e07fe46aae619076b6e31d5663e914
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Jun 28 02:19:11 2024 +0200

    Add missing items in makefile (#8177)

commit cb0b06a8a613f7a2ccb7253b2a3c00fdd397ba1c
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Thu Jun 27 22:08:42 2024 +0100

    `json`: update grammars/README w/ examples & note about additionalProperties (#8132)
    
    * json: update grammars/README
    
    * mention broken prefixItems
    
    * add mention to llama-gbnf-validator
    
    * json: explicit type: object for nested items object in cli example

commit 558f44bf83d78242d4e5c4ab98d0be9125cb9780
Author: loonerin <132926317+loonerin@users.noreply.github.com>
Date:   Thu Jun 27 15:01:23 2024 -0400

    CI: fix release build (Ubuntu+Mac) (#8170)
    
    * CI: fix release build (Ubuntu)
    
    PR #8006 changes defaults to build shared libs. However, CI for releases
    expects static builds.
    
    * CI: fix release build (Mac)
    
    ---------
    
    Co-authored-by: loonerin <loonerin@users.noreply.github.com>

commit 8172ee9da9921ca53d698c7438c2d792b3f3f2c8
Author: slaren <slarengh@gmail.com>
Date:   Thu Jun 27 20:04:39 2024 +0200

    cmake : fix deprecated option names not working (#8171)
    
    * cmake : fix deprecated option names not working
    
    * remove LlAMA_OPENMP

commit 16791b8f0b4526aafbf5d0e5bbbd2e99c2253418
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Jun 27 18:14:19 2024 +0200

    Add chatml fallback for cpp `llama_chat_apply_template` (#8160)
    
    * add chatml fallback for cpp `llama_chat_apply_template`
    
    * remove redundant code

commit ab3679112d4c49a215a3d31550a7720b202e9015
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jun 27 18:37:29 2024 +0300

    flake.lock: Update (#8071)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/e9ee548d90ff586a6471b4ae80ae9cfcbceb3420?narHash=sha256-4Zu0RYRcAY/VWuu6awwq4opuiD//ahpc2aFHg2CWqFY%3D' (2024-06-13)
      → 'github:NixOS/nixpkgs/d603719ec6e294f034936c0d0dc06f689d91b6c3?narHash=sha256-k3JqJrkdoYwE3fHE6xGDY676AYmyh4U2Zw%2B0Bwe5DLU%3D' (2024-06-20)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
    Co-authored-by: Philip Taron <philip.taron@gmail.com>

commit 97877eb10bd8e7f8023420b5b5300bcbdadd62dc
Author: jukofyork <69222624+jukofyork@users.noreply.github.com>
Date:   Thu Jun 27 15:48:07 2024 +0100

    Control vector loading fixes (#8137)
    
    * Fixed leak in llama_control_vector_load_one() and allow llama_control_vector_load() to grow
    
    * refactored `llama_control_vector_load_one()`
    
    * allow multiple directions for same layer in same file
    
    * llama_control_vector_load_one() and llama_control_vector_load() now break on error
    
    * removed unnecessary ggml_free() call

commit 387952651a8fc493f8c85ea4c9774bd4a5694f87
Author: Raj Hammeer Singh Hada <hammeerraj@gmail.com>
Date:   Thu Jun 27 20:09:29 2024 +0530

    Delete examples/llama.android/llama/CMakeLists.txt (#8165)
    
    * Delete examples/llama.android/llama/CMakeLists.txt
    
    https://github.com/ggerganov/llama.cpp/pull/8145#issuecomment-2194534244
    
    This file is not being used for building on Android. `llama.cpp/examples/llama.android/llama/src/main/cpp/CMakeLists.txt` is being used instead.
    
    * Update CMakeLists.txt
    
    Pick local llama.cpp files instead of fetching content from git

commit 6030c61281c8a7eb94eceb7396a608fac8b71555
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Thu Jun 27 16:27:41 2024 +0200

    Add Qwen2MoE 57B-A14B model identifier (#8158)
    
    * Add Qwen2MoE 57B-A14B
    
    * Add Qwen2MoE 57B-A14B

commit 85a267daaa1c6f8fd69160445bcb88717031d10c
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Jun 27 16:26:05 2024 +0200

    CUDA: fix MMQ stream-k for --split-mode row (#8167)

commit f675b20a3b7f878bf3be766b9a737e2c8321ff0d
Author: kustaaya <58045274+kustaaya@users.noreply.github.com>
Date:   Thu Jun 27 11:58:54 2024 +0300

    Added support for Viking pre-tokenizer (#8135)
    
    Co-authored-by: kustaaya <kustaaya@protonmail.com>

commit 911e35bb8bb2fd1c7d3f40f27e96ff432eae7e14
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Thu Jun 27 09:46:41 2024 +0200

    llama : fix CodeLlama FIM token checks (#8144)
    
    * account for space prefix character
    
    * use find instead

commit ac146628e47451c531a3c7e62e6a973a2bb467a0
Author: Raj Hammeer Singh Hada <hammeerraj@gmail.com>
Date:   Thu Jun 27 07:27:57 2024 +0530

    Fix llama-android.cpp for error - "common/common.h not found" (#8145)
    
    - Path seems to be wrong for the common.h header file in llama-android.cpp file. Fixing the path so the Android Build doesn't fail with the error "There is no file common/common.h"

commit 9b31a40c6ddabe552875b811d7127aa039ca9703
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Jun 27 01:50:09 2024 +0200

    clip : suppress unused variable warnings (#8105)
    
    * clip : suppress unused variable warnings
    
    This commit suppresses unused variable warnings for the variables e in
    the catch blocks.
    
    The motivation for this change is to suppress the warnings that are
    generated on Windows when using the MSVC compiler. The warnings are
    not displayed when using GCC because GCC will mark all catch parameters
    as used.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! clip : suppress unused variable warnings
    
    Remove e (/*e*/) instead instead of using GGML_UNUSED.
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit c70d117c37cc7876e775d1e2722208a50c52edb3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 26 23:25:22 2024 +0300

    scripts : fix filename sync

commit ae5d0f4b899ff2842bfca561370c945ad8d4368b
Author: slaren <slarengh@gmail.com>
Date:   Wed Jun 26 21:59:28 2024 +0200

    ci : publish new docker images only when the files change (#8142)

commit 31ec3993f6e050322a249c07af79dbde66ea6ddc
Author: slaren <slarengh@gmail.com>
Date:   Wed Jun 26 21:34:14 2024 +0200

    ggml : add GGML_CUDA_USE_GRAPHS option, restore GGML_CUDA_FORCE_CUBLAS (cmake) (#8140)

commit c7ab7b612cbdce04499575e713076a026af4b9c5
Author: slaren <slarengh@gmail.com>
Date:   Wed Jun 26 20:20:22 2024 +0200

    make : fix missing -O3 (#8143)

commit f2d48fffde76d959fdb0da37316bdc09e5518eb1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 26 19:39:19 2024 +0300

    sync : ggml

commit 4713bf3093d58a3e12368ab2ab5fc3630f27803e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 26 19:36:44 2024 +0300

    authors : regen

commit 0e814dfc42b4b57ad19598d239557b6a977ca16c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 26 19:32:07 2024 +0300

    devops : remove clblast + LLAMA_CUDA -> GGML_CUDA (#8139)
    
    ggml-ci

commit a95631ee97bb24861af6bdeec380270459631e8e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 26 19:26:13 2024 +0300

    readme : update API notes

commit f3f65429c44bb195a9195bfdc19a30a79709db7b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 26 18:33:02 2024 +0300

    llama : reorganize source code + improve CMake (#8006)
    
    * scripts : update sync [no ci]
    
    * files : relocate [no ci]
    
    * ci : disable kompute build [no ci]
    
    * cmake : fixes [no ci]
    
    * server : fix mingw build
    
    ggml-ci
    
    * cmake : minor [no ci]
    
    * cmake : link math library [no ci]
    
    * cmake : build normal ggml library (not object library) [no ci]
    
    * cmake : fix kompute build
    
    ggml-ci
    
    * make,cmake : fix LLAMA_CUDA + replace GGML_CDEF_PRIVATE
    
    ggml-ci
    
    * move public backend headers to the public include directory (#8122)
    
    * move public backend headers to the public include directory
    
    * nix test
    
    * spm : fix metal header
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * scripts : fix sync paths [no ci]
    
    * scripts : sync ggml-blas.h [no ci]
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 88540445615e77a0177fcca43aaa8e9d8eea6864
Author: Isaac McFadyen <isaac@imcf.me>
Date:   Wed Jun 26 02:29:28 2024 -0400

    Clarify default MMQ for CUDA and LLAMA_CUDA_FORCE_MMQ flag (#8115)
    
    * Add message about int8 support
    
    * Add suggestions from review
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

commit c8771ab5f89387cdd7d9a8a69280dac46b45e02f
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jun 26 08:28:02 2024 +0200

    CUDA: fix misaligned shared memory read (#8123)

commit 494165f3b6c4cbcd793123cb57fb3e1f5477f1db
Author: Eddie-Wang <wangjinheng1120@163.com>
Date:   Wed Jun 26 14:27:46 2024 +0800

    llama : extend llm_build_ffn() to support _scale tensors (#8103)

commit 9b2f16f8055265c67e074025350736adc1ea0666
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Wed Jun 26 01:46:35 2024 +0100

    `json`: better support for "type" unions (e.g. nullable arrays w/ typed items) (#7863)
    
    * json: better suport for "type" arrays (e.g. `{"type": ["array", "null"], "items": {"type": "string"}}`)
    
    * json: add test for type: [array, null] fix
    
    * update tests

commit 6777c544bdd8c5d9de3220d6e2557957bbbf2a4f
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Wed Jun 26 01:45:58 2024 +0100

    `json`: fix additionalProperties, allow space after enum/const (#7840)
    
    * json: default additionalProperty to true
    
    * json: don't force additional props after normal properties!
    
    * json: allow space after enum/const
    
    * json: update pydantic example to set additionalProperties: false
    
    * json: prevent additional props to redefine a typed prop
    
    * port not_strings to python, add trailing space
    
    * fix not_strings & port to js+py
    
    * Update json-schema-to-grammar.cpp
    
    * fix _not_strings for substring overlaps
    
    * json: fix additionalProperties default, uncomment tests
    
    * json: add integ. test case for additionalProperties
    
    * json: nit: simplify condition
    
    * reformat grammar integ tests w/ R"""()""" strings where there's escapes
    
    * update # tokens in server test: consts can now have trailing space

commit 163d50adaf8897d8b734d701ff332de6be63d484
Author: jukofyork <69222624+jukofyork@users.noreply.github.com>
Date:   Tue Jun 25 21:47:40 2024 +0100

    fixes #7999 (adds control vectors to all `build_XXX()` functions in `llama.cpp` [needs testing] (#8060)
    
    * fixes #7999
    
    The `build_command_r` forgot to add the control vector.
    
    * Fixes qwen2 too
    
    * Fixed all models' control vectors
    
    * Removed double calls to `cb(cur, "l_out", il)`
    
    * Moved control vector logic to llama_control_vector:apply_to()

commit 6fcbf6823553efabe52ed83e3c2a3329aa3387d1
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Tue Jun 25 21:14:35 2024 +0200

    llama : implement Unigram tokenizer needed by T5 and FLAN-T5 model families (#5763)
    
    * llama : add T5 model architecture, tensors and model header parameters
    
    * llama : add implementation of Unigram tokenizer with SentencePiece-like text normalization using precompiled charsmap
    
    ---------
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

commit e6bf007744eb06336a231ef39cf08146dd16d2ce
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Jun 25 21:07:28 2024 +0200

    llama : return nullptr from llama_grammar_init (#8093)
    
    * llama : return nullptr from llama_grammar_init
    
    This commit updates llama_grammar_init to return nullptr instead of
    throwing an exception.
    
    The motivation for this is that this function is declared inside an
    extern "C" block and is intended/may be used from C code which will not
    be able to handle exceptions thrown, and results in undefined behavior.
    
    On Windows and using MSVC the following warning is currently generated:
    ```console
    C:\llama.cpp\llama.cpp(13998,1): warning C4297: 'llama_grammar_init':
    function assumed not to throw an exception but does
    C:\llama.cpp\llama.cpp(13998,1): message :
    __declspec(nothrow), throw(), noexcept(true), or noexcept was specified
    on the function
    ```
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! llama : return nullptr from llama_grammar_init
    
    Add checks for nullptr when calling llama_grammar_init.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    Co-authored-by: Clint Herron <hanclinto@gmail.com>

commit 84631fe1504de40427dc4b4cdac92fa7ebf65955
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Tue Jun 25 20:06:20 2024 +0100

    `json`: support integer minimum, maximum, exclusiveMinimum, exclusiveMaximum (#7797)
    
    * json: support minimum for positive integer values
    
    * json: fix min 0
    
    * json: min + max integer constraints
    
    * json: handle negative min / max integer bounds
    
    * json: fix missing paren min/max bug
    
    * json: proper paren fix
    
    * json: integration test for schemas
    
    * json: fix bounds tests
    
    * Update json-schema-to-grammar.cpp
    
    * json: fix negative max
    
    * json: fix negative min (w/ more than 1 digit)
    
    * Update test-grammar-integration.cpp
    
    * json: nit: move string rules together
    
    * json: port min/max integer support to Python & JS
    
    * nit: move + rename _build_min_max_int
    
    * fix min in [1, 9]
    
    * Update test-grammar-integration.cpp
    
    * add C++11-compatible replacement for std::string_view
    
    * add min/max constrained int field to pydantic json schema example
    
    * fix merge
    
    * json: add integration tests for min/max bounds
    
    * reshuffle/merge min/max integ test cases
    
    * nits / cleanups
    
    * defensive code against string out of bounds (apparently different behaviour of libstdc++ vs. clang's libc++, can't read final NULL char w/ former)

commit dd047b476c8b904e0c25e5dbc5bee6ffde2f6e17
Author: slaren <slarengh@gmail.com>
Date:   Tue Jun 25 19:20:06 2024 +0200

    disable docker CI on pull requests (#8110)

commit 925c30956dd17723c3a25297bcd0a609aec60663
Author: joecryptotoo <80373433+joecryptotoo@users.noreply.github.com>
Date:   Tue Jun 25 08:13:27 2024 -0700

    Add healthchecks to llama-server containers (#8081)
    
    * added healthcheck
    
    * added healthcheck
    
    * added healthcheck
    
    * added healthcheck
    
    * added healthcheck
    
    * moved curl to base
    
    * moved curl to base

commit c8ad35955ad2c68db172dcd0e857423ab128518d
Author: Brian <mofosyne@gmail.com>
Date:   Tue Jun 25 22:03:25 2024 +1000

    Gguf dump start data offset via --data-offset and some extra refactor (#8054)
    
    * gguf-dump: add --data-offset
    
    * gguf-dump: add tensor data offset table
    
    * gguf-dump: refactor GGUFReader for clarity
    
    * gguf-dump: add --data-alignment
    
    * gguf-dump.py: Rename variables and adjust comments
    
    start_data_offset --> data_offset
    
    _build_tensors_info_fields --> _build_tensor_info

commit 49c03c79cda17913b72260acdc8157b742cee41c
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Jun 25 13:59:54 2024 +0200

    cvector: better prompt handling, add "mean vector" method (#8069)
    
    * remove completions file
    
    * fix inverted vector
    
    * add mean method
    
    * code style
    
    * remove inverted pca hotfix

commit 48e6b92cc378c937e59719f2c0f482bf76c9ca81
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Jun 25 13:56:49 2024 +0200

    Add chat template support for llama-cli (#8068)
    
    * add chat template support for llama-cli
    
    * add help message
    
    * server: simplify format_chat
    
    * more consistent naming
    
    * improve
    
    * add llama_chat_format_example
    
    * fix server
    
    * code style
    
    * code style
    
    * Update examples/main/main.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 3791ad219323389106dc3fd80814eb5bbb7b80de
Author: HanishKVC <hanishkvc@gmail.com>
Date:   Tue Jun 25 16:57:35 2024 +0530

    SimpleChat v3.1: Boolean chat request options in Settings UI, cache_prompt (#7950)
    
    * SimpleChat: Allow for chat req bool options to be user controlled
    
    * SimpleChat: Allow user to control cache_prompt flag in request
    
    * SimpleChat: Add sample GUI images to readme file
    
    Show the chat screen and the settings screen
    
    * SimpleChat:Readme: Add quickstart block, title to image, cleanup
    
    * SimpleChat: RePosition contents of the Info and Settings UI
    
    Make it more logically structured and flow through.
    
    * SimpleChat: Rename to apiRequestOptions from chatRequestOptions
    
    So that it is not wrongly assumed that these request options are
    used only for chat/completions endpoint. Rather these are used
    for both the end points, so rename to match semantic better.
    
    * SimpleChat: Update image included with readme wrt settings ui
    
    * SimpleChat:ReadMe: Switch to webp screen image to reduce size

commit f702a90e245499283d6de0b287701c723cda2a87
Author: HatsuneMikuUwU33 <173229399+HatsuneMikuUwU33@users.noreply.github.com>
Date:   Tue Jun 25 10:44:48 2024 +0200

    Update control vector help (#8104)

commit 083bacce14c1aaf9976aa40e8266cdc25ac749d3
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Tue Jun 25 10:19:20 2024 +0800

    [SYCL] Re-enabled mul_mat_batched_sycl (#8095)

commit 2df373ac40ea581ccca8a58c713f03ad9d4b658d
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Jun 25 01:22:33 2024 +0200

    CUDA: fix matrix multiplication algorithm choice (#8102)

commit 3b099bcd9cbf2434f90cbe40eba6fa2189ed1d02
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jun 24 22:15:33 2024 +0200

    CUDA: fix MMQ writeback for int8 tensor cores (#8100)

commit a818f3028d1497a51cb2b8eb7d993ad58784940e
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jun 24 17:43:42 2024 +0200

    CUDA: use MMQ instead of cuBLAS by default (#8075)

commit d62e4aaa02540c89be8b59426340b909d02bbc9e
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Mon Jun 24 14:13:39 2024 +0200

    gguf-py : fix tensor groups for encoder-decoder models in gguf-dump.py (#8090)
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>
    Co-authored-by: Brian <mofosyne@gmail.com>

commit 9a590c82262dd518137f85406e65e452fdf2aca3
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jun 24 12:41:23 2024 +0200

    CUDA: optimize MMQ int8 tensor core performance (#8062)
    
    * CUDA: optimize MMQ int8 tensor core performance
    
    * only a single get_mma_tile_x_k function
    
    * simplify code, make functions constexpr

commit 52fc8705a0617452df08333e1161838726c322b4
Author: Christian Zhou-Zheng <59622928+christianazinn@users.noreply.github.com>
Date:   Mon Jun 24 05:42:03 2024 -0400

    Option to split during conversion (#6942)
    
    * support splits in convert.py
    
    * Support split by size and dry run to write estimated shards/filesizes
    
    * Move split functionality to new GGUFManager class
    
    * fix improper function signature
    
    * tentative push of convert-hf-to-gguf support
    
    * resolve merge + SplitArguments for easier parsing
    
    * Fix eager tensor memory leak and remove convert.py changes
    
    Removed a memory leak caused by unexpected reference retention to eager tensors.
    
    Also removed GGUFManager functionality in convert.py in favor of specializing for convert-hf-to-gguf.py.
    
    * refactor SplitStrategy to be a deque
    
    Instead of having SplitStrategy have a `data` field that is a deque, just have SplitStrategy be a subclass of deque itself.
    
    * fix Q8 quantization
    
    * remove unnecessary imports in gguf_manager
    
    * fix final? merge issue
    
    * fix gguf_writer placement and remove comments
    
    * oops, actually fix gguf_writer placement
    
    * reduce duplicated code from gguf_writer
    
    * further simplify GGUFManager
    
    * simplify even further and standardize with GGUFWriter
    
    * reduce diffs with master
    
    * form shards while adding tensors, SHA256 sums agree with master
    
    * re-add type hint
    
    Co-authored-by: compilade <git@compilade.net>
    
    * GGUFWriter compatibility fix
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Shard dataclass and un-negative dont_add_architecture
    
    * type consistency in format_n_bytes_to_str
    
    * move kv keys to constants.py
    
    * make pathlib explicit
    
    * base-1024 bytes to base-1000
    
    * rename GGUFManager to GGUFWriterSplit
    
    * Update gguf-py/gguf/constants.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * fix convert-hf-to-gguf.py permissions
    
    * fix line endings
    
    * Update gguf-py/gguf/gguf_writer_split.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * convert-hf : restore executable file permission
    
    * examples/convert-legacy-llama.py: restore executable file permission
    
    * reinstate original gguf package import and fix type annotation
    
    * attempt to appease the linter
    
    * attempt 2 to appease the linter
    
    * attempt 3 to appease the linter
    
    * comma consistency
    
    * Update convert-hf-to-gguf.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * edit cmd line args
    
    * use simplification from #7827
    
    * kv/ti data are still wrong
    
    * try to refactor kv data (still fails)
    
    * fix ti data messiness
    
    * tidy up
    
    * fix linting
    
    * actually make the linter happy
    
    * cleanup round 1
    
    * remove SplitStrategy, SplitArguments
    
    * appease linter
    
    * fix typing and clean up
    
    * fix linting
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * progress bar, fix split logic
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * catch oversights
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * swap bar orders
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * compatibility fix
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Update convert-hf-to-gguf.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    ---------
    
    Co-authored-by: Brian <mofosyne@gmail.com>
    Co-authored-by: compilade <git@compilade.net>

commit 8cb508d0d5c024e12692370d85237b45469a004b
Author: slaren <slarengh@gmail.com>
Date:   Mon Jun 24 07:36:11 2024 +0200

    disable publishing the full-rocm docker image (#8083)

commit 646ef4a9cfb6f40236d3b9ef07ac03700b6efcc7
Author: Yann Follet <131855179+YannFollet@users.noreply.github.com>
Date:   Mon Jun 24 13:30:24 2024 +0800

    embedding : more cli arguments (#7458)
    
    * add parameters for embeddings
    --embd-normalize
    --embd-output-format
    --embd-separator
    description in the README.md
    
    * Update README.md
    
    fix tipo
    
    * Trailing whitespace
    
    * fix json generation, use " not '
    
    * fix merge master
    
    * fix code formating
    group of parameters // embedding
    print usage for embedding parameters
    
    ---------
    
    Co-authored-by: Brian <mofosyne@gmail.com>

commit de0d6a68ac99f307fe889c48e21124bc3b7ca29a
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Mon Jun 24 07:06:05 2024 +0200

    gguf-py, convert-hf : model conversion support for T5 and FLAN-T5 model variants (#5763)
    
    * gguf-py : add T5 model architecture
    
    * gguf-py : add separate tensors for encoder and decoder
    
    * gguf-py : add new model header parameters: decoder_start_token_id, attention.relative_buckets_count, tokenizer.ggml.remove_extra_whitespaces, tokenizer.ggml.precompiled_charsmap
    
    * convert-hf : add model conversion support for T5ForConditionalGeneration and T5WithLMHeadModel
    
    ---------
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

commit 95f57bb5d5b18ef0beb2702a0d6c06e46804075c
Author: slaren <slarengh@gmail.com>
Date:   Mon Jun 24 03:07:59 2024 +0200

    ggml : remove ggml_task_type and GGML_PERF (#8017)
    
    * ggml : remove ggml_task_type and GGML_PERF
    
    * check abort_callback on main thread only
    
    * vulkan : remove usage of ggml_compute_params
    
    * remove LLAMA_PERF

commit e112b610a1a75cb7fa8351e1a933e2e7a755a5ce
Author: Eddie-Wang <wangjinheng1120@163.com>
Date:   Mon Jun 24 02:27:57 2024 +0800

    llama : add support for BitnetForCausalLM (#7931)
    
    * hf bitnet v1
    
    * hf bitnet e2e v2
    
    * finish bitnet e2e
    
    * finish f16 hf bitnet e2e
    
    * remove unsed
    
    * finish bitnet i2 e2e
    
    * move i2s to quantize v1
    
    * move i2 to quantize
    
    * clean code
    
    * clean code 2
    
    * fix codestyle
    
    * fix code
    
    * fix
    
    * fix code
    
    * fix merge
    
    * remove unused
    
    * change table name
    
    * fix whitespace
    
    * delete redundant
    
    * i2_s to absmax
    
    * finish i2_s/i8_s vec_dot x86 simd
    
    * i2s->q22
    
    * fix code
    
    * remove block scale
    
    * add dequantize
    
    * fix seq
    
    * update avx2
    
    * remove q2_2
    
    * remove q22_grid
    
    * fix whitespace
    
    * reuse llm_build_kv
    
    * fix bo
    
    ---------
    
    Co-authored-by: root <root@wangjinheng>

commit 6a2f298bd784403c5c33eebb822217ec5d9b5590
Author: Aarni Koskela <akx@iki.fi>
Date:   Sun Jun 23 18:03:08 2024 +0300

    server : fix JSON-Scheme typo (#7975)

commit 11318d9aa1f668aa10407a5cb9614371af32f3ce
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Sun Jun 23 15:39:45 2024 +0200

    Fix typo in llama_set_embeddings comment (#8077)

commit b6b9a8e606da7764bd3649c2ea574979c85abd43
Author: slaren <slarengh@gmail.com>
Date:   Sun Jun 23 13:14:45 2024 +0200

    fix CI failures (#8066)
    
    * test-backend-ops : increase cpy max nmse
    
    * server ci : disable thread sanitizer

commit 45c0e2e4c1268c2d7c8c45536f15e3c9a731ecdc
Author: 0cc4m <picard12@live.de>
Date:   Sun Jun 23 10:21:25 2024 +0200

    Refactor Vulkan backend to allow multiple contexts (#7961)
    
    * Refactor Vulkan backend to allow multiple contexts
    
    * Fix too many shader groups called validation error in llama3 on AMD and Intel GPUs
    
    * Fix Vulkan debug build error

commit b5a5f34efadec8d8f0057b35cb04742abfeb2ef5
Author: Clint Herron <hanclinto@gmail.com>
Date:   Sat Jun 22 14:28:18 2024 -0400

    Removing extra blank lines that were breaking Lint. (#8067)

commit 3e58b0ee355f78be41cf5211b68426761339bc3c
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat Jun 22 18:11:30 2024 +0200

    cvector: fix CI + correct help message (#8064)
    
    * cvector: fix CI + correct help message
    
    * also correct --pca-iter

commit adf480c3ab3abedc964c8ae381476257583ae134
Author: HatsuneMikuUwU33 <173229399+HatsuneMikuUwU33@users.noreply.github.com>
Date:   Sat Jun 22 17:19:37 2024 +0200

    cvector-generator: Moe Moe Fixie-Fixie for Lots of Formats~! ♡(ᐢ ᴥ ᐢ)♡ (#8052)
    
    * Update negative.txt
    
    * Update positive.txt
    
    * Update cvector-generator.cpp
    
    * Update cvector-generator.cpp

commit 3aa184a8c7c553b5dfcc142d919f3db695df297a
Author: 0xspringtime <110655352+0xspringtime@users.noreply.github.com>
Date:   Sat Jun 22 09:37:41 2024 -0400

    convert-hf : change assert to exception (#8015)

commit 5b48cd53a87928db0c6447f0c9dac4db5802102d
Author: ddh0 <dylanhalladay02@icloud.com>
Date:   Sat Jun 22 07:16:10 2024 -0600

    Update llama-quantize ppl/file size output from LLaMA-v1 to Llama-3 values (#8058)
    
    Uses the values computed by @JohannesGaessler in PR #7413

commit c5a8d4b749352645afd4c024f85d6eca2ca72c6d
Author: Clint Herron <hanclinto@gmail.com>
Date:   Fri Jun 21 23:18:36 2024 -0400

    JSON Schema to GBNF integration tests (#7790)
    
    * Adding simple bare-bones test for end-to-end integration test for json validation against auto-generated JSON-schema grammars.
    
    * Adding additional examples as documented in #7789 . Also adding the ability to automatically output improperly failing grammars to debug output files so they can more easily be examined in the gbnf-validator program.
    
    * Uncommenting formerly commented tests so that they fail for others who are attempting to reproduce the bugs.
    
    * Merging improved schema test methods added by @ochafik in #7797
    
    * Adding #define to temporarily remove failing tests so that this PR can pass CI, but still be useful for other PRs that want to leverage the framework.
    
    * Fixing nits from ochafik. Removing escape slashes, adding additional failing cases, fixing some other strings.
    
    * Fixing grammar indentation to be consistent throughout file.

commit 557b653dc9ed91e8c313e87500e0050c775f81b6
Author: k.h.lai <adrian.k.h.lai@outlook.com>
Date:   Fri Jun 21 16:28:20 2024 +0800

    vulkan: detect multiple devices by deviceUUID instead of deviceID (#8022)
    
    * vulkan: detect multiple devices by deviceUUID instead of deviceID
    
    * vulkan: remove unneeded variables
    
    * vulkan: fix id query

commit 7d5e8777ae1d21af99d4f95be10db4870720da91
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Fri Jun 21 05:57:36 2024 +0000

    ggml : AVX IQ quants (#7845)
    
    * initial iq4_xs
    
    * fix ci
    
    * iq4_nl
    
    * iq1_m
    
    * iq1_s
    
    * iq2_xxs
    
    * iq3_xxs
    
    * iq2_s
    
    * iq2_xs
    
    * iq3_s before sllv
    
    * iq3_s
    
    * iq3_s small fix
    
    * iq3_s sllv can be safely replaced with sse multiply

commit a927b0f3dd9a86ee042cd2bdcc8c9da4a855926b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jun 21 08:51:28 2024 +0300

    llama : optimize long word tokenization with WPM (#8034)
    
    ggml-ci

commit 80ea089d771f0c2d97afa8bead80ded412f600d7
Author: Douglas Hanley <thesecretaryofwar@gmail.com>
Date:   Fri Jun 21 00:38:22 2024 -0500

    llama : allow pooled embeddings on any model (#7477)
    
    * create append_pooling operation; allow to specify attention_type; add last token pooling; update examples
    
    * find result_norm/result_embd tensors properly; update output allocation logic
    
    * only use embd output for pooling_type NONE
    
    * get rid of old causal_attn accessor
    
    * take out attention_type; add in llama_set_embeddings
    
    * bypass logits when doing non-NONE pooling

commit 0e64591e8290037db6412665a56354b789a0597e
Author: Shuichi Tsutsumi <shuichi0526@gmail.com>
Date:   Fri Jun 21 14:30:58 2024 +0900

    swiftui : enable stream updating (#7754)

commit b1ef562bc17fbf8ba436ddf2d89b78efd10d3e03
Author: Hamdoud Hakem <90524568+hamdoudhakem@users.noreply.github.com>
Date:   Thu Jun 20 21:01:15 2024 +0100

    requirements : Bump torch and numpy for python3.12 (#8041)

commit 17b291a6a581c47f24f99bad926b42617894f99f
Author: Hamdoud Hakem <90524568+hamdoudhakem@users.noreply.github.com>
Date:   Thu Jun 20 20:59:59 2024 +0100

    convert-hf : Fix the encoding in the convert-hf-to-gguf-update.py (#8040)

commit abd894ad96a242043b8e197ec130d8649eead22e
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Jun 20 16:40:13 2024 +0200

    common: fix warning (#8036)
    
    * common: fix warning
    
    * Update common/common.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit de391e4c803383bbea054b6edd016e78c024a74d
Author: luoyu-intel <yu.luo@intel.com>
Date:   Thu Jun 20 13:19:05 2024 +0000

    [SYCL] Fix windows build and inference (#8003)
    
    * add sycl preset
    
    * fix debug link error. fix windows crash
    
    * update README

commit d50f8897a797a5a03f31228d1b5a7b8130ee1bc2
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Jun 20 14:39:21 2024 +0200

    CUDA: stream-k decomposition for MMQ (#8018)
    
    * CUDA: stream-k decomposition for MMQ
    
    * fix undefined memory reads for small matrices

commit 2075a66a96cc1b04eabec7cf4b3051193d6f719e
Author: Michael de Gans <michael.john.degans@gmail.com>
Date:   Wed Jun 19 22:32:01 2024 -0700

    metal : fix `ggml_metal_supports_op` for BF16 (#8021)
    
    Currently the Metal backend does not support BF16. `ggml_metal_supports_op` was returning true in these cases, leading to a crash with models converted with `--leave-output-tensor`. This commit checks if the first few sources types are BF16 and returns false if that's the case.

commit ba5899315283eb1df3902363daf79bdc5eefe426
Author: sasha0552 <admin@sasha0552.org>
Date:   Wed Jun 19 23:57:10 2024 +0000

    server : fix smart slot selection (#8020)

commit a7854743c5e6216a6178824a603aa9479728ddd5
Author: Michael de Gans <michael.john.degans@gmail.com>
Date:   Wed Jun 19 13:10:42 2024 -0700

    un-ignore `build-info.cmake` and `build-info.sh` (#7996)
    
    * un-ignore `build-info.cmake` and `build-info.sh`
    
    I am assuming that ignoring them was unintentional. If they are ignored, some tools, like cargo, will consider the files inexistent, even if they're comitted, for the purpose of publishing. This leads to the build failing in such cases.
    
    * un-ignore `build-info.cpp.in`
    
    For the same reason as the previous two files.
    
    * Reorganize `.gitignore`
    
    * Add exceptions for files mentioned by @slaren
    
    I did leave .clang-tidy since it was explicitly ignored before.
    
    * Add comments for organization
    * Sort some lines for pretty
    * Test with `make` and `cmake` builds to ensure no build artifacts might be comitted
    
    * Remove `.clang-tidy` from `.gitignore`
    
    Per comment by @ggerganov
    
    * Remove `IDEWorkspaceChecks.plist` from root-level `.gitignore`

commit 9c77ec1d74874ee22bdef8f110e8e8d41389abf2
Author: slaren <slarengh@gmail.com>
Date:   Wed Jun 19 15:04:15 2024 +0200

    ggml : synchronize threads using barriers (#7993)

commit a04a953cab583f0229e7b4b506346e3e9a85c8d0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 19 13:04:36 2024 +0300

    codecov : remove (#8004)

commit 623494a478134432fd2d7ee40135770a3340674f
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Wed Jun 19 09:11:51 2024 +0800

    [SYCL] refactor (#6408)
    
    * seperate lower precision GEMM from the main files
    
    * fix workgroup size hardcode

commit 37bef8943312d91183ff06d8f1214082a17344a5
Author: jaime-m-p <167997752+jaime-m-p@users.noreply.github.com>
Date:   Tue Jun 18 18:40:52 2024 +0200

    tokenizer : BPE fixes (#7530)
    
    * Random test: add_bos_token, add_eos_token
    * Random test: add BPE models for testing
    * Custom regex split fails with codepoint 0
    * Fix falcon punctuation regex
    * Refactor llm_tokenizer_bpe: move code to constructor
    * Move 'add_special_bos/eos' logic to llm_tokenizer_bpe
    * Move tokenizer flags to vocab structure.
    * Default values for special_add_bos/eos
    * Build vocab.special_tokens_cache using vocab token types
    * Generalize 'jina-v2' per token attributes
    * Fix unicode whitespaces (deepseek-coder, deepseek-llm)
    * Skip missing byte tokens (falcon)
    * Better unicode data generation
    * Replace char32_t with uint32_t

commit 91c188d6c296bd3384f2a02a83b71187aa3d18b3
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Tue Jun 18 14:19:45 2024 +0200

    Only use FIM middle token if it exists (#7648)
    
    * Only use FIM middle if it exists
    
    * Only use FIM middle if it exists

commit 84f6de17f6a8602e7ff7f7c7bda36a73f510a2dd
Author: jojorne <jojorne@users.noreply.github.com>
Date:   Tue Jun 18 09:18:32 2024 -0300

    Fix no gcc pragma on Windows (#7751)

commit 61665277afde2add00c0d387acb94ed5feb95917
Author: Ulrich Drepper <drepper@gmail.com>
Date:   Tue Jun 18 14:00:14 2024 +0200

    Allow compiling with CUDA without CUDA runtime installed (#7989)
    
    On hosts which are not prepared/dedicated to execute code using CUDA
    it is still possible to compile llama.cpp with CUDA support by just
    installing the development packages.  Missing are the runtime
    libraries like /usr/lib64/libcuda.so* and currently the link step
    will fail.
    
    The development environment is prepared for such situations.  There
    are stub libraries for all the CUDA libraries available in the
    $(CUDA_PATH)/lib64/stubs directory.  Adding this directory to the end
    of the search path will not change anything for environments which
    currently work fine but will enable compiling llama.cpp also in case
    the runtime code is not available.

commit b96f9afb0d58b003ac8d1d0c94cd99393a3bc437
Author: Frank Mai <thxcode0824@gmail.com>
Date:   Tue Jun 18 15:11:40 2024 +0800

    chore: clean useless beam search param (#7985)
    
    Signed-off-by: thxCode <thxcode0824@gmail.com>

commit 1193778105c9a81bd38f72c61aaafbaf85dc9c04
Author: Abheek Gulati <abheekg@hotmail.com>
Date:   Mon Jun 17 23:57:41 2024 -0700

    readme : update UI list (#7943)

commit 5326bcceeb7dd34f16d0fe61b134d1e074a8e65d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 18 09:50:45 2024 +0300

    ggml : sync

commit e6ecc2be470e3c5c6c5c9d8b90aa83a1f7725084
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 18 09:37:20 2024 +0300

    whisper : use ggml_backend_sched (whisper/2239)
    
    * whisper : use ggml_backend_sched (wip)
    
    * use sched in whisper_allocr
    
    * whisper : single backend in whisper_context
    
    * whisper : remove whisper_state->backends_used
    
    * whisper : remove whisper_context->backend
    
    * whisper : reset scheduler after init
    
    * whisper : fix external encoder (e.g. CoreML)
    
    * whisper : cleanup
    
    * whisper : handle null GPU buffer types + fix sycl
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit a94e6ff8774b7c9f950d9545baf0ce35e8d1ed2f
Author: Ștefan-Gabriel Muscalu <legraphista@users.noreply.github.com>
Date:   Mon Jun 17 22:08:46 2024 +0300

    update: support Qwen2-57B-A14B (#7835)
    
    * update: convert-hf-to-gguf.py to support Qwen2-57B-A14B
    
    * fix: QWEN2MOE support for expert_feed_forward_length
    
    previously, expert ff was taken from n_ff (intermediate size) but it is now properly taken from LLM_KV_EXPERT_FEED_FORWARD_LENGTH
    
    n_ff_exp and n_ff_shared_exp are now properly calculated
    
    * update: convert-hf-to-gguf.py cleanup for Qwen2MoeForCausalLM
    
    * fix: QWEN2MOE support for expert_feed_forward_length
    
    previously, expert ff was taken from n_ff (intermediate size) but it is now properly taken from LLM_KV_EXPERT_FEED_FORWARD_LENGTH
    
    n_ff_exp and n_ff_shexp are now properly calculated

commit 5b6da187508f49a9fa9d95fa22ae804a0780d256
Author: Srihari-mcw <96763064+Srihari-mcw@users.noreply.github.com>
Date:   Mon Jun 17 23:53:17 2024 +0530

    Make updates to type cast based on compiler instead of OS (#7851)

commit 7c26775adb579e92b59c82e8084c07a1d0f75e9c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 17 19:40:01 2024 +0300

    llama : disable FA if KV head size do not match (#7982)

commit b473e95084c286780165568cf0f385f21141d68d
Author: Bryan Honof <bryanhonof@gmail.com>
Date:   Mon Jun 17 17:37:55 2024 +0200

    Add Nix and Flox install instructions (#7899)

commit 99052cd227c7182fcf53343d2e7d33bfa180a9cf
Author: slaren <slarengh@gmail.com>
Date:   Mon Jun 17 16:51:42 2024 +0200

    sched : offload_op also requires supports_op (#7977)

commit c637fcd34d135a9ff4f97d3a53ad03a910a4a31f
Author: Frank Mai <thxcode0824@gmail.com>
Date:   Mon Jun 17 22:11:08 2024 +0800

    fix: divide 0 exception in mamba (#7932)
    
    Signed-off-by: thxCode <thxcode0824@gmail.com>

commit 6a2f0b3474d479bda4ac2ee7cfd5dcdcf0be1f79
Author: Markus Tavenrath <mtavenrath@users.noreply.github.com>
Date:   Mon Jun 17 16:10:15 2024 +0200

    Implement non-mapped async IO for CUDA on Windows.  (#7896)
    
    * Implement non-mapped async IO for CUDA on Windows. On a fast Gen5 NVMe drive this change improves model load time by >3x while it should be the same (or slightly faster) on any other drive.
    
    * Free resources except for backend.
    
    * Change assertions to exceptions in llama_file, find correct cuda backend to create CUDA resources and respect the use_mmap flag again for CUDA.
    
    * Apply suggestions from code review
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Fix editorconfig and unused variable
    
    * Fix issues with Windows build
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 21be9cab94e0b5b53cb6edeeebf8c8c799baad03
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 17 11:09:20 2024 +0300

    rpc : fix load/store misaligned addresses (#7948)

commit 006167aaf6b6aaa4daa52961035f7460af19f469
Author: Brian <mofosyne@gmail.com>
Date:   Mon Jun 17 15:25:20 2024 +1000

    gguf-dump.py: add --markdown dump output (#7853)
    
    * gguf-dump.py: add --markdown dump output
    
    * gguf-dump.py: Add toc
    
    * gguf-dump.py: use standard tensor name lookup. Also add tensor ID field
    
    * gguf-dump.py: Add tensor overview count
    
    * gguf-dump.py: fix array preview
    
    * gguf-dump.py: markdownTableWithAlignmentSupport() added
    
    * Add type hints and spacing
    
    Co-authored-by: compilade <git@compilade.net>
    
    * gguf-dump.py: prettyfy dimention
    
    * gguf-dump: right align element count
    
    * gguf-dump.py: element count autosizing
    
    * Apply suggestions from code review
    
    Co-authored-by: compilade <git@compilade.net>
    
    ---------
    
    Co-authored-by: compilade <git@compilade.net>

commit df68d4fa5dc929217d3e64d673e099d7a417b206
Author: Neo Zhang <zhang.jianyu@outlook.com>
Date:   Mon Jun 17 11:17:07 2024 +0800

    [SYCL] Update README-sycl.md for Chapter "Recommended release" and "News" (#7946)
    
    * Update README-sycl.md
    
    * Update README-sycl.md
    
    * Update README-sycl.md
    
    * Update README-sycl.md

commit 43b35e38ba371f9a7faa6dca4c5d1e8f698ffd87
Author: Calvin Laurenson <calvin@laurenson.dev>
Date:   Sun Jun 16 15:23:04 2024 -0700

    Add support for sqrt on CUDA (#7953)
    
    * cuda sqrt support
    
    * enable cuda in pca
    
    * fix comments in pca
    
    * add test
    
    * add sqrt to ggml_backend_cuda_supports_op
    
    * fix test
    
    * new line
    
    * Use F32 sqrtf instead of F64 sqrt
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

commit 19b7a836f6658e18e973af532a5cc6ad6b3a27f8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 11 17:39:01 2024 +0300

    cuda : fix bounds check for src0 rows in MMVQ kernel (whisper/2231)
    
    * cuda : fix bounds check for src0 rows in MMVQ kernel
    
    * Update ggml-cuda/mmvq.cu
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

commit b5fcf8ef5c29df53cfff60e180b4992a3b2332a6
Author: Hong Bo PENG <penghb@cn.ibm.com>
Date:   Sun Jun 16 16:53:11 2024 +0800

    ggml : fix and optimize ppc64le (ggml/849)
    
    * fix compile issues introduced by loongarch_asx
    
    * restore quant changes to merge
    
    * fix compile issues introduced by loongarch_asx
    
    * further optimize by using vec_msum & vec_sum4s on ppc64le

commit 398105ff4373eea385ea8e8625cb417b2ae51134
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Sun Jun 16 10:51:18 2024 +0200

    ggml : remove duplicate include of ggml-common.h (ggml/853)
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit bc6c457fa35f6791e9a2bb61108e7d49e8fc98bd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 16 19:16:21 2024 +0300

    flake.lock: Update (#7951)

commit 52399254b3bceda279b4ea9111a983e32310166e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 16 14:51:40 2024 +0300

    unicode : avoid char32_t (#7957)
    
    ggml-ci

commit 6fe1c627413725ddc1f9e323f6b13fe388c53e0a
Author: hopkins385 <98618192+hopkins385@users.noreply.github.com>
Date:   Sun Jun 16 13:51:18 2024 +0200

    readme : update UI list [no ci] (#7958)

commit cddaf028adc738b5a7ecc60809cb78e0ba0f97c1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 16 14:50:12 2024 +0300

    ggml : fix handling of zero blocks in IQ quants (#7955)
    
    ggml-ci

commit c8a82194a888f68f259e0d0fa96f3332ac7c5cb6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 16 10:46:51 2024 +0300

    github : update pr template

commit 7c7836d9d4062d6858e3fb337b135c417ccee6ce
Author: 0cc4m <picard12@live.de>
Date:   Sun Jun 16 07:17:31 2024 +0200

    Vulkan Shader Refactor, Memory Debugging Option (#7947)
    
    * Refactor shaders, extract GLSL code from ggml_vk_generate_shaders.py into vulkan-shaders directory
    
    * Improve debug log code
    
    * Add memory debug output option
    
    * Fix flake8
    
    * Fix unnecessary high llama-3 VRAM use

commit 0c7b3595b9e5ad2355818e259f06b0dc3f0065b3
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat Jun 15 18:53:40 2024 +0200

    Add `cvector-generator` example (#7514)
    
    * add control-vector-generator
    
    * calc diff
    
    * add comments
    
    * proof-of-concept stdlib implementation
    
    Implements PCA and file writing using mostly standard libraries. The output is recognized as a functional control vector, but outputs gibberish.
    
    * param parsing, refactor, comments
    
    Added basic command-line parameters for outfile and one each positive/negative prompt.
    
    Refactored some messy code in PCA computation and GGUF exporting.
    
    Left a bunch of comments regarding further work needed.
    
    * example template completions
    
    Implements an example template set built from the positive/negative prompts like the control vector Python implementation.
    
    * add multi prompts, multi-thread for PCA
    
    * fix mem error
    
    * add debugs
    
    * fix matrix transpose multiplication
    
    you have got to be kidding me
    
    * preliminary template/multiprompt support
    
    model is running out of context and that ought to be fixed (segfaulting) but other than that it looks goodish
    
    * fix zero output & param parsing, functional templating
    
    fixed a bug where the output file had no tensor data/was all zero
    
    fixed a bug where single hyphen flags were not being correctly parsed
    
    implements creation of templated prompts from input (still need to adapt based on model)
    
    * fix square_diff matmul index range and CRLF->LF line endings
    
    fixed a logic error where square_diff would not multiply all rows
    
    fixed a formatting error where the provided completions.txt had CRLF line endings
    
    * add command-line args for num threads, num completions file lines, always reload model
    
    refactored a few things and did what the commit message says on the tin
    
    * code aestheticization
    
    * fix compiler warnings
    
    * in-series multithreading for prompt embedding?
    
    added commented-out code to attempt to start implementing mutlithreading for embedding in main
    
    * remove unnecessary multithreading
    
    * interim fix memory leak
    
    * translated everything but PCA (I think)
    
    * tentatively translate the rest
    
    * fix ggml errors and make new ones
    
    at least it compiles and runs
    
    * fix cb_eval
    
    * temporary commit while I move dev environments
    
    it finally outputs a functioning control vector - "functioning" in the sense that it can be loaded and it clearly has the right idea, but makes the model incoherent
    
    * update debug statements
    
    * pre-tokenize so we can allocate correct memory to ctx_diffs_wrapped
    
    * update comments
    
    * (wip) refactor
    
    * clean up PCA ggml implementation
    
    * fix shape of v_diff_original
    
    * add n_batch for pca
    
    * working version
    
    * remember to copy back the last_eigenvector
    
    * fix n_completions
    
    * bring back n_completions
    
    * default n_pca_batch to 20
    
    * fix macos build
    
    * add to makefile all targets
    
    * use ggml_format_name
    
    * add readme
    
    * fix .editorconfig
    
    * use ggml_backend_tensor_copy
    
    * attemp to fix compile problem on mac
    
    * fix compile warn
    
    * reuse allocr
    
    * move param parser to common
    
    * better error handling
    
    * clean up a bit
    
    * add print_usage
    
    * shorten help msg
    
    * beautify help msg
    
    * escape prompt by default
    
    * change compile target to llama-cvector-generator
    
    * typo
    
    * disable GPU for PCA
    
    * code style
    
    ---------
    
    Co-authored-by: Christian Zhou-Zheng <christianzhouzheng@gmail.com>

commit 7b2f4a7d193ef2475259bbe7656fcccfab4b1217
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Sat Jun 15 14:05:10 2024 +0800

    [SYCL] remove global variables (#7710)
    
    * separate DPCT helpers outside
    
    * replace global variables with context
    
    * remove useless extra
    
    * update mul_mat condition
    
    * remove duplicate buft initialization
    
    * remove duplicate extra and global work group size
    
    * remove useless backend check
    
    * remove duplicated extras
    
    * use macro for group_size and remove cuda-related

commit f8ec8877b75774fc6c47559d529dac423877bcad
Author: olexiyb <olexiyb@gmail.com>
Date:   Fri Jun 14 20:28:34 2024 +0300

    ci : fix macos x86 build (#7940)
    
    In order to use old `macos-latest` we should use `macos-12`
    
    Potentially will fix: https://github.com/ggerganov/llama.cpp/issues/6975

commit 76d66ee0be91e2bec93206e821ee1db8d023cff5
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Jun 14 18:41:49 2024 +0200

    CUDA: faster q2_K, q3_K MMQ + int8 tensor cores (#7921)
    
    * CUDA: faster q2_K, q3_K MMQ + int8 tensor cores
    
    * try CI fix
    
    * try CI fix
    
    * try CI fix
    
    * fix data race
    
    * rever q2_K precision related changes

commit 66ef1ceedf983773c8ceb4d925285d41d4e50e2a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jun 14 17:14:09 2024 +0300

    metal : utilize max shared memory for mul_mat_id (#7935)

commit e65bbf606c61f49dc06c7ac060cd5ba7ae446025
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Fri Jun 14 16:47:41 2024 +0300

    llama-bench : fix RPC indication (#7936)
    
    Show "<backend_name>+RPC" when RPC offloading is used

commit 6fcd1331efbfbb89c8c96eba2321bb7b4d0c40e4
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Fri Jun 14 12:20:04 2024 +0200

    llama : more checks before assuming FIM tokens (#7644)
    
    * More checks before assuming FIM tokens for Llama arch
    
    * extensive token check

commit 41b9260f18eb7f325c952006ac46afc1d0d8ad2f
Author: Elaine <elaine.zosa@gmail.com>
Date:   Fri Jun 14 13:16:49 2024 +0300

    convert : add Poro-34B-chat tokenizer support (#7713)
    
    * support for Poro chat pre-tokenizer
    
    * add support for Poro pre-tokenizer
    
    * Update convert-hf-to-gguf-update.py
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Change Poro-34B-chat to poro-chat
    
    * Change Poro-34B-chat to poro-chat
    
    * Update convert-hf-to-gguf-update.py
    
    * Update llama.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 172c8256840ffd882ab9992ecedbb587d9b21f15
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Thu Jun 13 15:18:44 2024 +0300

    rpc : fix ggml_backend_rpc_supports_buft() (#7918)

commit a55eb1bf0fa2fd84147bdfd384391e029d988253
Author: Galunid <karolek1231456@gmail.com>
Date:   Thu Jun 13 09:42:41 2024 +0200

    readme : Remove outdated instructions from README.md (#7914) [no ci]

commit f578b86b2123d0f92afbaa98a031df4d4464e582
Author: slaren <slarengh@gmail.com>
Date:   Thu Jun 13 03:11:35 2024 +0200

    move BLAS to a separate backend (#6210)
    
    * move BLAS to a separate backend
    
    * rename GGML_USE_OPENBLAS to GGML_USE_BLAS
    
    * alloc : reuse same buffer when the same buffer type if used multiple times
    
    * set number of threads automatically for openblas and blis
    
    * sched : print assignments when GGML_SCHED_DEBUG env variable is set
    
    * sched : allow ops with weights on an incompatible buffer type
    
    This will cause the weight to be copied to a backend that supports the
    op, which is very costly. The weight should have been stored in a buffer
    of a backend that can run the op, but llama.cpp cannot do this
    automatically at the moment.
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 1c641e6aac5c18b964e7b32d9dbbb4bf5301d0d7
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Thu Jun 13 00:41:52 2024 +0100

    `build`: rename main → llama-cli, server → llama-server, llava-cli → llama-llava-cli, etc... (#7809)
    
    * `main`/`server`: rename to `llama` / `llama-server` for consistency w/ homebrew
    
    * server: update refs -> llama-server
    
    gitignore llama-server
    
    * server: simplify nix package
    
    * main: update refs -> llama
    
    fix examples/main ref
    
    * main/server: fix targets
    
    * update more names
    
    * Update build.yml
    
    * rm accidentally checked in bins
    
    * update straggling refs
    
    * Update .gitignore
    
    * Update server-llm.sh
    
    * main: target name -> llama-cli
    
    * Prefix all example bins w/ llama-
    
    * fix main refs
    
    * rename {main->llama}-cmake-pkg binary
    
    * prefix more cmake targets w/ llama-
    
    * add/fix gbnf-validator subfolder to cmake
    
    * sort cmake example subdirs
    
    * rm bin files
    
    * fix llama-lookup-* Makefile rules
    
    * gitignore /llama-*
    
    * rename Dockerfiles
    
    * rename llama|main -> llama-cli; consistent RPM bin prefixes
    
    * fix some missing -cli suffixes
    
    * rename dockerfile w/ llama-cli
    
    * rename(make): llama-baby-llama
    
    * update dockerfile refs
    
    * more llama-cli(.exe)
    
    * fix test-eval-callback
    
    * rename: llama-cli-cmake-pkg(.exe)
    
    * address gbnf-validator unused fread warning (switched to C++ / ifstream)
    
    * add two missing llama- prefixes
    
    * Updating docs for eval-callback binary to use new `llama-` prefix.
    
    * Updating a few lingering doc references for rename of main to llama-cli
    
    * Updating `run-with-preset.py` to use new binary names.
    Updating docs around `perplexity` binary rename.
    
    * Updating documentation references for lookup-merge and export-lora
    
    * Updating two small `main` references missed earlier in the finetune docs.
    
    * Update apps.nix
    
    * update grammar/README.md w/ new llama-* names
    
    * update llama-rpc-server bin name + doc
    
    * Revert "update llama-rpc-server bin name + doc"
    
    This reverts commit e474ef1df481fd8936cd7d098e3065d7de378930.
    
    * add hot topic notice to README.md
    
    * Update README.md
    
    * Update README.md
    
    * rename gguf-split & quantize bins refs in **/tests.sh
    
    ---------
    
    Co-authored-by: HanClinto <hanclinto@gmail.com>

commit 963552903f51043ee947a8deeaaa7ec00bc3f1a4
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jun 12 17:41:51 2024 +0200

    CUDA: fix broken oob check for FA vec f32 kernel (#7904)

commit a9cae48003dfc4fe95b8f5c81682fc6e63425235
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 12 16:00:22 2024 +0300

    tests : add non-cont unary tests (#7857)
    
    * tests : add non-cont unary tests
    
    * ggml : update unary asserts and "supports_op"
    
    ggml-ci

commit bfaa676b0841617d4ef3596e63aca6be1a8eb1b5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 12 15:24:20 2024 +0300

    ggml : improve ggml_is_contiguous logic (#7856)
    
    * ggml : improve ggml_is_contiguous logic
    
    ggml-ci
    
    * ggml : support more contiguous cases
    
    ggml-ci

commit 704a35b183748954013bd875bbbfdd9eaca14e62
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 12 14:42:29 2024 +0300

    server : restore numeric prompts (#7883)

commit dcf752707d96eb305f546526c7bc5d01f0831130
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Wed Jun 12 17:05:35 2024 +0800

    update intel docker oneapi-basekit to 2024.1.1-devel-ubuntu22.04 (#7894)
    
    In addition this reverts a workaround we had to do to workaround the upstream issue with expired intel GPG package keys in 2024.0.1-devel-ubuntu22.04

commit f2b5764beb35583295e2475479c18f249b139b58
Author: Patrice Ferlet <metal3d@gmail.com>
Date:   Wed Jun 12 03:18:16 2024 +0200

    Fix a typo and add Fedora 40 pacakge to install for Vulkan (#7794) [no ci]
    
    Fix "appropiate" to "appropriate" and add Fedora 40 packages to install to compile with Vulkan support

commit 73bac2b11d7d3e20982fc9ee607625836387db8b
Author: k.h.lai <adrian.k.h.lai@outlook.com>
Date:   Wed Jun 12 03:26:05 2024 +0800

    vulkan: select only one device for single gpu with multiple drivers (#7582)

commit ef52d1d16afc695d798396cdd13594ea5e45a9dd
Author: 0cc4m <picard12@live.de>
Date:   Tue Jun 11 21:20:29 2024 +0200

    Update Vulkan RoPE implementation (#7818)
    
    * Update Vulkan RoPE implementation
    
    * Return nullptr on alloc_buffer when allocation fails, instead of throwing an exception
    
    Minor fixes
    
    * Fix segfault when running out of VRAM
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 14f83526cd27f638c856ea6eff08110b9860eb2a
Author: Deven Mistry <31466137+deven367@users.noreply.github.com>
Date:   Tue Jun 11 12:18:58 2024 -0400

    fix broken link in pr template (#7880) [no ci]
    
    * fix broken link in pr template
    
    * Update pull_request_template.md [no ci]
    
    ---------
    
    Co-authored-by: Brian <mofosyne@gmail.com>

commit 6fe42d073f0554eada93ac9d40574025aeedb703
Author: Brian <mofosyne@gmail.com>
Date:   Wed Jun 12 00:43:41 2024 +1000

    github: move PR template to .github/ root (#7868)

commit 148995e5e57b313cce2672f75610db58c6327a51
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Jun 11 14:45:40 2024 +0200

    llama-bench: more compact markdown tables (#7879)

commit 4bfe50f741479c1df1c377260c3ff5702586719e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 11 10:10:20 2024 +0300

    tests : check the Python version (#7872)
    
    ggml-ci

commit bdcb8f42221bc40c411150a009a3d3a30fa74722
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Jun 11 08:26:07 2024 +0200

    CUDA: int8 tensor cores for MMQ (q4_K, q5_K, q6_K) (#7860)

commit c2ce6c47e4f2d891bf29d8810832a3b310a8f205
Author: slaren <slarengh@gmail.com>
Date:   Tue Jun 11 07:59:20 2024 +0200

    fix CUDA CI by using a windows-2019 image (#7861)
    
    * try to fix CUDA ci with --allow-unsupported-compiler
    
    * trigger when build.yml changes
    
    * another test
    
    * try exllama/bdashore3 method
    
    * install vs build tools before cuda toolkit
    
    * try win-2019

commit b61eb9644d64e90123ac805436d95b94b3b4cc3f
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Tue Jun 11 02:22:57 2024 +0100

    json: refine constraint for whitespace to avoid runaways yet allow pretty print (#7866)

commit 396b18dfec2c56846e80362db70af09b9e1d70ba
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Tue Jun 11 01:00:30 2024 +0100

    `json`: document schema conversion in GBNF readme, align manual grammar examples & converters (#7841)
    
    * json: fix char pattern in grammar converters
    
    * json: prevent number precision & whitespace runaways in example grammars
    
    * json: add doc to grammar readme

commit 864a99e7a01d9422d2f55618dbe62c8099a2175c
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Mon Jun 10 18:32:10 2024 -0400

    cmake : fix CMake requirement for CUDA (#7821)

commit fd5ea0f897ecb3659d6c269ef6f3d833e865ead7
Author: slaren <slarengh@gmail.com>
Date:   Mon Jun 10 14:18:41 2024 +0200

    ci : try win-2019 on server windows test (#7854)

commit c28a83902cf6ab6a9e085ad6d4cc2e95c4ccfe40
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 10 15:00:15 2024 +0300

    examples : remove --instruct remnants (#7846)

commit d9da0e4986f121c727bdd9579a6688097b11602c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 10 14:59:55 2024 +0300

    server : improve "prompt" handling (#7847)

commit 1f0dabda8d5c131f9d4632aa41de74317cdd61fb
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jun 10 11:45:13 2024 +0200

    CUDA: use tensor cores for MMQ (#7676)
    
    * CUDA: int8 tensor cores for MMQ (legacy quants)
    
    * fix out-of-bounds writes
    
    * __builtin_assume -> GGML_CUDA_ASSUME
    
    * fix writeback returning too early

commit af4ae502ddaeb03cd5861273ca2e9a5ae4551db7
Author: Ben Ashbaugh <ben.ashbaugh@intel.com>
Date:   Mon Jun 10 02:21:31 2024 -0700

    use the correct SYCL context for host USM allocations (#7777)
    
    Signed-off-by: Ben Ashbaugh <ben.ashbaugh@intel.com>

commit 10ceba354a3b152ff425e9fa97f9caaef99a46b1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 10 02:04:50 2024 +0300

    flake.lock: Update (#7838)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/ad57eef4ef0659193044870c731987a6df5cf56b?narHash=sha256-SzDKxseEcHR5KzPXLwsemyTR/kaM9whxeiJohbL04rs%3D' (2024-05-29)
      → 'github:NixOS/nixpkgs/051f920625ab5aabe37c920346e3e69d7d34400e?narHash=sha256-4q0s6m0GUcN7q%2BY2DqD27iLvbcd1G50T2lv08kKxkSI%3D' (2024-06-07)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit e95beeb1fc4621826ddd616776dbdf717366bf5c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 9 20:19:35 2024 +0300

    imatrix : handle partial entries (#7833)

commit 57bf62ce7cb75cca589943e2050d29bff4026e76
Author: Nicolás Pérez <nicolas_perez@brown.edu>
Date:   Sun Jun 9 11:24:29 2024 -0400

    docs: Added initial PR template with directions for doc only changes and squash merges [no ci] (#7700)
    
    This commit adds pull_request_template.md and CONTRIBUTING.md . It focuses on explaining to contributors the need to rate PR complexity level, when to add [no ci] and how to format PR title and descriptions.
    
    Co-authored-by: Brian <mofosyne@gmail.com>
    Co-authored-by: compilade <git@compilade.net>

commit 3e2ee443159724e2d3a0741f6b167e599ec088aa
Author: mgroeber9110 <45620825+mgroeber9110@users.noreply.github.com>
Date:   Sun Jun 9 12:50:35 2024 +0200

    server: do not remove whitespace at the start of a completion chunk (#7830)

commit 42b53d192f4e3abf1b7c8e424628424504ea5dc5
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Jun 9 09:42:25 2024 +0200

    CUDA: revise q8_1 data layout for mul_mat_q (#7824)

commit 2decf57bc6e4a6b45176c3727d964a01161beecc
Author: sasha0552 <admin@sasha0552.org>
Date:   Sun Jun 9 06:39:25 2024 +0000

    convert-hf : set the model name based on cli arg, if present (#7693)
    
     `--model-name` argument was added a while ago but did not do anything.
    This commit fixes this issue and enables this feature.

commit 5795b941827fdec6c1662986de962badff456718
Author: compilade <git@compilade.net>
Date:   Sat Jun 8 22:47:25 2024 -0400

    convert-hf : match model part name prefix and suffix (#7687)
    
    In #7075, to fix the conversion of (some) models using model-00001-of-00001.safetensors instead of model.safetensors for a single model part we simply used the same logic as the part count to get the part names.
    
    But this doesn't always work correctly, like when unusual additional model files like consolidated.safetensors in https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 are present.
    
    This commit matching both the prefix and the suffix of the model part names should fix this problem without breaking any previously-supported upstream models. But according to report by @teleprint-me there is still some
    persistent problem, but shall do in the meantime.

commit ed9f2521185706481501a5e6d5315397b11802ff
Author: compilade <git@compilade.net>
Date:   Sat Jun 8 22:34:29 2024 -0400

    gguf-py : decouple adding metadata from writing in GGUFWriter (#7827)
    
    Main changes of this PR is to consolidate GGUFWriter.add_key and GGUFWriter.add_val into GGUFWriter.add_key_value.
    
    In addition use_temp_file is now opt-in instead of opt-out defaulting to False.
    
    Also GGUFWriter now does not require output file name until when actually writing to it.
    
    And GGUFWriter doesn't really need to eagerly prepare the data layout of the metadata

commit fe1e3917cfa0f9397a765cfd0aef880674d938d5
Author: slaren <slarengh@gmail.com>
Date:   Sun Jun 9 01:43:39 2024 +0200

    Revert "[SYCL] Update rpc-server.cpp to include SYCL backend (#7682)" (#7808)
    
    This reverts commit 9422c5e34bbd302493b77a8f6d546154a1f4fe82.

commit d4d915d351d1f1270d56184bdd46672893e8a5d8
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Sat Jun 8 20:21:08 2024 +0100

    url: save -mu downloads to new cache location (#7826)
    
    * url: save -mu download to new cache location
    
    * url: fs_get_cache_file_path util
    
    * url: tweak sig of fs_get_cache_file

commit 7a16ce7db2a74a223f0f3b9cee66d4539c5bce8f
Author: sasha0552 <admin@sasha0552.org>
Date:   Sat Jun 8 07:50:31 2024 +0000

    server : smart slot selection using Longest Common Prefix (#7728)
    
    * server : Smart selection of available slot using Longest Common Substring
    
    * add usage
    
    * remove trailing whitespaces
    
    * Use Longest Common Prefix (LCP) instead of LCS
    
    * Rename argument

commit da799b41891e34aac86ce4e173f9c4c0afd4fab3
Author: slaren <slarengh@gmail.com>
Date:   Fri Jun 7 19:47:49 2024 +0200

    vulkan : reuse parent extra for views (#7806)
    
    * vulkan : reuse parent extra for views
    
    * Fix validation error when multiple compute contexts are used in a graph
    
    ---------
    
    Co-authored-by: 0cc4m <picard12@live.de>

commit c00fad71e507ff386d42bd74846fe06d19dd63a4
Author: Christian Zhou-Zheng <59622928+christianazinn@users.noreply.github.com>
Date:   Fri Jun 7 08:56:01 2024 -0400

    gguf-split : change binary multi-byte units to decimal (#7803)

commit 27615f5ab21060d96953c9c1e223051ab2188f57
Author: intelmatt <61025942+intelmatt@users.noreply.github.com>
Date:   Fri Jun 7 05:15:07 2024 -0700

    cmake : fix BUILD_SHARED_LIBS=ON build (#7784)
    
    common depends on pthreads in Linux

commit 7027b27d765db95d4ac6b569d976e387a8715881
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Jun 7 11:15:49 2024 +0200

    server: update cache_prompt documentation [no ci] (#7745)

commit a5cabd76491f07494c5b8267f921c73f5e2bbfb4
Author: woodx <124784234+woodx9@users.noreply.github.com>
Date:   Fri Jun 7 15:09:45 2024 +0800

    server : do not get prompt in infill mode (#7286)
    
    * avoid to get prompt in infill mode and embedding mode
    
    * remove embedding mode
    
    * refactor format
    
    ---------
    
    Co-authored-by: wudexiang <wudexiang@bytedance.com>

commit d5c938cd7716b9a2ace49a43a469dfbffcff4d28
Author: pengxin99 <pengxin.yuan@intel.com>
Date:   Fri Jun 7 14:28:26 2024 +0800

    [SYCL] fix softmax r2r result wrong issue (#7811)

commit c9ee7118d5644dd3df70ea6878b36a9761616aab
Author: slaren <slarengh@gmail.com>
Date:   Fri Jun 7 08:01:29 2024 +0200

    check for nans in imatrix and quantize (#7807)
    
    * imatrix : detect nan/inf values
    
    * quantize : check imatrix for nan/inf values

commit ee459f40f65810a810151b24eba5b8bd174ceffe
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jun 6 19:19:59 2024 +0300

    server : fix --threads-http arg (#7801)

commit f83351f9a62a6262f1fc3d08f320033089cddfb5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jun 6 16:30:58 2024 +0300

    imatrix : migrate to gpt_params (#7771)
    
    * imatrix : migrate to gpt_params
    
    ggml-ci
    
    * imatrix : add --save-frequency cli arg
    
    * common : fix --no-ppl

commit ad675e1c67a05b16e4e12abe30dbecfc808e7b7e
Author: Clint Herron <hanclinto@gmail.com>
Date:   Thu Jun 6 06:08:52 2024 -0700

    Added support for . (any character) token in grammar engine. (#6467)
    
    * Added support for . (any characer) token in grammar engine.
    
    * Add integration tests for any-character symbol.

commit a143c04375828b1f72eb1a326115791b63e79345
Author: Mattheus Chediak <shammcity00@gmail.com>
Date:   Thu Jun 6 09:17:54 2024 -0300

    README minor fixes (#7798) [no ci]
    
    derievatives --> derivatives

commit 55b2d0849d3ec9e45e4a4d9e480f5fa7977872a6
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Thu Jun 6 10:07:06 2024 +0100

    grammars: x{min,max} repetition operator (#6640)
    
    * grammars: x{min,max} repetition operator + tweak +/*/? to avoid duplication of original over alternates
    
    * grammars: handle `x{n}` and fix `x{n,n}`
    
    * grammars: document new repetition operators
    
    * grammars: uniform use of int for min & max
    
    * grammars: refactor parser test
    
    * grammar: parsing tests w/ natural pretty print of updated expectations
    
    * grammars: much prettier print of expectations (+ TEST_GRAMMAR_PARSER_PRINT_ALL=1 to force all)
    
    * grammars: improve test pretty print again
    
    * grammars: pretty print rules and chars
    
    * grammars: fix copy rule skipping
    
    * grammars: disallow `a{,}` (not allowed in regexps)
    
    * Update common/grammar-parser.cpp
    
    Co-authored-by: Clint Herron <hanclinto@gmail.com>
    
    * grammars: fix copy rule skipping (again) & display of expectations
    
    * grammars: more test cases
    
    * grammars: update reps parsing to bring ? / * / + closer to before
    
    * json: use new GBNF repetitions{m,n} syntax
    
    * grammars: update performance gotchas w/ repetition advice
    
    * Update examples/json_schema_to_grammar.py
    
    Co-authored-by: Clint Herron <hanclinto@gmail.com>
    
    * Update examples/server/public/json-schema-to-grammar.mjs
    
    Co-authored-by: Clint Herron <hanclinto@gmail.com>
    
    * grammars: comment on rule repetitions
    
    * grammars: ensure unambiguous number alternatives
    
    * grammar: nit typo switched error msgs
    
    * grammar: nit numbering in comment
    
    * json: update numeric rule to be unambiguous
    
    * Apply suggestions from code review
    
    Co-authored-by: Clint Herron <hanclinto@gmail.com>
    
    * Update examples/server/public/json-schema-to-grammar.mjs
    
    Co-authored-by: Clint Herron <hanclinto@gmail.com>
    
    * json: fix integral-part
    
    * grammar: add repetition tests
    
    ---------
    
    Co-authored-by: Clint Herron <hanclinto@gmail.com>

commit f5d7b268ec4bf8628aa6ccc9f6631d0230dde76f
Author: Joan Fontanals <joan.fontanals.martinez@jina.ai>
Date:   Thu Jun 6 09:22:41 2024 +0200

    llama : add jina v2 base code (#7596)
    
    * feat: add changes to handle jina v2 base code
    
    * fix: do not complicate things
    
    * fix: fix the usage of the code model
    
    * fix: fix comments
    
    * fix: fix linting issues
    
    * fix: remove ollama patches
    
    * style : minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 2d08b7fbb483c14bd2b173d4cd51ea3a4f862e8f
Author: slaren <slarengh@gmail.com>
Date:   Thu Jun 6 07:19:49 2024 +0200

    docker : build only main and server in their images (#7782)
    
    * add openmp lib to dockerfiles
    
    * build only main and server in their docker images

commit d67caea0d6e6c303d31b01d0a010973e6c908dff
Author: slaren <slarengh@gmail.com>
Date:   Thu Jun 6 07:17:21 2024 +0200

    docker : add openmp lib (#7780)

commit 7672adeec7a79ea271058c63106c142ba84f951a
Author: Galunid <karolek1231456@gmail.com>
Date:   Wed Jun 5 19:07:24 2024 +0200

    Fix encoding in python scripts (#7733)

commit 7d1a378b8fb266782d9248538a661405aad80768
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jun 5 16:53:00 2024 +0200

    CUDA: refactor mmq, dmmv, mmvq (#7716)
    
    * CUDA: refactor mmq, dmmv, mmvq
    
    * fix out-of-bounds write
    
    * struct for qk, qr, qi
    
    * fix cmake build
    
    * mmq_type_traits

commit 2b3389677a833cee0880226533a1768b1a9508d2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 5 11:29:20 2024 +0300

    ggml : refactor rope norm/neox (#7634)
    
    * ggml : unify rope norm/neox (CPU)
    
    * ggml : fix compile warning
    
    * ggml : remove GLM rope mode
    
    ggml-ci
    
    * metal : better rope implementation
    
    ggml-ci
    
    * cuda : better rope implementation
    
    ggml-ci
    
    * naming : n_orig_ctx -> n_ctx_orig
    
    ggml-ci
    
    * dev : add reminders to update backends
    
    ggml-ci
    
    * vulkan : fix ggml_rope_ext() usage
    
    * cuda : fix array size + indents
    
    ggml-ci

commit 9973e81c5ccf4f31b3980f5aa73f5cfea8699860
Author: arch-btw <57669023+arch-btw@users.noreply.github.com>
Date:   Tue Jun 4 23:40:49 2024 -0700

    readme : remove -ins (#7759)
    
    -ins and --instruct were moved in https://github.com/ggerganov/llama.cpp/pull/7675
    
    I have adjusted the README accordingly.
    There was no trace of --chatml in the README.

commit c90dbe026b456a233f8f0fbe752212e6a0503ca2
Author: jaime-m-p <167997752+jaime-m-p@users.noreply.github.com>
Date:   Wed Jun 5 01:26:14 2024 +0200

    Fix per token atrributes bits (#7749)

commit b90dc566c1c615289b05b50d61680f23744a21e7
Author: agray3 <agray3@users.noreply.github.com>
Date:   Tue Jun 4 21:06:49 2024 +0100

    Allow number of nodes in CUDA graph to change (#7738)
    
    Previously the code would have failed to cope in the case that the
    number of nodes changes in an existing CUDA graph. This fixes the
    issue by removing an unnecessary conditional.

commit 1442677f92e45a475be7b4d056e3633d1d6f813b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 4 21:23:39 2024 +0300

    common : refactor cli arg parsing (#7675)
    
    * common : gpt_params_parse do not print usage
    
    * common : rework usage print (wip)
    
    * common : valign
    
    * common : rework print_usage
    
    * infill : remove cfg support
    
    * common : reorder args
    
    * server : deduplicate parameters
    
    ggml-ci
    
    * common : add missing header
    
    ggml-ci
    
    * common : remote --random-prompt usages
    
    ggml-ci
    
    * examples : migrate to gpt_params
    
    ggml-ci
    
    * batched-bench : migrate to gpt_params
    
    * retrieval : migrate to gpt_params
    
    * common : change defaults for escape and n_ctx
    
    * common : remove chatml and instruct params
    
    ggml-ci
    
    * common : passkey use gpt_params

commit 554c247caffed64465f372661f2826640cb10430
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 4 21:23:20 2024 +0300

    ggml : remove OpenCL (#7735)
    
    ggml-ci

commit 0cd6bd3483fa66124b76a8a8ac794d9ee18c70c1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 4 21:23:05 2024 +0300

    llama : remove beam search (#7736)

commit 5ca0944a153b65724d51b2f484139aa25ccb7a8b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 4 19:43:01 2024 +0300

    readme : remove obsolete Zig instructions (#7471)

commit adc9ff384121f4d550d28638a646b336d051bf42
Author: slaren <slarengh@gmail.com>
Date:   Tue Jun 4 14:32:42 2024 +0200

    llama-bench : allow using a different printer for stderr with -oe (#7722)
    
    compare-commits.sh : hide stdout, use -oe to print markdown

commit 987d743d6bc4cee4bde6820733ea33a2abc0afac
Author: Daniele <57776841+daniandtheweb@users.noreply.github.com>
Date:   Tue Jun 4 12:09:15 2024 +0000

    Improve hipBLAS support in CMake (#7696)
    
    * Improve hipBLAS support in CMake
    
    This improves the detection of the correct CMAKE_PREFIX_PATH when using different distributions or a self-built ROCm SDK.
    
    * Set ROCM_PATH correctly

commit b226c1227bcf6412076ecf787421135fd2c42ef0
Author: zhouwg <zhouwg2000@gmail.com>
Date:   Tue Jun 4 19:21:26 2024 +0800

    refine .gitignore (#7688)
    
    This adds tags and android ndk into the git ignore list

commit 3b38d48609280aa5f8ab7ea135a4351b2a5ee240
Author: jaime-m-p <167997752+jaime-m-p@users.noreply.github.com>
Date:   Tue Jun 4 09:17:17 2024 +0200

    Per token attributes (#7685)
    
    * Add per token attributes enum
    * Using phi-3 for testing 'rstrip'
    * Using jina-v2 for testing 'lstrip'
    * Brute force test for 'lstrip' and 'rstrip'
    * Implement 'rstrip' and 'lstrip'
    * Update phi-3 GGUF file (obsolete since 917dc8c)
    * Replace llama_token_type with llama_token_attribs

commit 6d1616944d9efd342ed2a4fd318722adfc9febcd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 4 10:01:09 2024 +0300

    ggml : prevent builds with -ffinite-math-only (#7726)
    
    This enforces a check that -fno-finite-math-only was set and that the operating
    compiling mode is not in finite maths mode. This is because during rewriting of
    silu and softmax for cpu #7154 there emerged an issue where the result that was
    observed when >1 slot was nondeterministic as found by @JohannesGaessler.
    
    @LostRuins narrowed the problem down to -ffinite-math-only which was theorised
    to be due to SiLU, instead of flushing small values to 0, returns NaN or some
    other garbage. @jart proposed a fix that @ggerganov then implemented in this fix
    
    ref https://github.com/ggerganov/llama.cpp/pull/7154#issuecomment-2145661825

commit bde7cd3cd949c1a85d3a199498ac98e78039d46f
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Mon Jun 3 20:03:26 2024 +0300

    llama : offload to RPC in addition to other backends (#7640)
    
    * llama : offload to RPC in addition to other backends
    
    * - fix copy_tensor being called on the src buffer instead of the dst buffer
    
    - always initialize views in the view_src buffer
    
    - add RPC backend to Makefile build
    
    - add endpoint to all RPC object names
    
    * add rpc-server to Makefile
    
    * Update llama.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit a5735e4426b19a3ebd0c653ad8ac01420458ee95
Author: Masaya, Kato <62578291+msy-kato@users.noreply.github.com>
Date:   Tue Jun 4 00:14:15 2024 +0900

    ggml : use OpenMP as a thread pool (#7606)
    
    * ggml: Added OpenMP for multi-threads processing
    
    * ggml : Limit the number of threads used to avoid deadlock
    
    * update shared state n_threads in parallel region
    
    * clear numa affinity for main thread even with openmp
    
    * enable openmp by default
    
    * fix msvc build
    
    * disable openmp on macos
    
    * ci : disable openmp with thread sanitizer
    
    * Update ggml.c
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 0b832d53ba0ffcc759c8d62ede3772dd62321f8e
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jun 3 16:28:58 2024 +0200

    make: fix debug options not being applied to NVCC (#7714)

commit 3d7ebf63123b8652fb7bbecef7ba731202309901
Author: 0cc4m <picard12@live.de>
Date:   Mon Jun 3 10:59:14 2024 +0200

    Vulkan Mixture of Experts (MoE) support (#7628)
    
    * Finish Vulkan mul_mat_id implementation
    
    * Add Vulkan sum_rows and div ops
    
    * Fix MUL_MAT_ID matrix matrix shader
    
    * Fix MUL_MAT_ID matrix vector shader dispatch size
    
    * Fix MUL_MAT_ID matrix vector shader and dispatch code
    
    * Update Vulkan CPU offload for MUL_MAT_ID
    
    * Fix crash when using split mode none and setting a main GPU

commit a10cda58d3199cd85305e0f03a8c6056714ae2e8
Author: Andy Tai <andy-tai@users.noreply.github.com>
Date:   Mon Jun 3 01:06:24 2024 -0700

    cmake : add pkg-config spec file for llama.cpp (#7702)

commit 6f28a333c1e3fdfdc7b4f9d0367f2b41a9b7e9d4
Author: zhangkaihuo <zhangkaihuo@gmail.com>
Date:   Mon Jun 3 15:49:30 2024 +0800

    llama : MiniCPM support tied embeddings (#7664)
    
    * support lm_head
    
    * remove the code block
    
    ---------
    
    Co-authored-by: zhangkaihuo <zhangkaihuo@modelbest.cn>

commit 549279d8049d78620a2b081e26edb654f83c3bbd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 3 08:34:43 2024 +0300

    llama : avoid double token-to-piece cache (#7654)
    
    ggml-ci

commit 9e405b6e2ecb888e860f7b92720b4809e21b3915
Author: woachk <24752637+woachk@users.noreply.github.com>
Date:   Mon Jun 3 07:32:16 2024 +0200

    kompute : implement op_getrows_f32 (#6403)
    
    op_getrows_f32 is required since https://github.com/ggerganov/llama.cpp/pull/6122
    for the Vulkan w/ Kompute backend to be functional.
    
    As such, implement this op to make this backend functional again.

commit 3413ae2193d0693f14bead02e5018f442cbf579b
Author: Dave Airlie <airlied@redhat.com>
Date:   Mon Jun 3 07:59:54 2024 +1000

    fix bug introduced in using calloc (#7701)
    
    compilade pointed this out on the previous MR

commit 1669810d7c2446af8425aa54ff6611bf6f14646c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 3 00:13:12 2024 +0300

    flake.lock: Update (#7686)
    
    Flake lock file updates:
    
    • Updated input 'flake-parts':
        'github:hercules-ci/flake-parts/8dc45382d5206bd292f9c2768b8058a8fd8311d9?narHash=sha256-/GJvTdTpuDjNn84j82cU6bXztE0MSkdnTWClUCRub78%3D' (2024-05-16)
      → 'github:hercules-ci/flake-parts/2a55567fcf15b1b1c7ed712a2c6fadaec7412ea8?narHash=sha256-iKzJcpdXih14qYVcZ9QC9XuZYnPc6T8YImb6dX166kw%3D' (2024-06-01)
    • Updated input 'flake-parts/nixpkgs-lib':
        'https://github.com/NixOS/nixpkgs/archive/50eb7ecf4cd0a5756d7275c8ba36790e5bd53e33.tar.gz?narHash=sha256-QBx10%2Bk6JWz6u7VsohfSw8g8hjdBZEf8CFzXH1/1Z94%3D' (2024-05-02)
      → 'https://github.com/NixOS/nixpkgs/archive/eb9ceca17df2ea50a250b6b27f7bf6ab0186f198.tar.gz?narHash=sha256-lIbdfCsf8LMFloheeE6N31%2BBMIeixqyQWbSr2vk79EQ%3D' (2024-06-01)
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/bfb7a882678e518398ce9a31a881538679f6f092?narHash=sha256-4zSIhSRRIoEBwjbPm3YiGtbd8HDWzFxJjw5DYSDy1n8%3D' (2024-05-24)
      → 'github:NixOS/nixpkgs/ad57eef4ef0659193044870c731987a6df5cf56b?narHash=sha256-SzDKxseEcHR5KzPXLwsemyTR/kaM9whxeiJohbL04rs%3D' (2024-05-29)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit 7c4e5b7eae26581869e782015d9deca947c34997
Author: Austin <77757836+teleprint-me@users.noreply.github.com>
Date:   Sun Jun 2 13:39:08 2024 -0400

    chore : add ignore rule for generated server themes (#7689)

commit 9422c5e34bbd302493b77a8f6d546154a1f4fe82
Author: nickp27 <nb.porter@gmail.com>
Date:   Sun Jun 2 19:13:54 2024 +1000

    [SYCL] Update rpc-server.cpp to include SYCL backend (#7682)
    
    * Update rpc-server.cpp to include SYCL backend
    
    Draft PR to address inclusion of SYCL backend for RPC server
    
    * Update rpc-server.cpp

commit e141ce624af57bdffbaf57014a044eb1d9689230
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jun 1 23:26:10 2024 +0200

    Fix FlashAttention debug test, FP32 assert (#7684)

commit 2e666832e6ac78194edf030bd1c295e21bdb022c
Author: Yazan Agha-Schrader <mountaiin@icloud.com>
Date:   Sat Jun 1 21:31:48 2024 +0200

    server : new UI (#7633)
    
    * ic
    
    * migrate my eary work
    
    * add the belonging stuff: css,favicon etc
    
    * de prompts
    
    * chore: Update HTML meta tags in index.html file
    
    * add api-key css classes
    
    * some necessary fixes
    
    * Add API key CSS classes and update styling in style.css
    
    * clean the code
    
    * move API to the top, rearrange param sliders. update css
    
    * add tooltips to the parameters with comprehensible explanations
    
    * fix FloatField and BoolField tooltips
    
    * fix grammar field width
    
    * use template literales for promptFormats.js
    
    * update const ModelGenerationInfo
    
    * remove ms per token, since not relevant for most webui users and use cases
    
    * add phi-3 prompt template
    
    * add phi3 to dropdown
    
    * add css class
    
    * update forgotten css theme
    
    * add user message suffix
    
    * fix chatml & add llama3 format
    
    * fix llama3 prompt template
    
    * more prompt format fixes
    
    * add more comon stop tokens
    
    * add missing char
    
    * do not separate with new line or comma
    
    * move prompt style
    
    * add hacky llama2 prompt solution, reduce redundancy in promptFormats.js
    
    * fix toggle state localstorage
    
    * add cmd-r prompt et reduce redundancy
    
    * set default prompt to empty
    
    * move files, clean code
    
    * fix css path
    
    * add a button to the new ui
    
    * move new ui to "/public" due to otherwise problematic CORS behaviour
    
    * include new ui in cpp
    
    * fix wrong link to old ui
    
    * renaming to ensure consistency
    
    * fix typos "prompt-format" -> "prompt-formats"
    
    * use correct indent
    
    * add new ui files to makefile
    
    * fix typo

commit 2ac95c9d5678d05e253691fb1f26471675bff5ad
Author: HanishKVC <hanishkvc@gmail.com>
Date:   Sat Jun 1 21:50:18 2024 +0530

    SimpleChat: Simple histogram/repeatMatching driven garbageTrimming, Settings UI, Streaming mode, OpenAi Compat (Model, Authorization Bearer), Save/Restore session, Auto Settings UI (#7548)
    
    * SimpleChat:DU:BringIn local helper js modules using importmap
    
    Use it to bring in a simple trim garbage at end logic, which is
    used to trim received response.
    
    Also given that importmap assumes esm / standard js modules, so
    also global variables arent implicitly available outside the
    modules. So add it has a member of document for now
    
    * SimpleChat:DU: Add trim garbage at end in loop helper
    
    * SimpleChat:DU:TrimGarbage if unable try skip char and retry
    
    * SimpleChat:DU: Try trim using histogram based info
    
    TODO: May have to add max number of uniq chars in histogram at
    end of learning phase.
    
    * SimpleChat:DU: Switch trim garbage hist based to maxUniq simple
    
    Instead of blindly building histogram for specified substring
    length, and then checking if any new char within specified min
    garbage length limit, NOW exit learn state when specified maxUniq
    chars are found. Inturn there should be no new chars with in
    the specified min garbage length required limit.
    
    TODO: Need to track char classes like alphabets, numerals and
    special/other chars.
    
    * SimpleChat:DU: Bring in maxType to the mix along with maxUniq
    
    Allow for more uniq chars, but then ensure that a given type of
    char ie numerals or alphabets or other types dont cross the
    specified maxType limit. This allows intermixed text garbage
    to be identified and trimmed.
    
    * SimpleChat:DU: Cleanup debug log messages
    
    * SimpleChat:UI: Move html ui base helpers into its own module
    
    * SimpleChat:DU:Avoid setting frequence/Presence penalty
    
    Some models like llama3 found to try to be over intelligent by
    repeating garbage still, but by tweaking the garbage a bit so that
    it is not exactly same. So avoid setting these penalties and let
    the model's default behaviour work out, as is.
    
    Also the simple minded histogram based garbage trimming from end,
    works to an extent, when the garbage is more predictable and
    repeatative.
    
    * SimpleChat:UI: Add and use a para-create-append helper
    
    Also update the config params dump to indicate that now one needs
    to use document to get hold of gMe global object, this is bcas of
    moving to module type js.
    
    Also add ui.mjs to importmap
    
    * SimpleChat:UI: Helper to create bool button and use it wrt settings
    
    * SimpleChat:UI: Add Select helper and use it wrt ChatHistoryInCtxt
    
    * SimpleChat:UI:Select: dict-name-value, value wrt default, change
    
    Take a dict/object of name-value pairs instead of just names.
    Inturn specify the actual value wrt default, rather than the
    string representing that value.
    
    Trap the needed change event rather than click wrt select.
    
    * SimpleChat:UI: Add Div wrapped label+element helpers
    
    Move settings related elements to use the new div wrapped ones.
    
    * SimpleChat:UI:Add settings button and bring in settings ui
    
    * SimpleChat:UI:Settings make boolean button text show meaning
    
    * SimpleChat: Update a bit wrt readme and notes in du
    
    * SimpleChat: GarbageTrim enable/disable, show trimmed part ifany
    
    * SimpleChat: highlight trim, garbage trimming bitmore aggressive
    
    Make it easy for end user to identified the trimmed text.
    
    Make garbage trimming logic, consider a longer repeat garbage
    substring.
    
    * SimpleChat: Cleanup a bit wrt Api end point related flow
    
    Consolidate many of the Api end point related basic meta data into
    ApiEP class.
    
    Remove the hardcoded ApiEP/Mode settings from html+js, instead use
    the generic select helper logic, inturn in the settings block.
    
    Move helper to generate the appropriate request json string based
    on ApiEP into SimpleChat class itself.
    
    * SimpleChat:Move extracting assistant response to SimpleChat class
    
    so also the trimming of garbage.
    
    * SimpleChat:DU: Bring in both trim garbage logics to try trim
    
    * SimpleChat: Cleanup readme a bit, add one more chathistory length
    
    * SimpleChat:Stream:Initial handshake skeleton
    
    Parse the got stream responses and try extract the data from it.
    
    It allows for a part read to get a single data line or multiple
    data line. Inturn extract the json body and inturn the delta
    content/message in it.
    
    * SimpleChat: Move handling oneshot mode server response
    
    Move handling of the oneshot mode server response into SimpleChat.
    
    Also add plumbing for moving multipart server response into same.
    
    * SimpleChat: Move multi part server response handling in
    
    * SimpleChat: Add MultiPart Response handling, common trimming
    
    Add logic to call into multipart/stream server response handling.
    
    Move trimming of garbage at the end into the common handle_response
    helper.
    
    Add new global flag to control between oneshot and multipart/stream
    mode of fetching response. Allow same to be controlled by user.
    
    If in multipart/stream mode, send the stream flag to the server.
    
    * SimpleChat: show streamed generative text as it becomes available
    
    Now that the extracting of streamed generated text is implemented,
    add logic to show the same on the screen.
    
    * SimpleChat:DU: Add NewLines helper class
    
    To work with an array of new lines. Allow adding, appending,
    shifting, ...
    
    * SimpleChat:DU: Make NewLines shift more robust and flexible
    
    * SimpleChat:HandleResponseMultiPart using NewLines helper
    
    Make handle_response_multipart logic better and cleaner. Now it
    allows for working with the situation, where the delta data line
    got from server in stream mode, could be split up when recving,
    but still the logic will handle it appropriately.
    
    ALERT: Rather except (for now) for last data line wrt a request's
    response.
    
    * SimpleChat: Disable console debug by default by making it dummy
    
    Parallely save a reference to the original func.
    
    * SimpleChat:MultiPart/Stream flow cleanup
    
    Dont try utf8-decode and newlines-add_append if no data to work on.
    
    If there is no more data to get (ie done is set), then let NewLines
    instance return line without newline at end, So that we dont miss
    out on any last-data-line without newline kind of scenario.
    
    Pass stream flag wrt utf-8 decode, so that if any multi-byte char
    is only partly present in the passed buffer, it can be accounted
    for along with subsequent buffer. At sametime, bcas of utf-8's
    characteristics there shouldnt be any unaccounted bytes at end,
    for valid block of utf8 data split across chunks, so not bothering
    calling with stream set to false at end. LATER: Look at TextDecoder's
    implementation, for any over intelligence, it may be doing..
    If needed, one can use done flag to account wrt both cases.
    
    * SimpleChat: Move baseUrl to Me and inturn gMe
    
    This should allow easy updating of the base url at runtime by the
    end user.
    
    * SimpleChat:UI: Add input element helper
    
    * SimpleChat: Add support for changing the base url
    
    This ensures that if the user is running the server with a
    different port or wants to try connect to server on a different
    machine, then this can be used.
    
    * SimpleChat: Move request headers into Me and gMe
    
    Inturn allow Authorization to be sent, if not empty.
    
    * SimpleChat: Rather need to use append to insert headers
    
    * SimpleChat: Allow Authorization header to be set by end user
    
    * SimpleChat:UI+: Return div and element wrt creatediv helpers
    
    use it to set placeholder wrt Authorization header.
    
    Also fix copy-paste oversight.
    
    * SimpleChat: readme wrt authorization, maybe minimal openai testing
    
    * SimpleChat: model request field for openai/equivalent compat
    
    May help testing with openai/equivalent web services, if they
    require this field.
    
    * SimpleChat: readme stream-utf-8 trim-english deps, exception2error
    
    * Readme: Add a entry for simplechat in the http server section
    
    * SimpleChat:WIP:Collate internally, Stream mode Trap exceptions
    
    This can help ensure that data fetched till that point, can be
    made use of, rather than losing it.
    
    On some platforms, the time taken wrt generating a long response,
    may lead to the network connection being broken when it enters
    some user-no-interaction related power saving mode.
    
    * SimpleChat:theResp-origMsg: Undo a prev change to fix non trim
    
    When the response handling was moved into SimpleChat, I had changed
    a flow bit unnecessarily and carelessly, which resulted in the non
    trim flow, missing out on retaining the ai assistant response.
    
    This has been fixed now.
    
    * SimpleChat: Save message internally in handle_response itself
    
    This ensures that throwing the caught exception again for higher
    up logic, doesnt lose the response collated till that time.
    
    Go through theResp.assistant in catch block, just to keep simple
    consistency wrt backtracing just in case.
    
    Update the readme file.
    
    * SimpleChat:Cleanup: Add spacing wrt shown req-options
    
    * SimpleChat:UI: CreateDiv Divs map to GridX2 class
    
    This allows the settings ui to be cleaner structured.
    
    * SimpleChat: Show Non SettingsUI config field by default
    
    * SimpleChat: Allow for multiline system prompt
    
    Convert SystemPrompt into a textarea with 2 rows. Reduce
    user-input-textarea to 2 rows from 3, so that overall
    vertical space usage remains same.
    
    Shorten usage messages a bit, cleanup to sync with settings ui.
    
    * SimpleChat: Add basic skeleton for saving and loading chat
    
    Inturn when ever a chat message (system/user/model) is added,
    the chat will be saved into browser's localStorage.
    
    * SimpleChat:ODS: Add a prefix to chatid wrt ondiskstorage key
    
    * SimpleChat:ODS:WIP:TMP: Add UI to load previously saved chat
    
    This is a temporary flow
    
    * SimpleChat:ODS:Move restore/load saved chat btn setup to Me
    
    This also allows being able to set the common system prompt
    ui element to loaded chat's system prompt.
    
    * SimpleChat:Readme updated wrt save and restore chat session info
    
    * SimpleChat:Show chat session restore button, only if saved session
    
    * SimpleChat: AutoCreate ChatRequestOptions settings to an extent
    
    * SimpleChat: Update main README wrt usage with server

commit 750f60c03e4d3f53fa51910551ce87a3d508d2d7
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jun 1 15:47:04 2024 +0200

    CUDA: fix Pascal FA, deq. KV to FP16 for batch > 8 (#7681)

commit 9b596417af11c9ac44fcae0fcfbc6f3665089083
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jun 1 08:44:14 2024 +0200

    CUDA: quantized KV support for FA vec (#7527)
    
    * CUDA: quantized KV support for FA vec
    
    * try CI fix
    
    * fix commented-out kernel variants
    
    * add q8_0 q4_0 tests
    
    * fix nwarps > batch size
    
    * split fattn compile via extern templates
    
    * fix flake8
    
    * fix metal tests
    
    * fix cmake
    
    * make generate_cu_files.py executable
    
    * add autogenerated .cu files
    
    * fix AMD
    
    * error if type_v != FP16 and not flash_attn
    
    * remove obsolete code

commit a323ec60af14a33d560df98f2cc41b4112cb4f80
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 31 22:23:04 2024 +0300

    server : update js (#7670)

commit 0515ad93f48df63bbff204eddb0cac75e8585c65
Author: Galunid <karolek1231456@gmail.com>
Date:   Fri May 31 17:42:33 2024 +0200

    convert-hf : Handle NotImplementedError in convert-hf-to-gguf (#7660)

commit c8047d538f3addab40e3112be60bb92e70ce1a50
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri May 31 16:26:21 2024 +0200

    scripts: update compare_llama_bench.py [no ci] (#7673)

commit 30e238b246f8002cc6eb7cb79afe242243f1f66d
Author: Daniele <57776841+daniandtheweb@users.noreply.github.com>
Date:   Fri May 31 14:00:29 2024 +0000

    Improve HIP compatibility (#7672)

commit 16926dff92d6d0efa8cbc0f44d30d63349532b38
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 31 15:04:58 2024 +0300

    readme : link homebrew discussion

commit 0c27e6f62eea80140daf152d7b6c154466614e5c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 31 14:17:10 2024 +0300

    ggml : fix loongson compile warnings (#7537)
    
    * ggml : fix loongson compile warnings
    
    ggml-ci
    
    * Fix loongarch quantize test fail.
    
    Fix unexpected error introduced during rebase code.
    
    * tests : disable json test due to lack of python on the CI node
    
    ggml-ci
    
    ---------
    
    Co-authored-by: junchao-loongson <zhaojunchao@loongson.cn>

commit 2e32f874e675f7bc5307cb7b4470ddbe090bab8f
Author: Galunid <karolek1231456@gmail.com>
Date:   Fri May 31 10:24:41 2024 +0200

    Somehow '**' got lost (#7663)

commit 1af511fc22cba4959dd8bced5501df9e8af6ddf9
Author: Galunid <karolek1231456@gmail.com>
Date:   Fri May 31 10:09:20 2024 +0200

    Add convert.py removal to hot topics (#7662)

commit 0541f06296753dbc59a57379eb54cec865a4c9f9
Author: Sertaç Özercan <852750+sozercan@users.noreply.github.com>
Date:   Thu May 30 16:57:16 2024 -0700

    [no ci] docs: add aikit to readme (#7650)
    
    Signed-off-by: Sertac Ozercan <sozercan@gmail.com>

commit 9022c33646fbf78da35f40c3f98576cc08c40ddf
Author: JohnnyB <jboero@users.noreply.github.com>
Date:   Thu May 30 21:32:38 2024 +0100

    Fixed painfully slow single process builds. (#7326)
    
    * Fixed painfully slow single process builds.
    
    * Added nproc for systems that don't default to nproc

commit 5921b8f089d3b7bda86aac5a66825df6a6c10603
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu May 30 19:01:41 2024 +0300

    llama : cache llama_token_to_piece (#7587)
    
    * llama : cache llama_token_to_piece
    
    ggml-ci
    
    * llama : use vectors and avoid has_cache
    
    ggml-ci
    
    * llama : throw on unknown tokenizer types
    
    ggml-ci
    
    * llama : print a log of the total cache size

commit 5dcdf946764fae49a8e2a90bf2f0960bde1c44e8
Author: Martin Delille <martin@delille.org>
Date:   Thu May 30 17:07:39 2024 +0200

    Fix conan badge display [no ci] (#7645)

commit 2e2340de1740b07f2418c04792607387d4dc1442
Author: Manuel <44313466+makuche@users.noreply.github.com>
Date:   Thu May 30 16:58:15 2024 +0200

    Add brew installation instruction to README [no ci] (#7616)

commit 7846540bd291e52cd4eee53882315760e05239be
Author: Martin Delille <martin@delille.org>
Date:   Thu May 30 14:52:50 2024 +0200

    readme : add Conan badge (#7638)

commit e6157f94c8f835f7f774b98409078867472a34fe
Author: Brian <mofosyne@gmail.com>
Date:   Thu May 30 21:55:36 2024 +1000

    github: add contact links to issues and convert question into research [no ci] (#7612)

commit 9c4c9cc83f7297a10bb3b2af54a22ac154fd5b20
Author: Galunid <karolek1231456@gmail.com>
Date:   Thu May 30 13:40:00 2024 +0200

    Move convert.py to examples/convert-legacy-llama.py (#7430)
    
    * Move convert.py to examples/convert-no-torch.py
    
    * Fix CI, scripts, readme files
    
    * convert-no-torch -> convert-legacy-llama
    
    * Move vocab thing to vocab.py
    
    * Fix convert-no-torch -> convert-legacy-llama
    
    * Fix lost convert.py in ci/run.sh
    
    * Fix imports
    
    * Fix gguf not imported correctly
    
    * Fix flake8 complaints
    
    * Fix check-requirements.sh
    
    * Get rid of ADDED_TOKENS_FILE, FAST_TOKENIZER_FILE
    
    * Review fixes

commit 59b0d077662fab430446b3119fa142f3291c45b2
Author: Chris Elrod <elrodc@gmail.com>
Date:   Thu May 30 07:32:55 2024 -0400

    faster avx512 exp implementation (#7551)
    
    * faster avx512 exp implementation
    
    * x->r
    
    * improve accuracy, handle special cases
    
    * remove `e`

commit d5c05821f3c3d6cabe8ac45776fe0ecb0da13eca
Author: junchao-loongson <68935141+junchao-loongson@users.noreply.github.com>
Date:   Thu May 30 17:30:10 2024 +0800

    ggml : fix loongarch build (O2 issue) (#7636)

commit 972b555ab935705f3437abd5909a5c46852811f6
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu May 30 09:52:39 2024 +0200

    README: explain parallel build [no ci] (#7618)

commit 3854c9d07f67de7f8cd6d86117bfaef47549b05a
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Thu May 30 14:19:08 2024 +0800

    [SYCL] fix intel docker (#7630)
    
    * Update main-intel.Dockerfile
    
    * workaround for https://github.com/intel/oneapi-containers/issues/70
    
    * reset intel docker in CI
    
    * add missed in server

commit eb57fee51f7b4d78039f003249873c2eb46f12f6
Author: Galunid <karolek1231456@gmail.com>
Date:   Thu May 30 02:10:40 2024 +0200

    gguf-py : Add tokenizer.ggml.pre to gguf-new-metadata.py (#7627)

commit 55d62262a99cd8bc28a1492975791fe433c8cc0f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 29 22:20:40 2024 +0300

    metal : remove invalid asserts (#7617)

commit 975ec63ff26cdf96156d1126d86f75a395fdc43a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 29 20:45:25 2024 +0300

    metal : add missing asserts (#7617)

commit fb76ec31a9914b7761c1727303ab30380fd4f05c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 29 20:17:31 2024 +0300

    ggml : fix YARN + add tests + add asserts (#7617)
    
    * tests : add rope tests
    
    ggml-ci
    
    * ggml : fixes (hopefully)
    
    ggml-ci
    
    * tests : add non-cont tests
    
    ggml-ci
    
    * cuda : add asserts for rope/norm + fix DS2
    
    ggml-ci
    
    * ggml : assert contiguousness
    
    * tests : reduce RoPE tests
    
    ggml-ci

commit cce3dcffc5695bd24835f04e6080070a2a119873
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 29 15:38:26 2024 +0300

    cuda : non-cont concat support (#7610)
    
    * tests : add non-cont concat tests
    
    * cuda : non-cont concat support
    
    ggml-ci

commit 210d99173dc82aafb48f6e39d787c387951fe3a9
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Wed May 29 14:45:44 2024 +0300

    llama-bench : add support for the RPC backend (#7435)

commit 87bdf2a199acd62e19814d7a4d0500a04a7f09f3
Author: slaren <slarengh@gmail.com>
Date:   Wed May 29 13:36:39 2024 +0200

    ggml : use atomic_flag for critical section (#7598)
    
    * ggml : use atomic_flag for critical section
    
    * add windows shims

commit 00281b7be32462754618c42ed93f95743af46627
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 29 14:31:18 2024 +0300

    scripts : remove mpi remnants

commit 2ab977282b02ccd6783fbbaec393c96886cf33b1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 29 14:29:52 2024 +0300

    sync : ggml

commit 72de268bec49f67e2883880f573c55cea32de736
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun May 26 18:35:23 2024 +0300

    ggml : restore ggml_rope_xpos_inplace (ggml/0)
    
    ggml-ci

commit 0e8d8bfd6caf1d0a8cbdf9d3d5c06fbbb9dfced8
Author: Akarshan Biswas <akarshanbiswas@fedoraproject.org>
Date:   Wed May 29 12:23:47 2024 +0530

    Add Arc A750 and Arch linux to readme-sycl.md as verified GPU model and Linux distro (#7605)

commit 504f0c340f6b5e04de682f6ddefdd3b81208df5d
Author: zhouwg <zhouwg2000@gmail.com>
Date:   Wed May 29 10:09:31 2024 +0800

    ggml : fix typo in ggml.c (#7603)

commit b864b50ce5e2beefc8c2fd31733e4e1a978b7754
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Wed May 29 07:00:24 2024 +0800

    [SYCL] Align GEMM dispatch (#7566)
    
    * align GEMM dispatch

commit 02c1ecad07f0e2d2febe8196271bcc64bdc9c006
Author: jaime-m-p <167997752+jaime-m-p@users.noreply.github.com>
Date:   Tue May 28 21:46:34 2024 +0200

    Tokenizer WPM fixes (#7500)
    
    * Update random test: add_bos_token.
    * Update random test: add WPM models for testing.
    * Build vocab.special_tokens_cache using vocab token types.
    * Fix and improve WPM preprocessing.
      - Fix unicode edge case combinations.
      - Split by whitspace in the same pass.
    * Discard all tokens when no matching found.

commit 6bd12ce409f949012935b7d1b15a21ffa473a565
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 28 22:22:50 2024 +0300

    sycl : fix assert (#7563)

commit 5442939fcc5e6ae41abf40612a95fd71377e487e
Author: Giuseppe Scrivano <giuseppe@scrivano.org>
Date:   Tue May 28 20:49:49 2024 +0200

    llama : support small Granite models (#7481)
    
    * Add optional MLP bias for Granite models
    
    Add optional MLP bias for ARCH_LLAMA to support Granite models.
    Partially addresses ggerganov/llama.cpp/issues/7116
    Still needs some more changes to properly support Granite.
    
    * llama: honor add_space_prefix from the model configuration
    
    propagate the add_space_prefix configuration from the HF model
    configuration to the gguf file and honor it with the gpt2 tokenizer.
    
    Signed-off-by: Giuseppe Scrivano <gscrivan@redhat.com>
    
    * llama: add support for small granite models
    
    it works only for the small models 3b and 8b.
    
    The convert-hf-to-gguf.py script uses the vocabulary size of the
    granite models to detect granite and set the correct configuration.
    
    Signed-off-by: Giuseppe Scrivano <gscrivan@redhat.com>
    
    ---------
    
    Signed-off-by: Giuseppe Scrivano <gscrivan@redhat.com>
    Co-authored-by: Steffen Roecker <sroecker@redhat.com>

commit 56411a950f255b523a9edd684fd1632752474399
Author: k.h.lai <adrian.k.h.lai@outlook.com>
Date:   Wed May 29 01:25:08 2024 +0800

    vulkan: properly initialize vulkan devices for LLAMA_SPLIT_MODE_NONE (#7552)

commit 2b737caae100cf0ac963206984332e422058f2b9
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Tue May 28 18:13:36 2024 +0300

    rpc : resource management rework (#7562)
    
    * rpc : resource management rework
    
    * address review comments

commit ee3dff6b8e39bb8c1cdea1782a7b95ef0118f970
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Tue May 28 17:07:05 2024 +0200

    Add support for DeepseekV2ForCausalLM (#7519)
    
    * common : increase max number of experts to 160
    
    * common : add tensors ATTN_Q_A, ATTN_Q_A_NORM, ATTN_Q_B, ATTN_KV_A_MQA, ATTN_KV_A_NORM, ATTN_KV_B needed by DeepSeek-V2 MLA (multi-head latent attention) architecture
    
    * common : add model header parameters: leading_dense_block_count, expert_feed_forward_length, expert_shared_count, expert_weights_scale, attention.q_lora_rank, attention.kv_lora_rank, rope.scaling.yarn_log_multiplier
    
    * convert-hf : add model conversion support for DeepseekV2ForCausalLM
    
    * llama : add model types for DeepSeek-V2 and DeepSeek-V2-Lite models
    
    * llama : add two new llm_build_moe_ffn() arguments: scale_w (whether to scale weights of selected MoE experts) and w_scale (numerical value of the scaling factor)
    
    * llama : add inference support for LLM_ARCH_DEEPSEEK2
    
    ---------
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

commit edc29433fa08b4e5aeb67649a29fc7713af13d04
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 28 15:04:09 2024 +0300

    tests : fix test-tokenizer-0.sh

commit 8b99e2aa66ba39e4e1114effea6ef7430881eca4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 28 13:55:35 2024 +0300

    llama : handle unknown utf8 bytes (#7588)

commit 271ff3fc44a6ecfcea3ebc192e67567d578b7772
Author: Brian <mofosyne@gmail.com>
Date:   Tue May 28 20:27:27 2024 +1000

    github: add refactor to issue template (#7561)
    
    * github: add refactor issue template [no ci]
    
    * Update 07-refactor.yml

commit e2b065071c5fc8ac5697d12ca343551faee465cc
Author: Neo Zhang <14088817+arthw@users.noreply.github.com>
Date:   Tue May 28 17:53:37 2024 +0800

    [SYCL]fix ggml_sycl_mul_mat_id() to match the change of api (#7436)
    
    * fix mul_mat_id to match the change of api
    
    * rm comment
    
    * rm unused or duplicated code, rename as review comment

commit 0548a4187f2e53b8fc6d9ff0f4c71988f708ff42
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 28 11:04:19 2024 +0300

    ggml : generalize GGML_OP_CONCAT (#7563)
    
    * ggml : generalize GGML_OP_CONCAT (WIP)
    
    ggml-ci
    
    * tests : add dim != 2 tests
    
    * metal : generalize concat kernel
    
    * tests : naming
    
    * cuda : generalize concat kernel
    
    ggml-ci
    
    * sycl : add warning and assert
    
    * ggml : fix op params handling
    
    * metal : bugfix kernel
    
    ggml-ci
    
    * ggml : reimplement CPU and Metal
    
    * cuda : add asserts
    
    ggml-ci
    
    * ggml : fix ptrs
    
    ggml-ci

commit 9335b969e86a222e247adacedf814d8abfff8847
Author: mgroeber9110 <45620825+mgroeber9110@users.noreply.github.com>
Date:   Tue May 28 06:55:51 2024 +0200

    server: do not remove whitespace at the start of a completion chunk (#7524)

commit c41767154eb82aa3fe7568fc816c3402b78eae94
Author: Nathan Epstein <nate2@umbc.edu>
Date:   Tue May 28 00:41:14 2024 -0400

    Markdownish code block fix (#7571)
    
    * markdownish codeblock fix
    
    * updating regexes

commit 74b239b3d5f067470d7ef5e26e2e059720572e32
Author: Ikko Eltociear Ashimine <eltociear@gmail.com>
Date:   Tue May 28 11:48:16 2024 +0900

    llava : update clip.h (#7580)
    
    overriden -> overridden

commit 852aafb163d32d5bad63c10bc323a02c28fec59d
Author: Djip007 <djip.perois@free.fr>
Date:   Tue May 28 01:40:47 2024 +0200

    update HIP_UMA #7399 (#7414)
    
    * update HIP_UMA #7399
    
    add use of hipMemAdviseSetCoarseGrain when LLAMA_HIP_UMA is enable.
    - get x2 on prompte eval and x1.5 on token gen with rocm6.0 on ryzen 7940HX iGPU (780M/gfx1103)
    
    * simplify code, more consistent style
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 0136966dafb452601c23f30395878d5a65ddc559
Author: kunnis <kunnis@users.noreply.github.com>
Date:   Mon May 27 18:40:12 2024 -0500

    adding in x64 targets to cmake presets (#7574)

commit 10b1e4587670feba2c7730a645accf8234873113
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon May 27 19:34:40 2024 +0200

    make: add --device-debug to NVCC debug flags (#7542)

commit 197c00681b80f9dea17d11a4436b6b8ef1be0ce8
Author: agray3 <agray3@users.noreply.github.com>
Date:   Mon May 27 18:33:42 2024 +0100

    Allow multiple copy function pointers for CUDA graph kernel param updates (#7565)
    
    CUDA graphs require parameter updates to kernels associated with
    GGML_OP_CPY nodes. Previously the implementation only checked for a
    single CUDA kernel in such nodes, but this caused a bug in cases where
    2 such kernels exist. This fixes the issue by using a vector to allow
    multiple function pointers to be stored and checked against.
    
    Fixes #7942

commit 95f84d5ce8b449a9b16009434aca800df504a02e
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Mon May 27 17:34:51 2024 +0100

    Fix q_xxs using mul_mat_q (#7459)

commit 5487593bc7ee0b65b9d2e2985b4b61dc77043101
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Mon May 27 13:34:09 2024 +0100

    Add freq factors (#7495)

commit 1d8fca72ae9154eec0e1c0a75cfaac3c50f08e4a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 27 12:10:19 2024 +0300

    metal : add GGML_OP_REPEAT kernels (#7557)
    
    ggml-ci

commit 62bfef5194d5582486d62da3db59bf44981b7912
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 27 10:38:39 2024 +0300

    metal : disable FA kernel for HS=256 (#7556)
    
    ggml-ci

commit eaf6e031741ca2d3aafeff3e0f4dd7557a974d2b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 27 09:24:13 2024 +0300

    llama : add comments about experimental flags (#7544)

commit d6ef0e77dd25f54fb5856af47e3926cf6f36c281
Author: Brian <mofosyne@gmail.com>
Date:   Mon May 27 10:54:30 2024 +1000

    github: add self sorted issue ticket forms (#7543)
    
    * github: add self sorted issue ticket forms [no ci]
    
    * github: consolidate BSD in bug issue ticket
    
    * github: remove contact from bug ticket template [no ci]
    
    * github: remove bios from os dropdown in bug report [no ci]

commit dff451cfa1f297348751ce6b538670e1ae9a7d5b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun May 26 18:54:56 2024 +0300

    flake.lock: Update (#7540)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/4a6b83b05df1a8bd7d99095ec4b4d271f2956b64?narHash=sha256-%2BNpbZRCRisUHKQJZF3CT%2Bxn14ZZQO%2BKjxIIanH3Pvn4%3D' (2024-05-17)
      → 'github:NixOS/nixpkgs/bfb7a882678e518398ce9a31a881538679f6f092?narHash=sha256-4zSIhSRRIoEBwjbPm3YiGtbd8HDWzFxJjw5DYSDy1n8%3D' (2024-05-24)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit d298382ad977ec89c8de7b57459b9d7965d2c272
Author: Brian <mofosyne@gmail.com>
Date:   Mon May 27 00:10:17 2024 +1000

    main: replace --no-special with --special (#7534)
    
    This also flips the default behavior of the output to not include control token by default.

commit 32a28217f475119926c603341e8273b26932b56a
Author: Galunid <karolek1231456@gmail.com>
Date:   Sun May 26 16:02:34 2024 +0200

    Fix aya-23 conversion scripts (#7539)

commit c429b33beb35f13934a4dfbe0c138d30b45e5d54
Author: Bartowski <ckealty1182@gmail.com>
Date:   Sun May 26 08:28:35 2024 -0400

    llama : add Smaug 70B support (#7402)

commit 9146d36fe7e3e911a07438c07efc1bae082f6390
Author: Aarni Koskela <akx@iki.fi>
Date:   Sun May 26 15:09:42 2024 +0300

    Readme: add akx/ggify to tools (#1484)

commit b9adcbbf92fc7096bee23fe61496d25652ebf765
Author: HanishKVC <hanishkvc@gmail.com>
Date:   Sun May 26 06:26:34 2024 +0530

    SimpleChat Completion Mode flexibility and cleanup, Settings gMe, Optional sliding window (#7480)
    
    * SimpleChat: A placeholder system prompt, Use usage msg in code
    
    Just have a alert msg wrt needing javascript enabled in html. And
    have usage message from js file. Update the usage message a bit.
    So also enable switch session wrt setup_ui call.
    
    Add a possible system prompt as a placeholder for the system-input.
    
    * SimpleChat:CompletionMode: Allow control of Role: prefix
    
    * SimpleChat:Completion: Avoid Role: prefix; Newline only in between
    
    In completion mode
    
    * avoid inserting Role: prefix before each role's message
    
    * avoid inserting newline at the begin and end of the prompt
      message. However if there are multiple role messages, then
      insert newline when going from one role's message to the
      next role's message.
    
    * SimpleChat:CompletionMode: Update readme/usage, trim textarea newline
    
    Readme update wrt completion mode behavior.
    
    Usage help updated wrt completion mode behavior.
    
    When changing from input to textarea elment wrt user input, the last
    newline at the end of the user input wrt textarea, was forgotten to be
    filtered, this is fixed now. However if user wants to have a explicit
    newline they can using shift+enter to insert a newline, that wont be
    removed. The extra newline removal logic uses substring and keyup to
    keep things simple and avoid some previously noted bugs wrt other
    events in the key path as well as IME composition etal.
    
    * SimpleChat:SC: Ensure proper clearing/reseting
    
    previous logic would have cleared/reset the xchat, without doing
    the same wrt iLastSys, thus leading to it pointing to a now non
    existent role-content entry.
    
    So if a user set a system prompt and used completion mode, it would
    have done the half stupid clear, after the model response was got.
    Inturn when user tries to send a new completion query, it would
    inturn lead to handle_user_submit trying to add/update system prompt
    if any, which will fail, bcas iLastSys will be still pointing to a
    non existant entry.
    
    This is fixed now, by having a proper clear helper wrt SC class.
    
    * SimpleChat: Update usage note and readme a bit
    
    * SimpleChat:Completion: clear any prev chat history at begining
    
    Previously any chat history including model response to a completion
    query would have got cleared, after showing the same to the user,
    at the end of handle_user_submit, rather than at the begining.
    
    This gave the flexibility that user could switch from chat mode
    to completion mode and have the chat history till then sent to
    the ai model, as part of the completion query. However this flow
    also had the issue that, if user switches between different chat
    sessions, after getting a completion response, they can no longer
    see the completion query and its response that they had just got.
    
    The new flow changes the clearing of chat history wrt completion
    mode to the begining of handle_user_submit, so that user doesnt
    lose the last completion mode query and response, till a new
    completion mode query is sent to the model, even if they were to
    switch between the chat sessions. At the same time the loss of
    flexibility wrt converting previous chat history into being part
    of the completion query implicitly doesnt matter, because now
    the end user can enter multiline queries.
    
    * SimpleChat:Try read json early, if available
    
    For later
    
    the server flow doesnt seem to be sending back data early, atleast
    for the request (inc options) that is currently sent.
    
    if able to read json data early on in future, as and when ai model
    is generating data, then this helper needs to indirectly update
    the chat div with the recieved data, without waiting for the
    overall data to be available.
    
    * SimpleChat: Rename the half asleep mis-spelled global var
    
    * SimpleChat: Common chat request options from a global object
    
    * SimpleChat: Update title, usage and readme a bit
    
    Keep the title simple so that print file name doesnt have chars
    that need to be removed.
    
    Update readme wrt some of the new helpers and options.
    
    Change Usage list to a list of lists, add few items and style it
    to reduce the margin wrt lists.
    
    * SimpleChat:ChatRequestOptions: max_tokens
    
    As some times based on the query from the user, the ai model may get
    into a run away kind of generation with repeatations etal, so adding
    max_tokens to try and limit this run away behaviour, if possible.
    
    * SimpleChat: Reduce max_tokens to be small but still sufficient
    
    * SimpleChat: Consolidate global vars into gMe, Display to user
    
    This allows the end user to see the settings used by the logic,
    as well as allows users to change/update the settings if they
    want to by using devel-tools/console
    
    * SimpleChat:SlidingWindow: iRecentUserMsgCnt to limit context load
    
    This is disabled by default. However if enabled, then in addition
    to latest system message, only the last N user messages, after the
    latest system message and its reponses from the ai model will be sent
    to the ai-model, when querying for a new response.
    
    This specified N also includes the latest user query.
    
    * SimpleChat: placeholder based usage hint for user-in textarea
    
    * SimpleChat: Try make user experience better, if possible
    
    Reduce chat history context sent to the server/ai-model to be
    just the system-prompt, prev-user-request-and-ai-response and
    cur-user-request, instead of the previous full chat history.
    This way if there is any response with garbage/repeatation, it
    doesnt mess with things beyond the next question, in some ways.
    
    Increase max_tokens to 1024, so that a relatively large previous
    reponse doesnt eat up the space available wrt next query-response.
    However dont forget that the server when started should also
    be started with a model context size of 1k or more, to be on
    safe side.
    
    Add frequency and presence penalty fields set to 1.2 to the set
    of fields sent to server along with the user query. So that
    the model is partly set to try avoid repeating text in its
    response.
    
    * SimpleChat:Add n_predict (equiv max_tokens) for llamacpp server
    
    The /completions endpoint of examples/server doesnt take max_tokens,
    instead it takes the internal n_predict, for now add the same on
    the client side, maybe later add max_tokens to /completions endpoint
    handling.
    
    * SimpleChat: Note about trying to keep things simple yet flexible

commit 9588f196b1d7b21bdff013fcf958c249576b2619
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 25 15:21:30 2024 +0300

    train : change default FA argument (#7528)

commit 3cbd23ed88c03a27e1eb6090ac4a8186ca9ac29a
Author: Brian <mofosyne@gmail.com>
Date:   Sat May 25 19:30:42 2024 +1000

    labeler: added Apple Metal detector (+Kompute) (#7529)
    
    * labeler: added Apple Metal detector [no ci]
    
    * labeler: add Kompute to detector [no ci]

commit 00c63907931bb08a0ed2b7e38cf44dd290143cb9
Author: Justine Tunney <jtunney@mozilla.com>
Date:   Sat May 25 05:04:03 2024 -0400

    main : don't print special tokens with --grammar (#6923)
    
    * main : don't print special tokens with --grammar
    
    The CLI interface was recently changed to print special control tokens
    like the </s> stop message one. This token shouldn't be printed if the
    grammar flag was passed, unless the grammar specifies it, because that
    breaks shell-scriptability.
    
    * main: use seperate stream for control characters
    
    * main: use dprintf and add --ctrl-token-no-out and --ctrl-token-fd-out
    
    * main: dprintf isn't part of the IEEE POSIX standard. Just use write().
    
    * main: remove --ctrl-token-fd-out in favor for fcntl() based detection
    
    * common.cpp: accidentally removed --interactive-first
    
    * main: only merge stdout and control token if not in conversation or grammar mode
    
    * main: rejig control token descriptor handling
    
    * main: must check pipe status on very top of program
    
    * main: renamed --no-special from  --ctrl-token-no-out and other refactoring
    
    * main: refactor ctrl_token_no_out --> no_special
    
    * llama: rename llama_token_is_control_token() to llama_token_is_control()
    
    * main: remove special token file descriptor feature (#5)
    
    ---------
    
    Co-authored-by: Brian <mofosyne@gmail.com>

commit faa0e6979a11dcb731e9d778ad42ceaa0302015e
Author: Masaya, Kato <62578291+msy-kato@users.noreply.github.com>
Date:   Sat May 25 17:42:31 2024 +0900

    ggml: aarch64: SVE kernels for q8_0_q8_0, q4_0_q8_0 vector dot (#7433)
    
    * Add SVE support for q4_0_q8_0 q8_0_q8_0
    
    * remove ifdef

commit 9791f402580838d7f8543ae7bc633ef265e436f0
Author: Elton Kola <eltonkola@gmail.com>
Date:   Sat May 25 04:11:33 2024 -0400

    android : module (#7502)
    
    * move ndk code to a new library
    
    * add gradle file

commit 902184dd3a9d6685e752b19027a48423742531db
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat May 25 05:30:59 2024 +0200

    fix missing slash in `fs_get_cache_directory()` (#7503)
    
    * fix missing slash in fs_get_cache_directory()
    
    * use LOCALAPPDATA for fs_get_cache_directory()
    
    * better code style

commit 57684331fc2d685f7d1f5775af0b9e47d1829833
Author: Mikko Juola <mikjuo@gmail.com>
Date:   Fri May 24 18:14:42 2024 -0700

    Make tokenize CLI tool have nicer command line arguments. (#6188)
    
    * Make tokenizer.cpp CLI tool nicer.
    
    Before this commit, tokenize was a simple CLI tool like this:
    
      tokenize MODEL_FILENAME PROMPT [--ids]
    
    This simple tool loads the model, takes the prompt, and shows the tokens
    llama.cpp is interpreting.
    
    This changeset makes the tokenize more sophisticated, and more useful
    for debugging and troubleshooting:
    
      tokenize [-m, --model MODEL_FILENAME]
               [--ids]
               [--stdin]
               [--prompt]
               [-f, --file]
               [--no-bos]
               [--log-disable]
    
    It also behaves nicer on Windows now, interpreting and rendering Unicode
    from command line arguments and pipes no matter what code page the user
    has set on their terminal.
    
    * style fix: strlen(str) == 0 --> *str == 0
    
    * Simplify tokenize.cpp; by getting rid of handling positional style arguments.
    
    It must now be invoked with long --model, --prompt etc. arguments only.
    Shortens the code.
    
    * tokenize.cpp: iostream header no longer required
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: brian khuu <mofosyne@gmail.com>

commit b83bab15a5d2a1e7807d09613a9b34309d86cfaa
Author: compilade <git@compilade.net>
Date:   Fri May 24 21:11:48 2024 -0400

    gguf-py : fix and simplify quantized shape round-trip (#7483)
    
    * gguf-py : fix and simplify quantized shape round-trip
    
    * gguf-py : remove unused import

commit d041d2ceaaf50e058622d92921b3e680ffa4e9e7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 24 18:59:06 2024 +0300

    flake.lock: Update (#7232)
    
    Flake lock file updates:
    
    • Updated input 'flake-parts':
        'github:hercules-ci/flake-parts/e5d10a24b66c3ea8f150e47dfdb0416ab7c3390e?narHash=sha256-yzcRNDoyVP7%2BSCNX0wmuDju1NUCt8Dz9%2BlyUXEI0dbI%3D' (2024-05-02)
      → 'github:hercules-ci/flake-parts/8dc45382d5206bd292f9c2768b8058a8fd8311d9?narHash=sha256-/GJvTdTpuDjNn84j82cU6bXztE0MSkdnTWClUCRub78%3D' (2024-05-16)
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/63c3a29ca82437c87573e4c6919b09a24ea61b0f?narHash=sha256-4cPymbty65RvF1DWQfc%2BBc8B233A1BWxJnNULJKQ1EY%3D' (2024-05-02)
      → 'github:NixOS/nixpkgs/4a6b83b05df1a8bd7d99095ec4b4d271f2956b64?narHash=sha256-%2BNpbZRCRisUHKQJZF3CT%2Bxn14ZZQO%2BKjxIIanH3Pvn4%3D' (2024-05-17)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit 27891f6db03de6e3fd5941983838c29bef253352
Author: Brian <mofosyne@gmail.com>
Date:   Fri May 24 23:47:56 2024 +1000

    docker.yml: disable light-intel and server-intel test (#7515)
    
    * docker.yml: disable light-intel test
    
    * docker.yml: disable server-intel test

commit fbca2f27fc7fa9aa4a8ad0357478fdb908472908
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Fri May 24 14:31:13 2024 +0200

    Add support for ArcticForCausalLM (#7020)
    
    * common : increase max number of experts to 128
    
    * common : add tensor LLM_TENSOR_FFN_NORM_EXPS for normalization before MoE that runs in parallel to attention + ffn
    
    * gguf-py : add architecture-specific block mappings that override selected general block mappings
    
    * convert-hf : add model conversion support for ArcticForCausalLM
    
    * convert-hf : use added_tokens_decoder from tokenizer_config.json to redefine tokens from SentencePiece model (only for ArcticForCausalLM)
    
    * llama : add inference support for LLM_ARCH_ARCTIC
    
    ---------
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

commit 0df0aa8e43c3378975269a51f9b876c8692e70da
Author: Neo Zhang <14088817+arthw@users.noreply.github.com>
Date:   Fri May 24 10:06:56 2024 +0800

    add build shared lib in win release package (#7438)

commit 74f33adf5f8b20b08fc5a6aa17ce081abe86ef2f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu May 23 17:43:18 2024 +0300

    readme : remove trailing space (#7469)

commit 1debe72737ea131cb52975da3d53ed3a835df3a6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu May 23 17:17:43 2024 +0300

    ggml : silence UB sanitizer error during iq2_xxs quantization (#0)

commit 007489e895bad02e4e54758bf0bdf2d6a4cdb7c1
Author: Tristan Druyen <tristan@vault81.mozmail.com>
Date:   Thu May 23 16:15:15 2024 +0200

    Fix phi3 chat template confusion with zephyr (#7449)
    
    * Fix phi3 template matching vs zephyr
    
    * Add regression test for new phi3 chat template
    
    * Implement review suggestions
    
    * Fix phi3 jinja test templates & match by <|end|>
    
    * Apply suggestion
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    * Add all phi3 template variants in tests
    
    * Remove unneeded message trimming
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    * Fix tests to not expect trimmed messages
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>

commit 8b94e799dfa482adf63419df4905dc79b37e179f
Author: Raj Hammeer Singh Hada <hammeerraj@gmail.com>
Date:   Thu May 23 18:00:13 2024 +0530

    readme : add Bunny in supported models [no ci] (#7469)

commit 3015851c5ac7334fb544a23a70a284c117b87044
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu May 23 14:29:26 2024 +0200

    llama : add getters for n_threads/n_threads_batch (#7464)
    
    * llama : add getters for n_threads/n_threads_batch
    
    This commit adds two new functions to the llama API. The functions
    can be used to get the number of threads used for generating a single
    token and the number of threads used for prompt and batch processing
    (multiple tokens).
    
    The motivation for this is that we want to be able to get the number of
    threads that the a context is using. The main use case is for a
    testing/verification that the number of threads is set correctly.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! llama : add getters for n_threads/n_threads_batch
    
    Rename the getters to llama_n_threads and llama_n_threads_batch.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 55ac3b7aeaf52f19786ed96e885d89521fc0f6c8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu May 23 15:28:14 2024 +0300

    ci : use Pythia models instead of OpenLlama (#7470)
    
    * ci : start using Pythia models over OpenLlama
    
    ggml-ci
    
    * ci : disable q2_k ppl tests
    
    * ci : use convert-hf-to-gguf.py
    
    * ci : update gg_get_model
    
    * ci : fix convert outfile name
    
    ggml-ci
    
    * llama : gptneox arch use F32 attn prec
    
    ggml-ci

commit dacfcebd6022175848e978f82811a244f1033038
Author: Victor Nogueira <felladrin@gmail.com>
Date:   Thu May 23 15:12:43 2024 +0300

    readme : add GPT-NeoX + Pythia to the list of supported models (#7491)

commit 9b82476ee9e73065a759f8bcc4cf27ec7ab2ed8c
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Thu May 23 11:49:53 2024 +0200

    Add missing inference support for GPTNeoXForCausalLM (Pythia and GPT-NeoX base models) (#7461)
    
    * convert-hf : add conversion of bloom-style qkv tensor to gpt-style qkv (code borrowed from BloomModel)
    
    * llama : add inference support for LLM_ARCH_GPTNEOX
    
    * llama : add model types for every Pythia variant and GPT-NeoX
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

commit a61a94e543e3c6877c087e80fca27a0313ce5fd5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu May 23 12:38:18 2024 +0300

    llama : rename n_ctx -> cache.size, less confusing (#0)

commit 152da28ae54139e3754189b9e6e1c28e11277502
Author: Brian <mofosyne@gmail.com>
Date:   Thu May 23 17:40:43 2024 +1000

    labeler.yml: add embedding label detector [no ci] (#7482)

commit d48c88cbd563b6cf0ce972e2f56796896e240736
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu May 23 10:00:44 2024 +0300

    ggml : remove ggml_flash_attn and ggml_flash_ff (#7463)
    
    ggml-ci

commit e84b71c2c6da6e69c8f815168ea836f9716a325e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu May 23 10:00:21 2024 +0300

    ggml : drop support for QK_K=64 (#7473)
    
    * ggml : drop support for QK_K=64
    
    ggml-ci
    
    * opencl : restore QK_K=256 define

commit 1b1e27cb49158123ef4902aa41eb368c9e76e6a1
Author: 0cc4m <picard12@live.de>
Date:   Thu May 23 08:59:59 2024 +0200

    Update vulkan rope implementation to support frequency factors (#7475)

commit fbf777d2b9c30e7569e3d1c149501c1e31d9b5b9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu May 23 09:43:24 2024 +0300

    main : minor (#7462)

commit cd93a28cb1446319af5e2f4b416174c3a8e43546
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu May 23 00:31:20 2024 +0200

    CUDA: fix FA out-of-bounds reads (#7479)

commit 1e374365d170b7f692fd7753c145e21bc14486c8
Author: HanishKVC <hanishkvc@gmail.com>
Date:   Wed May 22 23:23:21 2024 +0530

    SimpleChat: a simple and dumb web front end for testing /chat/completions and /completions end points and try chat (#7350)
    
    * SimpleChat: Add a skeletal html page
    
    Contains a div placeholder for showing chat messages till now
    
    a text-input for allowing user to enter next chat message/query
    to the model.
    
    a submit button to allow sending of the user entered message and
    chat till now to the model.
    
    * SimpleChat: A js skeleton with SimpleChat class
    
    Allows maintaining an array of chat message.
    
    Allows adding chat message (from any of the roles be it system,
    user, assistant, ...)
    
    Allows showing chat messages till now, in a given div element.
    
    * SimpleChat: request_json, globals, startme
    
    * SimpleChatJS: Roles Class, submitClick
    
    Define Role class with static members corresponding to the roles.
    
    Update startme to
    
    * Get hold of the ui elements.
    
    * Attach a click handler to submit button, which adds the user input
      to xchats array and shows the chat messages till now in chat div
      element.
    
    Trap DOMContentLoaded to trigger startme
    
    * SimpleChat:HTML: Bring in the js file
    
    * SimpleChat: Rather value wrt input text element
    
    * SimpleChat: Also add completions related prompt
    
    * SimpleChat: Use common helper logic wrt json data
    
    * SimpleChat: Move handling of submit request into its own func
    
    * SimpleChat: Try handshake with llm over its web service endpoint
    
    * SimpleChat:JS: Extract model response and show to user
    
    * SimpleChat:JS: Messages/Prompt, indicate working to end user
    
    * SimpleChat: Try keep input element in view
    
    * SimpleChat: Diff user/assistant msgs, Make input wider
    
    Also show a default message to user
    
    Also add some metas
    
    * SimpleChat: Move into its own sub directory to avoid confusion
    
    * SimpleChat:sh: Add simple shell script to run python3 http.server
    
    So one needs to run the llm server locally
    then run this script and access it using a local browser
    
    * SimpleChat:JS: Try trap enter key press wrt input text field
    
    So user can either press submit button or press enter key
    
    * SimpleChat: Allow user to select chat or completion mode
    
    * SimpleChat: Dont submit if already submitted and waiting
    
    Also make chat the default selection wrt mode
    
    * SimpleChat:JS: Handle difference in response
    
    Try read the assistance response from appropriate field in the
    response got.
    
    Also examples/server seems to return the response in a slightly
    different field, so try account for that also.
    
    * SimpleChat:JS: Force completion mode be single message by default
    
    * SimpleChat: Add a simple readme file
    
    * SimpleChat:HTML: Cleanup/structure UI a bit, Add input for system
    
    * SimpleChat:Allow system prompt to be set, if provided before user
    
    * SimpleChat: Ignore empty user input, without trimming
    
    * SimpleChat:Alert user if they provide sysprompt late or change it
    
    * SimpleChat: Move handling systemprompt into its own func
    
    * SimpleChat:HTML: Add a style for system role message
    
    * SimpleChat: Update the readme file
    
    * SimpleChat:CSS: Move style info into its own css file
    
    To keep it simple, clean and seperate so that things are not
    unnecessarily cluttered.
    
    * SimpleChat:CSS: Allow for chat div to be scrollable
    
    * SimpleChat:JS: Try ensure the last entry in chat is visible
    
    Needed because now only the chat div is scrollable and not the full
    page.
    
    In last commit the chat div size was fixed to 75% vertical height,
    so the full page no longer scrolls, so the old bring user-input
    element to view wont work, instead now the last element in the
    chat div should be brought into view.
    
    * SimpleChat:JS: bottom of element visible, Set focus to user input
    
    As the generated text could be multiple lines and occupy more space
    that the full scrollable div's vertical space, make the bottom of
    the last element (which can be such a generated text) in the div
    visible by scrolling.
    
    Ensure that the user input box has focus
    
    * SimpleChat: Update notes a bit. Try keep browser happy
    
    Avoid browser quirk mode with DOCTYPE.
    
    Help with accessibility a bit by specifying the language explicitly.
    
    Specify the char encoding explicitly, inturn utf-8 is a safe bet,
    even with intermixing of languages if reqd in future.
    
    Add a cache-control http-equiv meta tag, which in all probability
    will be ignored.
    
    Defer js loading and execution, just for fun and future, not that
    critical here as it stands now.
    
    * SimpleChat:HTML:Group user input+btn together; Note about multichat
    
    * SimpleChat:JS: Allow for changing system prompt anytime for future
    
    * SimpleChat:Readme: Note about handle_systemprompt begin/anytime
    
    * SimpleChat:HTML: Add viewport meta for better mobile friendliness
    
    Without this the page content may look too small.
    
    * SimpleChat:HtmlCss: Cleanup UI flow
    
    set margin wrt vmin rather than vw or vh so portrait/landscape ok.
    
    Use flex and flex-grow to put things on the same line as well as
    distribute available space as needed. Given two main elements/line
    so it remains simple.
    
    In each line have one element with grows and one sits with a basic
    comfortably fixed size.
    
    * SimpleChat: textarea for multiline user chat, inturn shift+enter 4 enter
    
    * SimpleChat: Make vertical layout better responsive (flex based)
    
    Also needed to make things cleaner and properly usable whether
    landscape or portrait, after changing to multiline textarea rather
    than single line user input.
    
    Avoid hardcoding the chat-till-now display area height, instead
    make it a flex-growable within a flex column of ui elements within
    a fixed vertical area.
    
    * SimpleChat: Rename simplechat.html to index.html, update readme
    
    Instead of providing a seperate shell script, update the readme wrt
    how to run/use this web front end.
    
    * SimpleChat: Screen fixed view and scrolling, Printing full
    
    * SimpleChat:JS:CI: Avoid space at end of jsdoc param line
    
    * SimpleChat:JS: MultiChat initial skeleton
    
    Will help maintain multiple independent chats in future
    
    * SimpleChat:JS: Move system prompt begin/anytime into SimpleChat
    
    * SimpleChat:JS:Keep MultiChatUI simple for now
    
    Worry about different chats with different servers for later.
    
    * SimpleChat:JS: Move handle submit into MultiChat, build on same
    
    Create an instance of MultiChatUI and inturn a instance of chat
    session, which is what the UI will inturn work on.
    
    * SimpleChat:JS: Move to dictionary of SimpleChat, instead of array
    
    * SimpleChat: Move ui elements into MultiChatUI, Update el IDs
    
    Move ui elements into MultiChatUI, so that current handleUserSubmit
    doesnt need to take the element arguments. Also in future, when
    user is allowed to switch between different chat sessions, the
    UI can be updated as needed by using the elements in UI already
    known to MultiChatUI instance.
    
    Rename the element ids' so that they follow a common convention,
    as well as one can identify what the element represents in a more
    consistant manner.
    
    * SimpleChat:MCUI:Show available chat sessions, try switch btw them
    
    Previous commits brought in / consolidated existing logic into
    MultiChatUI class.
    
    Now start adding logic towards multichat support
    
    * show buttons indicating available chat sessions
    
    * on sessin button click, try switch to that session
    
    * SimpleChat:MCUI: Store and use current chat session id
    
    Also
    
    allow to switch chat session optionally, wrt some of the related
    helpers.
    
    setup for two chat sessions by default.
    
    * SimpleChat:MCUI: Delay enabling user-input to avoid race
    
    Re-enable user-input, only after response to a user query has been
    updated to the chat-div. This ensures that if user tries to switch
    chat session, it wont be allowed till chat-request-response flow is
    done.
    
    * SimpleChat: Take care of system prompt
    
    Helper to get the latest system prompt and inturn use same to
    set the system prompt ui, when switching.
    
    Ensure that system prompt is set if and when enter key is pressed.
    
    * SimpleChat:GetSystemLatest, fix a oversight.
    
    * SimpleChat:MCUI: Allow selected chat-session btn to be highlighted
    
    Also have a general helper for setting class of children.
    
    * SimpleChat:Cleanup corners
    
    Show system prompt in chat space, when it is set by pressing enter,
    as a feedback to user.
    
    Alert user, if they try to switch chat session in the middle of
    waiting for a response from the ai model.
    
    * SimpleChat:MCUI: Ensure req-resp failure doesnt lock up things
    
    * SimpleChat:MCUI: Support for new chat sessions
    
    Also a general create button helper.
    
    * SimpleChat:MCUI: CreateSessionBtn helper, use wrt NewChat
    
    Also fix a oversight wrt using stale data wrt the list of chat
    sessions.
    
    * SimpleChat:MCUI: NewChat btn first before existing chat sessions
    
    * SimpleChat:MCUI:CornerCases:Skip new chat, show only if current
    
    Skip NewChat if user cancels or if one waiting for response from
    the ai model.
    
    Dont show a chat with newly got ai model response, if current chat
    session has changed, some how. Chat session shouldnt be allowed to
    change, if there is a pending response, but still as a additional
    sanity check.
    
    * SimpleChat: Update readme, title, show usage if no chat to show
    
    * SimpleChat: Cleanup the log/dialog messages a bit

commit 197ff91462dd05bb9a3be03578114abf0c355536
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 22 20:05:38 2024 +0300

    build : remove zig (#7471)

commit 6ff13987ad1a9519bee13dd98b6a21cd98979aab
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 22 20:04:20 2024 +0300

    common : normalize naming style (#7462)
    
    * common : normalize naming style
    
    ggml-ci
    
    * common : match declaration / definition order
    
    * zig : try to fix build

commit 38c03478a37e460ecd3a21155b338a83bfed7f90
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed May 22 17:58:25 2024 +0200

    CUDA: fix FA out-of-bounds writes (#7465)

commit b18532a4efeca8796fea8e36195c81cbfd596a4a
Author: slaren <slarengh@gmail.com>
Date:   Wed May 22 16:10:46 2024 +0200

    phi3 : duplicate rope factors in each layer (#7447)
    
    * phi3 : duplicate rope factors in each layer
    
    phi3 : set phi-3 model type as 14B
    
    model loader : simplify the process for duplicating model tensors
    
    llama-bench : remove default pg test
    
    * replace bool parameters in llama_model_loader with named flags

commit fcda1128bc5f8eb7e1811708fe9d9867b9aec815
Author: k.h.lai <adrian.k.h.lai@outlook.com>
Date:   Wed May 22 20:53:21 2024 +0800

    vulkan: add workaround for iterator boundary check to fix clang-cl debug build (#7426)

commit 03d8900ebe062355e26a562379daee5f17ea099f
Author: Justine Tunney <jtunney@mozilla.com>
Date:   Wed May 22 07:08:18 2024 -0400

    llama : add missing model type names (#7445)

commit 9b3d83318931aa98c487baaa977626931d059e6a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 22 12:36:37 2024 +0300

    cuda : fix compile warning (#7454)

commit 95fb0aefab568348da159efdd370e064d1b35f97
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed May 22 10:24:29 2024 +0200

    CUDA: remove incorrect precision check (#7454)

commit 3e5faa85032ec3106a2ad831bf412be9ff139f47
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 22 11:01:35 2024 +0300

    cuda : fix rope + add tests (#7452)
    
    * cuda : fix rope pos data
    
    ggml-ci
    
    * ggml : drop mode & 1 == 1 support for ggml_rope
    
    ggml-ci
    
    * ggml : support freq_factors for f16 rope (CPU)
    
    ggml-ci
    
    * tests : add rope tests using frequency factors
    
    ggml-ci

commit 201cc11afa0a1950e1f632390b2ac6c937a0d8f0
Author: liuwei-git <14815172+liuwei-git@users.noreply.github.com>
Date:   Wed May 22 04:28:32 2024 +0800

    llama : add phi3 128K model support (#7225)
    
    * add phi3 128k support in convert-hf-to-gguf
    
    * add phi3 128k support in cuda
    
    * address build warnings on llama.cpp
    
    * adjust index value in cuda long rope freq factors
    
    * add long rope support in ggml cpu backend
    
    * make freq factors only depend on ctx size
    
    * remove unused rope scaling type 'su' frin gguf converter
    
    * fix flint warnings on convert-hf-to-gguf.py
    
    * set to the short freq factor when context size is small than trained context size
    
    * add one line of comments
    
    * metal : support rope freq_factors
    
    * ggml : update ggml_rope_ext API to support freq. factors
    
    * backends : add dev messages to support rope freq. factors
    
    * minor : style
    
    * tests : update to use new rope API
    
    * backends : fix pragma semicolons
    
    * minor : cleanup
    
    * llama : move rope factors from KV header to tensors
    
    * llama : remove tmp assert
    
    * cuda : fix compile warning
    
    * convert : read/write n_head_kv
    
    * llama : fix uninitialized tensors
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 6369bf04336ab60e5c892dd77a3246df91015147
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 21 23:03:42 2024 +0300

    metal : handle F16 inf values, fix FA partial offload (#7434)
    
    ggml-ci

commit e402de364b643cb89ea9f43057733b5d36298670
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Tue May 21 20:40:00 2024 +0100

    `grammars`: fix resampling logic regression (#7424)

commit fcf6538ba6702c55eaec70da9a75c81d04900a72
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue May 21 19:27:12 2024 +0200

    CUDA: fix unused warning in mmq.cu (#7442)

commit c3f8d583560b4f261fa21c976793e538c60cd66c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 21 19:53:48 2024 +0300

    tests : test-tokenizer-0.sh print more info (#7402)

commit 11474e756de3f56b760986e73086d40e787e52f8
Author: Amir <amir_zia@outlook.com>
Date:   Tue May 21 17:13:12 2024 +0300

    examples: cache hf model when --model not provided (#7353)
    
    * examples: cache hf model when --model not provided
    
    * examples: cache hf model when --model not provided
    
    * examples: cache hf model when --model not provided
    
    * examples: cache hf model when --model not provided
    
    * examples: cache hf model when --model not provided

commit d8ee90222791afff2ab666ded4cb6195fd94cced
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue May 21 16:02:12 2024 +0200

    CUDA: deduplicate mmq code (#7397)

commit d7e852c1bc8e85bf62a6f1aede08cd2de723404a
Author: jaime-m-p <167997752+jaime-m-p@users.noreply.github.com>
Date:   Tue May 21 14:39:48 2024 +0200

    Tokenizer SPM fixes for phi-3 and llama-spm (bugfix) (#7425)
    
    * Update brute force test: add_special
    * Update brute force test: default values for add_bos_token and add_eos_token
    * Enable rtrim when pre-inserting BOS
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    * Revert "server : fix test regexes"

commit 917dc8cfa67a72fb7c8bf7392270da3bf4833af4
Author: jaime-m-p <167997752+jaime-m-p@users.noreply.github.com>
Date:   Mon May 20 20:15:57 2024 +0200

    Tokenizer SPM fixes for phi-3 and llama-spm (#7375)
    
    * Update brute force test: special tokens
    * Fix added tokens
      - Try to read 'added_tokens.json'.
      - Try to read 'tokenizer_config.json'.
      - Try to read 'tokenizer.json'.
    * Fix special tokens rtrim
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    * server : fix test regexes

commit fabf30b4c4fca32e116009527180c252919ca922
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 20 19:35:28 2024 +0300

    llama : remove Persimmon (#7408)
    
    * llama : remove Persimmon
    
    * requirements : remove

commit 20385cebcc4bb3f6dd10f989573c11864d70d53d
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon May 20 18:15:38 2024 +0200

    perplexity: update README FP16 results [no ci] (#7413)

commit db10f01310beea8a1ef7798651b9d692fd1149d0
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Mon May 20 16:36:55 2024 +0300

    rpc : track allocated buffers (#7411)
    
    * rpc : track allocated buffers
    
    ref: #7407
    
    * rpc : pack rpc_tensor tightly

commit 3bc10cb485dd7efa4da6c64e73ad0c9e2bfe0821
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 20 15:10:03 2024 +0300

    server : fix temperature + disable some tests (#7409)
    
    * server : fix temperature
    
    * server : disable tests relying on parallel determinism
    
    * ci : change server Debug -> RelWithDebInfo

commit 6bf9b66fa3f263ca2175dcb5f6d0a658581e1dfb
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Mon May 20 12:08:23 2024 +0100

    [SYCL] Update SYCL upscale operation (#7321)
    
    * Update SYCL upscale operation
    
    * Formatting
    
    * Remove messages

commit 26cd4237bc499f7144d76f440aa775d749b170bb
Author: Bingan <70050083+binganao@users.noreply.github.com>
Date:   Mon May 20 17:55:34 2024 +0800

    Update README.md (#7410)

commit 213e90ed73f8ac3cd3026dc3f086beae0d414f96
Author: Herman Semenov <GermanAizek@yandex.ru>
Date:   Mon May 20 07:33:21 2024 +0000

    ggml-opencl, llama: using reserve() if count already known (#7272)

commit 65c58207ece92ad213f4bfd0f91dcb2dfb664f5b
Author: junchao-loongson <68935141+junchao-loongson@users.noreply.github.com>
Date:   Mon May 20 15:19:21 2024 +0800

    ggml : add loongarch lsx and lasx support (#6454)
    
    * add loongarch lsx and lasx optimize code
    
    * Add loongarch compilation support to makefile
    
    * revert stb_image.h
    
    * opt bytes_from_nibbles_32 and sum_i16_pairs_float
    
    * fix undeclared
    
    * format code
    
    * update
    
    * update 2
    
    ---------
    
    Co-authored-by: Jinyang He <hejinyang@loongson.cn>

commit 1cc0155d04918cb3017afa472acea51b77483c4a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 20 10:16:41 2024 +0300

    server : tuning tests (#7388)
    
    * server : don't pass temperature as string
    
    * server : increase timeout
    
    * tests : fix the fix 0.8f -> 0.8
    
    ggml-ci
    
    * tests : set explicit temperature

commit e932094d58f513d5996c3efc9f6fed8238894c57
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 20 08:56:05 2024 +0300

    server : return error on too large embedding input (#7389)

commit 2789baf480ba7dc9281b65f601f0918e58920f54
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 20 08:55:09 2024 +0300

    tests : fix --keep_split -> --keep-split (#7374)

commit 33c8d50accd6dca73c9c4af00a05e24209c160fe
Author: Srihari-mcw <96763064+Srihari-mcw@users.noreply.github.com>
Date:   Sun May 19 19:18:39 2024 -0700

    Add provisions for windows support for BF16 code including CMake provision for enabling AVX512_BF16 (#7258)

commit d359f30921a9f62a0fd299c412ff3f270286fea6
Author: slaren <slarengh@gmail.com>
Date:   Mon May 20 01:17:03 2024 +0200

    llama : remove MPI backend (#7395)

commit 1ea2a0036e88172d6c8bf7e1a1989a03894dc955
Author: Fred Douglas <43351173+fredlas@users.noreply.github.com>
Date:   Sun May 19 11:37:04 2024 -0500

    quantize : fix --keep-split check (#7374)

commit f030ec1f7a72aa825b2104823946551b9ec5dfc1
Author: 0cc4m <picard12@live.de>
Date:   Sun May 19 17:19:53 2024 +0200

    Vulkan Embedding Fix (#7360)
    
    * Fix empty Vulkan host buffers
    
    Add fp32 fp16 matmul shader
    
    Fix matmul shader alignment
    
    * Remove deprecated tensor->backend uses
    
    * Fix Vulkan validation errors on embedding models with no offloaded layers
    
    * Fix Vulkan llava segfault when not offloading layers

commit e4e6f67be6a8a697f5f89a28c98934e53c99c359
Author: slaren <slarengh@gmail.com>
Date:   Sun May 19 17:08:46 2024 +0200

    ggml : fix another case of quants nans (#7387)

commit 5ca49cbecda27ce0a7266658fc3b640bff3ed386
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun May 19 16:46:13 2024 +0200

    ggml: implement quantized KV cache for FA (#7372)

commit 1b01f06db0cff5f5f600bb754fc39fde565ed56a
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun May 19 16:26:02 2024 +0200

    server: add test for token probs (#7347)

commit 41858392e17abead21735309bf17cb55183d8c31
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun May 19 16:06:33 2024 +0200

    server: fix seed being reported back (#7382)

commit 6aade19ee74b896c59929676629340b36be3e22c
Author: Anas Ahouzi <112881240+aahouzi@users.noreply.github.com>
Date:   Sun May 19 14:46:46 2024 +0200

    Add StableLM2 pre-tokenizer (#7349)
    
    * Add StableLM pre-tokenizer
    
    * Fix space
    
    * Fix trailing whitespace

commit ab33f7a338593f6cf1ae98b10b6f8684f63bd72c
Author: slaren <slarengh@gmail.com>
Date:   Sun May 19 14:19:37 2024 +0200

    cuda : clear error after buffer allocation failure (#7376)

commit e23b974f4cf9270d05062d446f406e3ff55d9451
Author: Brian <mofosyne@gmail.com>
Date:   Sun May 19 20:51:03 2024 +1000

    labeler.yml: Use settings from ggerganov/llama.cpp [no ci] (#7363)
    
    https://github.com/actions/labeler#using-configuration-path-input-together-with-the-actionscheckout-action
    Recommends the use of checkout action to use the correct repo context
    when applying settings for PR labels
    
    e.g.
    
        steps:
        - uses: actions/checkout@v4 # Uploads repository content to the runner
          with:
            repository: "owner/repositoryName" # The one of the available inputs, visit https://github.com/actions/checkout#readme to find more
        - uses: actions/labeler@v5
          with:
            configuration-path: 'path/to/the/uploaded/configuration/file'

commit 854d365abab7194b5013a523f72da19860c3c550
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun May 19 11:01:01 2024 +0300

    cmake : update android comments (#7341)

commit f5bf761747988ee1832766f7d1433739aff810da
Author: fraxy-v <65565042+fraxy-v@users.noreply.github.com>
Date:   Sun May 19 01:44:42 2024 +0300

    Capture CUDA logging output (#7298)
    
    * logging: output capture in cuda module
    
    * fix compile error
    
    * fix: vsnprintf terminates with 0, string use not correct
    
    * post review
    
    * Update llama.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Update llama.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 059031b8c40e1f4ba60586842c5b1ed3ddf61842
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 18 18:55:54 2024 +0300

    ci : re-enable sanitizer runs (#7358)
    
    * Revert "ci : temporary disable sanitizer builds (#6128)"
    
    This reverts commit 4f6d1337ca5a409dc74aca8c479b7c34408a69c0.
    
    * ci : trigger

commit 511182eabb36f6ec9776e2b3c4d7e16d93d0ac0d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 18 13:40:39 2024 +0300

    android : use "ci-android" branch for CI (#7341)
    
    * android : use "ci-android" branch for CI
    
    * ggml : disable SIMD exp and silu for 32-bit ARM
    
    ggml-ci
    
    * android : do not fetch, use add_subdirectory instead
    
    * cmake : provide binary dir

commit 133d99c59980139f5bb75922c8b5fca67d7ba9b8
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat May 18 12:36:25 2024 +0200

    CUDA: deduplicate FlashAttention code (#7352)

commit cb42c294279bc4a0a4e926a7b5a5568049f12fa7
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat May 18 11:10:47 2024 +0200

    server: correct --threads documentation [no ci] (#7362)

commit d233b507cd19fcc2d8d8963ecc6a3eb7a33f2ecc
Author: Engininja2 <139037756+Engininja2@users.noreply.github.com>
Date:   Sat May 18 02:05:17 2024 -0600

    cuda : add half2 __shfl_xor() for ROCm 5.5 (#7263)

commit 0f98acfac6cc561dc57586bfff778405e42b576b
Author: Steffen Röcker <sroecker@gmail.com>
Date:   Sat May 18 10:04:55 2024 +0200

    llama : add support for larger Granite Code Models (20B, 34B) (#7324)
    
    Tie the weights for ARCH_STARCODER to support the larger Granite code models.
    Partially addresses ggerganov/issues/7116
    
    There still remains to be a few things to fix.
    Currently requires `--override-kv tokenizer.ggml.add_bos_token=bool:false`

commit ca57e0f35e33f714b9a6c2c4482b87bfe059c819
Author: strawberrymelonpanda <152940198+strawberrymelonpanda@users.noreply.github.com>
Date:   Sat May 18 00:57:08 2024 -0700

    perplexity : ndot progress and show stats with < 100 tasks (#7348)
    
    Fix floating point error with ndot printing, allow end stats on lower task numbers if multiple-choice tasks.

commit c1b295eea5c49887a066559527a74e8b94fe9db0
Author: 0cc4m <picard12@live.de>
Date:   Sat May 18 08:10:58 2024 +0200

    Update and fix Vulkan soft_max and argsort implementations (#7237)
    
    * Update and fix Vulkan softmax implementation
    
    * Update and fix Vulkan argsort implementation

commit de731963441ff128248259e1b99573d75264d210
Author: Brian <mofosyne@gmail.com>
Date:   Sat May 18 16:04:23 2024 +1000

    github-actions-labeler: initial commit (#7330)
    
    * github-actions-labeler: initial commit [no ci]
    
    * github actions: remove priority auto labeling [no ci]

commit b49a13dd2fa9c94c2c19a8c248bb7fa45499f9a8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 18 08:46:20 2024 +0300

    convert : fix set_vocab_sentencepiece (#6866)
    
    * convert : fix set_vocab_sentencepiece
    
    * Update convert-hf-to-gguf.py

commit 05834841dcb4f922983ea976539c70472272df9a
Author: slaren <slarengh@gmail.com>
Date:   Sat May 18 02:39:54 2024 +0200

    ggml : fix quants nans when all the group weights are very close to zero (#7313)

commit ef277de2add255a08b2b909ebfbf70364d1f4dc4
Author: Engininja2 <139037756+Engininja2@users.noreply.github.com>
Date:   Fri May 17 18:39:25 2024 -0600

    cmake : fix typo in AMDGPU_TARGETS (#7356)

commit b43272afa29a64dcb8bcf26a96a05bac40792b92
Author: jaime-m-p <167997752+jaime-m-p@users.noreply.github.com>
Date:   Sat May 18 01:09:13 2024 +0200

    Unicode codepoint flags for custom regexs (#7245)
    
    * Replace CODEPOINT_TYPE_* with codepoint_flags
    * Update and bugfix brute force random test
    * Deterministic brute force random test
    * Unicode normalization NFD
    * Get rid of BOM

commit 0fc1e820a9900a3dd08ddd3c6abe6604c53b689b
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri May 17 18:54:52 2024 +0200

    CUDA: faster large batch FA without tensor cores (#7314)

commit 82ca83db3c8d45df559c03a4225b6eb34808a2db
Author: Gavin Zhao <gavinzhaojw@protonmail.com>
Date:   Fri May 17 11:03:03 2024 -0400

    ROCm: use native CMake HIP support (#5966)
    
    Supercedes #4024 and #4813.
    
    CMake's native HIP support has become the
    recommended way to add HIP code into a project (see
    [here](https://rocm.docs.amd.com/en/docs-6.0.0/conceptual/cmake-packages.html#using-hip-in-cmake)).
    This PR makes the following changes:
    
    1. The environment variable `HIPCXX` or CMake option
    `CMAKE_HIP_COMPILER` should be used to specify the HIP
    compiler. Notably this shouldn't be `hipcc`, but ROCm's clang,
    which usually resides in `$ROCM_PATH/llvm/bin/clang`. Previously
    this was control by `CMAKE_C_COMPILER` and `CMAKE_CXX_COMPILER`.
    Note that since native CMake HIP support is not yet available on
    Windows, on Windows we fall back to the old behavior.
    
    2. CMake option `CMAKE_HIP_ARCHITECTURES` is used to control the
    GPU architectures to build for. Previously this was controled by
    `GPU_TARGETS`.
    
    3. Updated the Nix recipe to account for these new changes.
    
    4. The GPU targets to build against in the Nix recipe is now
    consistent with the supported GPU targets in nixpkgs.
    
    5. Added CI checks for HIP on both Linux and Windows. On Linux, we test
    both the new and old behavior.
    
    The most important part about this PR is the separation of the
    HIP compiler and the C/C++ compiler. This allows users to choose
    a different C/C++ compiler if desired, compared to the current
    situation where when building for ROCm support, everything must be
    compiled with ROCm's clang.
    
    ~~Makefile is unchanged. Please let me know if we want to be
    consistent on variables' naming because Makefile still uses
    `GPU_TARGETS` to control architectures to build for, but I feel
    like setting `CMAKE_HIP_ARCHITECTURES` is a bit awkward when you're
    calling `make`.~~ Makefile used `GPU_TARGETS` but the README says
    to use `AMDGPU_TARGETS`. For consistency with CMake, all usage of
    `GPU_TARGETS` in Makefile has been updated to `AMDGPU_TARGETS`.
    
    Thanks to the suggestion of @jin-eld, to maintain backwards
    compatibility (and not break too many downstream users' builds), if
    `CMAKE_CXX_COMPILER` ends with `hipcc`, then we still compile using
    the original behavior and emit a warning that recommends switching
    to the new HIP support. Similarly, if `AMDGPU_TARGETS` is set but
    `CMAKE_HIP_ARCHITECTURES` is not, then we forward `AMDGPU_TARGETS`
    to `CMAKE_HIP_ARCHITECTURES` to ease the transition to the new
    HIP support.
    
    Signed-off-by: Gavin Zhao <git@gzgz.dev>

commit f4bd8b3d260bb09491ba63c77ab7012b744362ef
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Fri May 17 17:25:44 2024 +0300

    rpc : set SO_REUSEADDR for the server socket (#7320)
    
    ref: #7293

commit 51e9d02599336e62948d29f1d6c05addeb921ac2
Author: Brian <mofosyne@gmail.com>
Date:   Fri May 17 22:40:14 2024 +1000

    Added a single test function script and fix debug-test.sh to be more robust (#7279)
    
    * run-single-test.sh: added a single test function script and fix debug-test.sh to be more robust
    
    * debug-test.sh: combined execute and gdb test mode via -g flag
    
    * debug-test.sh: refactor
    
    * debug-test: refactor for clarity
    
    * debug-test.sh: comment style changes
    
    * debug-test.sh: fix gdb

commit d273c1402b25086fd91aef2467ac13f2e49fa0ea
Author: Aarni Koskela <akx@iki.fi>
Date:   Fri May 17 15:11:45 2024 +0300

    py : convert-hf-to-gguf-update improvements (#7340)
    
    * convert-hf-to-gguf-update: automate updating
    
    * convert-hf-to-gguf-update: improve download
    
    * share requests session for performance
    * create directories only when needed, don't skip downloads when empty directory encountered
    * be more graceful about errors

commit 27b040691cbe45314147c2745e891a38e9c048d4
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Fri May 17 13:24:38 2024 +0200

    llama : use n_embd_head_v when reshaping kqv (#7327)
    
    * llama : use n_embd_head_v instead of n_embd_head_k when reshaping kqv
    
    * llama : use n_embd_v_gqa and n_embd_head_v instead of n_embd_k_gqa and n_embd_head_k when making a view of cached value vectors.
    
    ---------
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

commit 29c60d8cddcfd14fa8a6bf023a6c4eb8692c76ba
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri May 17 09:59:57 2024 +0200

    tokenization: add warning for double BOS (#7332)

commit 359cbe3f46c90ce6f5151005e411b8fb74f8139e
Author: Herman Semenov <GermanAizek@yandex.ru>
Date:   Fri May 17 07:08:49 2024 +0000

    ggml-quants, llama : removed excess checks (#7274)

commit e18bc6aaf3b547890609ed254ee5248e720e5840
Author: amd-lalithnc <lalithnc@amd.com>
Date:   Fri May 17 12:31:58 2024 +0530

    convert : fix Qwen/Qwen-7b conversion (#7308)

commit ee94172d33399d2e814ca05c8a3ff8c523ebb093
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Fri May 17 10:00:17 2024 +0300

    server : add support for the RPC backend (#7305)
    
    ref: #7292

commit 934266c0e0b2aa9781fdba2deb112c161ff038a9
Author: Justine Tunney <jtunney@mozilla.com>
Date:   Fri May 17 02:58:52 2024 -0400

    ggml : rewrite silu and softmax for cpu (#7154)
    
    This change upstreams llamafile's vectorized expf() functions. This lets
    us compute softmax and silu more accurately than the short[65536] lookup
    table that GGML previously used to make this operation go faster. We can
    support aarch64 and sse2+ with the worst case rounding error of 2ulp. It
    makes make -j8 tests && ./tests/test-backend-ops -o SOFT_MAX -b CPU perf
    go 1.5x faster for SSE2+FMA, 1.9x faster for AVX2+FMA and 2.1x on AVX512

commit 9c4fdcbec8c7fcc428e723b0d8a1cf1f351ba642
Author: Leon Knauer <git@leonknauer.com>
Date:   Fri May 17 02:11:03 2024 +0200

    [Server] Added --verbose option to README [no ci] (#7335)

commit 24ecb58168dce81646c2ed425690a106591c8c6d
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Thu May 16 20:43:45 2024 +0200

    Revert "server bench: fix bench not waiting for model load (#7284)" (#7334)
    
    This reverts commit 583fd6b000ec9ad1b465b5c98524f4a0ae388077.

commit 9afdffe70ebf3166d429b4434783bb0b7f97bdeb
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Wed May 15 16:04:40 2024 +0300

    rpc : get available mem for the CPU backend
    
    This can be overridden with the -m command line option
    
    ref: #7293

commit 3b3963c55c8332e33533c44b2aa882b0e45f8292
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Wed May 15 15:29:07 2024 +0300

    rpc : add command line arg for specifying backend memory
    
    ref: #7293

commit dda64fc17c97820ea9489eb0cc9ae8b8fdce4926
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Thu May 16 02:15:23 2024 -0400

    convert : get general.name from model dir, not its parent (#5615)
    
    Co-authored-by: Brian <mofosyne@gmail.com>

commit 0350f5815218c483fb3026a86adc44a115481625
Author: Herman Semenov <GermanAizek@yandex.ru>
Date:   Thu May 16 06:14:24 2024 +0000

    grammar, json, llama: replace push on emplace if it possible (#7273)

commit ad52d5c259344888b06fd5acd3344c663dd0621d
Author: Vaibhav Srivastav <vaibhavs10@gmail.com>
Date:   Thu May 16 07:38:43 2024 +0200

    doc: add references to hugging face GGUF-my-repo quantisation web tool. (#7288)
    
    * chore: add references to the quantisation space.
    
    * fix grammer lol.
    
    * Update README.md
    
    Co-authored-by: Julien Chaumond <julien@huggingface.co>
    
    * Update README.md
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Julien Chaumond <julien@huggingface.co>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 172b78210aae0e54d3668c5de14200efab9fac23
Author: Max Krasnyansky <quic_maxk@quicinc.com>
Date:   Wed May 15 22:36:43 2024 -0700

    ci: fix bin/Release path for windows-arm64 builds (#7317)
    
    Switch to Ninja Multi-Config CMake generator to resurect bin/Release path
    that broke artifact packaging in CI.

commit 13ad16af1231ab2d245d35df3295bcfa23de1305
Author: Max Krasnyansky <max.krasnyansky@gmail.com>
Date:   Wed May 15 19:47:36 2024 -0700

    Add support for properly optimized Windows ARM64 builds with LLVM and MSVC (#7191)
    
    * logging: add proper checks for clang to avoid errors and warnings with VA_ARGS
    
    * build: add CMake Presets and toolchian files for Windows ARM64
    
    * matmul-int8: enable matmul-int8 with MSVC and fix Clang warnings
    
    * ci: add support for optimized Windows ARM64 builds with MSVC and LLVM
    
    * matmul-int8: fixed typos in q8_0_q8_0 matmuls
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * matmul-int8: remove unnecessary casts in q8_0_q8_0
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 8f7080bf48828b538bc9387c3d150bbd4fb4cf2d
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed May 15 23:41:03 2024 +0200

    readme : remove stray double quote (#7310)
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit e1b40ac3b94824d761b5e26ea1bc5692706029d9
Author: kunnis <kunnis@users.noreply.github.com>
Date:   Wed May 15 12:59:12 2024 -0500

    ggml : use dynamic thread scheduling for matrix multiplication (#6915)
    
    * Just reordering some structs.
    
    * Adding in the calls to mm_pause
    
    * Passing around the state
    
    * Renaming and moving a bunch of variables around.
    
    * Extracting the logic to it's own function.
    
    * Moving some variable definitions into the chunk function.
    
    * Moving some variables around
    
    * moving src1_cont inside
    
    * Moving row_size
    
    * adding the current_chunk
    
    * Reorg the code.
    
    * Formatting to match the orig patch
    
    * starting to setup the chunking variables
    
    * Starting the buildup of the loop
    
    * The yield shouldn't be necessary.
    
    * adding the looping structure based on the chunk configuration.
    
    * Add in the re-chunking code.
    
    * Making it much more likely to rechunk.
    
    * disable resizing if numa is enabled.
    
    * Updating comments with what we've learned.
    
    * Fix formatting
    
    * Couple more formatting fixes.
    
    * More style fixes.
    
    * Fix Warnings
    
    * Going with unused because there's conditional logic that needs it.
    
    * Update ggml.c
    
    * Update ggml.c
    
    ---------

commit dc020985b8755dd6aa93a2f002f43c3ede808cce
Author: agray3 <agray3@users.noreply.github.com>
Date:   Wed May 15 14:44:49 2024 +0100

    Avoid unnecessarily disabling CUDA graphs (#7302)
    
    As discussed in PR #6766, CUDA graphs were being disabled in the presence of long prompts.
    This fixes the issue by avoiding the consective update counter from incrementing unnecessarily
    for tokens in which cuda graphs are disabled due to batch size > 1.

commit 344f9126cc0d15891fde9472fe40b8572628ad7d
Author: slaren <slarengh@gmail.com>
Date:   Wed May 15 15:08:48 2024 +0200

    ggml : tag ggml_tensor::backend as deprecated (#7290)

commit 9a17ab914b0aa7353389c656a3f2a0f086726868
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Wed May 15 13:26:30 2024 +0100

    Add missing " (#7303)

commit ea3b0590ee33d3573eb8ef76f88cc60f36d2a38d
Author: dm4 <sunrisedm4@gmail.com>
Date:   Wed May 15 20:01:12 2024 +0800

    embedding : free the batch after execution (#7297)

commit 29499bb59383c2a8c5d557a90abb08b696cef7f6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 15 13:23:41 2024 +0300

    sync : ggml

commit 48aa8fd1f213a69b41569f809cc954f24dbc4366
Author: John Balis <phobossystems@gmail.com>
Date:   Wed May 15 03:52:33 2024 -0500

    ggml : add `ggml_upscale_ext` (ggml/814)
    
    * initial commit with CPU implementation of upscale to shape and test, cuda implementation next
    
    * experimental commit to see if dst shape is correct
    
    * test version
    
    * test
    
    * removed unnecessary params
    
    * refactor
    
    * fixed tests
    
    * ggml : metal impl + cleanup + sycl dev warnings
    
    * patched ggml_upscale cuda op to handle non-contiguous tensors, added test for non-contiguous behavior
    
    * metal : fix upsacle op to support nb00 + style
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 583fd6b000ec9ad1b465b5c98524f4a0ae388077
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed May 15 08:44:16 2024 +0200

    server bench: fix bench not waiting for model load (#7284)

commit 9f773486ab78d65f5cca3f7e31c862b7043bf721
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 14 19:14:38 2024 +0300

    script : sync ggml-rpc

commit e8a7fd4fb06d82f663850c21fcf86c0fb98ad9b4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 14 19:09:30 2024 +0300

    metal : support FA without mask + add asserts (#7278)
    
    * ggml : fa without mask + add asserts
    
    ggml-ci
    
    * metal : support non-contiguous KV
    
    ggml-ci

commit a5e3fde8578d54b98d941344a4da150669af200d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 14 15:33:16 2024 +0300

    sync : ggml
    
    ggml-ci

commit f308ea705974dff62a1fe5367d776ad9d5109239
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 13 11:01:07 2024 +0300

    metal : tune soft_max number of threads (whisper/0)

commit c3c88f296a72432edb697ac8026dbf2ec18f2b21
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun May 12 20:36:31 2024 +0300

    ggml : try fix ppc64 (whisper/0)

commit 182adefcf36fc5f4263082ff032c0796fda65578
Author: Przemysław Pawełczyk <przemoc@gmail.com>
Date:   Wed May 8 17:33:43 2024 +0200

    ggml : expose SSE3 and SSSE3 for MSVC when AVX is available (whisper/2128)

commit 0d26d8ccd8caebab75af697c0275f599075fdacf
Author: Hong Bo PENG <penghb@cn.ibm.com>
Date:   Sun May 12 17:17:18 2024 +0800

    ggml : optimize for ppc64le using VSX intrinsics (ggml/784)
    
    * optimize for ppc64le using VSX intrinsics
    
    * 1. code clean up by removing comments about overflow concern.
    
    2. fix typo in suffix of scaling.
    
    * Continue to fix typo in suffix of scaling for QK_K <> 256
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 4f0263633b40e94e8b69fd6e7e4395cfedfd5c12
Author: Steve Grubb <ausearch.1@gmail.com>
Date:   Tue May 14 10:11:24 2024 -0400

    server: free sampling contexts on exit (#7264)
    
    * server: free sampling contexts on exit
    
    This cleans up last leak found by the address sanitizer.
    
    * fix whitespace
    
    * fix whitespace

commit 1265c670fd8e41e1947352c96c5179adda97fb2c
Author: Brian <mofosyne@gmail.com>
Date:   Tue May 14 23:10:39 2024 +1000

    Revert "move ndk code to a new library (#6951)" (#7282)
    
    This reverts commit efc8f767c8c8c749a245dd96ad4e2f37c164b54c.

commit 5e31828d3e35c76ecfee665bc23771a4bec1d130
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Tue May 14 14:27:19 2024 +0300

    ggml : add RPC backend (#6829)
    
    * ggml : add RPC backend
    
    The RPC backend proxies all operations to a remote server which runs a
    regular backend (CPU, CUDA, Metal, etc).
    
    * set TCP_NODELAY
    
    * add CI workflows
    
    * Address review comments
    
    * fix warning
    
    * implement llama_max_devices() for RPC
    
    * Address review comments
    
    * Address review comments
    
    * wrap sockfd into a struct
    
    * implement get_alignment and get_max_size
    
    * add get_device_memory
    
    * fix warning
    
    * win32 support
    
    * add README
    
    * readme : trim trailing whitespace
    
    * Address review comments
    
    * win32 fix
    
    * Address review comments
    
    * fix compile warnings on macos

commit 541600201e6480f54ae09e58d16b154d4b4b331d
Author: slaren <slarengh@gmail.com>
Date:   Tue May 14 09:33:42 2024 +0200

    llama : disable pipeline parallelism with nkvo (#7265)

commit efc8f767c8c8c749a245dd96ad4e2f37c164b54c
Author: Elton Kola <eltonkola@gmail.com>
Date:   Tue May 14 03:30:30 2024 -0400

    move ndk code to a new library (#6951)

commit e0f556186b6e1f2b7032a1479edf5e89e2b1bd86
Author: Haggai Nuchi <h.nuchi@gmail.com>
Date:   Mon May 13 22:25:56 2024 -0700

    Add left recursion check: quit early instead of going into an infinite loop (#7083)
    
    * Add left recursion check: quit early instead of going into an infinite loop
    
    * Remove custom enum, rename left recursion check and move to "grammar internal" section, add handling for edge case where a leftmost nonterminal may be empty
    
    * Remove unnecessary declaration

commit 27f65d6267cf22a44c5ccefa7765d53a05bd1259
Author: Ryuei <louixs@users.noreply.github.com>
Date:   Tue May 14 14:20:47 2024 +0900

    docs: Fix typo and update description for --embeddings flag (#7026)
    
    - Change '--embedding' to '--embeddings' in the README
    - Update the description to match the latest --help output
    - Added a caution about defining physical batch size

commit ee52225067622babc277371511b8124884e1c797
Author: compilade <git@compilade.net>
Date:   Mon May 13 14:10:51 2024 -0400

    convert-hf : support direct Q8_0 conversion (#7234)
    
    * convert-hf : support q8_0 conversion
    
    * convert-hf : add missing ftype
    
    This was messing with the checksums otherwise.
    
    * convert-hf : add missing ftype to Baichuan and Xverse
    
    I didn't notice these on my first pass.

commit 614d3b914e1c3e02596f869649eb4f1d3b68614d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 13 17:15:15 2024 +0300

    llama : less KV padding when FA is off (#7257)
    
    ggml-ci

commit 30e70334f71b3bd115024affcf98cac3d79aaa95
Author: k.h.lai <adrian.k.h.lai@outlook.com>
Date:   Mon May 13 22:02:36 2024 +0800

    llava-cli: fix base64 prompt (#7248)

commit 1c570d8beeebad95872dc738ea542a4a0022f78a
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon May 13 13:03:27 2024 +0200

    perplexity: add BF16 vs. FP16 results (#7150)

commit 948f4ec7c5bff92b18e63303f2b2d1645bccd943
Author: Neo Zhang <14088817+arthw@users.noreply.github.com>
Date:   Mon May 13 18:11:26 2024 +0800

    [SYCL] rm wait() (#7233)

commit 9aa672490c848e45eaa704a554e0f1f6df995fc8
Author: Joan Fontanals <joan.fontanals.martinez@jina.ai>
Date:   Mon May 13 10:35:14 2024 +0200

    llama : rename jina tokenizers to v2 (#7249)
    
    * refactor: rename jina tokenizers to v2
    
    * refactor: keep refactoring non-breaking

commit b1f8af1886e8187db6bb2a9b87cfc1c0f175f629
Author: Brian <mofosyne@gmail.com>
Date:   Mon May 13 12:56:47 2024 +1000

    convert.py: Outfile default name change and additional metadata support (#4858)
    
    * convert.py: Outfile default name change and additional metadata support
    
    * convert.py: don't stringify Metadata load method output
    
    * convert.py: typo fix
    
    * convert.py: fix metadata format to sync with LLM_KV_NAMES in llama.cpp

commit e586ee42595500c53938e937b6b6ad5353ad76dc
Author: Benjamin Findley <39356821+Kartoffelsaft@users.noreply.github.com>
Date:   Sun May 12 19:40:08 2024 -0700

    change default temperature of OAI compat API from 0 to 1 (#7226)
    
    * change default temperature of OAI compat API from 0 to 1
    
    * make tests explicitly send temperature to OAI API

commit cbf75894d256f1861f6409565db599365de3d4b8
Author: Neo Zhang <14088817+arthw@users.noreply.github.com>
Date:   Mon May 13 08:04:29 2024 +0800

    [SYCL] Add oneapi runtime dll files to win release package (#7241)
    
    * add oneapi running time dlls to release package
    
    * fix path
    
    * fix path
    
    * fix path
    
    * fix path
    
    * fix path
    
    ---------
    
    Co-authored-by: Zhang <jianyu.zhang@intel.com>

commit 0d5cef78aeafae4d4e6d56e2d4bcda771af58cc9
Author: Neo Zhang <14088817+arthw@users.noreply.github.com>
Date:   Mon May 13 08:02:55 2024 +0800

    [SYCL] update CI with oneapi 2024.1 (#7235)
    
    Co-authored-by: Zhang <jianyu.zhang@intel.com>

commit dc685be46622a8fabfd57cfa804237c8f15679b8
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun May 12 19:40:45 2024 +0200

    CUDA: add FP32 FlashAttention vector kernel (#7188)
    
    * CUDA: add FP32 FlashAttention vector kernel
    
    * fixup! CUDA: add FP32 FlashAttention vector kernel
    
    * fixup! fixup! CUDA: add FP32 FlashAttention vector kernel
    
    * fixup! fixup! fixup! CUDA: add FP32 FlashAttention vector kernel

commit 6f1b63606fc68a09d62d1d74dbd156c35219026d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun May 12 18:30:23 2024 +0300

    cmake : fix version cmp (#7227)

commit b228aba91ac2cd9eb90e9d423ba1d0d20e0117e2
Author: slaren <slarengh@gmail.com>
Date:   Sun May 12 02:29:33 2024 +0200

    remove convert-lora-to-ggml.py (#7204)

commit 7bd4ffb78062587e4012a1c24186223f09b1bc70
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 11 21:36:20 2024 +0300

    metal : fix warnings (skipme) (#0)

commit 1622ac023f42e5e01c163321cd98c6596aa9402d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 11 21:35:05 2024 +0300

    sync : ggml

commit 6aeff24f8b91e145e92d17ec7ce3adc4ef60b8e9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 11 16:57:53 2024 +0300

    metal : fix indent (ggml/0)

commit 325756d28df7d018a7bac424e1b3bc8acb4ecf07
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 11 16:25:50 2024 +0300

    ggml : resolve merge (ggml/0)
    
    ggml-ci

commit fed0108491a3a3cbec6c6480dc8667ffff9d7659
Author: Josh Ramer <josh.ramer@icloud.com>
Date:   Sat May 11 12:26:35 2024 -0500

    Scripting & documenting debugging one test without anything else in the loop. (#7096)
    
    * A little documentation that shares my quick tips for working in the repository.
    
    * Update startup-testing-debugging.md
    
    * script that shows a menu of tests to pick from & run the debugger on
    
    * debug-test.sh: Refactor CLI help message
    
    * debug-test.sh: documentation update
    
    * debug-test.sh: CLI Help output corrections
    
    * debug-test.sh: minor doc fix
    
    ---------
    
    authored-by: Josh Ramer <ubuntu@ip-172-31-32-53.ec2.internal>
    Assisted-by: brian khuu <mofosyne@gmail.com>

commit 72c177c1f6c16693eee319d4ebd4eaab5e630dd2
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat May 11 17:28:10 2024 +0200

    fix system prompt handling (#7153)

commit 5a419926b0c4efab0531401aea91522aaea9fd07
Author: compilade <git@compilade.net>
Date:   Sat May 11 11:06:26 2024 -0400

    convert-hf : support bfloat16 conversion (#7158)
    
    * convert-hf : support bfloat16 conversion
    
    * gguf-py : flake8 fixes
    
    * convert-hf : add missing space after comma
    
    * convert-hf : get bit-exact same output as ./quantize
    
    The quantization version was missing.
    
    * convert-hf : don't round bf16 NANs
    
    * convert-hf : save some memory with np.int16 intermediate bf16 weights
    
    * convert-hf : more closely match llama.cpp with which weights to keep in f32
    
    * convert-hf : add --outtype auto-f16
    
    A reason for this to exist is for model quantizers who want an initial
    GGUF with the most fidelity to the original model while still using
    a 16-bit float type instead of 32-bit floats.
    
    * convert-hf : remove a semicolon because flake8 doesn't like it
    
    It's a reflex from when programming in C/C++, I guess.
    
    * convert-hf : support outtype templating in outfile name
    
    * convert-hf : rename --outtype auto-f16 to --outtype auto

commit fae9d234b6606693704eca62fe4aefbb6c6abb45
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 11 12:02:39 2024 +0300

    sync : ggml
    
    ggml-ci

commit f5ef34e428f3886544590ecb2d532e4d333c114c
Author: Justina Cho <justcho5@gmail.com>
Date:   Wed May 1 14:44:26 2024 -0700

    feat: implemented sigmoid function (ggml/806)
    
    * added sigmoid function
    
    * implemented metal kernel for sigmoid
    
    * implemented cuda kernel for sigmoid
    
    * added sigmoid unary op and incremented count

commit ef0d5e3ec9f99003af3ff326384816c02850ea3f
Author: Borislav Stanimirov <b.stanimirov@abv.bg>
Date:   Thu Apr 25 17:24:07 2024 +0300

    build: fix and ignore msvc warnings (ggml/805)

commit 3292733f95d4632a956890a438af5192e7031c12
Author: CrispStrobe <154636388+CrispStrobe@users.noreply.github.com>
Date:   Sat May 11 10:18:35 2024 +0200

    convert : skip unaccessible HF repos (#7210)

commit 988631335a20d06497f58be0b8ba13adb4323a22
Author: Steve Grubb <ausearch.1@gmail.com>
Date:   Sat May 11 04:13:02 2024 -0400

    server : free llama_batch on exit (#7212)
    
    * [server] Cleanup a memory leak on exit
    
    There are a couple memory leaks on exit of the server. This hides others.
    After cleaning this up, you can see leaks on slots. But that is another
    patch to be sent after this.
    
    * make tab into spaces

commit f99e1e456eaf69cc38c1982a2693ce41c0f897ef
Author: Haoxiang Fei <tonyfettes@tonyfettes.com>
Date:   Sat May 11 16:12:06 2024 +0800

    llama : lookup word in vocab before doing BPE merges (#7193)
    
    * fix: llama-3 ignore_merges
    
    * test: add test for llama-3 bpe ignore_merges
    
    * fix: set ignore_merges only for llama-3
    
    * fix: test-tokenizer-1-bpe --ingore-merges detection
    
    * fix: copy to fix fallthrough
    
    * fix: change ignore_merges to bool
    
    * fix: add ignore merges tests to cmake
    
    * llama : alternative merge ignore logic
    
    ---------
    
    Co-authored-by: Haoxiang Fei <feihaoxiang@idea.edu.cn>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 5ae3426b0b64672991563d4c28b2018b9f961467
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat May 11 10:11:28 2024 +0200

    server: fix reported top tokens for temperature 0 (#7203)

commit b83cc3f5b303ff30c52874b2d5864dc6385ebf9f
Author: Joan Fontanals <jfontanalsmartinez@gmail.com>
Date:   Sat May 11 09:46:09 2024 +0200

    llama : add Jina Embeddings architecture (#6826)
    
    * feat: first things to do
    
    * feat: create tensors for Jina architecture
    
    * fix: use other tensors
    
    * feat: embedding gets results
    
    * fix: fix usage of ALIBI
    
    * fix: clean prints
    
    * fix: do some cleanup unused vars
    
    * fix: revert changes to Makefile and CMakeLists
    
    * fix: revert some changes
    
    * fix: fix small detail
    
    * fix: fix convert formatting
    
    * fix: fix linting and editor
    
    * feat: set proper vocab settings
    
    * fix: JinaBertForMaskedLM registration
    
    * feat: support q_normalization and k_normalization in Jina arch
    
    * feat: handle gpt2 tokenizer with Jina architecture
    
    * feat: example comments in embedding
    
    * feat: rename Jina Bert to Jina Bert V2
    
    * fix: add some changes as per review
    
    * feat: proper KQ_pos for Jina embeddings
    
    * feat: add capacity to load models ES and DE for Spanish
    
    * llama : fix pre-tokenizers
    
    * ggml : full ALiBi support
    
    * ggml : update ggml_soft_max_ext() CUDA, SYCL
    
    * ggml : ggml_flash_attn_ext() support ALiBi (CPU)
    
    * ggml : ggml_flash_attn_ext() support ALiBi (Metal)
    
    * ggml : fix warning
    
    * ggml : ggml_flash_attn_ext() support ALiBi (CUDA)
    
    ggml-ci
    
    * minor : clean-up
    
    * embedding : add warning about missing SEP
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 9cb317f77e53067f7a138cc89ef7657148eae8e6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 11 10:32:41 2024 +0300

    ggml : full ALiBi support (#7192)
    
    * ggml : full ALiBi support
    
    * ggml : update ggml_soft_max_ext() CUDA, SYCL
    
    * ggml : ggml_flash_attn_ext() support ALiBi (CPU)
    
    * ggml : ggml_flash_attn_ext() support ALiBi (Metal)
    
    * ggml : fix warning
    
    * ggml : ggml_flash_attn_ext() support ALiBi (CUDA)
    
    ggml-ci
    
    * ggml : fix assert message
    
    * vulkan : add dev notes
    
    * ggml : require mask when using ALiBi
    
    ggml-ci
    
    * convert : fix convert for refact models

commit e849648888a11de13aaaa4cb2eda3f5a9c7b444d
Author: slaren <slarengh@gmail.com>
Date:   Fri May 10 18:03:54 2024 +0200

    llama-bench : add pp+tg test type (#7199)

commit 18e437665ce626dddbd79119aa7498493e7cb13b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 10 18:20:10 2024 +0300

    metal : fix flash attention kernel requirements (#7169)
    
    * metal : fix flash attention kernel requirements
    
    ggml-ci
    
    * metal : fix ggml_metal_supports_op
    
    ggml-ci

commit 8c660242d708d3913a2adc2b6e4a9ee9cf5e4ce7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 10 17:53:04 2024 +0300

    convert : print "ignore_merges" field

commit 25c6e82e7a1ad25a42b0894e87d9b5c557409516
Author: slaren <slarengh@gmail.com>
Date:   Fri May 10 14:28:01 2024 +0200

    llama : use n_vocab to differentiate between mistral 7B and llama3 8B (#7200)

commit 4e3880978f8b1bf546dd4e6f3b524d6b8739c49c
Author: Justine Tunney <jtunney@mozilla.com>
Date:   Fri May 10 07:01:08 2024 -0400

    Fix memory bug in grammar parser (#7194)
    
    The llama.cpp grammar parser had a bug where forgetting to add a closing
    quotation mark to strings would cause parsing to crash. Anyone running a
    server on a public endpoint is advised to upgrade. To reproduce this bug
    
        ./llamafile -m foo.gguf -p bar --grammar 'root::="'
    
    Credit for discovering and reporting this issue goes to Eclypsium
    Security Researcher Richard Johnson <Richard.johnson@eclypsium.com>.

commit f89fe2732c5709f6e86d5f4aee2e6d2a561f2eb2
Author: HanishKVC <hanishkvc@gmail.com>
Date:   Fri May 10 15:51:58 2024 +0530

    Main+: optionally allow special tokens from user in interactive mode (#7097)
    
    @hanishkvc added a new `--interactive-specials` flag which would allow for inserting special tokens from user side into the embedding stream.

commit d11afd665241c1b3910ab5f040d0216403019d87
Author: Andrei <abetlen@gmail.com>
Date:   Fri May 10 02:41:10 2024 -0400

    llava : fix moondream support (#7163)
    
    * Revert "Revert "llava : add support for moondream vision language model (#6899)""
    
    This reverts commit 9da243b36ac0b9d609adfaaa4c8f1cc8c592f737.
    
    * Fix num_positions and embeddings initialization

commit 8c570c9496212073079476651c7517c02581101f
Author: Ouadie EL FAROUKI <ouadie.elfarouki@codeplay.com>
Date:   Fri May 10 01:32:15 2024 +0100

    Minor arithmetic improvement to mmvq wrapper kernel (#7172)

commit eaf4bd8b399a6ed4b834f91d22409d2a05c4d266
Author: slaren <slarengh@gmail.com>
Date:   Fri May 10 01:04:12 2024 +0200

    eval-callback : fix conversion to float (#7184)

commit befddd0f15de6efb15d7e7f5b527dfb671f4196f
Author: 0cc4m <picard12@live.de>
Date:   Thu May 9 20:39:54 2024 +0200

    Vulkan Bugfixes and Improvements (#7084)
    
    * Modify mat mat mul shader for mul_mat_id, modify mat vec mul shaders for single call batch operation
    
    * Further work towards MoE, disabled for now
    
    * Disable MoE code (not ready yet), fix a number of bugs in shaders and Vulkan code
    
    * Add softmax with f16 mask and pos buffer support
    
    * Disable mul_mat_id shaders for now
    
    * Fix flake8
    
    * Fix validation errors caused by empty buffers on larger batch sizes

commit d46dbc76f8770caec0175f1e57777173c70556a0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu May 9 16:40:42 2024 +0300

    readme : add scheduled server workflow status badge

commit 0961d866044143eefec5b525e991611f73fd4f6e
Author: l3utterfly <gc.pthzfoldr@gmail.com>
Date:   Thu May 9 22:32:40 2024 +0900

    readme : add app (#6371)
    
    * added Layla to supported UIs
    
    * Update README.md

commit 43248e559472556f368988575d9fba906b3eb139
Author: jaime-m-p <167997752+jaime-m-p@users.noreply.github.com>
Date:   Thu May 9 15:30:44 2024 +0200

    llama3 custom regex split (#6965)
    
    * merged the changes from deepseeker models to main branch
    
    * Moved regex patterns to unicode.cpp and updated unicode.h
    
    * Moved header files
    
    * Resolved issues
    
    * added and refactored unicode_regex_split and related functions
    
    * Updated/merged the deepseek coder pr
    
    * Refactored code
    
    * Adding unicode regex mappings
    
    * Adding unicode regex function
    
    * Added needed functionality, testing remains
    
    * Fixed issues
    
    * Fixed issue with gpt2 regex custom preprocessor
    
    * unicode : fix? unicode_wstring_to_utf8
    
    * lint : fix whitespaces
    
    * tests : add tokenizer tests for numbers
    
    * unicode : remove redundant headers
    
    * tests : remove and rename tokenizer test scripts
    
    * tests : add sample usage
    
    * gguf-py : reader prints warnings on duplicate keys
    
    * llama : towards llama3 tokenization support (wip)
    
    * unicode : shot in the dark to fix tests on Windows
    
    * unicode : first try custom implementations
    
    * convert : add "tokenizer.ggml.pre" GGUF KV (wip)
    
    * llama : use new pre-tokenizer type
    
    * convert : fix pre-tokenizer type writing
    
    * lint : fix
    
    * make : add test-tokenizer-0-llama-v3
    
    * wip
    
    * models : add llama v3 vocab file
    
    * llama : adapt punctuation regex + add llama 3 regex
    
    * minor
    
    * unicode : set bomb
    
    * unicode : set bomb
    
    * unicode : always use std::wregex
    
    * unicode : support \p{N}, \p{L} and \p{P} natively
    
    * unicode : try fix windows
    
    * unicode : category support via std::regex
    
    * unicode : clean-up
    
    * unicode : simplify
    
    * llama3 custom regex split
    
    * convert : add convert-hf-to-gguf-update.py
    
    ggml-ci
    
    * lint : update
    
    * convert : add falcon
    
    ggml-ci
    
    * unicode : normalize signatures
    
    * lint : fix
    
    * lint : fix
    
    * convert : remove unused functions
    
    * convert : add comments
    
    * convert : exercise contractions
    
    ggml-ci
    
    * Using char32_t for codepoints
    
    * lint : fix
    
    * already exists unicode_tolower()
    
    * Typing
    
    * Restore BOM
    
    * cmake : refactor test targets
    
    * tests : refactor vocab tests
    
    ggml-ci
    
    * tests : add more vocabs and tests
    
    ggml-ci
    
    * unicode : cleanup
    
    * scripts : ignore new update script in check-requirements.sh
    
    * Fix merge
    
    * models : add phi-3, mpt, gpt-2, starcoder
    
    * tests : disable obsolete
    
    ggml-ci
    
    * tests : use faster bpe test
    
    ggml-ci
    
    * llama : more prominent warning for old BPE models
    
    * tests : disable test-tokenizer-1-bpe due to slowness
    
    ggml-ci
    
    * Move unused variable value
    
    * GPT2 custom regex split
    
    * Add alternative regex for custom aplit llama3
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Style
    
    * Add bruteforce random tests for token encoding
    
    * wip: fixing unicode codepoint ranges
    
    * Fix merge
    
    * Unicode tables: separator, lowercase, uppercase and whitespace
    
    * llama3 custom regex split: fix \s
    
    * Restore BOM
    
    * Style
    
    * wip: generate NDF table
    
    * Ignore special tokens for testing
    
    * Clean gen-unicode-data.py
    
    * Refactor random tokenizer test
    
    * lint : fix
    
    * tests : add fail test for llama-bpe
    
    ---------
    
    Co-authored-by: Jaggzh <jaggz.h@gmail.com>
    Co-authored-by: Kazim Abrar Mahi <kazimabrarmahi135@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: jaime-m-p <>

commit a743d76a01f23038b2c85af1e9048ee836767b44
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu May 9 14:32:02 2024 +0200

    CUDA: generalize FP16 fattn vec kernel (#7061)
    
    * CUDA: generalize FP16 fattn vec kernel
    
    * disable unsupported head sizes for AMD in test
    
    * try AMD fix
    
    * fix batch size 2-8
    
    * partially revert changes

commit f31ec120bc36c6270e4948e6a065a7c4cfa0c404
Author: Galunid <karolek1231456@gmail.com>
Date:   Thu May 9 14:13:05 2024 +0200

    Add warning if token is invalid (#7173)

commit fd9f92b154850014146f61717cd292a59a5cee5a
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu May 9 13:03:29 2024 +0200

    llama : update llama_timings.n_p_eval setting (#7160)
    
    This commit changes the value assigned to llama_timings.n_p_eval when
    ctx->n_p_eval is 0 to be 1 instead of 1 which is the current value.
    
    The motivation for this change is that if session caching is enabled,
    for example using the `--prompt-cache main-session.txt` command line
    argument for the main example, and if the same prompt is used then on
    subsequent runs, the prompt tokens will not actually be passed to
    llama_decode, and n_p_eval will not be updated by llama_synchoronize.
    
    But the value of n_p_eval will be set 1 by llama_get_timings because
    ctx->n_p_eval will be 0. This could be interpreted as 1 token was
    evaluated for the prompt which could be misleading for applications
    using this value.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 22842164bcae3251b81ad9e497a16ef66833cb9e
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Thu May 9 12:56:00 2024 +0200

    gguf-py : add special token modification capability (#7166)
    
    * Add special token modification capability
    
    To be able to fix/amend special tokens in a GGUF let's add two new arguments:
    * `--special-token <name> <value>` where `<name>` can be bos, eos, prefix, middle, etc. while `<value>` is the token value, f.ex. `"<｜fim▁begin｜>"`
    * `--special-token-by-id <name> <id>` where `<id>` is the ID of the token, f.ex. 32006
    
    So, in order to f.ex. add fill-in-middle tokens to a GGUF you would do the following:
    ```bash
    python3 gguf-new-metadata.py input.gguf output.gguf --special-token prefix "<｜fim▁begin｜>" --special-token middle "<｜fim▁hole｜>" --special-token suffix "<｜fim▁end｜>"
    ```
    
    * improve help text
    
    * flake--
    
    * fix multiple tokens warning
    
    * make script executable
    
    * switch to namedtuple, no need to dataclass
    
    * typing++
    
    * add progress bar
    
    * Add special token modification capability
    
    To be able to fix/amend special tokens in a GGUF let's add two new arguments:
    * `--special-token <name> <value>` where `<name>` can be bos, eos, prefix, middle, etc. while `<value>` is the token value, f.ex. `"<｜fim▁begin｜>"`
    * `--special-token-by-id <name> <id>` where `<id>` is the ID of the token, f.ex. 32006
    
    So, in order to f.ex. add fill-in-middle tokens to a GGUF you would do the following:
    ```bash
    gguf-new-metadata.py input.gguf output.gguf --special-token prefix "<｜fim▁begin｜>" --special-token middle "<｜fim▁end｜>" --special-token suffix "<｜fim▁hole｜>"
    ```
    (yes, fim_end is the `middle` token, because completion is a `prefix`/`suffix`/`middle` sequence (where `middle` is unfilled))
    or
    ```bash
    gguf-new-metadata.py input.gguf output.gguf --special-token prefix "<fim_prefix>" --special-token middle "<fim_middle>" --special-token suffix "<fim_suffix>"
    ```
    etc...
    
    NB: The tokens have to exist already, trying to add non-existent token name/IDs will be ignored (with a warning), while non-existent values will fail (with an error).
    
    * improve help text
    
    * flake--
    
    * fix multiple tokens warning
    
    * make script executable
    
    * switch to namedtuple, no need to dataclass
    
    * typing++
    
    * add progress bar
    
    * fail on invalid token id

commit 47345248827f426038098d016ed11975021b4919
Author: Albert Jin <albert.jin@gmail.com>
Date:   Thu May 9 17:34:37 2024 +0800

    opencl : alignment size converted from bits to bytes (#7090)
    
    * opencl alignment size should be converted from bits to bytes
    
    Reference: https://registry.khronos.org/OpenCL/specs/3.0-unified/html/OpenCL_API.html#CL_DEVICE_MEM_BASE_ADDR_ALIGN
    
    > Alignment requirement (in bits) for sub-buffer offsets.
    
    * Update ggml-opencl.cpp for readability using division instead of shift
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

commit 07cd41d0965829463eff73eda3348aedbfd3a444
Author: Ahmet Zeer <ahmed.zeer@std.yildiz.edu.tr>
Date:   Thu May 9 11:16:45 2024 +0300

    TypoFix (#7162)

commit 4426e2987b566f09c7aa96ada9706cc778637620
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Wed May 8 19:55:32 2024 -0400

    cmake : fix typo (#7151)

commit f98eb31c517c95960df1d0abc48002787f145f3b
Author: compilade <git@compilade.net>
Date:   Wed May 8 18:16:38 2024 -0400

    convert-hf : save memory with lazy evaluation (#7075)
    
    * convert-hf : begin refactoring write_tensor
    
    * convert : upgrade to sentencepiece v0.2.0
    
    * convert-hf : remove unused n_dims in extra_*_tensors
    
    * convert-hf : simplify MoE weights stacking
    
    * convert-hf : flake8 linter doesn't like semicolons
    
    * convert-hf : allow unusual model part names
    
    For example, loading `model-00001-of-00001.safetensors` now works.
    
    * convert-hf : fix stacking MoE expert tensors
    
    `torch.stack` and `torch.cat` don't do the same thing.
    
    * convert-hf : fix Mamba conversion
    
    Tested to work even with a SentencePiece-based tokenizer.
    
    * convert : use a string for the SentencePiece tokenizer path
    
    * convert-hf : display tensor shape
    
    * convert-hf : convert norms to f32 by default
    
    * convert-hf : sort model part names
    
    `os.listdir` is said to list files in arbitrary order.
    Sorting the file names should let "model-00009-of-00042.safetensors"
    be loaded before "model-00010-of-00042.safetensors".
    
    * convert-hf : use an ABC for Model again
    
    It seems Protocol can't be used as a statically type-checked ABC,
    because its subclasses also can't be instantiated. (why did it seem to work?)
    
    At least there's still a way to throw an error when forgetting to define
    the `model_arch` property of any registered Model subclasses.
    
    * convert-hf : use a plain class for Model, and forbid direct instantiation
    
    There are no abstract methods used anyway,
    so using ABC isn't really necessary.
    
    * convert-hf : more consistent formatting of cmdline args
    
    * convert-hf : align the message logged for converted tensors
    
    * convert-hf : fix Refact conversion
    
    * convert-hf : save memory with lazy evaluation
    
    * convert-hf : flake8 doesn't like lowercase L as a variable name
    
    * convert-hf : remove einops requirement for InternLM2
    
    * convert-hf : faster model parts loading
    
    Instead of pre-loading them all into a dict, iterate on the tensors
    in the model parts progressively as needed in Model.write_tensors
    
    Conversion for some architectures relies on checking for the presence
    of specific tensor names, so for multi-part models, the weight map is read
    from the relevant json file to quickly get these names up-front.
    
    * convert-hf : minor changes for consistency
    
    * gguf-py : add tqdm as a dependency
    
    It's small, and used for a progress bar
    in GGUFWriter.write_tensors_to_file

commit bc4bba364fb96d908f2698e908648df5e6f55e02
Author: agray3 <agray3@users.noreply.github.com>
Date:   Wed May 8 21:55:49 2024 +0100

    Introduction of CUDA Graphs to LLama.cpp (#6766)
    
    * DRAFT: Introduction of CUDA Graphs to LLama.cpp
    
    * FIx issues raised in comments
    
    * Tidied to now only use CUDA runtime (not mixed with driver calls)
    
    * disable for multi-gpu and batch size > 1
    
    * Disable CUDA graphs for old GPU arch and with env var
    
    * added missing CUDA_CHECKs
    
    * Addressed comments
    
    * further addressed comments
    
    * limit to GGML_ALLOW_CUDA_GRAPHS defined in llama.cpp cmake
    
    * Added more comprehensive graph node checking
    
    * With mechanism to fall back if graph capture fails
    
    * Revert "With mechanism to fall back if graph capture fails"
    
    This reverts commit eb9f15fb6fcb81384f732c4601a5b25c016a5143.
    
    * Fall back if graph capture fails and address other comments
    
    * - renamed GGML_ALLOW_CUDA_GRAPHS to GGML_CUDA_USE_GRAPHS
    
    - rename env variable to disable CUDA graphs to GGML_CUDA_DISABLE_GRAPHS
    
    - updated Makefile build to enable CUDA graphs
    
    - removed graph capture failure checking in ggml_cuda_error
      using a global variable to track this is not thread safe, but I am also not safistied with checking an error by string
      if this is necessary to workaround some issues with graph capture with eg. cuBLAS, we can pass the ggml_backend_cuda_context to the error checking macro and store the result in the context
    
    - fixed several resource leaks
    
    - fixed issue with zero node graphs
    
    - changed fixed size arrays to vectors
    
    - removed the count of number of evaluations before start capturing, and instead changed the capture mode to relaxed
    
    - removed the check for multiple devices so that it is still possible to use a single device, instead checks for split buffers to disable cuda graphs with -sm row
    
    - changed the op for checking batch size to GGML_OP_ADD, should be more reliable than GGML_OP_SOFT_MAX
    
    - code style fixes
    
    - things to look into
      - VRAM usage of the cudaGraphExec_t, if it is significant we may need to make it optional
      - possibility of using cudaStreamBeginCaptureToGraph to keep track of which ggml graph nodes correspond to which cuda graph nodes
    
    * fix build without cuda graphs
    
    * remove outdated comment
    
    * replace minimum cc value with a constant
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit c12452c7aec8a02264afc00196a13caa591a13ac
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed May 8 21:53:08 2024 +0200

    JSON: [key] -> .at(key), assert() -> GGML_ASSERT (#7143)

commit 9da243b36ac0b9d609adfaaa4c8f1cc8c592f737
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 8 22:14:39 2024 +0300

    Revert "llava : add support for moondream vision language model (#6899)"
    
    This reverts commit 46e12c4692a37bdd31a0432fc5153d7d22bc7f72.

commit bd1871fa2b1bf8a081b43ba9bc85f8ffd46fac46
Author: JohnnyB <jboero@users.noreply.github.com>
Date:   Wed May 8 20:12:06 2024 +0100

    server : add themes + favicon (#6848)
    
    * Added themes support with two sample themes and a favicon.
    
    * Newline
    
    * Newline
    
    * Newline
    
    * Trailing whitespace
    
    * Increased opacity for contrast
    
    * Increase opacity.
    
    Check actions cancelled for some other priority job and I can't seem to manually re-run them, so MOAR OPACITY
    
    * Opacity action trigger.
    
    Trying to re-trigger the cancelled action.
    
    * One more opacity adjustment
    
    This Actions pipeline is failing for random issues.
    
    * Delete examples/server/themes/buttons_top/completion.js
    
    This will be served from the static string built-in to server.
    
    * Delete examples/server/themes/buttons_top/index.js
    
    This will be served from the static string built-in to server.
    
    * Delete examples/server/themes/wild/completion.js
    
    This will be served from the static string built-in to server.
    
    * Delete examples/server/themes/buttons_top/json-schema-to-grammar.mjs
    
    This will be served from the static string built-in to server.
    
    * Delete examples/server/themes/wild/index.js
    
    This will be served from the static string built-in to server.
    
    * Delete examples/server/themes/wild/json-schema-to-grammar.mjs
    
    This will be served from the static string built-in to server.
    
    * Replaced underscore.

commit 26458af1d63c85195cd96cd1673051e332d06d30
Author: Gilad S <giladgd@users.noreply.github.com>
Date:   Wed May 8 22:08:10 2024 +0300

    metal : use `vm_allocate` instead of `posix_memalign` on macOS (#7078)
    
    * fix: use `malloc` instead of `posix_memalign` in `ggml-metal.m` to make it not crash Electron proccesses
    
    * fix: typo
    
    * fix: use `vm_allocate` instead of `posix_memalign`
    
    * fix: don't call `newBufferWithBytesNoCopy` with `NULL` when `ggml_metal_host_malloc` returns `NULL`
    
    * fix: use `vm_allocate` only on macOS

commit 83330d8cd6491e53e1aca4c5dfc47e039b3c04ff
Author: Dawid Potocki <github@dawidpotocki.com>
Date:   Thu May 9 02:32:32 2024 +1200

    main : add --conversation / -cnv flag (#7108)

commit 465263d0cf1e8f8bc41948332dbd009d27a68590
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Wed May 8 14:29:23 2024 +0000

    sgemm : AVX Q4_0 and Q8_0 (#6891)
    
    * basic avx implementation
    
    * style
    
    * combine denibble with load
    
    * reduce 256 to 128 (and back!) conversions
    
    * sse load
    
    * Update sgemm.cpp
    
    * oops
    
    oops

commit 911b3900dded9a1cfe0f0e41b82c7a29baf3a217
Author: Johan <JohanAR@users.noreply.github.com>
Date:   Wed May 8 14:27:58 2024 +0200

    server : add_special option for tokenize endpoint (#7059)

commit ad211edef5db1f1fb955874b7ca6a67bd0c88708
Author: 20kdc <asdd2808@gmail.com>
Date:   Wed May 8 13:22:32 2024 +0100

    convert.py : --vocab-only generates false but valid params (#7027)
    
    An example of how this might be used in the style of baby-llama will be attached with this PR.

commit 229ffff872f8ad0d21c997d18ee7a23692ae60a0
Author: Ren Xuancheng <jklj077@users.noreply.github.com>
Date:   Wed May 8 20:06:43 2024 +0800

    llama : add BPE pre-tokenization for Qwen2 (#7114)
    
    * Add BPE pre-tokenization for Qwen2.
    
    * minor : fixes
    
    ---------
    
    Co-authored-by: Ren Xuancheng <17811943+jklj077@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 1fd9c1741d864d01cd7ec6d67227b92d7bfabf22
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed May 8 13:24:14 2024 +0200

    clean up json_value & server_log (#7142)

commit 4cd621c26de2095cd7c4464bdec5fe2e696ef3f3
Author: DAN™ <dranger003@gmail.com>
Date:   Wed May 8 06:43:23 2024 -0400

    convert : add BPE pre-tokenization for DBRX (#7132)
    
    * Add BPE pre-tokenization for DBRX.
    
    * Add vocab GGUFs.
    
    * Remove test.
    
    * Remove GGUFs.

commit 7e0b6a7b3ba94ff624dc27c1e0e735fded8819b8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 8 12:47:07 2024 +0300

    py : also print the normalizers

commit acdce3cdef6fc2f0b7b5623231fd7762c0884d1c
Author: Brian <mofosyne@gmail.com>
Date:   Wed May 8 18:54:39 2024 +1000

    compare-llama-bench.py: add missing basicConfig (#7138)
    
    * compare-llama-bench.py: add missing basicConfig
    
    * compare-llama-bench.py: Add line break between error message and print_help()
    
    * Add regular print() markdown table

commit 3855416027cb25d9a708ffa5581cf503a87856a6
Author: Justine Tunney <jtunney@mozilla.com>
Date:   Wed May 8 02:30:09 2024 -0400

    ggml : introduce bfloat16 support (#6412)
    
    * Introduce bfloat16 support
    
    Many models on Hugging Face (e.g. Mistral, TinyLLaMA) use bfloat16 as
    their canonical floating point format.
    
          ┌sign
          │
          │   ┌exponent
          │   │
          │   │      ┌mantissa
          │   │      │
          │┌──┴───┐┌─┴───┐
        0b0000000000000000 brain16
    
    This encoding has the same number of exponent bits as float32. That
    makes conversion relatively straightforward, even in the absence of
    hardware support. For example, converting brain16 to binary32 means
    simply shifting 16 bits to the left.
    
          ┌sign
          │
          │   ┌exponent
          │   │
          │   │      ┌mantissa
          │   │      │
          │┌──┴───┐┌─┴───────────────────┐
        0b00000000000000000000000000000000 IEEE binary32
    
    The issue is that converting bf16 to fp16 can result in information
    loss. Only 13% of bf16 numbers can be precisely represented in fp16
    which in practice ends up being 99.71% of Mistral 7b v0.2's weights
    however there is currently no way other than fp32 to get the others
    
          ┌sign
          │
          │  ┌exponent
          │  │
          │  │    ┌mantissa
          │  │    │
          │┌─┴─┐┌─┴──────┐
        0b0000000000000000 IEEE binary16
    
    This change fixes that, by adding a bf16 data type to GGML. Support
    for CPU inference has been implemented along with optimizations for
    the AVX2, AVX512, and AVX512BF16 ISAs. Perplexity on Mistral 7b 0.2
    improves somewhere around -0.0024 to -0.0046 compared to using fp16
    
    * Remove GGML code that's not needed
    
    * Minimize the GGML API surface area for BF16
    
    * Remove bf16 luts
    
    * Make the GGML header look nicer
    
    * Fix documentation
    
    * Apply ggerganov's fixes for test-backend-ops
    
    * Add BF16 code for new ggml_validate_row_data() function

commit c0e6fbf8c380718102bd25fcb8d2e55f8f9480d1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 8 09:14:50 2024 +0300

    metal : fix unused warning

commit c780e75305dba1f67691a8dc0e8bc8425838a452
Author: Jeximo <jeximo@gmail.com>
Date:   Tue May 7 21:26:43 2024 -0300

    Further tidy on Android instructions README.md (#7077)
    
    * Further tidy on Android instructions README.md
    
    Fixed some logic when following readme direction
    
    * Clean up redundent information
    
    A new user arriving will see simple directions on llama.cpp homepage
    
    * corrected puncuation
    
    Period after cmake, colon after termux
    
    * re-word for clarity
    
    method seems to be more correct, instead of alternative in this context
    
    * Organized required packages per build type
    
    building llama.cpp with NDK on a pc doesn't require installing clang, cmake, git, or wget in termux.
    
    * README.md
    
    corrected title
    
    * fix trailing whitespace

commit 48b2f9c1fc71ab7df5432be2ed9fa7cdf5e8405e
Author: jukofyork <69222624+jukofyork@users.noreply.github.com>
Date:   Wed May 8 01:24:16 2024 +0100

    Fixed save_imatrix to match old behaviour for MoE (#7099)
    
    * Fixed save_imatrix to match old behaviour for MoE
    
    This fix is simple and clear, but unnecessarily doubles the memory overhead..
    
    * Fixed missing idx variable
    
    * Unconditionally increment ncall
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Fixed 2 bugs in save_imatrix()
    
    - Fixed segfault bug because the counts vector needed to be created.
    - Fixed pre-existing bug didn't actually add to the counts for "--combine" option.
    
    * ncall needs summing too
    
    * Trailing whitespace
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit af0a5b616359809ce886ea433acedebb39b12969
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue May 7 23:07:58 2024 +0200

    server: fix incorrectly reported token probabilities (#7125)
    
    * server: normalize token probabilities
    
    * fix temperature == 0.0f

commit b6aa6702030320a3d5fbc2508307af0d7c947e40
Author: nopperl <54780682+nopperl@users.noreply.github.com>
Date:   Tue May 7 19:39:43 2024 +0000

    Fix OLMo HF to GGUF conversion (#6910)

commit 260b7c65296fba0568eeb1ff05244ea0be206b54
Author: Kyle Mistele <kyle@mistele.com>
Date:   Tue May 7 13:44:29 2024 -0500

    server : update readme with undocumented options (#7013)

commit 53d6c52e227dedef347b21e28febcfb9caeecdad
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 7 21:43:13 2024 +0300

    readme : update hot topics

commit 3af34c1d1b0da47f85b95f60922abeded1cb5d33
Author: RhinoDevel <RhinoDevel@users.noreply.github.com>
Date:   Tue May 7 19:51:31 2024 +0200

    main : update log text (EOS to EOG) (#7104)
    
    * Update log text (EOS to EOG)
    
    The log text "found EOS" is no longer always correct, here, because there is now an is-EOG check that also returns true for EOT.
    
    * Improve log msg. further by using "an" instead of "some".
    
    As suggested, to avoid misunderstanding (no multiple EOG tokens found, just one).

commit 04976db7a819fcf8bfefbfc09a3344210b79dd27
Author: omahs <73983677+omahs@users.noreply.github.com>
Date:   Tue May 7 17:20:33 2024 +0200

    docs: fix typos (#7124)
    
    * fix typo
    
    * fix typos
    
    * fix typo
    
    * fix typos
    
    * fix typo
    
    * fix typos

commit 947d3ad27d94f1addef76b5d64c314618f063933
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 7 11:08:49 2024 +0300

    ci : add GG_BUILD_EXTRA_TESTS_0 env (#7098)
    
    * ci : add GG_BUILD_EXTRA_TESTS_0 env
    
    ggml-ci
    
    * Update run.sh
    
    ggml-ci

commit 858f6b73f6e57a62523d16a955d565254be889b4
Author: William Tambellini <william.tambellini@gmail.com>
Date:   Mon May 6 11:12:14 2024 -0700

    Add an option to build without CUDA VMM (#7067)
    
    Add an option to build ggml cuda without CUDA VMM
    resolves
    https://github.com/ggerganov/llama.cpp/issues/6889
    https://forums.developer.nvidia.com/t/potential-nvshmem-allocated-memory-performance-issue/275416/4

commit b3a995b416e13ae3123a117a743e11d0ede0ca4c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 6 18:36:06 2024 +0300

    flake.lock: Update (#7079)
    
    Flake lock file updates:
    
    • Updated input 'flake-parts':
        'github:hercules-ci/flake-parts/9126214d0a59633752a136528f5f3b9aa8565b7d?narHash=sha256-sB4SWl2lX95bExY2gMFG5HIzvva5AVMJd4Igm%2BGpZNw%3D' (2024-04-01)
      → 'github:hercules-ci/flake-parts/e5d10a24b66c3ea8f150e47dfdb0416ab7c3390e?narHash=sha256-yzcRNDoyVP7%2BSCNX0wmuDju1NUCt8Dz9%2BlyUXEI0dbI%3D' (2024-05-02)
    • Updated input 'flake-parts/nixpkgs-lib':
        'github:NixOS/nixpkgs/d8fe5e6c92d0d190646fb9f1056741a229980089?dir=lib&narHash=sha256-iMUFArF0WCatKK6RzfUJknjem0H9m4KgorO/p3Dopkk%3D' (2024-03-29)
      → 'https://github.com/NixOS/nixpkgs/archive/50eb7ecf4cd0a5756d7275c8ba36790e5bd53e33.tar.gz?narHash=sha256-QBx10%2Bk6JWz6u7VsohfSw8g8hjdBZEf8CFzXH1/1Z94%3D' (2024-05-02)
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/7bb2ccd8cdc44c91edba16c48d2c8f331fb3d856?narHash=sha256-Drmja/f5MRHZCskS6mvzFqxEaZMeciScCTFxWVLqWEY%3D' (2024-04-25)
      → 'github:NixOS/nixpkgs/63c3a29ca82437c87573e4c6919b09a24ea61b0f?narHash=sha256-4cPymbty65RvF1DWQfc%2BBc8B233A1BWxJnNULJKQ1EY%3D' (2024-05-02)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit bcdee0daa7c5e8e086b719e5eb4073b00df70e01
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 6 09:31:30 2024 +0300

    minor : fix trailing whitespace

commit 628b299106d1e9476fdecb3cbe546bf5c60f1b89
Author: kunnis <kunnis@users.noreply.github.com>
Date:   Sun May 5 07:17:47 2024 -0500

    Adding support for the --numa argument for llama-bench. (#7080)

commit 8f8acc8683a00ce40fa5a81161a079d2167126e6
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Sun May 5 13:38:55 2024 +0200

    Disable benchmark on forked repo (#7034)
    
    * Disable benchmark on forked repo
    
    * only check owner on schedule event
    
    * check owner on push also
    
    * more readable as multi-line
    
    * ternary won't work
    
    * style++
    
    * test++
    
    * enable actions debug
    
    * test--
    
    * remove debug
    
    * test++
    
    * do debug where we can get logs
    
    * test--
    
    * this is driving me crazy
    
    * correct github.event usage
    
    * remove test condition
    
    * correct github.event usage
    
    * test++
    
    * test--
    
    * event_name is pull_request_target
    
    * test++
    
    * test--
    
    * update ref checks

commit ca3632602091e959ed2ad4c09c67a7c790b10d31
Author: Lyle Dean <dean@lyle.dev>
Date:   Sun May 5 06:21:46 2024 +0100

    readme : add note that LLaMA 3 is not supported with convert.py (#7065)

commit 889bdd76866ea31a7625ec2dcea63ff469f3e981
Author: DAN™ <dranger003@gmail.com>
Date:   Sun May 5 01:19:30 2024 -0400

    command-r : add BPE pre-tokenization (#7063)
    
    * Add BPE pre-tokenization for Command-R/R+.
    
    * Bump transformers convert requirement.
    
    * command-r : add individual digits regex
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 6fbd43221167bf96112f899daf22c127b282cbcf
Author: Brian <mofosyne@gmail.com>
Date:   Sun May 5 15:07:48 2024 +1000

    py : logging and flake8 suppression refactoring (#7081)
    
    Set one as executable and add basicConfig()
    to another. Also added noqa tag to test scripts.

commit 842500144ee02c8b0a46d2cc43880f8d80998fa5
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat May 4 18:56:22 2024 +0200

    gguf-split: add --no-tensor-first-split (#7072)

commit cf768b7e71cbcc9886c753ae963c2b68893d02e4
Author: Jeximo <jeximo@gmail.com>
Date:   Sat May 4 13:10:15 2024 -0300

    Tidy Android Instructions README.md (#7016)
    
    * Tidy Android Instructions README.md
    
    Remove CLBlast instructions(outdated), added OpenBlas.
    
    * don't assume git is installed
    
    Added apt install git, so that git clone works
    
    * removed OpenBlas
    
    Linked to Linux build instructions
    
    * fix typo
    
    Remove word "run"
    
    * correct style
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * correct grammar
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * delete reference to Android API
    
    * remove Fdroid reference, link directly to Termux
    
    Fdroid is not required
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Update README.md
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit fcd84a0f5a584ab5271745d7ffef21c8a6bc7b0c
Author: viric <viric@viric.name>
Date:   Sat May 4 15:26:53 2024 +0200

    Fix Linux /sys cpu path to guess number of cores (#7064)

commit 03fb8a002df2e96104f9e06de9c78d2a8ed91e92
Author: maor-ps <154728172+maor-ps@users.noreply.github.com>
Date:   Sat May 4 12:06:40 2024 +0300

    If first token generated from the server is the stop word the server will crash (#7038)
    
    This will reproduce the issue in llama13b
    {
    'prompt': 'Q: hello world \nA: ',
     'stop': ['\n'],
     'temperature': 0.0,
     'n_predict': 10,
     'cache_prompt': True,
     'n_probs': 10
    }

commit 92139b90af4841d7fd060b526bdd443b621770ff
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 4 08:32:32 2024 +0300

    tests : add test-tokenizer-0.sh + fix some tokenizers (#7036)
    
    * tests : add test-tokenizer-0.sh
    
    * unicode : add all unicode number ranges
    
    * starcoder : fix pre-tokenizer
    
    * tests : add test that fails with DeepSeek tokenizers
    
    * falcon : fix regex
    
    * unicode : regenerate unicode tables
    
    * refact : add tokenizer model
    
    * lint : fix
    
    * tests : disable failing tests
    
    ggml-ci
    
    * refact : add tests files
    
    ggml-ci
    
    * convert : print -> logging
    
    ggml-ci
    
    * lint : fix
    
    * unicode : digit -> number
    
    * phi-3 : update

commit a2ac89d6efb41b535778bfeaecaae8fe295b6ed3
Author: Brian <mofosyne@gmail.com>
Date:   Sat May 4 05:36:41 2024 +1000

    convert.py : add python logging instead of print() (#6511)
    
    * convert.py: add python logging instead of print()
    
    * convert.py: verbose flag takes priority over dump flag log suppression
    
    * convert.py: named instance logging
    
    * convert.py: use explicit logger id string
    
    * convert.py: convert extra print() to named logger
    
    * convert.py: sys.stderr.write --> logger.error
    
    * *.py: Convert all python scripts to use logging module
    
    * requirements.txt: remove extra line
    
    * flake8: update flake8 ignore and exclude to match ci settings
    
    * gh-actions: add flake8-no-print to flake8 lint step
    
    * pre-commit: add flake8-no-print to flake8 and also update pre-commit version
    
    * convert-hf-to-gguf.py: print() to logger conversion
    
    * *.py: logging basiconfig refactor to use conditional expression
    
    * *.py: removed commented out logging
    
    * fixup! *.py: logging basiconfig refactor to use conditional expression
    
    * constant.py: logger.error then exit should be a raise exception instead
    
    * *.py: Convert logger error and sys.exit() into a raise exception (for atypical error)
    
    * gguf-convert-endian.py: refactor convert_byteorder() to use tqdm progressbar
    
    * verify-checksum-model.py: This is the result of the program, it should be printed to stdout.
    
    * compare-llama-bench.py: add blank line for readability during missing repo response
    
    * reader.py: read_gguf_file() use print() over logging
    
    * convert.py: warning goes to stderr and won't hurt the dump output
    
    * gguf-dump.py: dump_metadata() should print to stdout
    
    * convert-hf-to-gguf.py: print --> logger.debug or ValueError()
    
    * verify-checksum-models.py: use print() for printing table
    
    * *.py: refactor logging.basicConfig()
    
    * gguf-py/gguf/*.py: use __name__ as logger name
    
    Since they will be imported and not run directly.
    
    * python-lint.yml: use .flake8 file instead
    
    * constants.py: logger no longer required
    
    * convert-hf-to-gguf.py: add additional logging
    
    * convert-hf-to-gguf.py: print() --> logger
    
    * *.py: fix flake8 warnings
    
    * revert changes to convert-hf-to-gguf.py for get_name()
    
    * convert-hf-to-gguf-update.py: use triple quoted f-string instead
    
    * *.py: accidentally corrected the wrong line
    
    * *.py: add compilade warning suggestions and style fixes

commit 433def286e98751bf17db75dce53847d075c0be5
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri May 3 15:24:30 2024 +0200

    llama : rename ctx to user_data in progress_callback (#7045)
    
    * llama : rename ctx to user_data in progress_callback
    
    This commit renames the `ctx` parameter to `user_data` in the
    `llama_progress_callback` typedef.
    
    The motivation for this is that other callbacks use `user_data` or
    `data`, and using `ctx` in this case might be confusing as it could be
    confused with `llama_context`.
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 60325fa56f61c228464c9f065db3aa6a61f2156e
Author: Bartowski <ckealty1182@gmail.com>
Date:   Thu May 2 19:49:09 2024 -0400

    Remove .attention from skipped tensors to match more accurately (#7051)

commit 6ecf3189e00a1e8e737a78b6d10e1d7006e050a2
Author: alwqx <kenan3015@gmail.com>
Date:   Thu May 2 23:56:41 2024 +0800

    chore: fix typo in llama.cpp (#7032)
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>

commit b0d943de179ad5dbd83d51f327fb566066f4ccda
Author: Andrew Downing <andrew2085@gmail.com>
Date:   Wed May 1 17:31:30 2024 -0400

    Update LOG_IMPL and LOG_TEE_IMPL (#7029)
    
    ROCm clang defines _MSC_VER which results in the wrong implementation of LOG_IMPL and LOG_TEE_IMPL being compiled.
    
    This fixes https://github.com/ggerganov/llama.cpp/issues/6972

commit 8d608a81b7bd170f700648f8214e6f3279d4d715
Author: l3utterfly <gc.pthzfoldr@gmail.com>
Date:   Thu May 2 04:27:41 2024 +0900

    main : fix off by one error for context shift (#6921)

commit 3ea0d36000e2314baba7e9fc6a97f08670a6f7e4
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed May 1 17:52:55 2024 +0200

    Server: add tests for batch size, different seeds (#6950)

commit 1613ef8d8eb2479ba55c4d598e08c8f3f18a0fed
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed May 1 14:46:37 2024 +0200

    CUDA: CUDART < 11.7 workaround for __hmax, __hmax2 (#7019)

commit c4ec9c0d3d67e6b33638e6dad86419e6fd5ffe01
Author: slaren <slarengh@gmail.com>
Date:   Wed May 1 07:13:59 2024 +0200

    ci : exempt confirmed bugs from being tagged as stale (#7014)

commit a8f9b076316e16aadd0791015b3bfd446fe1e904
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Apr 30 23:36:27 2024 +0200

    perplexity: more statistics, added documentation (#6936)
    
    * perplexity: more statistics, added documentation
    
    * add LLaMA 3 8b scoreboard

commit f364eb6fb5d46118a76fa045f487318de4c24961
Author: Kevin Gibbons <bakkot@gmail.com>
Date:   Tue Apr 30 08:14:02 2024 -0700

    switch to using localizedDescription (#7010)

commit 77e15bec6217a39be59b9cc83d6b9afb6b0d8167
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 30 15:52:21 2024 +0300

    metal : remove deprecated error code (#7008)

commit a68a1e7ed060ee5af2d638585d19e3510ddbf16c
Author: Kevin Gibbons <bakkot@gmail.com>
Date:   Tue Apr 30 02:34:50 2024 -0700

    metal : log more info on error (#6987)

commit 9c67c2773d4b706cf71d70ecf4aa180b62501960
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 30 12:16:08 2024 +0300

    ggml : add Flash Attention (#5021)
    
    * ggml : add ggml_flash_attn_ext API
    
    * ggml : fix GQA support in ggml_flash_attn_ext
    
    * ggml : online attention (CPU)
    
    * metal : initial implementation
    
    * metal : f16 precision
    
    * metal : reduce branches
    
    * metal : specialize for head size
    
    * wip : 8 rows per simd group
    
    * wip : 4 rows per simd group
    
    * wip : template for rows per warp
    
    * metal : parallelize across KV size
    
    * metal : parallel reduce across heads
    
    * metal : efficient flash_attn_f16 implementation
    
    * metal : avoid redundant loads of the attention
    
    * metal : scale and mask in matrix form
    
    * metal : fix comment
    
    * llama : avoid ggml_cast, use F32 query
    
    * metal : add parallel reduce version (disabled)
    
    * metal : move output into local memory + optimize
    
    - the result from each simdgroup now stays in the registers
    - significantly reduced SRAM usage
    - more efficient skipping of -INF blocks
    - avoid simdgroup barrier in hot loop
    - add comments
    
    * metal : add tests, fix scaling, support C > 32
    
    * metal : improve precision
    
    * ggml : fix f16 mad
    
    * metal : minor
    
    * metal : support Q > 8
    
    * tests : add ATTN tests
    
    * metal : disable buffer allocation logs
    
    * tests : more
    
    * metal : faster inner loop for C == 32
    
    * metal : fix array initialization
    
    * tests : ifdef
    
    * ggml : switch to padded F16 mask for ggml_soft_max, ggml_flash_attn_ext
    
    * ggml : fix ggml_soft_max mask requirement
    
    * cuda : fix soft_max to use correct mask size
    
    * cuda : add flash_attn kernel (wip)
    
    * metal : optimize softmax for C > 32
    
    * metal : optimize softmax
    
    * tests : minor fix
    
    * cuda : avoid zeroing fragments
    
    * tests : update dims
    
    * cuda : fix __hisinf() result check
    
    * cuda : avoid warp_reduce for smax
    
    * cuda : use int instead of int64_t
    
    Noticeably improves performance (thanks to Johannes)
    
    * cuda : make loops use the same loop values
    
    Thanks Johannes again for the tip
    
    * cuda : unroll some of the loops
    
    * cuda : avoid __hisinf branches
    
    * cuda : use half2 in softmax
    
    * cuda : switch to 1 warp for bs > 16
    
    * cuda : speed-up reduce part of the kernel
    
    * cuda : unroll Q*K^T loop
    
    * cuda : fix -INF block check
    
    * cuda : simplify softmax
    
    * cuda : fix matrix names
    
    * cuda : minor
    
    * llama : adapt to F16 KQ_pos
    
    * llama : adapt new models to F16 KQ_mask
    
    * ggml : fix F16 store (ARM NEON)
    
    * llama : fix type of KQ_mask and KQ_pos
    
    * ggml : fix CPU soft_max
    
    * tests : add hs=256
    
    * cuda : fix build
    
    * metal : improve perf via smaller int registers
    
    * cuda : adapt soft_max to F16 mask and pos
    
    * CUDA: faster FlashAttention, kernel for bs == 1
    
    * 16 cols for Phi-2
    
    * no vec for hs, no hs==256 ncols==32 for Volta
    
    * adjust kernel selection logic
    
    * 4 warps, 256 stride for all D
    
    * no ncols == 64
    
    * Multiple parallel blocks for batch size 1
    
    * fix compile warnings
    
    * fix excessive KQ_b loads
    
    * fix cmake build
    
    * fix KV cache padding, NaN from INFINITY (#6438)
    
    * llama : flash_attn cparam + fix defrag
    
    * server: support flash_attn param
    
    * server: bench: enable flash_attn param
    
    * CUDA: refactor host code, dyn. par. blocks
    
    * fix flash_attn_vec_f16 race condition
    
    * flush softmax exp below threshold to 0
    
    * store temp KQ in registers
    
    * Calculate KQ as FP32 if KQV has GGML_PREC_F32
    
    * Add __hgt2_mask implementation for CUDA 11
    
    * fix KQ FP32 precision fpr parallel_blocks > 1
    
    * llama-bench : add -fa,--flash-attn arg
    
    * metal : add BS=1 kernel for flash attention (#6508)
    
    * metal : add BS=1 kernel for flash attention (wip)
    
    * metal : support more than 1 warps
    
    * metal : opts
    
    * metal : opt
    
    * metal : switch to parallel reduce
    
    * metal : reduce registers
    
    * metal : simplify
    
    * metal : initial FA vec kernel
    
    * metal : use F32 attention accumulators
    
    * batched-bench : add fattn arg
    
    * llama : simplify llama_build_kv_store
    
    ggml-ci
    
    * llama : adapt build_olmo to changes
    
    * ggml : fix arm fp16 store on windows
    
    * metal : clean-up
    
    * metal : clean-up kernel code
    
    * metal : minor
    
    * tests : remove benchmarks
    
    ggml-ci
    
    * ggml : fix avx512 const correctness
    
    ggml-ci
    
    * ggml : fix soft_max with bias on CPU
    
    ggml-ci
    
    * common : print --flash-attn in help
    
    * ggml : fix num dimensions in ggml_flash_attn_ext
    
    * llama : force disable flash attention for incompatible models
    
    * ggml : ggml_soft_max support F16/F32 mask/pos
    
    ggml-ci
    
    * cuda : uint -> uint32_t
    
    * cuda : "constexpr dim3" -> "const dim3"
    
    ggml-ci
    
    * cuda : try to fix __hgt2_mask
    
    ggml-ci
    
    * ggml : add TODO's for F16/F32 mask/pos support in other backends
    
    * llama : replace bool need_kq_pos with use_alibi
    
    * llama : prep ALiBi support for BERT models
    
    ggml-ci
    
    * llama : fix n_batch requirements
    
    ggml-ci
    
    * cont
    
    * server : add help for --flash-attn arg
    
    * llama : disable FA for AMD
    
    * tests : remove TMP_ATTN_BENCH
    
    ggml-ci
    
    * llama : support save/load state with FA enabled
    
    ggml-ci
    
    * ci : add CUDA save-load-state tests
    
    ggml-ci
    
    * llama : llama_kv_cache_clear zeroes data + fix save-load seq
    
    ggml-ci
    
    * llama : fix copy-paste errors, add TODO
    
    * llama : disallow incompatible states
    
    * llama : update llama_state_get_size after v_trans field
    
    * metal : remove tmp log
    
    * llama : add static reminder for llama_state_get_size
    
    * metal : fix max nsg
    
    ggml-ci
    
    * ci : fix arg order
    
    ggml-ci
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    Co-authored-by: Pierrick HYMBERT <pierrick.hymbert@gmail.com>

commit 952d03dbead16e4dbdd1d3458486340673cc2465
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 30 11:05:25 2024 +0300

    convert : use utf8 encoding (#7000)
    
    * convert : use utf8 encoding
    
    * convert : update instructions and warning message

commit 8843a98c2ba97a25e93319a104f9ddfaf83ce4c4
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Tue Apr 30 00:52:50 2024 +0100

    Improve usability of --model-url & related flags (#6930)
    
    * args: default --model to models/ + filename from --model-url or --hf-file (or else legacy models/7B/ggml-model-f16.gguf)
    
    * args: main & server now call gpt_params_handle_model_default
    
    * args: define DEFAULT_MODEL_PATH + update cli docs
    
    * curl: check url of previous download (.json metadata w/ url, etag & lastModified)
    
    * args: fix update to quantize-stats.cpp
    
    * curl: support legacy .etag / .lastModified companion files
    
    * curl: rm legacy .etag file support
    
    * curl: reuse regex across headers callback calls
    
    * curl: unique_ptr to manage lifecycle of curl & outfile
    
    * curl: nit: no need for multiline regex flag
    
    * curl: update failed test (model file collision) + gitignore *.gguf.json

commit b8c1476e44cc1f3a1811613f65251cf779067636
Author: Clint Herron <hanclinto@gmail.com>
Date:   Mon Apr 29 14:40:14 2024 -0400

    Extending grammar integration tests (#6644)
    
    * Cleaning up integration tests to share code between tests and make it simpler to add new tests.
    
    * Add tests around quantifiers to ensure both matching and non-matching compliance.
    
    * Add slightly more complex grammar with quantifiers to test references with quantifiers.
    
    * Fixing build when C++17 is not present.
    
    * Separating test calls to give more helpful stack traces on failure. Adding verbose messages to give visibility for what is being tested.
    
    * Adding quotes around strings to explicitly show whitespace
    
    * Removing trailing whitespace.
    
    * Implementing suggestions from @ochafik -- grammars and test strings now print and flush before tests to aid in debugging segfaults and whatnot.
    
    * Cleaning up forgotten symbols. Modifying simple test to use test harness. Added comments for more verbose descriptions of what each test is accomplishing.
    
    * Unicode symbol modifications to hopefully make log easier to parse visually.

commit 5539e6fdd1b97e0c37c54d34df67f334fc344a26
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Mon Apr 29 19:56:59 2024 +0200

    main : fix typo in comment in main.cpp (#6985)
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit b8a7a5a90fd3187175d84227dad705ade395ba46
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Mon Apr 29 17:02:45 2024 +0100

    build(cmake): simplify instructions (`cmake -B build && cmake --build build ...`) (#6964)
    
    * readme: cmake . -B build && cmake --build build
    
    * build: fix typo
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * build: drop implicit . from cmake config command
    
    * build: remove another superfluous .
    
    * build: update MinGW cmake commands
    
    * Update README-sycl.md
    
    Co-authored-by: Neo Zhang Jianyu <jianyu.zhang@intel.com>
    
    * build: reinstate --config Release as not the default w/ some generators + document how to build Debug
    
    * build: revert more --config Release
    
    * build: nit / remove -H from cmake example
    
    * build: reword debug instructions around single/multi config split
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    Co-authored-by: Neo Zhang Jianyu <jianyu.zhang@intel.com>

commit d2c898f746a527f09effb061829e68b2e1812a28
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 29 18:36:39 2024 +0300

    ci : tmp disable gguf-split (#6983)
    
    ggml-ci

commit 544f1f10adeec3d5ed91c4d3bd3f903904ecea63
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 29 17:55:02 2024 +0300

    ggml : fix __MSC_VER -> _MSC_VER (#6977)
    
    ggml-ci

commit ffe666572f98a686b17a2cd1dbf4c0a982e5ac0a
Author: cpumaxx <163466046+cpumaxx@users.noreply.github.com>
Date:   Mon Apr 29 07:34:24 2024 -0700

    llava-cli : multiple images (#6969)
    
    Co-authored-by: root <root@nenya.lothlorien.ca>

commit 24affa7db3c9db148854b0ab4fd63de8bca7d898
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 29 17:06:19 2024 +0300

    readme : update hot topics

commit f4ab2a41476600a98067a9474ea8f9e6db41bcfa
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 29 16:58:41 2024 +0300

    llama : fix BPE pre-tokenization (#6920)
    
    * merged the changes from deepseeker models to main branch
    
    * Moved regex patterns to unicode.cpp and updated unicode.h
    
    * Moved header files
    
    * Resolved issues
    
    * added and refactored unicode_regex_split and related functions
    
    * Updated/merged the deepseek coder pr
    
    * Refactored code
    
    * Adding unicode regex mappings
    
    * Adding unicode regex function
    
    * Added needed functionality, testing remains
    
    * Fixed issues
    
    * Fixed issue with gpt2 regex custom preprocessor
    
    * unicode : fix? unicode_wstring_to_utf8
    
    * lint : fix whitespaces
    
    * tests : add tokenizer tests for numbers
    
    * unicode : remove redundant headers
    
    * tests : remove and rename tokenizer test scripts
    
    * tests : add sample usage
    
    * gguf-py : reader prints warnings on duplicate keys
    
    * llama : towards llama3 tokenization support (wip)
    
    * unicode : shot in the dark to fix tests on Windows
    
    * unicode : first try custom implementations
    
    * convert : add "tokenizer.ggml.pre" GGUF KV (wip)
    
    * llama : use new pre-tokenizer type
    
    * convert : fix pre-tokenizer type writing
    
    * lint : fix
    
    * make : add test-tokenizer-0-llama-v3
    
    * wip
    
    * models : add llama v3 vocab file
    
    * llama : adapt punctuation regex + add llama 3 regex
    
    * minor
    
    * unicode : set bomb
    
    * unicode : set bomb
    
    * unicode : always use std::wregex
    
    * unicode : support \p{N}, \p{L} and \p{P} natively
    
    * unicode : try fix windows
    
    * unicode : category support via std::regex
    
    * unicode : clean-up
    
    * unicode : simplify
    
    * convert : add convert-hf-to-gguf-update.py
    
    ggml-ci
    
    * lint : update
    
    * convert : add falcon
    
    ggml-ci
    
    * unicode : normalize signatures
    
    * lint : fix
    
    * lint : fix
    
    * convert : remove unused functions
    
    * convert : add comments
    
    * convert : exercise contractions
    
    ggml-ci
    
    * lint : fix
    
    * cmake : refactor test targets
    
    * tests : refactor vocab tests
    
    ggml-ci
    
    * tests : add more vocabs and tests
    
    ggml-ci
    
    * unicode : cleanup
    
    * scripts : ignore new update script in check-requirements.sh
    
    * models : add phi-3, mpt, gpt-2, starcoder
    
    * tests : disable obsolete
    
    ggml-ci
    
    * tests : use faster bpe test
    
    ggml-ci
    
    * llama : more prominent warning for old BPE models
    
    * tests : disable test-tokenizer-1-bpe due to slowness
    
    ggml-ci
    
    ---------
    
    Co-authored-by: Jaggzh <jaggz.h@gmail.com>
    Co-authored-by: Kazim Abrar Mahi <kazimabrarmahi135@gmail.com>

commit 3f167476b11efa7ab08f6cacdeb8cab0935c1249
Author: David Renshaw <dwrenshaw@gmail.com>
Date:   Mon Apr 29 09:35:45 2024 -0400

    sampling : use std::random_device{}() for default random seed (#6962)

commit 3055a4180557c6cbe29eacc8284c9e070ac10eab
Author: Christian Zhou-Zheng <59622928+christianazinn@users.noreply.github.com>
Date:   Mon Apr 29 09:34:41 2024 -0400

    convert : fix conversion of some BERT embedding models (#6937)

commit 577277ffd203b190c3dc2ab3e737946dc432132c
Author: Przemysław Pawełczyk <przemoc@gmail.com>
Date:   Mon Apr 29 15:08:20 2024 +0200

    make : change GNU make default CXX from g++ to c++ (#6966)

commit ca7f29f568803bee4c92d1b3e41c7d721b0dc570
Author: Przemysław Pawełczyk <przemoc@gmail.com>
Date:   Mon Apr 29 14:59:47 2024 +0200

    ci : add building in MSYS2 environments (Windows) (#6967)

commit c4f708a93f1df5e35167f9313e000d381298be7f
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Apr 29 14:36:22 2024 +0200

    llama : fix typo LAMMAFILE -> LLAMAFILE (#6974)

commit e00b4a8f816ebc45b98a46e5f5231359b9a017e0
Author: DAN™ <dranger003@gmail.com>
Date:   Sun Apr 28 18:38:44 2024 -0400

    Fix more int overflow during quant (PPL/CUDA). (#6563)
    
    * Fix more int overflow during quant.
    
    * Fix some more int overflow in softmax.
    
    * Revert back to int64_t.

commit 7bb36ccf91b8a2e92b182dd75624f1fd7cb205ac
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sun Apr 28 17:36:18 2024 +0200

    gguf : enforce that tensor names are unique (#6905)
    
    * not allow adding duplicated tensor name
    
    * no duplicated tensor while reading gguf
    
    * typo
    
    * throw exception inside llama_model_loader
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit ce023f6f2ff34fbe840e32e65d443d2fed7393de
Author: Neo Zhang <14088817+arthw@users.noreply.github.com>
Date:   Sun Apr 28 22:40:31 2024 +0800

    add device version in device list (#6959)
    
    Co-authored-by: arthw <>

commit 6e472f58e40cd4acf6023e15c75a2700535c5f0b
Author: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
Date:   Sun Apr 28 00:18:27 2024 +0000

    flake.lock: Update
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/5c24cf2f0a12ad855f444c30b2421d044120c66f?narHash=sha256-XtTSSIB2DA6tOv%2Bl0FhvfDMiyCmhoRbNB%2B0SeInZkbk%3D' (2024-04-19)
      → 'github:NixOS/nixpkgs/7bb2ccd8cdc44c91edba16c48d2c8f331fb3d856?narHash=sha256-Drmja/f5MRHZCskS6mvzFqxEaZMeciScCTFxWVLqWEY%3D' (2024-04-25)

commit 4dba7e8114d84241c842b986e008af8b88d1a019
Author: mgroeber9110 <45620825+mgroeber9110@users.noreply.github.com>
Date:   Sat Apr 27 21:02:06 2024 +0200

    Replace "alternative" boolean operator in conditional compilation directive (#6949)

commit b7368332e24c5b2c8038bf8267f43632783fcc35
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Apr 27 17:50:48 2024 +0200

    ci: server: tests python env on github container ubuntu latest / fix n_predict (#6935)
    
    * ci: server: fix python env
    
    * ci: server: fix server tests after #6638
    
    * ci: server: fix windows is not building PR branch

commit 928e0b7013c862cf10701957b3d654aa70f11bd8
Author: agray3 <agray3@users.noreply.github.com>
Date:   Fri Apr 26 19:08:30 2024 +0100

    Reset schedule earlier to allow overlap with ggml graph computation on device (#6933)
    
    * Reset schedule earlier to allow overlap with graph computation on device

commit 0c4d489e29e53589bf13a801fe7c94b7b546d8f6
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Apr 26 20:06:33 2024 +0200

    quantize: add imatrix and dataset metadata in GGUF (#6658)
    
    * imatrix: save the dataset file used in the output file
    
    * llama: support kv overrides type string string
    
    * common: factorize KV Overrides parsing between common and server
    
    * quantize: add imatrix n entries and dataset KV metadata
    quantize: factorize KV Overrides parsing between common
    #6656
    
    * llama: remove kv override str_value initialization as it does not compile on some toolchain
    
    * quantize: add imatrix m_last_call as `quantize.imatrix.chunks_count`
    
    * quantize: add imatrix filename in KV
    
    * llama: add llama_model_kv_override_free
    
    * common: add llama_model_kv_override_free
    common: free kv override if used after model loading
    
    * llama: finally move the string KV override value to the stack
    
    * llama : minor
    
    * no need to add a NUL to the std::vector, std::string can be initialized from a pair of iterators.
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * kv override: ensure string termination
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>

commit 017e6999b5184234370b22a2f868e1be911e8d88
Author: slaren <slarengh@gmail.com>
Date:   Fri Apr 26 18:39:58 2024 +0200

    add basic tensor data validation function (#6884)
    
    * add basic tensor data validation function
    
    * add --check-tensors command line argument
    
    tensor validation is disabled by default and can be enabled by adding
    `--check-tensors` to the command line arguments.
    
    quantize always validates tensors.

commit e2764cd7ca1112d9303eba9e81c9935ee67352ff
Author: slaren <slarengh@gmail.com>
Date:   Fri Apr 26 17:07:42 2024 +0200

    gguf : fix mismatch between alloc and free functions (#6929)

commit 4b1c3c98b442a4c84a788fff6323f6fa7e678a5b
Author: Justine Tunney <jtunney@mozilla.com>
Date:   Fri Apr 26 10:05:33 2024 -0400

    llamafile : use 64-bit integers in sgemm (#6928)

commit bbe3c6e76157a5d806fdc155451f0ca8936248ee
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Apr 26 12:27:25 2024 +0200

    ci: server: fix python installation (#6925)

commit 7f5ff558eed0f732af8f25c2ab0645610bdec80c
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Apr 26 12:15:30 2024 +0200

    server: stop generation at `n_ctx_train` if `n_predict` is not set (#6638)
    
    * server: cap n_predict if not set to n_ctx_train
    
    * server: fix infinite loop
    
    * server: infinite loop, move in process_token
    server: infinite loop: set stop limit to true
    
    * minor: spaces
    
    * minor: spaces
    
    * server: include prompt tokens in the EOS limit

commit 9e4e077ec50fde6049b128662c72d37a3c28e34b
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Apr 26 11:11:51 2024 +0200

    ci: server: fix python installation (#6922)

commit 83b72cb086ce46a33dececc86bfe4648b6120aa8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Apr 26 10:41:53 2024 +0300

    Merge pull request from GHSA-p5mv-gjc5-mwqv
    
    * always use calloc
    
    clamp n_kv on failure to read a kv
    
    * ggml : alternative ctx->header.n_kv update
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit d4a9afc1009f0da88d04b2c5f672d81d5ae94675
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Apr 26 09:27:49 2024 +0200

    ci: server: fix python installation (#6918)

commit 7d641c26ac73956a54b204cabefe85c764823678
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Apr 26 09:26:59 2024 +0200

    ci: fix concurrency for pull_request_target (#6917)

commit 5790c8dac1a4f0aa80b4efee3b962d8c04c829e8
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Apr 26 09:26:16 2024 +0200

    bench: server add stop word for PHI-2 (#6916)

commit 46e12c4692a37bdd31a0432fc5153d7d22bc7f72
Author: vik <vikhyatk@gmail.com>
Date:   Thu Apr 25 12:38:31 2024 -0700

    llava : add support for moondream vision language model (#6899)
    
    * add support for moondream vision language model
    
    This required making the following changes to the CLIP model:
    
    1. Support for patch embedding bias.
    2. Make class embedding and pre-layernorm optional.
    3. Add support for post-layernorm.
    
    * Update examples/llava/clip.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit dba497e0c1eff27cf0e85d1d73c86fa08e1b5557
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 25 21:31:17 2024 +0300

    cmake : restore LLAMA_LLAMAFILE_DEFAULT

commit fa0b4ad25208d5ec2df6b998ccb29b49b249e82c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 25 18:59:51 2024 +0300

    cmake : remove obsolete ANDROID check

commit d6e1d44f16e5580cf47f13325ff49960bf13ca37
Author: slaren <slarengh@gmail.com>
Date:   Thu Apr 25 17:59:03 2024 +0200

    llama : synchronize before get/set session data (#6911)

commit 853d06ffe26621176d60909a6e3f7c4cd067b305
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 25 17:06:27 2024 +0300

    ci : tmp disable slow tests

commit 3fe0596c1817a6114ffffb6dbfd6c36ca7815dc7
Author: BarfingLemurs <128182951+BarfingLemurs@users.noreply.github.com>
Date:   Thu Apr 25 09:52:28 2024 -0400

    readme : update model list (#6908)
    
    * Update README.md
    
    * missing space
    
    * llama3 !

commit 0ead1f1072fdc70720f92e008a294dc74a826b1d
Author: slaren <slarengh@gmail.com>
Date:   Thu Apr 25 15:23:47 2024 +0200

    llama : check that all the tensor data is in the model file (#6885)
    
    * llama : check that all the tensor data is in the model file
    
    * also check for unsigned overflow

commit 51543729ff5b1361ebfb164875c93dff0850d5fe
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 25 15:48:25 2024 +0300

    ggml : fix redefinition of vaddvq_f32 for 32-bit ARM (#6906)

commit 4ab99d8d4762869506bb675b36501fd7c2af00b2
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Apr 25 14:38:14 2024 +0200

    clip : rename lerp function to avoid conflict (#6894)
    
    This commit renamesthe lerp (linear interpolation) function in clip.cpp
    to avoid a conflict with the lerp function in the <cmath> standard C++
    library when using c++20.
    
    The motivation for this change is to enable projects that use c++20 to
    be able to compile clip.cpp without having to resort to patching it. The
    lerp function was added to cmath in version C++20 (202002L) and is why
    this is not causing any issue at the moment as C++11/C++17 is currently
    used by llama.cpp.
    
    I realize that llama.cpp uses either C++11 (or C++17 in the case for
    SYCL) but wanted to ask if this would be an acceptable change just the
    same.
    
    Refs: https://en.cppreference.com/w/cpp/numeric/lerp
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 54770413c484660d021dd51b5dbacab7880b8827
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 25 15:12:28 2024 +0300

    ggml : fix MIN / MAX macros (#6904)
    
    ggml-ci

commit aa750c1ede6232c91de890a14a7731d6daa2bc8e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 25 14:27:20 2024 +0300

    tests : minor bash stuff (#6902)
    
    * tests : minor bash stuff
    
    ggml-ci
    
    * llama : fix build
    
    ggml-ci
    
    * tests : fix CUR_DIR -> ROOT_DIR
    
    ggml-ci
    
    * tests : fix fname
    
    ggml-ci

commit 1966eb2615242f224bf9ca939db8905ab6a174a0
Author: jiez <373447296@qq.com>
Date:   Thu Apr 25 18:29:35 2024 +0800

    quantize : add '--keep-split' to quantize model into shards (#6688)
    
    * Implement '--keep-split' to quantize model into several shards
    
    * Add test script
    
    * Update examples/quantize/quantize.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Split model correctly even if tensor id is out-of-order
    
    * Update llama_model_quantize_params
    
    * Fix preci failures
    
    ---------
    
    Co-authored-by: z5269887 <z5269887@unsw.edu.au>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 784e11dea1f5ce9638851b2b0dddb107e2a609c8
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Apr 24 21:29:13 2024 +0200

    README: add graphic for matrix multiplication (#6881)

commit b4e4b8a9351d918a56831c73cf9f25c1837b80d1
Author: Douglas Hanley <thesecretaryofwar@gmail.com>
Date:   Wed Apr 24 08:10:07 2024 -0500

    llama : add llama_get_pooling_type function (#6862)
    
    * add llama_get_pooling_type function
    
    * fix argument name, move with ctx funcs

commit 3fe847b5747676ee1bf90371c46ed0bc66b57240
Author: mgroeber9110 <45620825+mgroeber9110@users.noreply.github.com>
Date:   Wed Apr 24 12:54:24 2024 +0200

    server : do not apply Markdown formatting in code sections (#6850)

commit 37246b1031b1680c0dcaf20aef736d6b446203fa
Author: Kyle Mistele <kyle@mistele.com>
Date:   Wed Apr 24 05:15:29 2024 -0500

    common : revert showing control tokens by default for server (#6860)
    
    * fix: revert showing control tokens by default
    
    * feat: revert changes to default behavior of llama_token_to_piece; provide overridden declaration to receive "bool special" param to toggle showing control tokens
    
    * feat: use the overridden declaration of llama_token_to_piece from common/common.cpp to specify "false" so that control tokens are not shown in chat completion responses"
    
    * common : simplify
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 28103f4832e301a9c84d44ff0df9d75d46ab6c76
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Apr 24 11:08:36 2024 +0200

    Server: fix seed for multiple slots (#6835)
    
    * Server: add tests for consistent results
    
    * sampling: separate rng per sampling context

commit c0d1b3e03e27634ac2871761f5033cf9324d472d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 24 12:00:07 2024 +0300

    ggml : move 32-bit arm compat in ggml-impl.h (#6865)
    
    ggml-ci

commit abd3314064cd3c513f9eef34c3ba6c23a107442c
Author: Tristan Druyen <tristan@vault81.mozmail.com>
Date:   Wed Apr 24 10:52:37 2024 +0200

    llama : add phi 3 chat template (#6857)
    
    * Add phi 3 chat template & tests
    
    * test : fix chat template result
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 3fec68be4e9577fc53158366d3b3af039c17bb1f
Author: Junyang Lin <justinlin930319@hotmail.com>
Date:   Wed Apr 24 15:16:21 2024 +0800

    convert : add support of codeqwen due to tokenizer (#6707)
    
    * add support of codeqwen due to tokenizer
    
    * override load_hparams
    
    * fix typo
    
    * fix load_params
    
    * convert : fix whitespace
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit c8297c6af5693555652c40b95974b95d49d2674d
Author: liuwei-git <14815172+liuwei-git@users.noreply.github.com>
Date:   Wed Apr 24 15:00:37 2024 +0800

    llama : add phi3 support (#6852)
    
    * add explicit phi3 support
    
    * add explicit phi3 support
    
    * remove unused code
    
    * convert : add BOS token
    
    * llama : match EOT token <|end|>
    
    * llama : minor / style
    
    * llama : tabs -> spaces
    
    * convert : fix lint checks
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 4e96a812b3ce7322a29a3008db2ed73d9087b176
Author: Anas Ahouzi <112881240+aahouzi@users.noreply.github.com>
Date:   Tue Apr 23 02:53:18 2024 +0200

    [SYCL] Windows default build instructions without -DLLAMA_SYCL_F16 flag activated (#6767)
    
    * Fix FP32/FP16 build instructions
    
    * Fix typo
    
    * Recommended build instruction
    
    Co-authored-by: Neo Zhang Jianyu <jianyu.zhang@intel.com>
    
    * Recommended build instruction
    
    Co-authored-by: Neo Zhang Jianyu <jianyu.zhang@intel.com>
    
    * Recommended build instruction
    
    Co-authored-by: Neo Zhang Jianyu <jianyu.zhang@intel.com>
    
    * Add comments in Intel GPU linux
    
    ---------
    
    Co-authored-by: Anas Ahouzi <112881240+aahouzi-intel@users.noreply.github.com>
    Co-authored-by: Neo Zhang Jianyu <jianyu.zhang@intel.com>

commit 192090bae47960f0d38d4967abe398a5d190057e
Author: Justine Tunney <jtunney@mozilla.com>
Date:   Mon Apr 22 15:00:36 2024 -0400

    llamafile : improve sgemm.cpp (#6796)
    
    * llamafile : improve sgemm.cpp
    
    - Re-enable by default
    - Fix issue described in #6716
    - Make code more abstract, elegant, and maintainable
    - Faster handling of weirdly shaped `m` an `n` edge cases
    
    * Address review comments
    
    * Help clang produce fma instructions
    
    * Address review comments

commit e931888d5024de814ce7119a18d6a959bfff3821
Author: Dave Airlie <airlied@gmail.com>
Date:   Tue Apr 23 00:05:06 2024 +1000

    ggml : fix calloc argument ordering. (#6820)
    
    Latest gcc complains here:
    /home/airlied/devel/llama.cpp/ggml-alloc.c: In function ‘ggml_gallocr_new_n’:
    /home/airlied/devel/llama.cpp/ggml-alloc.c:374:59: warning: ‘calloc’ sizes specified with ‘sizeof’ in the earlier argument and not in the later argument [-Wcalloc-transposed-args]
      374 |     ggml_gallocr_t galloc = (ggml_gallocr_t)calloc(sizeof(struct ggml_gallocr), 1);
          |                                                           ^~~~~~
    /home/airlied/devel/llama.cpp/ggml-alloc.c:374:59: note: earlier argument should specify number of elements, later size of each element
    
    and a bunch more.
    
    calloc is specified to take nmemb first then size, so realign the code.
    
    In a couple of places there was a * x, 1 so I fixed those to use calloc properly.

commit 8960fe86ae075c846c5df8848230d1904ba8877f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 22 15:41:11 2024 +0300

    llama : fix typo in <|im_end|> token text (#6745)

commit c0956b09ba845a7cd787d5580d7c8b96e80f40f5
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Mon Apr 22 13:22:54 2024 +0200

    ci: fix job are cancelling each other (#6781)

commit e9b4a1bf68c18beff4e33f23ea62c1245b296915
Author: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
Date:   Sun Apr 21 00:17:47 2024 +0000

    flake.lock: Update
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/1042fd8b148a9105f3c0aca3a6177fd1d9360ba5?narHash=sha256-3sbWO1mbpWsLepZGbWaMovSO7ndZeFqDSdX0hZ9nVyw%3D' (2024-04-10)
      → 'github:NixOS/nixpkgs/5c24cf2f0a12ad855f444c30b2421d044120c66f?narHash=sha256-XtTSSIB2DA6tOv%2Bl0FhvfDMiyCmhoRbNB%2B0SeInZkbk%3D' (2024-04-19)

commit 5cf5e7d490dfdd2e70bface2d35dfd14aa44b4fb
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Sun Apr 21 18:48:53 2024 +0100

    `build`: generate hex dump of server assets during build (#6661)
    
    * `build`: generate hex dumps of server assets on the fly
    
    * build: workaround lack of -n on gnu xxd
    
    * build: don't use xxd in cmake
    
    * build: don't call xxd from build.zig
    
    * build: more idiomatic hexing
    
    * build: don't use xxd in Makefile (od hackery instead)
    
    * build: avoid exceeding max cmd line limit in makefile hex dump
    
    * build: hex dump assets at cmake build time (not config time)

commit 40f74e4d739e9250431cf339ae7588b28d8d0663
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 21 18:36:45 2024 +0300

    llama : add option to render special/control tokens (#6807)
    
    * make : fix common dep on llama.h
    
    * llama : add option to render special tokens
    
    * readme : add API change notice
    
    ggml-ci
    
    * swift : fix build

commit b9cc76d87e3d7ae5900f19d4fe8f8976d0a35888
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 21 16:47:57 2024 +0300

    ggml : fix ggml_backend_cpu_supports_op() for CPY (#0)

commit 7dbdba5690ca61b3ee8c92cfac8e7e251042e787
Author: Wouter <9594229+DifferentialityDevelopment@users.noreply.github.com>
Date:   Sun Apr 21 15:03:39 2024 +0200

    llama : add llama-3 chat template (#6751)
    
    * Added llama-3 chat template
    
    * Update llama.cpp
    
    Co-authored-by: Samuel Tallet <36248671+SamuelTallet@users.noreply.github.com>
    
    * Update llama.cpp
    
    Co-authored-by: Samuel Tallet <36248671+SamuelTallet@users.noreply.github.com>
    
    * Update tests/test-chat-template.cpp
    
    Co-authored-by: Samuel Tallet <36248671+SamuelTallet@users.noreply.github.com>
    
    * Added EOS stop sequence according to https://github.com/ggerganov/llama.cpp/pull/6751#issuecomment-2065602862
    
    * Removed adding of BOS token before first message
    
    * Removed bos token from expected output from llama-3
    
    * Update tests/test-chat-template.cpp
    
    Co-authored-by: Rene Leonhardt <65483435+reneleonhardt@users.noreply.github.com>
    
    * Update tests/test-chat-template.cpp
    
    Co-authored-by: Rene Leonhardt <65483435+reneleonhardt@users.noreply.github.com>
    
    * Added <|end_of_text|> as another stop token
    
    * Reverted last change of adding the end_of_text stop word for llama 3
    
    ---------
    
    Co-authored-by: Wouter Tichelaar <tichelaarw@spar.net>
    Co-authored-by: Samuel Tallet <36248671+SamuelTallet@users.noreply.github.com>
    Co-authored-by: Rene Leonhardt <65483435+reneleonhardt@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit c1386c936e9fbc38eb2816c711ab28f13355708e
Author: pmysl <piotr.myslinski@outlook.com>
Date:   Sun Apr 21 14:49:30 2024 +0200

    gguf-py : add IQ1_M to GGML_QUANT_SIZES (#6761)

commit e8d35f47cb8cb4002fca02e18aaa1cb9fa21d6f1
Author: Jan Boon <jan.boon@kaetemi.be>
Date:   Sun Apr 21 20:35:40 2024 +0800

    doc : add link to falcon (#6789)

commit 2cca09d509c0e114acc16cb347c3cdd86e5f1b40
Author: Mohammadreza Hendiani <mohammad.r.hendiani@gmail.com>
Date:   Sun Apr 21 16:02:05 2024 +0330

    readme : add Fedora instructions (#6783)
    
    * added fedora to list of distros that may need the package (the packages have the same name on Fedora)
    
    * how to add clblast that is avalible in the fedora repos

commit 89b0bf0d5dc123cc1053708f5f84b89e52cdc80b
Author: Justine Tunney <jtunney@mozilla.com>
Date:   Sun Apr 21 08:19:04 2024 -0400

    llava : use logger in llava-cli (#6797)
    
    This change removes printf() logging so llava-cli is shell scriptable.

commit b97bc3966e852adb626c90be64fd48282800f504
Author: Pedro Cuenca <pedro@huggingface.co>
Date:   Sun Apr 21 13:50:41 2024 +0200

    llama : support Llama 3 HF conversion (#6745)
    
    * Support Llama 3 conversion
    
    The tokenizer is BPE.
    
    * style
    
    * Accept suggestion
    
    Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>
    
    * llama : add llama_token_is_eog()
    
    ggml-ci
    
    * llama : auto-detect more EOT tokens when missing in KV data
    
    * convert : replacing EOS token is a hack
    
    * llama : fix codegemma EOT token + add TODOs
    
    * llama : fix model type string for 8B model
    
    ---------
    
    Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit b8109bc0139f15a5b321909f47510b89dca47ffc
Author: Jan Boon <jan.boon@kaetemi.be>
Date:   Sun Apr 21 00:29:50 2024 +0800

    doc : server tests require llama to be built with curl enabled (#6788)

commit aed82f6837a3ea515f4d50201cfc77effc7d41b4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 20 13:27:12 2024 +0300

    common : try to fix Android CI (#6780)
    
    * common : disable get_math_cpu_count() until Android CI gets fixed
    
    * common : another try

commit 0e4802b2ecbaab04b4f829fde4a3096ca19c84b5
Author: loonerin <132926317+loonerin@users.noreply.github.com>
Date:   Fri Apr 19 13:03:35 2024 -0400

    ci: add ubuntu latest release and fix missing build number (mac & ubuntu) (#6748)

commit 637e9a86c220718d008b54842dfd294aa96d3b7a
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Apr 19 13:19:01 2024 +0200

    server: static: upstream upgrade (#6765)

commit 9958c81b798a5872087b30b360e4674871f2479e
Author: nopperl <54780682+nopperl@users.noreply.github.com>
Date:   Fri Apr 19 09:35:54 2024 +0000

    Implement the OLMo architecture (#6741)
    
    * implement olmo architecture
    
    * remove unused variable
    
    * remove unused moe branch
    
    * remove check for weight
    
    * remove superfluous moe, bias and rope tensors
    
    * clarified comment
    
    * fix clamp_kqv setting
    
    * remove obsolete parameter name filter

commit 8b1b1f4982d3e9b994308d05a1c8b9e45c23edb5
Author: Austin <77757836+teleprint-me@users.noreply.github.com>
Date:   Fri Apr 19 03:16:45 2024 -0400

    train : add general name (#6752)
    
    * llama : make general.name optional
    
    * train: Add 'general.name' to model metadata
    
    Signed-off-by: teleprint-me <77757836+teleprint-me@users.noreply.github.com>
    
    ---------
    
    Signed-off-by: teleprint-me <77757836+teleprint-me@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit bca40e98149c7b673558ddd7a3ebeffef789349d
Author: Neo Zhang <14088817+arthw@users.noreply.github.com>
Date:   Fri Apr 19 09:16:31 2024 +0800

    fix wrong parameter in cmd in readme-sycl.md (#6755)
    
    Co-authored-by: jianyuzh <jianyu.zhang@intel.com>

commit 0d56246f4b9764158525d894b96606f6163c53a8
Author: slaren <slarengh@gmail.com>
Date:   Thu Apr 18 15:18:48 2024 +0200

    ggml : group all experts in a single ggml_mul_mat_id (#6505)
    
    * ggml : group all experts in a single ggml_mul_mat_id
    cuda : improve mmid row copy
    
    * cuda : fix bin bcast with non-cont src0
    
    * test-backend-ops : only run all mul mat tests for base types
    
    * llama : disable moe offloading with SYCL
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 03c0946d73c63ea73e1d85015b7088298443d438
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Thu Apr 18 13:49:01 2024 +0200

    convert : support models with multiple chat templates (#6588)
    
    * Support converting models with multiple chat templates
    
    Adds the following metadata:
    * tokenizer.chat_templates
    * tokenizer.chat_template.<name1>
    * tokenizer.chat_template.<name2>
    * tokenizer.chat_template.<...>
    
    Where `tokenizer.chat_templates` is an array of the template names (except `default`), `default` is added to the regular `tokenizer.chat_template`.
    
    * replace filtered characters with underscore
    
    * New script to add/modify/remove metadata
    
    This scripts creates a copy of a GGUF file and allows you to add/modify/remove metadata in the process.
    
    Most importantly this allows you to update chat templates, either as a string or directly from an updated tokenizer_config.json file.
    
    * Add files via upload
    
    add new script to project/readme
    
    * flake--

commit e11b2e6e1e18522ca7cf129600875a0f6fb9307d
Author: Ren Xuancheng <jklj077@users.noreply.github.com>
Date:   Thu Apr 18 19:38:04 2024 +0800

    Qwen2 : assume tied weights if lm_head/output weights is missing (#6738)

commit c71bfd736ee99a56e697697b39240f2ee06ed26d
Author: slaren <slarengh@gmail.com>
Date:   Thu Apr 18 09:04:47 2024 +0200

    llama : fix compatibility with old 2 expert models (#6735)

commit 3b8f1ec4b18770531d0b1d792f3edf08254e4f0c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 17 23:58:26 2024 +0300

    llamafile : tmp disable + build sgemm.o when needed (#6716)
    
    * build : sgemm.o only when needed
    
    ggml-ci
    
    * llamafile : tmp disable due to MoE bug
    
    ggml-ci

commit 8dd1ec8b3ffbfa2d26e82e672cea89f5eeb2f141
Author: Yaroslav <yaroslav.yashin@me.com>
Date:   Wed Apr 17 14:47:50 2024 +0200

    readme : add UI (#6724)
    
    * Update README.md
    
    * Update README.md
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit facb8b56f8fd3bb10a693bf0943ae9d69d0828ef
Author: Zheng.Deng <32841220+dengzheng-cloud@users.noreply.github.com>
Date:   Wed Apr 17 04:51:07 2024 +0800

    convert : fix autoawq gemma (#6704)
    
    * fix autoawq quantized gemma model convert error
    
    using autoawq to quantize gemma model will include a lm_head.weight tensor in model-00001-of-00002.safetensors. it result in this situation that convert-hf-to-gguf.py can't map lm_head.weight. skip loading this tensor could prevent this error.
    
    * change code to full string match and print necessary message
    
    change code to full string match and print a short message to inform users that lm_head.weight has been skipped.
    
    ---------
    
    Co-authored-by: Zheng.Deng <32841220+CUGfred@users.noreply.github.com>

commit 532c1737a14bb4b99747e6f460874947df37e450
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 16 23:50:38 2024 +0300

    llama : make general.name optional (#6709)

commit 666867b799ddd9da7dfdc905ece291ecf286effa
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 16 23:50:22 2024 +0300

    ggml : fix llamafile sgemm wdata offsets (#6710)
    
    ggml-ci

commit 8cc91dc63c0df397d644a581b2cbeea74eb51ae0
Author: Justine Tunney <jtunney@mozilla.com>
Date:   Tue Apr 16 14:55:30 2024 -0400

    ggml : add llamafile sgemm (#6414)
    
    This change upstreams llamafile's cpu matrix multiplication kernels
    which improve image and prompt evaluation speed. For starters, Q4_0
    and Q8_0 weights should go ~40% faster on CPU. The biggest benefits
    are with data types like f16 / f32, which process prompts 2x faster
    thus making them faster than quantized data types for prompt evals.
    
    This change also introduces bona fide AVX512 support since tinyBLAS
    is able to exploit the larger register file. For example, on my CPU
    llama.cpp llava-cli processes an image prompt at 305 tokens/second,
    using the Q4_K and Q4_0 types, which has always been faster than if
    we used f16 LLaVA weights, which at HEAD go 188 tokens/second. With
    this change, f16 LLaVA performance leap frogs to 464 tokens/second.
    
    On Intel Core i9-14900K this change improves F16 prompt perf by 5x.
    For example, using llama.cpp at HEAD with Mistral 7b f16 to process
    a 215 token prompt will go 13 tok/sec. This change has fixes making
    it go 52 tok/sec. It's mostly thanks to my vectorized outer product
    kernels but also because I added support for correctly counting the
    number of cores on Alderlake, so the default thread count discounts
    Intel's new efficiency cores. Only Linux right now can count cores.
    
    This work was sponsored by Mozilla who's given permission to change
    the license of this code from Apache 2.0 to MIT. To read more about
    what's improved, and how it works, see: https://justine.lol/matmul/

commit dbceec87c0221ec952e69448df6a71f1372a7487
Author: Ashish <1856117+ashishdatta@users.noreply.github.com>
Date:   Tue Apr 16 08:48:35 2024 -0700

    llama : add StableLM2 12B (#6635)
    
    * StableLM2 12B support for huggingface -> GGUF
    
    * StableLM12 tensormapping and constants
    
    * StableLM-2-12b model support
    
    * fix
    
    * Added 12B support
    
    * Removed autoformatting; resolved bug where model_arch was not selecting StableLM2
    
    * Formatting
    
    * Do QK norm stacking in model conversion step
    
    * Converge StableLM and StableLM2 code to simplify graph construction
    
    * Fix accidental removal
    
    * Removed warnings
    
    * Revert formatter
    
    * Move QK norm stack to private function so it's easier to read
    
    * refactor stablelm graph builder to support 1.6, 3b and 12b more efficiently
    
    * Proper check for None type for new_name to avoid crash; formatting; revert change to base class `write_tensors()`
    
    * Format
    
    * Formatting
    
    * format
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Fix incorrect check for K norm
    
    * space after commas; Keep indentation multiple of 4 spaces
    
    * Flake8 format
    
    * Removed unnecessary conditional branches
    
    * Removed unused comment
    
    * Fixed incorrect tensor passing
    
    * Format
    
    ---------
    
    Co-authored-by: compilade <git@compilade.net>

commit f4dea7da1841a92d2788b0535063abf2f0e28461
Author: Shijie <821898965@qq.com>
Date:   Tue Apr 16 23:40:48 2024 +0800

    llama : add qwen2moe (#6074)
    
    * support qwen2moe
    
    * fix-review
    
    * metal : support unary ops for nelements % 4 != 0
    
    * metal : require contiguousness for float4 unary kernels
    
    * metal : require contiguousness for float4 unary kernels (cont)
    
    * fix-review
    
    * names : for brevity "SHARED_EXP" -> "SHEXP"
    
    * llama : reuse build_moe_ffn()
    
    * llama : add model type name
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 8a56075b07a8b571bf95a912ffdce4c928c2b414
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Apr 16 08:34:06 2024 +0200

    gritlm : add --outdir option to hf.sh script (#6699)
    
    This commit updates the hf.sh script usage to include the --outdir option
    and specifies the models directory as the output directory.
    
    The motivation for this is to avoid cluttering the root directory with
    model files.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 58227ffdeb4fb89cacb0cffaadf76b1914324ad3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 16 09:28:33 2024 +0300

    perplexity : require positive --ctx-size arg (#6695)

commit 4fbd8098e63670c6ae11a8adc350f5ba191cfda3
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Apr 16 08:13:13 2024 +0200

    gguf : add special tokens metadata for FIM/Infill (#6689)
    
    This commit adds special token metadata for Fill-In-the-Middle
    (FIM)/Infill to the GGUF model.
    
    The motivation for this is that currently there is support for CodeLlama
    but other models exist now like CodeGemma, but the different models use
    different token ids for the special tokens and this commit allows for
    supporting multiple models.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 7593639ce335e8d7f89aa9a54d616951f273af60
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Mon Apr 15 18:35:21 2024 +0100

    `main`: add --json-schema / -j flag (#6659)
    
    * main: add --json-schema / -j
    
    * json: move json-schema-to-grammar to common lib
    
    * json: fix zig build

commit 132f55795e51094954f1b1f647f97648be724a3a
Author: compilade <git@compilade.net>
Date:   Mon Apr 15 08:56:55 2024 -0400

    llama : fix restoring the number of outputs from state files (#6687)

commit 3272896d79465fd2befef3425dac8e83933b7ea4
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Mon Apr 15 14:18:47 2024 +0200

    server : revert "minor layout improvements" (#6684)
    
    This reverts commit b3a96f27f065a828f08c5d89ff60aab5361188fe.

commit 7fc16a2c32650d46e79b382ecc6720f58c82f31b
Author: Steven Prichard <spprichard20@gmail.com>
Date:   Mon Apr 15 05:14:46 2024 -0500

    swift : linux support (#6590)
    
    - Package.swift now supports conditional compilation based on OS
    - Allows for package to be used by SPM on Non-Apple platforms
    
    Co-authored-by: Steven Prichard <steven.prichard@justeattakeaway.com>

commit 17e98d4c96a583d420f12046bc92102381dbd28e
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Mon Apr 15 17:12:26 2024 +0800

    fix mul_mat_id() for new input, make the ut pass (#6682)

commit 1958f7e06ca2d2e3ab5698cc67513ba359144d8e
Author: David Renshaw <dwrenshaw@gmail.com>
Date:   Sun Apr 14 15:24:15 2024 -0400

    llama : add missing kv clear in llama_beam_search (#6664)

commit 04fbc5f23e9708136ff03ebe194c7a7e965a7ca4
Author: Chao Jiang <jc19chaoj@zoho.com>
Date:   Mon Apr 15 00:16:34 2024 +0800

    Add Command R chat template (#6650)
    
    * Add chat template for command-r model series
    
    * Fix indentation
    
    * Add chat template test for command-r models and update the implementation to trim whitespaces
    
    * Remove debug print

commit f184dd920852d6d372b754f871ee06cfe6f977ad
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 14 16:55:30 2024 +0300

    flake.lock: Update (#6669)

commit 422c2aff1c9735853c9d8f5162104e41a364adc4
Author: Dave <dave-fl@users.noreply.github.com>
Date:   Sun Apr 14 07:14:19 2024 -0400

    Added support for GGML_OP_CLAMP in Metal (#6662)
    
    * Added support for GGML_OP_CLAMP in Metal
    
    * Corrected size
    
    ---------
    
    Co-authored-by: dave-fl <dave@Davids-MacBook-Pro.local>

commit 8800226d65d5c98cd34eede6a6c05c78405c52da
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Sun Apr 14 13:12:59 2024 +0200

    Fix --split-max-size (#6655)
    
    * Fix --split-max-size
    
    Byte size calculation was done on int and overflowed.
    
    * add tests.sh
    
    * add examples test scripts to ci run
    
    Will autodiscover examples/*/tests.sh scripts and run them.
    
    * move WORK_PATH to a subdirectory
    
    * clean up before and after test
    
    * explicitly define which scripts to run
    
    * add --split-max-size to readme

commit e689fc4e912feb19085be6894f475a873759cbfe
Author: Jaemin Son <woalsdnd@gmail.com>
Date:   Sun Apr 14 20:12:36 2024 +0900

    [bug fix] convert github repository_owner to lowercase (#6673)

commit a4ec34e1cd68560e7ff16e21ddc6962b6abd3d1d
Author: James A Capozzoli <157492257+jac-jim@users.noreply.github.com>
Date:   Sun Apr 14 04:40:18 2024 -0400

    convert : enable the `--use-temp-file` cli flag (#6645)

commit de17e3f7455dc7fd298cc61d86798533b9ca7a29
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Sun Apr 14 10:42:29 2024 +0800

    fix memcpy() crash, add missed cmd in guide, fix softmax (#6622)
    
    * disable mmap to fix memcpy crash, add missed cmd in guide, fix softmax
    
    * refactor to disable mmap for SYCL backend
    
    * fix compile error in other os
    
    * refactor the solution, use host buf to fix it, instead of disable mmap
    
    * keep to support mmap()
    
    * use host buff to reduce malloc times
    
    * revert to malloc/free solution, for threaad safe

commit b5e7285baffb0da8a6619567b52d8e67de41291d
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Apr 14 00:21:55 2024 +0200

    CUDA: fix matrix multiplication logic for tests (#6667)

commit 4bd0f93e4ab4fe6682e7d0241c1bdec1397e954a
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Apr 13 11:33:52 2024 +0200

    model: support arch `DbrxForCausalLM` (#6515)
    
    * model: dbrx convert to gguf
    #6344
    
    * llama: support dbrx
    #6344
    
    * doc: dbrx: add the model as supported
    
    * scripts: get-wikitext-2 add unzip
    
    * llama: increase maximum experts allowed
    
    * llama: factorize moe graph implementation between grok, mixtral and dbrx
    
    
    ---------
    
    Co-authored-by: Megha Agarwal <16129366+megha95@users.noreply.github.com>

commit ab9a3240a9da941fdef5cd4a25f2b97c2f5a67aa
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Fri Apr 12 19:43:38 2024 +0100

    JSON schema conversion: ⚡️ faster repetitions, min/maxLength for strings, cap number length (#6555)
    
    * json: rename python schema converter to make import easier
    
    * server: skip null json_schema / grammar fields
    
    * json: deps management for primitive rules (+ allow null values)
    
    * json: optimize repetitions for minItems/maxItems and regexps: `a{,3}` goes from `"a"? "a"? "a"?` (explosive combos) to `(a (a (a)?)?)?`
    
    * grammars: add troubleshooting section to readme
    
    * json: cap length of numbers to 15 digits before/after decimal point
    
    (avoids infinite gen, e.g. "one third" -> `0.333333333333...`)
    
    * json: unify all repetition code (w/ or w/o sep)
    
    * json: support string minLength/maxLength
    
    * server+json: update server/README w/ result_format
    
    * nits
    
    * json: fix type error w/ python 3.8
    
    * json: fix server/README (json_schema in /completion vs. result_format in /v1/chat/completions)
    
    * json: simplify DOT `{"type": "string", "pattern": "^.$"}`
    
    * json: remove recursion in opt_repetitions (avoids Python stack overflow)
    
    * json: rm dead code
    
    * json: rm useless assert & ggml.h import

commit fbbc030ba93561fac842af994c5c6c4c1147f13b
Author: slaren <slarengh@gmail.com>
Date:   Fri Apr 12 18:13:20 2024 +0200

    metal : unify mul_mv_id kernels (#6556)

commit 4cc120c7443cf9dab898736f3c3b45dc8f14672b
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Apr 12 14:11:46 2024 +0200

    infill : add download instructions for model (#6626)
    
    * infill : add download instructions for model
    
    This commit adds instructions on how to download a CodeLlama model
    using the `hf.sh` script. This will download the model and place it
    in the `models` directory which is the same model use later by the
    infill example.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! infill : add download instructions for model
    
    Clarify the reason for using CodeLlama.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 24ee66ed0d908d156bd0d1747b63a636a495cd7a
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Apr 12 13:49:21 2024 +0200

    server : coherent log output for KV cache full (#6637)

commit 91c736015b66ba1d0b82cbae6313b6d5eaa61b68
Author: jiez <373447296@qq.com>
Date:   Fri Apr 12 18:45:06 2024 +0800

    llama : add gguf_remove_key + remove split meta during quantize (#6591)
    
    * Remove split metadata when quantize model shards
    
    * Find metadata key by enum
    
    * Correct loop range for gguf_remove_key and code format
    
    * Free kv memory
    
    ---------
    
    Co-authored-by: z5269887 <z5269887@unsw.edu.au>

commit 5c4d767ac028c0f9c31cba3fceaf765c6097abfc
Author: Rene Leonhardt <65483435+reneleonhardt@users.noreply.github.com>
Date:   Fri Apr 12 10:52:36 2024 +0200

    chore: Fix markdown warnings (#6625)

commit ef21ce4ccb41164cb52997bd2210d92bc6a6c5d1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Apr 12 11:49:58 2024 +0300

    imatrix : remove invalid assert (#6632)

commit dee7f8d6928cc680cc969f7d93f98c3e24dcad41
Author: MasterYi1024 <39848311+MasterYi1024@users.noreply.github.com>
Date:   Fri Apr 12 16:28:12 2024 +0800

    Correct free memory and total memory. (#6630)
    
    Co-authored-by: MasterYi <zouxiaoyi@kylinos.cn>

commit 81da18e71ccfc196d4516fbea5dc3a6a1f92dccb
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Apr 12 10:26:47 2024 +0200

    eval-callback: use ggml_op_desc to pretty print unary operator name (#6631)

commit 9ed2737acc233716374860e6b2ea7399c4aae29e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Apr 12 11:15:05 2024 +0300

    ci : disable Metal for macOS-latest-cmake-x64 (#6628)

commit 04a5ac211ef40936295980b7cdf0ba6e97093146
Author: Clint Herron <hanclinto@gmail.com>
Date:   Thu Apr 11 21:44:50 2024 -0400

    Optimization: eliminate addition of redundant stacks when advancing grammar. (#6616)

commit f7001ccc5aa359fcf41bba19d1c99c3d25c9bcc7
Author: Clint Herron <hanclinto@gmail.com>
Date:   Thu Apr 11 17:44:48 2024 -0400

    As suggested by @slaren, disabling Metal for test to fix CI build on OSX from #6576 (#6619)

commit a474f50ebb3e10be3371562f75f3f573f1a86b5f
Author: Nikolas <127742645+nneubacher@users.noreply.github.com>
Date:   Thu Apr 11 21:56:29 2024 +0200

    Refactor Error Handling for CUDA (#6575)
    
    * Refactor Error Handling for CUDA
    
    Add guidance for setting CUDA_DOCKER_ARCH to match GPU compute capability for CUDA versions < 11.7. Include link to NVIDIA's CUDA GPUs documentation for compute capability reference.
    
    * Update Makefile
    
    Improved wording
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

commit cbaadc92942c50aab599a9e4c163afc1f44f7c26
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Thu Apr 11 19:47:34 2024 +0100

    grammars: 1.5x faster inference w/ complex grammars (vector reserves / reuses) (#6609)
    
    * grammars: reserve rejects & next candidates
    
    * grammars: reuse new_stacks
    
    * grammars: fix missing sig change in llama.h
    
    * grammars: fix test (api changed)
    
    * grammars: update gbnf-validator.cpp
    
    * grammars: simpler syntax (no swap)

commit 1bbdaf6ecda6f0a360dfb307b256fcb6838c560b
Author: Hugo Roussel <hugo.rous@gmail.com>
Date:   Thu Apr 11 19:52:21 2024 +0200

    ci: download artifacts to release directory (#6612)
    
    When action download-artifact was updated to v4, the default download path changed.
    This fix binaries not being uploaded to releases.

commit f4183afe6a22f356ee222a710686ae7f83dbd949
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Apr 11 15:22:47 2024 +0200

    scripts : add --outdir option to hf.sh (#6600)
    
    * scripts : add --outdir option to hf.sh
    
    This commit adds an option to the hf.sh script that allows the user to
    specify an output directory for the downloaded file.
    
    The motivation for this changes is that examples that use the hf.sh
    script to download models from huggingface can now specify the output
    directory, perhaps to the `models` directory to keep them in one place
    and not clutter the root directory.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! scripts : add --outdir option to hf.sh
    
    Fix format of the --outdir option in the usage message.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit b804b1ef77351d2a11be945462c6c251710476cb
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Thu Apr 11 14:51:07 2024 +0200

    eval-callback: Example how to use eval callback for debugging (#6576)
    
    * gguf-debug: Example how to use ggml callback for debugging
    
    * gguf-debug: no mutex, verify type, fix stride.
    
    * llama: cv eval: move cb eval field in common gpt_params
    
    * ggml_debug: use common gpt_params to pass cb eval.
    Fix get tensor SIGV random.
    
    * ggml_debug: ci: add tests
    
    * ggml_debug: EOL in CMakeLists.txt
    
    * ggml_debug: Remove unused param n_batch, no batching here
    
    * ggml_debug: fix trailing spaces
    
    * ggml_debug: fix trailing spaces
    
    * common: fix cb_eval and user data not initialized
    
    * ci: build revert label
    
    * ggml_debug: add main test label
    
    * doc: add a model: add a link to ggml-debug
    
    * ggml-debug: add to make toolchain
    
    * ggml-debug: tests add the main label
    
    * ggml-debug: ci add test curl label
    
    * common: allow the warmup to be disabled in llama_init_from_gpt_params
    
    * ci: add curl test
    
    * ggml-debug: better tensor type support
    
    * gitignore : ggml-debug
    
    * ggml-debug: printing also the sum of each tensor
    
    * ggml-debug: remove block size
    
    * eval-callback: renamed from ggml-debug
    
    * eval-callback: fix make toolchain
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 8228b66dbc16290c5cbd70e80ab47c068e2569d8
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed Apr 10 20:16:48 2024 +0200

    gguf : add option to not check tensor data (#6582)
    
    This commit adds an option to the gguf example to not check the tensor
    data.
    
    The motivation for this is that it can be nice to use the gguf tool to
    read other .gguf files that were not created by the gguf tool.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit b3a96f27f065a828f08c5d89ff60aab5361188fe
Author: Ralph Soika <ralph.soika@imixs.com>
Date:   Wed Apr 10 19:18:25 2024 +0200

    minor layout improvements (#6572)
    
    * minor layout improvements
    
    * added missing file, run deps.sh locally

commit 4f407a0a353dae4726c74cc33250b623a4911dd7
Author: slaren <slarengh@gmail.com>
Date:   Wed Apr 10 17:24:14 2024 +0200

    llama : add model types for mixtral (#6589)

commit 65c64dc36f9bca5b3f100614cdd02bf12d6b3e49
Author: slaren <slarengh@gmail.com>
Date:   Wed Apr 10 15:23:12 2024 +0200

    convert.py : add consolidated.safetensors for mixtral 8x22b (#6587)

commit 67fac4b95fcccfda8ab965e9ba4992a9ddf3a25f
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Wed Apr 10 08:58:48 2024 +0200

    docs : how to add a model (#6565)
    
    * docs: how to add a model
    
    * docs: model: typo and docs
    
    * docs: model: add prevision on RoPE
    
    * docs: model: rephrasing README.md
    
    * docs: model: rephrasing README.md
    
    * docs: model: README.md fix trailing spaces
    
    * docs : some fixes
    
    * Update README.md
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 29122d32ac8075ad27b7a8be05dcad2b7e7a5f9e
Author: Artem Zinnatullin <ceo@abstractny.gay>
Date:   Wed Apr 10 00:49:12 2024 -0600

    readme : fix ROCm link (#6579)

commit b231b37b095f172b26b1300ca442a147ea048b64
Author: sjxx <63994076+ylsdamxssjxxdd@users.noreply.github.com>
Date:   Wed Apr 10 14:34:00 2024 +0800

    readme : update UI list (#6560)

commit ba5e134e073ec6837078c874aba44a702944a676
Author: Jiří Sejkora <Sejseloid@gmail.com>
Date:   Wed Apr 10 00:23:02 2024 +0200

    readme: fix typo in amdgpu target name (#6573)

commit 1b67731e184e27a465b8c5476061294a4af668ea
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Tue Apr 9 13:44:08 2024 -0400

    BERT tokenizer fixes (#6498)
    
    Key changes:
    * BERT conversion: fix abuse of LlamaHfVocab, do not set BOS or EOS
    * Nomic Embed conversion: pad vocab instead of slicing embedding tensor
    * llama_tokenize: handle added special tokens like HF does

commit c4a3a4ff47d62d2503ddf9bd91b58c21f04fe3c3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 9 20:29:06 2024 +0300

    sync : ggml

commit 400d5d722d7edf7de0cf24a18c42b183c65047d2
Author: Ed Lee <edilee@mozilla.com>
Date:   Tue Apr 9 01:31:47 2024 -0700

    server : detect search query to start webchat (#6554)

commit 5dc9dd7152dedc6046b646855585bd070c91e8c8
Author: Carolinabanana <140120812+Carolinabanana@users.noreply.github.com>
Date:   Tue Apr 9 09:16:13 2024 +0100

    llama : add Command R Plus support (#6491)
    
    * Add Command R Plus GGUF
    
    * Add Command R Plus GGUF
    
    * Loading works up to LayerNorm2D
    
    * Export new tensors in 1D so they are not quantized.
    
    * Fix embedding layer based on Noeda's example
    
    * Whitespace
    
    * Add line
    
    * Fix unexpected tokens on MPS. Re-add F16 fix. ((Noeda)
    
    * dranger003: Fix block index overflow in CUDA dequantizing.
    
    * Reverted blocked multiplication code as it still has issues and could affect other Llama arches
    
    * export norms as f32
    
    * fix overflow issues during quant and other cleanup
    
    * Type convention
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * dranger003: Fix more int overflow during quant.
    
    ---------
    
    Co-authored-by: S <seast@Ss-Mac-Studio.local>
    Co-authored-by: S <s@example.com>
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit e11a8999b5690f810c2c99c14347f0834e68c524
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 9 09:23:19 2024 +0300

    license : update copyright notice + add AUTHORS (#6405)
    
    * license : add AUTHORS
    
    * authors : update
    
    * scipts : add LICENSE and gen-authors.sh to sync

commit cc4a95426d17417d3c83f12bdb514fbe8abe2a88
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 8 22:25:49 2024 +0300

    llama : fix attention layer count sanity check (#6550)
    
    * llama : fix attention layer count sanity check
    
    * llama : fix parentheses in attention layer count sanity check
    
    There was otherwise a warning when compiling.
    
    ---------
    
    Co-authored-by: Francis Couture-Harpin <git@compilade.net>

commit cecd8d3c98b48f51aaa1d4c729e55bd319f6799c
Author: kunnis <kunnis@users.noreply.github.com>
Date:   Mon Apr 8 10:44:19 2024 -0500

    Comment explaining a decision (#6531)

commit b73e564b16086845a8b4fffd26e22685d3e0c3db
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 8 16:23:01 2024 +0300

    quantize : fix precedence of cli args (#6541)

commit e3c337d87ca650972105a51c6ce302dd236c07ad
Author: Rick G <26732651+TheFlipbook@users.noreply.github.com>
Date:   Mon Apr 8 06:02:30 2024 -0700

    llama : support negative ith in llama_get_ API (#6519)
    
    * llama_sampling_sample with default args is more naively usable
    
    * Batches populated by either llama_batch_get_one or llama_batch_add work with default args
      * Previously get_one could use the default argument
      * Previously add should usually have used the last index where logits[idx] == true
    * This hopefully encourages the use of llama_batch_add
      * By giving expected results when using default arguments.
    * Adds "negative indexing" feature to llama_get_logits_ith and llama_get_embeddings_ith
    * Believed to work with any currently well behaved program
      * Default arg now works for both cases (previously would give strange results for add case)
      * Any non-negative number is unaffected and behaves as previously
      * Negative arguments were previously invalid.
    * Implemented as a special case of indexing as suggested by @compilade in https://github.com/ggerganov/llama.cpp/pull/6519
    
    * Fixed mismatch type errors
    
    * cited in macOS CI tests
    * Missed in original updates based on PR feedback in https://github.com/ggerganov/llama.cpp/pull/6519

commit beea6e1b16e783a0886e78dec01002a8c00db24d
Author: Jan Boon <jan.boon@kaetemi.be>
Date:   Mon Apr 8 20:43:30 2024 +0800

    llama : save and restore kv cache for single seq id (#6341)
    
    * llama : save and restore kv cache for single seq id
    
    * remove trailing whitespace
    
    * respond error in case there's no space in the kv cache
    
    * add kv seq save restore to test case
    
    * add --slot-save-path arg to enable save restore and restrict save location
    
    * Returning 0 for some cases, instead of asserting.
    
    * cleanup error cases
    
    * rename sequence state functions
    
    * rename state get set functions
    
    * add previous function names back in with DEPRECATED notice
    
    * update doc
    
    * adjust endpoints to preferred style
    
    * fix restoring zero cell count
    
    * handle seq rm return value
    
    * unused param
    
    * keep in the size check
    
    * fix return types
    
    * add server test case for slot save restore
    
    * cleanup
    
    * add cake
    
    * cleanup style
    
    * add special
    
    * removing a whole sequence never fails
    
    * move sequence state file functionality from server to llama to match session api and add version tags
    
    * catch exceptions on save as well
    
    * error log messages
    
    * check types for stricter restore
    
    * update server doc
    
    * readme : update API changes date
    
    * strict filename validation
    
    * move include, reject bom as well
    
    * also reject empty filename
    
    * reject whitespace and trailing dot
    
    ---------
    
    Co-authored-by: Martin Evans <martindevans@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 87fb5b4234d4b9c56ac94cf7aa229c8fd7defdb0
Author: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
Date:   Mon Apr 8 13:56:01 2024 +0530

    remove row=1 cond (#6532)

commit d752327c3338d5b9634121d651c0105f2c933f9b
Author: Firat <firatkiral@gmail.com>
Date:   Mon Apr 8 00:48:29 2024 -0700

    Adding KodiBot to UI list (#6535)
    
    KodiBot is free and open source ai chat app released under the GNU General Public License.

commit 855f54402e866ed19d8d675b56a81c844c64b325
Author: Mark Fairbairn <thebaron88@gmail.com>
Date:   Sun Apr 7 19:52:19 2024 +0100

    Change Windows AMD example to release build to make inference much faster. (#6525)

commit b909236c0bf0b6e872af95df9490492ecec310ac
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 7 21:25:30 2024 +0300

    flake.lock: Update (#6517)
    
    Flake lock file updates:
    
    • Updated input 'flake-parts':
        'github:hercules-ci/flake-parts/f7b3c975cf067e56e7cda6cb098ebe3fb4d74ca2' (2024-03-01)
      → 'github:hercules-ci/flake-parts/9126214d0a59633752a136528f5f3b9aa8565b7d' (2024-04-01)
    • Updated input 'flake-parts/nixpkgs-lib':
        'github:NixOS/nixpkgs/1536926ef5621b09bba54035ae2bb6d806d72ac8?dir=lib' (2024-02-29)
      → 'github:NixOS/nixpkgs/d8fe5e6c92d0d190646fb9f1056741a229980089?dir=lib' (2024-03-29)
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/d8fe5e6c92d0d190646fb9f1056741a229980089' (2024-03-29)
      → 'github:NixOS/nixpkgs/fd281bd6b7d3e32ddfa399853946f782553163b5' (2024-04-03)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit e0717e751e12af13f4eedaae8bbbd608e40d7e54
Author: DAN™ <dranger003@gmail.com>
Date:   Sun Apr 7 13:33:59 2024 -0400

    Add GritLM as supported models. (#6513)

commit c37247796b4d45bdbbc8259afffb80208ad8fe55
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 7 17:05:51 2024 +0300

    sync : ggml

commit f77261a7c525fa1fa47b18a3d78cd308ae41cafc
Author: Slava Primenko <primenko.s@gmail.com>
Date:   Thu Apr 4 14:49:24 2024 +0200

    ggml: bypass code incompatible with CUDA < 11.1 (whisper/2020)
    
    `cudaHostRegisterReadOnly` parameter was only introduced in CUDA 11.1
    
    See this issue for more details:
    https://github.com/ggerganov/examples/whisper/whisper.cpp/issues/2007

commit 43e8995e754b8e04642e92822055d193a3272b37
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 7 16:08:12 2024 +0300

    scripts : sync ggml-cuda folder

commit 9472bce30800a581071478a839bf93abf404c893
Author: limitedAtonement <limitedAtonement@users.noreply.github.com>
Date:   Sun Apr 7 07:05:40 2024 -0400

    Run make to build the project (#6457)

commit d4f220a5ccdc6308173c1a31fad21d7c3fbc96c1
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Sun Apr 7 10:55:59 2024 +0800

    support/fix OPs GGML_TYPE_IQ4_NL, GGML_TYPE_IQ4_XS, GGML_TYPE_IQ3_XXS, GGML_TYPE_IQ3_S, GGML_TYPE_IQ2_XXS, GGML_TYPE_IQ2_XS, GGML_TYPE_IQ2_S, GGML_TYPE_IQ1_S, GGML_TYPE_IQ1_M (#6521)

commit 54ea0698fbf87e36a5d68a98c95f6bdd0fb91557
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 6 17:43:15 2024 +0300

    sync : ggml

commit b66aec675c1571a06b3570b858ae711246f96f84
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed Apr 3 22:57:20 2024 +0200

    backend : fix typo in scheduler documentation (ggml/781)
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 57dd02c44b2a0eb79e28f6c5eb8242a5d2d3174d
Author: Clint Herron <hanclinto@gmail.com>
Date:   Sat Apr 6 10:31:33 2024 -0400

    Tests: Added integration tests for GBNF parser  (#6472)
    
    * Added integration tests for GBNF parser to validate correctness of parsing, as well as correctness of string matching. Intended for use to pin behavior while working on performance improvements.
    
    * Fixing whitespace errors and cleaning error message alert to be clearer.
    
    * Removing hacky include to llama.cpp from grammar integration test now that needed functions are available via internal API.
    
    * Comment cleanup.
    
    * Reorganizing tests for readability.
    
    * Cleaning up debug message to make a bit more sense.

commit 75cd4c77292034ecec587ecb401366f57338f7c0
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Apr 6 05:40:47 2024 +0200

    ci: bench: support sse and fix prompt processing time / server: add tokens usage in stream OAI response (#6495)
    
    * ci: bench: support sse and fix prompt processing time
    server: add tokens usage in stream mode
    
    * ci: bench: README.md EOL
    
    * ci: bench: remove total pp and tg as it is not accurate
    
    * ci: bench: fix case when there is no token generated
    
    * ci: bench: change to the 95 percentile for pp and tg as it is closer to what the server exports in metrics
    
    * ci: bench: fix finish reason rate

commit a8bd14d55717754a1f48313a846a2b16fa998ad2
Author: Brian <mofosyne@gmail.com>
Date:   Sat Apr 6 05:41:38 2024 +1100

    gguf.py : add licence and version to gguf writer (#6504)

commit d0f5deebf898f8186a10148a03a56909ba05fc0b
Author: Hoang Nguyen <hugo53@users.noreply.github.com>
Date:   Fri Apr 5 11:39:43 2024 -0700

    readme : update UI list (#6503)
    
    * Add MindMac to UI list
    
    * Update proprietary description
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 87e21bbacd830437ab653cf03b6f26d45c15395d
Author: Ting Sun <suntcrick@gmail.com>
Date:   Sat Apr 6 01:34:53 2024 +0700

    bench : make n_batch and n_ubatch configurable in Batched bench (#6500)
    
    * bench: make n_batch and n_ubatch configurable
    
    * bench: update doc for batched bench

commit 1b496a745c315022df2d919374052e6004ced8d3
Author: Ouadie EL FAROUKI <ouadie.elfarouki@codeplay.com>
Date:   Fri Apr 5 14:35:06 2024 +0100

    [SYCL] Fixed minor bug when enabling FP16 for non intel targets (#6464)
    
    * moved INTEL_MKL guard from gemm_impl to gemm (wrapper)
    
    * Update ggml-sycl.cpp
    
    Co-authored-by: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
    
    ---------
    
    Co-authored-by: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>

commit a307375c02cac45cff53cf2520330b43fecc7718
Author: alexpinel <93524949+alexpinel@users.noreply.github.com>
Date:   Thu Apr 4 18:22:50 2024 +0100

    readme : add Dot to UI list (#6487)

commit b660a5729e1e7508671d3d0515fd7efaeaeb85b9
Author: Jun Jie <71215065+junnjiee16@users.noreply.github.com>
Date:   Fri Apr 5 01:16:37 2024 +0800

    readme : fix typo (#6481)

commit 0a1d889e27d6aaa3293dd2c692b849a9bcf4b474
Author: Ed Lepedus <ed.lepedus@googlemail.com>
Date:   Thu Apr 4 17:31:22 2024 +0100

    server: add cURL support to server Dockerfiles (#6474)
    
    * server: add cURL support to `full.Dockerfile`
    
    * server: add cURL support to `full-cuda.Dockerfile` and `server-cuda.Dockerfile`
    
    * server: add cURL support to `full-rocm.Dockerfile` and `server-rocm.Dockerfile`
    
    * server: add cURL support to `server-intel.Dockerfile`
    
    * server: add cURL support to `server-vulkan.Dockerfile`
    
    * fix typo in `server-vulkan.Dockerfile`
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 7dda1b727ef1730783a6077136d28b83e70dd397
Author: Minsoo Cheong <54794500+mscheong01@users.noreply.github.com>
Date:   Fri Apr 5 01:30:53 2024 +0900

    ci: exempt master branch workflows from getting cancelled (#6486)
    
    * ci: exempt master branch workflows from getting cancelled
    
    * apply to bench.yml

commit c666ba26c39d5c07e07b4e1e411332f408e309ad
Author: Ewout ter Hoeven <E.M.terHoeven@student.tudelft.nl>
Date:   Thu Apr 4 17:08:55 2024 +0200

    build CI: Name artifacts (#6482)
    
    Name the artifacts in the build CI, so that they get uploaded with separate names, instead of all put into the same `artifact` ZIP.
    
    It might be possible to further simplify the packing step (in future PRs).

commit 2e66913e5f56209f4c949f98e431925b78e7e84d
Author: Shakhar Dasgupta <shakhardasgupta@gmail.com>
Date:   Thu Apr 4 11:03:00 2024 -0400

    server: allow penalizing repetition of newlines on server webpage (#6431)

commit 8120efee1d9931b514aeb5a047209d576f23286c
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Thu Apr 4 16:59:04 2024 +0200

    ci: bench fix concurrency for workflow trigger dispatch with sha1 (#6478)

commit a74401f0e5ebb15fa4d8b6619d1baa6ea9179123
Author: limitedAtonement <limitedAtonement@users.noreply.github.com>
Date:   Thu Apr 4 10:30:02 2024 -0400

    Correct README link (#6458)
    
    README is called README.md.

commit 7a2c92637ae265654a68f62e6a7610b358255d3f
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Thu Apr 4 11:57:58 2024 +0200

    ci: bench: add more ftype, fix triggers and bot comment (#6466)
    
    * ci: bench: change trigger path to not spawn on each PR
    
    * ci: bench: add more file type for phi-2: q8_0 and f16.
    - do not show the comment by default
    
    * ci: bench: add seed parameter in k6 script
    
    * ci: bench: artefact name perf job
    
    * Add iteration in the commit status, reduce again the autocomment
    
    * ci: bench: add per slot metric in the commit status
    
    * Fix trailing spaces

commit 4bcd6b959ca3991084ad1d8464caf2a734e29b1d
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Apr 4 09:49:21 2024 +0200

    common: remove duplicate check for curl (#6471)
    
    This commit removes one of the two identical checks for curl being NULL
    in llama_load_model_from_url.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 9b84ae1806cded4d6683c7b810925da5ead40607
Author: Clint Herron <hanclinto@gmail.com>
Date:   Thu Apr 4 03:44:28 2024 -0400

    examples : add GBNF validator program (#5948)
    
    * Revising GBNF validator program to be much simpler.
    
    * Changing from streams to using cstdio
    
    * Adding final newline character.

commit 4399f13fb9462cd06f3f154d0aee738425000fea
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 4 09:34:58 2024 +0300

    server : remove obsolete --memory-f32 option

commit 1a43c7254ed5de91d09274b853db843c518f1b64
Author: Xiao-Yong Jin <jinxiaoyong@gmail.com>
Date:   Thu Apr 4 01:33:48 2024 -0500

    server : add option to disable KV offload (#6468)

commit 72d73af65132792a52433952ca4729b01c36cde2
Author: Clint Herron <hanclinto@gmail.com>
Date:   Thu Apr 4 02:32:53 2024 -0400

    convert : fix for lint error complaining of bare except (#6470)

commit 5fb1574c8112c757fc202fdae279da883f34e610
Author: Fattire <528174+fat-tire@users.noreply.github.com>
Date:   Wed Apr 3 13:22:57 2024 -0700

    A few small fixes to server's README docs (#6428)
    
    * Typo fix to server's README.md
    
    Fix minor typo ("tonen") in server README.
    
    * server readme grammar/style fixes.
    
    Quickly went through this file to look for inconsistencies in
    presentation of defaults, flag options, and looked for typos
    and grammar issues.
    
    Not perfect, but hopefully improved.
    
    * Update README.md
    
    Remove an extra space before newline.

commit 60cdf40cc32f0ad4cb11e0ca8fd38f3b93d8d640
Author: JH23X <165871467+JH23X@users.noreply.github.com>
Date:   Wed Apr 3 20:09:52 2024 +0200

    server : handle exception on wrong type in request (#6452)
    
    Co-authored-by: Jonas Holzner <jonas.holzner.external@hensoldt.net>

commit bb43cf7e9d86d69ffd9c7f008f75db890a35b45a
Author: bryanSwk <93190252+bryanSwk@users.noreply.github.com>
Date:   Thu Apr 4 02:05:10 2024 +0800

    llama : add SEA-LION support (#6448)
    
    * initial commit for sealion support
    
    * add sealion support
    
    * minor fix
    
    * q/k ln and pos_embd only if required
    
    * Apply suggestions from code review
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * minor : clear whitespaces
    
    ---------
    
    Co-authored-by: bryan <bryansiow@aisingapore.org>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 9f62c0173d964972849251c8ad12fc356f5b7896
Author: Ewout ter Hoeven <E.M.terHoeven@student.tudelft.nl>
Date:   Wed Apr 3 20:01:13 2024 +0200

    ci : update checkout, setup-python and upload-artifact to latest (#6456)
    
    * CI: Update actions/checkout to v4
    
    * CI: Update actions/setup-python to v5
    
    * CI: Update actions/upload-artifact to v4

commit 5d4f12e4624bf4435ba26753814bedd4e9b62803
Author: Ed Lepedus <ed.lepedus@googlemail.com>
Date:   Wed Apr 3 18:56:37 2024 +0100

    server: add cURL support to `server.Dockerfile` (#6461)

commit 154d4ee39c38e231fb5c3910df14d2fc920fdf1b
Author: Francisco Melo <43780565+francis2tm@users.noreply.github.com>
Date:   Wed Apr 3 18:53:37 2024 +0100

    readme : add feature-rich rust bindings (#6465)

commit e69945d953b651674d6e6978022edf00c3a34d03
Author: Joyce <joycebrum@google.com>
Date:   Wed Apr 3 14:48:07 2024 -0300

    security : create policy (#6354)
    
    * Create SECURITY.md
    
    Signed-off-by: Joyce <joycebrum@google.com>
    
    * Fix: link on SECURITY.md
    
    Signed-off-by: Joyce <joycebrum@google.com>
    
    * Fix: link on SECURITY.md
    
    Signed-off-by: Joyce <joycebrum@google.com>
    
    * minor
    
    * fix
    
    * fix
    
    ---------
    
    Signed-off-by: Joyce <joycebrum@google.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit db214fa578e00b01e0884fc2725c9349608bdab5
Author: Abhishek Gopinath K <31348521+overtunned@users.noreply.github.com>
Date:   Wed Apr 3 21:12:52 2024 +0530

    Missing tokenizer.model error during gguf conversion (#6443)
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>

commit 1ff4d9f3d683f02ef8a12e04bfac84300c44bd3a
Author: kaizau <kaizau@users.noreply.github.com>
Date:   Wed Apr 3 23:24:31 2024 +0800

    Add OpenChat, Alpaca, Vicuna chat templates (#6397)
    
    * Add openchat chat template
    
    * Add chat template test for openchat
    
    * Add chat template for vicuna
    
    * Add chat template for orca-vicuna
    
    * Add EOS for vicuna templates
    
    * Combine vicuna chat templates
    
    * Add tests for openchat and vicuna chat templates
    
    * Add chat template for alpaca
    
    * Add separate template name for vicuna-orca
    
    * Remove alpaca, match deepseek with jinja output
    
    * Regenerate chat template test with add_generation_prompt
    
    * Separate deepseek bos from system message
    
    * Match openchat template with jinja output
    
    * Remove BOS token from templates, unprefix openchat

commit 076b08649ecc3b0e1c0709c2a086a63eddd1bf32
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 3 16:11:15 2024 +0300

    readme : update hot topics

commit 08a0c0206075556e82aca0feafad530dcc5f1426
Author: slaren <slarengh@gmail.com>
Date:   Wed Apr 3 15:07:05 2024 +0200

    ggml : mul_mat_id use the same tensor for all the experts (#6387)
    
    * ggml : update mul_mat_id to use the same tensor for all the experts
    
    * update cuda
    
    * minor
    
    * update metal
    
    * update test-backend-ops
    
    * fix cuda
    
    * Update ggml-metal.m
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * update convert.py
    
    * update convert-hf-to-gguf.py
    
    * update convert.py for mixtral hf models
    
    * Update convert-hf-to-gguf.py
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * cuda : support non-pow-2 number of experts
    
    * allow quantize to work for split and merged experts models in the same way
    
    * cleanup + disable mmap automatically with split tensors models
    
    * update imatrix
    
    * test-backend-ops : test qwen argsort
    
    * update grok model loading
    
    * llama : add merged experts tensors to the grok tensor map
    
    * minor
    
    * gguf : bump version
    
    * fix quantizing of merged experts
    
    * convert-hf-to-gguf.py : update grok (untested)
    
    * make linter happy
    
    * cuda/argsort : use shared memory instead of pool memory
    
    * convert : fix grok tensor names
    
    * metal : add support for non-pow-2 argsort
    
    * llama : more loader cleanup, better error checking
    
    * cuda : fix warning
    
    * llama : still use mmap for loading old models, but copy the data to a host buffer
    
    * add review note
    
    * llama : remove ffn tensor counting + add sanity check
    
    ggml-ci
    
    * convert : fix handling of n_experts == None
    
    ggml-ci
    
    * imatrix : fix ncall counters
    
    * llama : produce error if imatrix size does not match
    
    * quantize : terminate on errors + trace logs
    
    ggml-ci
    
    * metal : pad shared memory to 16 bytes
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 52604860f93063ef98863921da697576af1c7665
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Wed Apr 3 10:34:40 2024 +0800

    [SYCL] Disable iqx on windows as WA (#6435)
    
    * disable iqx on windows as WA
    
    * array instead of global_memory

commit f87f7b898651339fe173ddf016ca826163e899d8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 1 19:05:57 2024 +0300

    flake.lock: Update (#6402)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/44d0940ea560dee511026a53f0e2e2cde489b4d4' (2024-03-23)
      → 'github:NixOS/nixpkgs/d8fe5e6c92d0d190646fb9f1056741a229980089' (2024-03-29)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit 33a52448061cfd2ea44da9e6cb30b2ec22e2f6d0
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Apr 1 13:30:43 2024 +0200

    compare-llama-bench.py: fix long hexsha args (#6424)

commit 226e819371eec0f298d9075198394a07b23ecfa9
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Mon Apr 1 12:36:40 2024 +0200

    ci: server: verify deps are coherent with the commit (#6409)
    
    * ci: server: verify deps are coherent with the commit
    
    * ci: server: change the ref to build as now it's a pull event target

commit c50a82ce0f71558cbb8e555146ba124251504b38
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 31 11:56:30 2024 +0300

    readme : update hot topics

commit 37e7854c104301c5b5323ccc40e07699f3a62c3e
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Mar 30 11:36:07 2024 +0100

    ci: bench: fix Resource not accessible by integration on PR event (#6393)

commit c342d070c64a1ffe35d22c1b16b672e684a30297
Author: Mohammadreza Hendiani <hendiani.mohammadreza@gmail.com>
Date:   Sat Mar 30 01:29:56 2024 +0330

    Fedora build update (#6388)
    
    * fixed deprecated address
    
    * fixed deprecated address
    
    * fixed deprecated address
    
    * Added 'Apache-2.0' SPDX license identifier due to 'kompute.cc' submodule licensing. Explanation of licensing method: https://docs.fedoraproject.org/en-US/legal/spdx/#_and_expressions
    
    * Added 'Apache-2.0' SPDX license identifier due to 'kompute.cc' submodule licensing. Explanation of licensing method: https://docs.fedoraproject.org/en-US/legal/spdx/#_and_expressions
    
    * Added 'Apache-2.0' SPDX license identifier due to 'kompute.cc' submodule licensing. Explanation of licensing method: https://docs.fedoraproject.org/en-US/legal/spdx/#_and_expressions
    
    * reverted back to only the MIT license

commit f7fc5f6c6f3700da311de7fe93977c81798f02d3
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Mar 29 22:34:44 2024 +0100

    split: allow --split-max-size option (#6343)
    
    * split by max size
    
    * clean up arg parse
    
    * split: ok
    
    * add dry run option
    
    * error on 0 tensors
    
    * be positive
    
    * remove next_metadata_size

commit ba0c7c70ab5b15f1f2be7fb0dfbe0366dda30d6c
Author: 0cc4m <picard12@live.de>
Date:   Fri Mar 29 17:29:21 2024 +0100

    Vulkan k-quant mmq and ggml-backend offload functionality (#6155)
    
    * Fix Vulkan no kv offload incoherence
    
    * Add k-quant mul mat mat shaders
    
    * Rework working buffer allocation, reduces vram use noticeably
    
    Clean up cpu assist code, replaced with ggml-backend offload function
    
    * Default to all dedicated GPUs
    
    * Add fallback for integrated GPUs if no dedicated GPUs are found
    
    * Add debug info which device is allocating memory
    
    * Fix Intel dequant issue
    
    Fix validation issue
    
    * Fix Vulkan GGML_OP_GET_ROWS implementation
    
    * Clean up merge artifacts
    
    * Remove Vulkan warning

commit d48ccf3ad4fea5b9ede209c7f40be65371987bfe
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 29 17:45:46 2024 +0200

    sync : ggml (#6351)
    
    * sync : ggml
    
    ggml-ci
    
    * cuda : move GGML_CUDA_DMMV constants to dmmv.cuh
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 069574775cea67d8a977904e4d534aff47a671f4
Author: hxer7963 <hxer7963@gmail.com>
Date:   Fri Mar 29 21:37:03 2024 +0800

    [Model] Add support for xverse (#6301)
    
    * Support xverse model convert to gguf format.
    
    * 1. Convert xverse models to gguf;
    2. Add LLM_ARCH_XVERSE inference in llama.cpp;
    3. Add xverse item in Supported models in README.md;
    
    * * gguf-py: remove redundant logs
    * llama: remove the init_mapping_prefetch custom parameter
    
    * llama.cpp: Include the changes from #6122 to exclude the unused outputs of the last layers.
    
    * - Fix format issues
    - Remove duplicate set kqv_out to llm_build_kv
    
    * Update llama.cpp
    
    ---------
    
    Co-authored-by: willhe <willhe@xverse.cn>
    Co-authored-by: willhe <hexin@xverse.cn>

commit cfde806eb95de06d84162bdee593dad33a1d2693
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 29 14:34:28 2024 +0200

    ci : fix BGE wget (#6383)
    
    ggml-ci

commit b910287954d4462fd3d48938336770e258459677
Author: zhouwg <6889919+zhouwg@users.noreply.github.com>
Date:   Fri Mar 29 15:33:46 2024 +0800

    readme : add project (#6356)
    
    * readme: add Android UI binding
    
    * Update README.md

commit 809398709041ee854fbbad9b344bcfdcd3712d59
Author: Matt Clayton <156335168+mattjcly@users.noreply.github.com>
Date:   Fri Mar 29 03:27:42 2024 -0400

    cmake : add explicit metal version options (#6370)
    
    * cmake: add explicit metal version options
    
    * Update CMakeLists.txt
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 057400a3fd457f4f214684eeb171444663b47a23
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Mar 29 08:23:22 2024 +0100

    llama : remove redundant reshape in build_kv_store (#6369)
    
    * llama: remove redundant reshape in build_kv_store
    
    This commit removes the reshape of the V matrix in the build_kv_store.
    
    The motivation for this is that V matrix has the shape:
    ```console
    (gdb) p *v_cur
    $46 = {type = GGML_TYPE_F32, backend = GGML_BACKEND_TYPE_CPU,
           buffer = 0x0, ne = {4096, 512, 1, 1}, nb = {4, 16384, 8388608,
           8388608}, op = GGML_OP_MUL_MAT, op_params = {
           0 <repeats 16 times>}, flags = 0, grad = 0x0,
           src = {0xb496b0, 0x7ffef1c40950, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
           0x0, 0x0}, perf_runs = 0, perf_cycles = 0, perf_time_us = 0,
           view_src = 0x0, view_offs = 0, data = 0x0,
           name = "Vcur-0", '\000' <repeats 57 times>, extra = 0x0,
           padding = "\000\000\000\000\000\000\000"}
    ```
    And after reshaping this tensor we get:
    ```console
    gdb) p *ggml_reshape_2d(ctx, v_cur, n_embd_v_gqa, n_tokens)
    $44 = {type = GGML_TYPE_F32, backend = GGML_BACKEND_TYPE_CPU,
           buffer = 0x0, ne = {4096, 512, 1, 1}, nb = {4, 16384, 8388608,
           8388608}, op = GGML_OP_RESHAPE, op_params = {
           0 <repeats 16 times>}, flags = 0, grad = 0x0,
           src = {0x7ffef1c40e00, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
           0x0}, perf_runs = 0, perf_cycles = 0, perf_time_us = 0,
           view_src = 0x7ffef1c40e00, view_offs = 0, data = 0x0,
           name = "Vcur-0 (reshaped)", '\000' <repeats 46 times>, extra = 0x0,
           padding = "\000\000\000\000\000\000\000"}
    ```
    I noticed that the `src` and `view_src` fields are different but that the
    dimensions are the same. From the code comment it seems like the reshape
    call is not needed and perhaps the above can motivate the removal of the
    reshape call.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * llama : add assert
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit b75c38166cf0c66793a32d5c3d6cb69929dd083d
Author: Pedro Cuenca <pedro@huggingface.co>
Date:   Fri Mar 29 08:15:00 2024 +0100

    convert : allow conversion of Mistral HF models (#6144)
    
    * Allow conversion of Mistral HF models
    
    * Homogenize Llama, Mistral, Mixtral under the same entry.
    
    * Fix tokenizer, permute tensors
    
    * Use sentencepiece tokenizer, or fall back to hfft.
    
    * convert-hf : small fix for mypy
    
    * convert-hf : fix duplicated block_count
    
    * convert-hf : add vocab size to metadata
    
    ---------
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>

commit bfe7dafc9cf96b9a09ead347fed9a547930fc631
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 28 22:56:03 2024 +0200

    readme : add notice for UI list

commit 5106ef482c65ac60ac14da9a68c7b37bca4c6993
Author: Ouadie EL FAROUKI <ouadie.elfarouki@codeplay.com>
Date:   Thu Mar 28 16:01:47 2024 +0000

    [SYCL] Revisited & updated SYCL build documentation (#6141)
    
    * Revisited & updated SYCL build documentation
    
    * removed outdated comment
    
    * Addressed PR comments
    
    * Trimed white spaces
    
    * added new end line

commit be55134a535f7218c53f39211755b1c7550851b2
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Thu Mar 28 11:44:36 2024 -0400

    convert : refactor vocab selection logic (#6355)

commit 66ba56025602270152f5ba5234f3a80be3dee1c9
Author: Ziang Wu <97337387+ZiangWu-77@users.noreply.github.com>
Date:   Thu Mar 28 22:33:10 2024 +0800

    llava : fix MobileVLM (#6364)
    
    * fix empty bug
    
    * Update MobileVLM-README.md
    
    added more results on devices
    
    * Update MobileVLM-README.md
    
    * Update MobileVLM-README.md
    
    * Update MobileVLM-README.md
    
    * Update MobileVLM-README.md
    
    * Update MobileVLM-README.md
    
    * Update MobileVLM-README.md
    
    * Update examples/llava/MobileVLM-README.md
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update MobileVLM-README.md
    
    remove gguf links
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 0308f5e3d7bf9879f818b1a4ae589ff36b242af5
Author: compilade <113953597+compilade@users.noreply.github.com>
Date:   Thu Mar 28 08:05:54 2024 -0400

    llama : fix command-r inference when omitting outputs (#6367)

commit 28cb9a09c4d10a489be1238abe7a858dcd4d65f2
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Thu Mar 28 11:27:56 2024 +0100

    ci: bench: fix master not schedule, fix commit status failed on external repo (#6365)

commit cfc4d75df6399b36153ef739f2c1abee4c114bb8
Author: Ting Sun <suntcrick@gmail.com>
Date:   Thu Mar 28 16:51:06 2024 +0800

    doc: fix outdated default value of batch size (#6336)
    
    * doc: fix outdated default value of batch size
    
    * doc: add doc for ubatch-size

commit 6902cb7f2e3479f364ee177118200fb7e4e9fc92
Author: Eric Zhang <34133756+EZForever@users.noreply.github.com>
Date:   Thu Mar 28 16:50:48 2024 +0800

    server : stop gracefully on SIGTERM (#6348)

commit d2d8f389960a106b66313ff1621bdb1aaaaaa285
Author: hutli <hutli@hutli.hu>
Date:   Wed Mar 27 19:17:30 2024 +0100

    nix: removed unnessesary indentation

commit d39b308eaf0ac91c2e1f432bf66751193a470a56
Author: hutli <hutli@hutli.hu>
Date:   Wed Mar 27 19:14:28 2024 +0100

    nix: moved blas availability check to package inputs so it is still overridable

commit c87397664964e5a2a21de1877d504b23a2a35332
Author: hutli <hutli@hutli.hu>
Date:   Wed Mar 27 18:10:08 2024 +0100

    using blas.meta.available to check host platform

commit dbb03e2b9c4fead73062926b6a134f28d3a3b46d
Author: hutli <jensstaermose@hotmail.com>
Date:   Wed Mar 27 17:25:05 2024 +0100

    only using explicit blas if hostPlatform is allowed

commit e9f17dc3bf0da76c8b35130f9ca2fda5246c418e
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Tue Mar 26 16:22:42 2024 +0000

    nix: .#windows: proper cross-compilation set-up
    
    Take all dependencies from the cross stage, rather tha only stdenv

commit 22a462cc1f69873f7d4c6d0201bd93478afa2ecb
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Tue Mar 26 16:22:07 2024 +0000

    nix: package: don't introduce the dependency on python
    
    - The generic /usr/bin/env shebangs are good enough
    - Python deps are provisioned in the devShells
    - We need to be able to leave python out at least on windows (currently breaks eval)

commit f6a0f5c6422200764b7929064c39dcf4bb0e9cd6
Author: hutli <jensstaermose@hotmail.com>
Date:   Thu Feb 15 14:25:04 2024 +0100

    nix: .#widnows: init
    
    initial nix build for windows using zig
    
    mingwW64 build
    
    removes nix zig windows build
    
    removes nix zig windows build
    
    removed unnessesary glibc.static
    
    removed unnessesary import of pkgs in nix
    
    fixed missing trailing newline on non-windows nix builds
    
    overriding stdenv when building for crosscompiling to windows in nix
    
    better variables when crosscompiling windows in nix
    
    cross compile windows on macos
    
    removed trailing whitespace
    
    remove unnessesary overwrite of "CMAKE_SYSTEM_NAME" in nix windows build
    
    nix: keep file extension when copying result files during cross compile for windows
    
    nix: better checking for file extensions when using MinGW
    
    nix: using hostPlatform instead of targetPlatform when cross compiling for Windows
    
    using hostPlatform.extensions.executable to extract executable format

commit d0e2f6416bd43eddb70137f6b96c7bc3d0246102
Author: Ziang Wu <97337387+ZiangWu-77@users.noreply.github.com>
Date:   Thu Mar 28 12:03:30 2024 +0800

    doc: fix typo in MobileVLM-README.md (#6181)

commit 25f4a613c4ed6451162a87cb90be10d610b49f0f
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Thu Mar 28 08:55:24 2024 +0800

    [SYCL] fix set main gpu crash (#6339)

commit a016026a3ac16d8c9b993a3573f19b9556d67de4
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Wed Mar 27 20:26:49 2024 +0100

    server: continuous performance monitoring and PR comment (#6283)
    
    * server: bench: init
    
    * server: bench: reduce list of GPU nodes
    
    * server: bench: fix graph, fix output artifact
    
    * ci: bench: add mermaid in case of image cannot be uploaded
    
    * ci: bench: more resilient, more metrics
    
    * ci: bench: trigger build
    
    * ci: bench: fix duration
    
    * ci: bench: fix typo
    
    * ci: bench: fix mermaid values, markdown generated
    
    * typo on the step name
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    * ci: bench: trailing spaces
    
    * ci: bench: move images in a details section
    
    * ci: bench: reduce bullet point size
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>

commit 53c7ec53d5eca26b2c0c648605543a5fa6c12817
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Wed Mar 27 16:17:46 2024 +0000

    nix: ci: dont test cuda and rocm (for now)
    
    Until https://github.com/ggerganov/llama.cpp/issues/6346 is resolved

commit e5b89a441af23a74b861b0bf8db3239139041876
Author: slaren <slarengh@gmail.com>
Date:   Wed Mar 27 15:07:50 2024 +0100

    ggml : fix bounds checking of zero size views (#6347)

commit 3a0345970ed0353fa857df3c8a62a2b3318b1364
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 27 15:02:49 2024 +0200

    make : whitespace

commit 1e13987fba5a536965ef942f2c86549d62cef50b
Author: howlger <eclipse@voormann.de>
Date:   Wed Mar 27 12:15:44 2024 +0100

    embedding : show full embedding for single prompt (#6342)
    
    * embedding : show full embedding for single prompt
    
    To support the use case of creating an embedding for a given prompt, the entire embedding and not just the first part needed to be printed.
    
    Also, show cosine similarity matrix only if there is more than one prompt, as the cosine similarity matrix for a single prompt is always `1.00`.
    
    * Update examples/embedding/embedding.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit e82f9e2b833d88cd2b30123ef57346c2cb8abd99
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Wed Mar 27 08:16:40 2024 +0000

    [SYCL] Fix batched impl for NVidia GPU (#6164)
    
    * Fix batched impl
    
    * Maintain previous behaviour for igpu
    
    * retrigger CI
    
    ---------
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>

commit cbc83436197cde617cad696e665879c20df77daa
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Wed Mar 27 08:44:27 2024 +0100

    Make IQ1_M work for QK_K = 64 (#6327)
    
    * iq1_m: make it work for QK_K = 64 (WIP)
    
    * iq1_m: make it work for QK_K = 64 (scalar and AVX2)
    
    * iq1_m: QK_K = 64 seems to work on Metal and ARM_NEON
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit e562b9714b9b3e242361a7f74bbbeb00f6bd99ac
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Wed Mar 27 08:23:10 2024 +0100

    common : change --no-penalize-nl to --penalize-nl (#6334)
    
    * Change --no-penalize-nl to --penalize-nl
    
    * Update documentation too

commit 2ab4f00d25c0682a74472412d66454df211e4b0e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 27 09:16:02 2024 +0200

    llama2c : open file as binary (#6332)

commit 1740d6dd4e00dedda87d6820b0e0d8e70dade340
Author: Mateusz Charytoniuk <mateusz.charytoniuk@protonmail.com>
Date:   Wed Mar 27 08:08:59 2024 +0100

    readme : add php api bindings (#6326)
    
    * add php bindings to readme
    
    * readme : add link to PR
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 0642b22cd12af278d6e7e459b73411947c169381
Author: Eric Zhang <34133756+EZForever@users.noreply.github.com>
Date:   Wed Mar 27 13:55:29 2024 +0800

    server: public: use relative routes for static files (#6325)
    
    server: public: support custom `api_url`, default to relative base path

commit a4f569e8a316cbd33d2b4de94b694d111507475a
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Wed Mar 27 09:47:06 2024 +0800

    [SYCL] fix no file in win rel (#6314)

commit 32c8486e1f0297393cb22ac0a0d26a6b17ad4d54
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Tue Mar 26 17:46:21 2024 -0400

    wpm : portable unicode tolower (#6305)
    
    Also use C locale for ispunct/isspace, and split unicode-data.cpp from unicode.cpp.

commit 557410b8f06380560155ac7fcb8316d71ddc9837
Author: compilade <113953597+compilade@users.noreply.github.com>
Date:   Tue Mar 26 10:46:41 2024 -0400

    llama : greatly reduce output buffer memory usage (#6122)
    
    * llama : greatly reduce logits memory usage
    
    * llama : more compact state saving and reloading
    
    * llama : fix lctx.n_outputs not being set before building graph
    
    * perplexity : adapt to the logits API changes
    
    * perplexity : fix Winogrande, use correct logits for second choice start
    
    The first logits used to evaluate the second choice were not from
    the end of the common prefix; instead, they were the logits from the end
    of the first choice. This has been corrected.
    
    The previous implementation sometimes had outliers in the scores of
    choices for some tasks, and the logic to skip choices words
    in the log-likelihood evaluation probably was an attempt to reduce those,
    but it was complex and didn't quite seem to be the right thing.
    
    This is simpler now, and the outlier scores aren't there anymore.
    
    * perplexity : normalize spaces and punctuation in Winogrande sentences
    
    * llama : fix embedding conditions
    
    * llama : fix llama_get_embeddings_ith when the resulting id is 0
    
    * llama : fix wrong n_outputs in llama_set_inputs
    
    A mismatch happened when using a smaller n_ubatch than n_batch and then using
    llama_batch_get_one(). The decision of what n_outputs should be now almost
    fully depends on how lctx.n_outputs is set in llama_decode_internal.
    The conditions are simpler this way.
    
    * llama : when saving the state, recalculate n_outputs
    
    This ensures the correct number of outputs for the entire previous batch
    is stored in the session file, even when n_ubatch is smaller than n_batch.
    
    * llama : fix not-skipping outputs of non-causal models
    
    * llama : fix running a batch with n_outputs == 0
    
    It previously worked because lctx.inp_out_ids was not initialized,
    so it pointed to some garbage address which was somehow still valid when I
    ran my tests.
    
    * llama : keep same graph topology even when n_outputs == 0
    
    * ggml : saner ggml_can_repeat with empty tensors
    
    *  ggml : future-proof ggml_is_empty by using GGML_MAX_DIMS - 1
    
    * ggml : do not multi-thread ops returning empty tensors
    
    * ggml : make ggml_is_empty public and work with views
    
    * llama : use a vector for ctx->output_ids
    
    * llama : rework reallocation logic for llama_output_reserve
    
    Now comparing the actual size with the new total size of the output buffer
    to allow more efficient enabling and disabling of the embeddings
    and/or logits output in the future.
    
    * ggml : skip empty tensors in all backends
    
    * llama : fix llama_output_reserve nullptr deref when new_size is 0
    
    * perplexity : make Winogrande work as it does on master
    
    The problems with the Winogrande implementation will
    need to be fixed in a separate PR to ease review.
    
    * llama : clearer error messages for invalid logits or embeddings ids
    
    * llama : assert all models that can have inp_out_ids
    
    Since the graph topology is now constant, this presence check
    can be done even when there are no outputs.
    
    * llama : assert logits and embd buffers exist before writing to them
    
    * llama : handle errors from llama_output_reserve at call sites
    
    * perplexity : make hellaswag and multiple-choice outputs identical to master
    
    Due to how the KV cache is updated, the logprobs for tokens in a batch
    are very slightly affected by the other tokens present in the batch,
    so to make hellaswag and multiple-choice return exactly the same results
    as on master, the last token of each sequence needs to be evaluated
    even though its output is not used at all.
    
    This will probably be changed back in the future to make these benchmarks
    a tiny bit faster.
    
    * perplexity : fix division by zero when using less than 100 multiple-choice tasks
    
    * llama : allow loading state saved with a different ctx size
    
    When loading a session file, the context size is now only required to be
    at least enough to load the KV cells contained in that session file,
    instead of requiring to use exactly the same context size as when saving.
    
    Doing this enables the use-case of extending or shrinking the context size
    of a saved session.
    
    This breaks existing session files because the meaning of kv_buf_size
    is slightly changed (previously it was the size of the whole KV cache,
    now it's only the size of the saved part of it). This allows for
    finer-grained sanity checks when loading in an effort to keep kv_buf_size
    useful even when the kv_size is changed.
    
    * llama : minor
    
    ggml-ci
    
    * readme : update recent API changes, and warn about Vulkan
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 55c1b2a3bbd470e9e2a3a0618b92cf64a885f806
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Mar 26 15:21:27 2024 +0100

    IQ1_M: 1.75 bpw quantization (#6302)
    
    * iq1_m: basics
    
    * iq1_m: basics-2
    
    * iq1_m: CUDA dequantize works
    
    Very 1st shot I get PPL = 9.76 for LLaMA-v2-7B.
    
    * iq1_m: separate shifts for each group of 8 in a block
    
    We get
    PPL(LLaMA-v2-7B ) = 9.2810
    PPL(LLaMA-v2-13B) = 6.8105
    
    Not bad, but slightly higher than
      sqrt(PPL(IQ1_S) * PPL(IQ2_XXS))
    which is the expected outcome given that IQ1_M is
    halfway between IQ1_S and IQ2_XXS in terms of bpw.
    From this, we would expect
     PPL = 9.14 for LLaMA-v2-7B
     PPL = 6.63 for LLaMA-v2-13B
    
    * iq1_m: go to 3-bit scales
    
    There is slight increase in PPL, but the 0.0625 bpw reduction
    in size is totally worth it.
    
    We now have
    PPL(LLaMA-v2-7B ) = 9.4469 at 1.96 bpw
    PPL(LLaMA-v2-13B) = 6.8717 at 1.93 bpw
    PPL(LLaMA-v2-70B) = 4.8568 at 1.85 bpw
    
    * iq1_m: scalar dot product
    
    * iq1_m: AVX2 dot product
    
    * iq1_m: very slightly faster AVX2 dot product
    
    * iq1_m: ARM_NEON dot product
    
    Works, but very slow (10.5 t/s)
    
    * iq1_m: Metal - dequantize works, dot product does not
    
    * iq1_m: Metal now works
    
    About the same performance as iq1_s.
    
    * iq1_m: minor
    
    * iq1_m: checking pure iq1_m quantization
    
    It is pretty bad: PPL(LLaMA-v2-7B) = 34 if we quantize output.weight
    with Q4_K.
    
    * iiq1_m: slightly faster ARM_NEON dot product
    
    10.5 t/s -> 11.65 t/s
    
    * iq1_m: faster ARM_NEON dot product
    
    11.65 t/s -> 14.9 t/s
    
    * iq1_m: another minor ARM_NEON dot product improvement
    
    14.9 -> 15.0 t/s
    
    * iq1_m: small PPL improvement via super-block scale adjustment
    
    After quantizing block scales redo the super-block scale fit.
    
    PPL(LLaMA-v2-7B ) = 9.3346
    PPL(LLaMA-v2-13B) = 6.8419
    PPL(LLaMA-v2-70B) = 4.8294
    PPL(Mistral-7B  ) = 8.1624
    
    * iq1_m: adapt to CUDA refactoring
    
    * iq1_m: remove unused variable
    
    We have progressed to warnings being errors.
    
    * iq1_m: add to backend-ops tests
    
    * iq1_m: fix Windows ARM
    
    * iq1_m: use common definition of iq1m_scale_t
    
    * cuda: assert -> NO_DEVICE_CODE
    
    * iq1_M: PR comments
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit e097633f63fdd26d492844f7eff056e4083fd9eb
Author: Pedro Cuenca <pedro@huggingface.co>
Date:   Tue Mar 26 13:32:19 2024 +0100

    convert-hf : fix exception in sentencepiece with added tokens (#6320)

commit d25b1c31b07c3675443a55a828dd58cfef5a241c
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Mar 26 13:09:30 2024 +0100

    quantize : be able to override metadata by key (#6321)
    
    * quantize: be able to override metadata by key
    
    * minor : spacing
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit deb7240100da99555b9ab9dc635021e591fceaf5
Author: Minsoo Cheong <54794500+mscheong01@users.noreply.github.com>
Date:   Tue Mar 26 18:11:46 2024 +0900

    embedding : adjust `n_ubatch` value (#6296)
    
    * embedding: assign `n_ubatch` value, print error on `n_batch` overflow
    
    * Update examples/embedding/embedding.cpp
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    * use %ld instead of %lld
    
    * Revert "use %ld instead of %lld"
    
    This reverts commit ea753ede90a86a0699f65878cc8e2020ff5eabb8.
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>

commit 3d032ece8e6973441273601ca2981130608e287d
Author: Jan Boon <jan.boon@kaetemi.be>
Date:   Tue Mar 26 16:47:43 2024 +0800

    server : add `n_discard` parameter (#6300)

commit e190f1fca6f60d80944f9e8709d343a025c4d245
Author: Joseph Stahl <1269177+josephst@users.noreply.github.com>
Date:   Mon Mar 25 20:51:46 2024 -0400

    nix: make `xcrun` visible in Nix sandbox for precompiling Metal shaders (#6118)
    
    * Symlink to /usr/bin/xcrun so that `xcrun` binary
    is usable during build (used for compiling Metal shaders)
    
    Fixes https://github.com/ggerganov/llama.cpp/issues/6117
    
    * cmake - copy default.metallib to install directory
    
    When metal files are compiled to default.metallib, Cmake needs to add this to the install directory so that it's visible to llama-cpp
    
    Also, update package.nix to use absolute path for default.metallib (it's not finding the bundle)
    
    * add `precompileMetalShaders` flag (defaults to false) to disable precompilation of metal shader
    
    Precompilation requires Xcode to be installed and requires disable sandbox on nix-darwin

commit 280345968dabc00d212d43e31145f5c9961a7604
Author: slaren <slarengh@gmail.com>
Date:   Tue Mar 26 01:16:01 2024 +0100

    cuda : rename build flag to LLAMA_CUDA (#6299)

commit b06c16ef9f81d84da520232c125d4d8a1d273736
Author: Christian Kögler <ck3d@gmx.de>
Date:   Mon Mar 25 18:52:45 2024 +0100

    nix: fix blas support (#6281)
    
    Since no blas was provided to buildInputs, the executable is built without blas support.
    
    This is a backport of NixOS/nixpkgs#298567

commit 1f2fd4e727a707f85d58e0b56075c3e6334d18d8
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Mar 25 18:33:15 2024 +0100

    tests : include IQ2_XXS and IQ2_XS in test-quantize-fns (#6303)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 43139cc528909c3de8c144ed174e09a1f7912a80
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 25 17:22:27 2024 +0200

    flake.lock: Update (#6266)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/d691274a972b3165335d261cc4671335f5c67de9' (2024-03-14)
      → 'github:NixOS/nixpkgs/44d0940ea560dee511026a53f0e2e2cde489b4d4' (2024-03-23)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit 2f34b865b62b1d2b5eb8a27885e4de220deeacbd
Author: slaren <slarengh@gmail.com>
Date:   Mon Mar 25 15:43:22 2024 +0100

    cuda : fix LLAMA_CUDA_F16 build (#6298)

commit ae1f211ce2138448b47ebb148e25c58406845278
Author: slaren <slarengh@gmail.com>
Date:   Mon Mar 25 13:50:23 2024 +0100

    cuda : refactor into multiple files (#6269)

commit ad3a0505e3b6cd777259ee35e61d428357ffc565
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Mar 25 09:42:17 2024 +0100

    Server: clean up OAI params parsing function (#6284)
    
    * server: clean up oai parsing function
    
    * fix response_format
    
    * fix empty response_format
    
    * minor fixes
    
    * add TODO for logprobs
    
    * update docs

commit 95ad616cddda50273e955bfe192328acd9aa4896
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Mon Mar 25 15:52:41 2024 +0800

    [SYCL] fix SYCL backend build on windows is break by LOG() error (#6290)
    
    * fix LOG() error for SYCL, enhance erro check by CI
    
    * rollback to bash
    
    * add newline at end of file

commit 64e7b47c6986221f2ff5c57c89dfc018bb0e9e6d
Author: Minsoo Cheong <54794500+mscheong01@users.noreply.github.com>
Date:   Mon Mar 25 16:38:22 2024 +0900

    examples : add "retrieval" (#6193)
    
    * add `retrieval` example
    
    * add README
    
    * minor fixes
    
    * cast filepos on print
    
    * remove use of variable sized array
    
    * store similarities in separate vector
    
    * print error on insufficient batch size
    
    * fix error message printing
    
    * assign n_batch value to n_ubatch
    
    * fix param definitions
    
    * define retrieval-only parameters in retrieval.cpp
    
    * fix `--context-file` option to be provided multiple times for multiple files
    
    * use vector for `query_emb`
    
    * add usage description in README
    
    * fix merge conflict
    
    * fix usage printing
    
    * remove seed setting
    
    * fix lint
    
    * increase file read buffer size
    
    * retrieval : minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 7733f0c76081b2a69b5f8b192db2db7c43629d58
Author: Justine Tunney <jtunney@gmail.com>
Date:   Mon Mar 25 01:39:56 2024 -0400

    ggml : support AVX512VNNI (#6280)
    
    This change causes some quants (e.g. Q4_0, Q8_0) to go faster on some
    architectures (e.g. AMD Zen 4).

commit a32b77c4b2c1808654d0b952f26c37d73d2e746b
Author: Rick G <26732651+TheFlipbook@users.noreply.github.com>
Date:   Sun Mar 24 14:45:56 2024 -0700

    Fix heap corruption from wmode out-of-bound writes on windows (#6272)
    
    * would throw error on VS2022 on GGML_FREE(wmode)
    * wchar_t is usually 2 bytes, but malloc wants bytes
      * therefore `*wmode_p++ = (wchar_t)*mode;` could write off the end of the allocation
    * Fixes error possibly introduced by https://github.com/ggerganov/llama.cpp/pull/6248

commit a0e584defd8c16e7a51ab895f595df0448d710d0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 24 16:18:45 2024 +0200

    imatrix : fix wname for mul_mat_id ops (#6271)
    
    * imatrix : fix wname for mul_mat_id ops
    
    * also filter tensor names in mul_mat_id ops
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 7aed0ffe6855e9cadcf413f288753af4566bfeb8
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Mar 24 14:21:17 2024 +0100

    Fixed lookup compilation issues on Windows (#6273)

commit ea279d56091b90fbbe063b598f5229d95f58ef68
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Mar 24 09:57:06 2024 +0100

    ci : close inactive issue, increase operations per run (#6270)

commit 586e7bc561be88e929a9afca7e67d8ead00c53bd
Author: Minsoo Cheong <54794500+mscheong01@users.noreply.github.com>
Date:   Sun Mar 24 17:54:07 2024 +0900

    sampling : deduplicated code for probability distribution access (#6240)
    
    * sampling: remove duplicated code for probability distribution access
    
    * free original_logits
    
    * fix original_logits allocation
    
    * fixes based on review @cebtenzzre
    
    * change function name to `llama_sampling_prepare`

commit ddf65685105a39a57b1e7f80c3aa502a6313af24
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Sun Mar 24 12:04:25 2024 +0800

    [SYCL] offload op (#6217)
    
    
    * remove no USM methods
    
    * leave the schedule to ggml_backend_sched entirely

commit d03224ac9840351023ff8abcf4aa0542258a53df
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Sun Mar 24 09:44:01 2024 +0800

    Support build win release for SYCL  (#6241)
    
    * support release win
    
    * fix value
    
    * fix value
    
    * fix value
    
    * fix error
    
    * fix error
    
    * fix format

commit 94d1b3b4119209efcdd08df0dceaecbd1fe7f85c
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Sat Mar 23 18:48:02 2024 -0400

    use _wfopen instead of fopen on Windows (#6248)
    
    also fix missing #defines before windows.h, and BPE LF token on MSVC

commit 95562175f83a49755ff6fd3bad09409417c8e6f9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 23 21:35:23 2024 +0200

    gitignore : gguf-split

commit f482bb2e4920e544651fb832f2e0bcb4d2ff69ab
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Mar 23 18:07:00 2024 +0100

    common: llama_load_model_from_url split support  (#6192)
    
    * llama: llama_split_prefix fix strncpy does not include string termination
    common: llama_load_model_from_url:
     - fix header name case sensitive
     - support downloading additional split in parallel
     - hide password in url
    
    * common: EOL EOF
    
    * common: remove redundant LLAMA_CURL_MAX_PATH_LENGTH definition
    
    * common: change max url max length
    
    * common: minor comment
    
    * server: support HF URL options
    
    * llama: llama_model_loader fix log
    
    * common: use a constant for max url length
    
    * common: clean up curl if file cannot be loaded in gguf
    
    * server: tests: add split tests, and HF options params
    
    * common: move llama_download_hide_password_in_url inside llama_download_file as a lambda
    
    * server: tests: enable back Release test on PR
    
    * spacing
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * spacing
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * spacing
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 1997577d5e121568ae39f538021733ccd4278c23
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Mar 23 18:00:38 2024 +0100

    server: docs: `--threads` and `--threads`, `--ubatch-size`, `--log-disable` (#6254)

commit 476b0251b27fb64c575507024a671e639d675594
Author: Julius Arkenberg <arki05@users.noreply.github.com>
Date:   Sat Mar 23 17:41:53 2024 +0100

    llama : add grok-1 support (#6204)
    
    * Add support for Grok model architecture
    
    * Revert convert-hf-to-gguf to default options
    
    * Fixed f_norm_rms_eps bug
    
    * Fix whitespaces
    
    * llama : fix grok rope type
    
    * llama : minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 21cad01b6e6e1a96f99391f95e8ea8ae25c8288e
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Mar 23 17:18:13 2024 +0100

    split: add gguf-split in the make build target (#6262)

commit 1b26aebe4de4f048ac99996efd8a2c9af150904d
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Mar 23 13:18:45 2024 +0100

    server: flush stdout after logging in both text and json layout (#6253)

commit 50ccaf5eacb50a2ca378a4ef0dc7aeb45fead652
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Mar 23 01:24:36 2024 +0100

    lookup: complement data from context with general text statistics (#5479)
    
    * lookup: evaluation tools, use corpus/previous gens
    
    * fixup! lookup: evaluation tools, use corpus/previous gens
    
    * fixup! lookup: evaluation tools, use corpus/previous gens
    
    * fixup! lookup: evaluation tools, use corpus/previous gens
    
    * fixup! lookup: evaluation tools, use corpus/previous gens

commit 56a00f0a2f48a85376f48b5ce77699df781631ae
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 22 21:10:39 2024 +0200

    common : default --hf-file to --model (#6234)

commit 92397d87a45a09b5449d845a64856f177cd7a920
Author: fraxy-v <65565042+fraxy-v@users.noreply.github.com>
Date:   Fri Mar 22 20:49:06 2024 +0200

    convert-llama2c-to-ggml : enable conversion of GQA models (#6237)
    
    * convert-llama2c-to-ggml: enable conversion of multiqueries, #5608
    
    * add test in build action
    
    * Update build.yml
    
    * Update build.yml
    
    * Update build.yml
    
    * gg patch

commit 1d0331c12a2f2a6296b471232bd4e66fbf06e6a1
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Mar 22 19:47:14 2024 +0100

    quantize: options for output and token embedding tensors qtype (#6239)
    
    * quantize: be able to specify the output tensor type
    
    * quantize: be able to specify the token embedding tensor type
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit dba1af612926cbd4ebe2d876277af1e3305177e0
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Mar 22 19:00:01 2024 +0100

    llama_model_loader: support multiple split/shard GGUFs (#6187)
    
    * split: support in llama_model_loader
    
    * avoid copying the entire vector
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * split: move llama_tensor_offset to llama_model_loader
    
    * llama_model_loader: PR feedbacks:
     - use only one gguf_context for metadata only
     - store all ggml_context in a vector as the files and mappings
     - store all weights in a vector along with the source tensor
     - rename ctx_gguf to meta
     - rename ctx_meta to contexts
    
    * avoid copying the entire vector
    
    * Simplify this by making these optional, switch some layer creation tensor optional
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Handle optional tensors
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * llama_model_loader: fail if backend cannot allocate buffer
    
    * fix mmap buffer management
    
    * llama_model_loader: map file to backend buffer if the allocation succeeds only
    
    * llama_model_loader: only map tensors included in the context
    
    * llama_model_loader: minor, use same variable name for consistency, fix spacing in types cast
    
    * llama_model_loader: fail if any of backend buffer cannot be allocated
    
    * spacing
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * fix loop over pointer
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * llama_model_loader: if n_tensors declared not equals to loaded tensors in split, throw an exception instead of asserting
    
    * llama_model_loader: ensure mappings vector has the expected size
    
    * llama_model_loader:  use at instead of operator[] if this should never add to the map.
    
    * llama_model_loader: immediately add the backend buffer to the model buffers in order to free them if an error occurs in the next allocation. Reserve the expected size.
    
    * llama_model_loader: be sure the model mappings has enough capacity before allocating backend buffer
    
    * llama_model_loader: fix map -> unordered map
    
    * llama_split_prefix: use a clearer version, not pass split path len but dest max len.
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    * llama : minor
    
    ggml-ci
    
    * llama : introduce some typedef helpers
    
    * docs: add model shard in hot topic
    
    * llama_model_loader: put mapping in a unique_ptr from the moment it is allocated
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * fix llama_split_prefix
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>

commit ee804f6223777019cf921e0d99cc24669313ab98
Author: Minsoo Cheong <54794500+mscheong01@users.noreply.github.com>
Date:   Sat Mar 23 02:15:06 2024 +0900

    ci: apply concurrency limit for github workflows (#6243)

commit 80bd33bc2c4be352697dc8473339f25e1085d117
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 22 15:33:38 2024 +0200

    common : add HF arg helpers (#6234)
    
    * common : add HF arg helpers
    
    * common : remove defaults

commit e80f06d2a194be62ab5b1cd7ef7c7a5b241dd4fb
Author: Nexesenex <124105151+Nexesenex@users.noreply.github.com>
Date:   Fri Mar 22 14:32:02 2024 +0100

    llama : correction of the attn.v.weight quantization for IQ3_XS (#6209)
    
    IQ3_XS was not mentioned, IQ3_S and IQ3_M were present twice.
    
    That PR corrects this in the manner which was probably intended initially.

commit f77a8ffd3bbde77b7819823b0c006fd8c2d5cae4
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Fri Mar 22 13:09:07 2024 +0000

    tests : conditional python & node json schema tests (#6207)
    
    * json: only attempt python & node schema conversion tests if their bins are present
    
    Tests introduced in https://github.com/ggerganov/llama.cpp/pull/5978
    disabled in https://github.com/ggerganov/llama.cpp/pull/6198
    
    * json: orange warnings when tests skipped
    
    * json: ensure py/js schema conv tested on ubuntu-focal-make
    
    * json: print env vars in test

commit 72114edf068a9b2f54d3b09e9a198f611be397e8
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Fri Mar 22 13:07:44 2024 +0000

    json-schema-to-grammar : fix order of props + non-str const/enum (#6232)
    
    * json: ordered json in server/schema converter to respect orig order
    
    * json: ws nits
    
    * json: support non-string const / enums

commit 2f0e81e053b41ca28e73a841e7bdbf9820baaa57
Author: slaren <slarengh@gmail.com>
Date:   Fri Mar 22 14:05:31 2024 +0100

    cuda : add LLAMA_CUDA_NO_PEER_COPY to workaround broken ROCm p2p copy (#6208)
    
    * cuda : add LLAMA_CUDA_NO_PEER_COPY to workaround broken ROCm p2p copy
    
    * add LLAMA_CUDA_NO_PEER_COPY to HIP build

commit 29ab270e65975785cdca3243a3de71ccebc1252a
Author: Xiaoyi Chen <cxychina@gmail.com>
Date:   Fri Mar 22 04:29:49 2024 -0700

    readme : add RecurseChat to the list of UIs (#6219)

commit 6b8bb3a31d260a01020497c5f01025c34222fcb9
Author: Jan Boon <kaetemi@gmail.com>
Date:   Fri Mar 22 19:12:05 2024 +0800

    server : fix n_keep always showing as 0 in response (#6211)

commit 68e210b3543e0cc71268bee0920441747679ee13
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 22 13:08:28 2024 +0200

    server : enable continuous batching by default (#6231)

commit b3e94f26bab8e3b5338b5902e5af5bb01894cb4a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 22 11:35:53 2024 +0200

    metal : proper assert for mat-mat memory alignment (#6225)
    
    * metal : proper assert for mat-mat memory alignment
    
    ggml-ci
    
    * readme : add notice about the bug fix
    
    * metal : fix the fix
    
    ggml-ci

commit b2075fd6a578f5685060df4baa90ae9e48e98c70
Author: Vaibhav Srivastav <vaibhavs10@gmail.com>
Date:   Fri Mar 22 08:53:43 2024 +0100

    ci : add CURL flag for the mac builds (#6214)

commit 95d576b48ebf582b112d1c9cf4eed7142fa4e464
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 22 09:36:03 2024 +0200

    metal : pad n_ctx by 32 (#6177)
    
    * metal : require ne00 >= 128 for mat-mat kernels
    
    ggml-ci
    
    * llama : pad n_ctx by 32
    
    ggml-ci

commit 59c17f02de8fdf7b084d6100b875b7e2bc07a83b
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Fri Mar 22 15:19:37 2024 +0800

    add blog link (#6222)

commit fa046eafbc70bf97dcf39843af0323f19a8c9ac3
Author: DAN™ <dranger003@gmail.com>
Date:   Thu Mar 21 21:32:42 2024 -0400

    Fix params underscore convert to dash. (#6203)
    
    * Fix params underscore convert to dash.
    
    * Update common/common.cpp
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit be07a03217376c53b1d95e5f658adbbbb2831555
Author: Jan Boon <kaetemi@gmail.com>
Date:   Fri Mar 22 06:41:24 2024 +0800

    server : update readme doc from `slot_id` to `id_slot` (#6213)

commit d0a71233fbf8ade8ef06ad8e6b81d1d7b254895f
Author: slaren <slarengh@gmail.com>
Date:   Thu Mar 21 19:54:28 2024 +0100

    cuda : disable host register by default (#6206)

commit f372c49ccdc561ab96fb3c7d2b7cbc0f89a4b359
Author: semidark <me@semidark.net>
Date:   Thu Mar 21 11:52:35 2024 -0600

    Corrected typo to wrong file (#6199)
    
    The stated file `./devops/main-server.Dockerfile` does not exist. I figure that `.devops/server-intel.Dockerfile` was meant.

commit 924ce1dce7fdf54894b462d03e7b608a6e574c32
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 21 16:20:05 2024 +0200

    tests : disable system() calls (#6198)
    
    ggml-ci

commit 03a8f8fafec2e21009be9fe7297d92e5d4538964
Author: slaren <slarengh@gmail.com>
Date:   Thu Mar 21 13:59:53 2024 +0100

    cuda : fix LLAMA_CUDA_F16 build (#6197)

commit cfd3be76e37dab92c846d75a2421178f20db4a11
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Mar 21 13:59:38 2024 +0100

    ggml : same IQ4_NL quantization for CPU/CUDA/Metal (#6196)
    
    * Make quantize_row_iq4_nl do the same thing is quantization on CUDA
    
    * Make quantize_row_iq4_nl do the same thing is quantization on CUDA
    
    This time for real. backend-ops tests pass.
    
    * Now fix test-quantize-fns
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 5b7b0ac8dfdd800c0fd0dc69b69991e8cb19fb46
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Thu Mar 21 11:50:43 2024 +0000

    json-schema-to-grammar improvements (+ added to server) (#5978)
    
    * json: fix arrays (disallow `[,1]`)
    
    * json: support tuple types (`[number, string]`)
    
    * json: support additionalProperties (`{[k: string]: [string,number][]}`)
    
    * json: support required / optional properties
    
    * json: add support for pattern
    
    * json: resolve $ref (and support https schema urls)
    
    * json: fix $ref resolution
    
    * join: support union types (mostly for nullable types I think)
    
    * json: support allOf + nested anyOf
    
    * json: support any (`{}` or `{type: object}`)
    
    * json: fix merge
    
    * json: temp fix for escapes
    
    * json: spaces in output and unrestricted output spaces
    
    * json: add typings
    
    * json:fix typo
    
    * Create ts-type-to-grammar.sh
    
    * json: fix _format_literal (json.dumps already escapes quotes)
    
    * json: merge lit sequences and handle negatives
    
    {"type": "string", "pattern": "^({\"question\": \"[^\"]+\", \"response\": \"[^\"]+\"}\\n)+$"}
    
    * json: handle pattern repetitions
    
    * Update json-schema-to-grammar.mjs
    
    * Create regex-to-grammar.py
    
    * json: extract repeated regexp patterns to subrule
    
    * Update json-schema-to-grammar.py
    
    * Update json-schema-to-grammar.py
    
    * Update json-schema-to-grammar.py
    
    * json: handle schema from pydantic Optional fields
    
    * Update json-schema-to-grammar.py
    
    * Update json-schema-to-grammar.py
    
    * Update ts-type-to-grammar.sh
    
    * Update ts-type-to-grammar.sh
    
    * json: simplify nullable fields handling
    
    * json: accept duplicate identical rules
    
    * json: revert space to 1 at most
    
    * json: reuse regexp pattern subrules
    
    * json: handle uuid string format
    
    * json: fix literal escapes
    
    * json: add --allow-fetch
    
    * json: simplify range escapes
    
    * json: support negative ranges in patterns
    
    * Delete commit.txt
    
    * json: custom regex parser, adds dot support & JS-portable
    
    * json: rm trailing spaces
    
    * Update json-schema-to-grammar.mjs
    
    * json: updated server & chat `( cd examples/server && ./deps.sh )`
    
    * json: port fixes from mjs to python
    
    * Update ts-type-to-grammar.sh
    
    * json: support prefixItems alongside array items
    
    * json: add date format + fix uuid
    
    * json: add date, time, date-time formats
    
    * json: preserve order of props from TS defs
    
    * json: port schema converter to C++, wire in ./server
    
    * json: nits
    
    * Update json-schema-to-grammar.cpp
    
    * Update json-schema-to-grammar.cpp
    
    * Update json-schema-to-grammar.cpp
    
    * json: fix mjs implementation + align outputs
    
    * Update json-schema-to-grammar.mjs.hpp
    
    * json: test C++, JS & Python versions
    
    * json: nits + regen deps
    
    * json: cleanup test
    
    * json: revert from c++17 to 11
    
    * json: nit fixes
    
    * json: dirty include for test
    
    * json: fix zig build
    
    * json: pass static command to std::system in tests (fixed temp files)
    
    * json: fix top-level $refs
    
    * json: don't use c++20 designated initializers
    
    * nit
    
    * json: basic support for reserved names `{number:{number:{root:number}}}`
    
    * Revamp test cmake to allow args (WORKING_DIRECTORY needed for JSON test)
    
    * json: re-ran server deps.sh
    
    * json: simplify test
    
    * json: support mix of additional props & required/optional
    
    * json: add tests for some expected failures
    
    * json: fix type=const in c++, add failure expectations for non-str const&enum
    
    * json: test (& simplify output of) empty schema
    
    * json: check parsing in test + fix value & string refs
    
    * json: add server tests for OAI JSON response_format
    
    * json: test/fix top-level anyOf
    
    * json: improve grammar parsing failures
    
    * json: test/fix additional props corner cases
    
    * json: fix string patterns (was missing quotes)
    
    * json: ws nit
    
    * json: fix json handling in server when there's no response_format
    
    * json: catch schema conversion errors in server
    
    * json: don't complain about unknown format type in server if unset
    
    * json: cleaner build of test
    
    * json: create examples/json-schema-pydantic-example.py
    
    * json: fix date pattern
    
    * json: move json.hpp & json-schema-to-grammar.{cpp,h} to common
    
    * json: indent 4 spaces
    
    * json: fix naming of top-level c++ function (+ drop unused one)
    
    * json: avoid using namespace std
    
    * json: fix zig build
    
    * Update server.feature
    
    * json: iostream -> fprintf
    
    * json: space before & refs for consistency
    
    * json: nits

commit 1943c0198125a0da1a200390e82cf461f9080d99
Author: Vaibhav Srivastav <vaibhavs10@gmail.com>
Date:   Thu Mar 21 10:30:40 2024 +0100

    ci : fix indentation error (#6195)

commit 5e43ba87429d85acbde97d16b2f48d9de992cc80
Author: Vaibhav Srivastav <vaibhavs10@gmail.com>
Date:   Thu Mar 21 10:13:12 2024 +0100

    build : add mac pre-build binaries (#6182)
    
    * Initial commit - add mac prebuilds.
    
    * forward contribution credits for building the workflow.
    
    * minor : remove trailing whitespaces
    
    ---------
    
    Co-authored-by: Nicolas Patry <Narsil@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 76aa30a26353f597e4fbe3cf776772ae812af89a
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Mar 21 08:27:57 2024 +0100

    Add ability to use Q5_0, Q5_1, and IQ4_NL for quantized K cache (#6183)
    
    * k_cache: be able to use Q5_0
    
    * k_cache: be able to use Q5_1 on CODA
    
    * k_cache: be able to use Q5_0 on Metal
    
    * k_cache: be able to use Q5_1 on Metal
    
    * k_cache: be able to use IQ4_NL - just CUDA for now
    
    * k_cache: be able to use IQ4_NL on Metal
    
    * k_cache: add newly added supported types to llama-bench and CUDA supports_op
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit c5b8595e3f4f4ed319ef71c9c9d868d1b7a27626
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Thu Mar 21 06:10:52 2024 +0000

    Add nvidia and amd backends (#6157)

commit 42e21c68826f2e56b9592dccd9f3c43895b6890d
Author: slaren <slarengh@gmail.com>
Date:   Thu Mar 21 01:47:46 2024 +0100

    cuda : fix conflict with std::swap (#6186)

commit 1c51f98adcbad40e3c41f0a6ffadeb723190b417
Author: slaren <slarengh@gmail.com>
Date:   Wed Mar 20 21:03:26 2024 +0100

    cuda : print the returned error when CUDA initialization fails (#6185)

commit f9c7ba34476ffc4f13ae2cdb1aec493a16eb8d47
Author: Ziang Wu <97337387+ZiangWu-77@users.noreply.github.com>
Date:   Wed Mar 20 23:29:51 2024 +0800

    llava : update MobileVLM-README.md (#6180)

commit 272935b281fee5c683e3d6d1eb580b84553cf503
Author: Ziang Wu <97337387+ZiangWu-77@users.noreply.github.com>
Date:   Wed Mar 20 23:02:32 2024 +0800

    llava : add MobileVLM_V2 backup (#6175)
    
    * Add MobileVLM_V2 backup
    
    * Update MobileVLM-README.md
    
    * Update examples/llava/MobileVLM-README.md
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update examples/llava/convert-image-encoder-to-gguf.py
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * clip :  fix whitespace
    
    * fix deifinition mistake in clip.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit ccf58aa3ec4d20b10162ba40898dc038ad4c3fad
Author: slaren <slarengh@gmail.com>
Date:   Wed Mar 20 14:42:59 2024 +0100

    cuda : refactor to remove global resources (#6170)
    
    * cuda : refactor to remove global resources

commit 91f8ad167dcd24b54615b468d9dd764ebe1d37ad
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed Mar 20 13:30:36 2024 +0100

    Server: version bump for httplib and json (#6169)
    
    * server: version bump for httplib and json
    
    * fix build
    
    * bring back content_length

commit 6b7e76d28cdf4361740054140708c71828453522
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 20 14:17:34 2024 +0200

    gitignore : ignore curl-related files

commit bc0baab2ea8f806961dd3fb9f534cfeb59a2e1fc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 20 14:14:32 2024 +0200

    server : allow to override -ngl in tests (#6170)

commit d795988d9e09f75412efe2134512914c42780ad5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 20 13:29:49 2024 +0200

    Revert "llava : add a MobileVLM_V2-1.7B backup (#6152)"
    
    This reverts commit f8c4e745e1e728204ab26dbadf52853545e6789c.

commit f8c4e745e1e728204ab26dbadf52853545e6789c
Author: Ziang Wu <97337387+ZiangWu-77@users.noreply.github.com>
Date:   Wed Mar 20 19:20:37 2024 +0800

    llava : add a MobileVLM_V2-1.7B backup (#6152)
    
    * Add MobileVLM_V2 backup
    
    * Update MobileVLM-README.md
    
    * Update examples/llava/MobileVLM-README.md
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update examples/llava/convert-image-encoder-to-gguf.py
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * clip :  fix whitespace
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 47cc7a7bf9793f81a6dfe7c2096c499403f64dd6
Author: Karthick <j.karthic2004@gmail.com>
Date:   Wed Mar 20 16:32:34 2024 +0530

    Server: Handle n_keep parameter in the request (#6174)

commit bd60d82d0cc8b6852ec535495a5042dbdf05de24
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Wed Mar 20 01:33:49 2024 -0400

    server tests : more pythonic process management; fix bare `except:` (#6146)
    
    * server tests : remove seemingly redundant newlines in print()
    
    * server tests : use built-in subprocess features, not os.kill and psutil
    
    * server tests : do not catch e.g. SystemExit; use print_exc
    
    * server tests: handle TimeoutExpired exception
    
    * server tests: fix connect on dual-stack systems
    
    * server: tests: add new tokens regex on windows generated following new repeat penalties default changed in (#6127)
    
    * server: tests: remove the hack on windows since now we get the good socket family
    
    * server: tests: add new tokens regex following new repeat penalties default changed in (#6127)
    
    * server: tests: add new tokens regex following new repeat penalties default changed in (#6127)
    
    ---------
    
    Co-authored-by: Pierrick HYMBERT <pierrick.hymbert@gmail.com>

commit 6c0b287748327741b113d7d6018b68c63039b1c5
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Wed Mar 20 11:21:41 2024 +0800

    update readme sycl for new update (#6151)
    
    * update readme sycl for new update
    
    * Update README-sycl.md
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
    
    * Update README-sycl.md
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
    
    * Update README-sycl.md
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
    
    * Update README-sycl.md
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
    
    * Update README-sycl.md
    
    Co-authored-by: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
    
    * Update README-sycl.md
    
    Co-authored-by: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
    
    * update by review comments
    
    * update w64devkit link
    
    * update for verify device id part
    
    * Update README-sycl.md
    
    Co-authored-by: Meng, Hengyu <airdldl@163.com>
    
    ---------
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
    Co-authored-by: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
    Co-authored-by: Meng, Hengyu <airdldl@163.com>

commit d26e8b669dbf1f5f5a0afe4d2d885e86cf566302
Author: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
Date:   Wed Mar 20 08:28:49 2024 +0530

    increase igpu cluster limit (#6159)

commit d8b009a9456bf5284376149f3deb09300a37701a
Author: DAN™ <dranger003@gmail.com>
Date:   Tue Mar 19 12:16:09 2024 -0400

    Remove undeed header file. (#6158)

commit d0d5de42e5a65865b5fddb6f5c785083539b74c3
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Tue Mar 19 12:05:44 2024 +0100

    gguf-split: split and merge gguf per batch of tensors (#6135)
    
    * gguf-split: split and merge gguf files per tensor
    
    * gguf-split: build with make toolchain
    
    * gguf-split: rename `--split-tensors-size` to `--split-max-tensors`. Set general.split_count KV to all split
    
    * split : minor style + fix compile warnings
    
    * gguf-split: remove --upload not implemented
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit b80cf3b2d1dee0ad325f7a794fecc66befce7336
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 19 10:21:54 2024 +0200

    common : disable repeat penalties by default (#6127)

commit 970a48060ab9a6cc67aa063870323781c2a7bd7d
Author: slaren <slarengh@gmail.com>
Date:   Tue Mar 19 09:06:54 2024 +0100

    ci : exempt some labels from being tagged as stale (#6140)

commit 4c28b8252907561165827125d2d1a4bad6926ac6
Author: DAN™ <dranger003@gmail.com>
Date:   Tue Mar 19 01:59:36 2024 -0400

    common : print usage on '-h' and '--help' (#6145)

commit 2d15886bb092c3b780c676b5cc57ff3337af9c83
Author: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
Date:   Sun Mar 17 06:37:44 2024 +0000

    flake.lock: Update
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/9df3e30ce24fd28c7b3e2de0d986769db5d6225d' (2024-03-06)
      → 'github:NixOS/nixpkgs/d691274a972b3165335d261cc4671335f5c67de9' (2024-03-14)

commit d199ca79f279e84ebe27caafe0aa59c461d88969
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Mon Mar 18 12:49:02 2024 -0400

    mpt : implement backwards compatiblity with duped output tensor (#6139)

commit 104f5e0fc156d48476258295457cafeec2a2af10
Author: Felix <stenbackfelix@gmail.com>
Date:   Mon Mar 18 16:40:22 2024 +0100

    clip : fix memory leak (#6138)

commit 5e1b7f94a03e0b3b8e4578625bbdadc7bbd2b93c
Author: slaren <slarengh@gmail.com>
Date:   Mon Mar 18 16:33:44 2024 +0100

    backend : set max split inputs to GGML_MAX_SRC (#6137)

commit ac9ee6a4ad740bc1ee484ede43e9f92b5af244c1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 18 13:45:38 2024 +0200

    ci : disable stale issue messages (#6126)

commit 4f6d1337ca5a409dc74aca8c479b7c34408a69c0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 18 13:45:27 2024 +0200

    ci : temporary disable sanitizer builds (#6128)

commit 2bf8d0f7c4cc1235755ad06961ca761e458c5e55
Author: slaren <slarengh@gmail.com>
Date:   Mon Mar 18 11:03:04 2024 +0100

    backend : offload large batches to GPU (#6083)
    
    * backend : offload large batches to GPU
    
    * fix hip
    
    * code cleanup
    
    * fix CUDA split buffers
    
    * Update ggml-backend-impl.h
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    * cuda : fix memset without set_device
    
    * imatrix : remove sched affix from weight names
    
    * sched : add a new split if the current one has too many inputs
    reduce max inputs per split
    more cleanup
    
    * update backends
    
    ggml-ci
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

commit 496bc79bc2b79bfd6124b8687a8dbd6a646e9b06
Author: DAN™ <dranger003@gmail.com>
Date:   Mon Mar 18 04:27:44 2024 -0400

    common : tidy-up argument parsing (#6105)
    
    * Tidy-up argument parsing.
    
    * Missing ref.
    
    * common : minor
    
    * common : add static classifier
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 9b03719ad712e2dc36c5c0c20f352bf3e4bda332
Author: Thérence <13496987+Royalphax@users.noreply.github.com>
Date:   Mon Mar 18 09:17:00 2024 +0100

    convert : add support for CamembertModel architecture (#6119)
    
    Adding support for CamembertModel architecture used by :
    https://huggingface.co/dangvantuan/sentence-camembert-large

commit 3a6efdd03c46c5ba08e43880d34260c02dd9999b
Author: Romain D <90720+Artefact2@users.noreply.github.com>
Date:   Mon Mar 18 09:04:41 2024 +0100

    convert : use f32 outtype for bf16 tensors (#6106)
    
    The old behaviour is to use f16, but bf16 to f16 is not a lossless conversion.
    Change the outtype to f32 to default to a lossless conversion.

commit d01b3c4c32357567f3531d4e6ceffc5d23e87583
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Mar 17 19:12:37 2024 +0100

    common: llama_load_model_from_url using --model-url (#6098)
    
    * common: llama_load_model_from_url with libcurl dependency
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit cd776c37c945bf58efc8fe44b370456680cb1b59
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 17 19:51:57 2024 +0200

    ci : close all stale issues at once (#6115)

commit dc0f6125487dcfbff913360f9d877bc0ccf6aa57
Author: GainLee <perfecter.gen@gmail.com>
Date:   Mon Mar 18 01:12:22 2024 +0800

    ggml:fix finding transfer queue family index error (#6094)
    
    Co-authored-by: GainLee <ligen@meizu.com>

commit c47cf414efafb8f60596edc7edb5a2d68065e992
Author: AmirAli Mirian <37371367+amiralimi@users.noreply.github.com>
Date:   Sat Mar 16 11:52:02 2024 -0400

    ggml : add AVX512F SIMD (#6088)

commit b5f4ae09c3244ae1644b67c03ed9f4227ab25ad2
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Sat Mar 16 16:46:29 2024 +0100

    gritlm : add initial README.md (#6086)
    
    * gritlm: add initial README.md to examples/gritlm
    
    This commit adds a suggestion for an initial README.md for the gritlm
    example.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! gritlm: add initial README.md to examples/gritlm
    
    Use the `scripts/hf.sh` script to download the model file.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! gritlm: add initial README.md to examples/gritlm
    
    Fix editorconfig-checker error in examples/gritlm/README.md.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit dfbfdd60f90207404039c6578d709231496831d9
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat Mar 16 16:42:08 2024 +0100

    readme : add wllama as a wasm binding (#6100)

commit 15961ec04dbd59d21d8984d42e4c0f7e7e7d320a
Author: DAN™ <dranger003@gmail.com>
Date:   Sat Mar 16 11:39:15 2024 -0400

    common : refactor nested if causing error C1061 on MSVC (#6101)
    
    * Refactor nested if causing error C1061 on MSVC.
    
    * Revert back and remove else's.
    
    * Add flag to track found arguments.

commit a56d09a4407f29c21e149b44fd5308f83aa1cb09
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Mar 16 13:20:53 2024 +0100

    ci : close inactive issue with workflow (#6053)
    
    * issues: ci - close inactive issue with workflow
    
    * ci: close issue, change workflow schedule time

commit d84c48505f60bcd358b82a751d40418c4d235643
Author: slaren <slarengh@gmail.com>
Date:   Fri Mar 15 22:14:16 2024 +0100

    llama : fix Baichuan2 13B (#6092)

commit 877b4d0c628cc70dddb5df72ed8fc14d126ca7e8
Author: Theia Vogel <theia@vgel.me>
Date:   Fri Mar 15 13:43:02 2024 -0700

    llama : add support for control vectors (#5970)
    
    * control vector api and implementation
    
    * control-vectors : minor code style updates
    
    * disable control vector when data == nullptr
    
    use -1 for disabled range (also on init) in case we ever support controlling layer 0 (embeddings)
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 12247f4c69a173b9482f68aaa174ec37fc909ccf
Author: Andrew Canis <andrew.canis@gmail.com>
Date:   Fri Mar 15 16:41:22 2024 -0400

    llama : add Command-R support (#6033)
    
    Information about the Command-R 35B model (128k context) can be found at:
            https://huggingface.co/CohereForAI/c4ai-command-r-v01
    
    Based on the llama2 model with a few changes:
    
    1) New hyper parameter to scale output logits (logit_scale)
    2) Uses LayerNorm instead of RMSNorm
    3) Transfomer layers have a single shared LayerNorm that feeds into both the
       self-attention and FFN layers in parallel. There is no post-attention LayerNorm.
    4) No support for Rotary Position Embeddings (RoPE) scaling
    5) No biases used
    
    Find GGUF files here:
            https://huggingface.co/andrewcanis/c4ai-command-r-v01-GGUF
    
    To convert model to GGUF format yourself:
    
    1) Download Command-R Hugging Face safetensors:
            git lfs install
            git clone https://huggingface.co/CohereForAI/c4ai-command-r-v01
    
    2) Run:
            python3 convert-hf-to-gguf.py --outtype f16 ./c4ai-command-r-v01

commit 4e9a7f7f7fb6acbddd1462909c8d696e38edbfcc
Author: Ting Lou <ting.lou@gmail.com>
Date:   Fri Mar 15 22:31:05 2024 +0800

    llava : change API to pure C style for Rust FFI bindgen (#6079)
    
    Co-authored-by: Lou Ting <louting.t@alibaba-inc.com>

commit 3020327f6cd6d2ce50528dd65f4b199d2ea8b1ae
Author: slaren <slarengh@gmail.com>
Date:   Fri Mar 15 13:24:03 2024 +0100

    cuda : disable unused cudaLaunchHostFunc code (#6078)

commit 46acb3676718b983157058aecf729a2064fc7d34
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Fri Mar 15 18:53:53 2024 +0800

    fix set main gpu error (#6073)

commit 131b0584096ee9df4d07cb28759dfea6efe6475f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 15 11:36:50 2024 +0200

    make : ggml-metal.o depends on ggml.h

commit 753e36f650fa2a5869f89188d9ee745dc74cf14b
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Fri Mar 15 09:26:20 2024 +0000

    [SYCL] Fix non-intel device selection (#6042)
    
    * Fix non-intel device selection
    
    * Update ggml-sycl.cpp
    
    Co-authored-by: Neo Zhang Jianyu <jianyu.zhang@intel.com>
    
    * Update ggml-sycl.cpp
    
    Co-authored-by: Neo Zhang Jianyu <jianyu.zhang@intel.com>
    
    ---------
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
    Co-authored-by: Neo Zhang Jianyu <jianyu.zhang@intel.com>

commit 7ce2c77f88e1ca66ec48417e56f91746bac018c2
Author: Ondřej Čertík <ondrej@certik.us>
Date:   Fri Mar 15 02:46:51 2024 -0600

    gguf : add support for I64 and F64 arrays (#6062)
    
    * gguf : add support for I64 and F64 arrays
    
    GGML currently does not support I64 or F64 arrays and they are not often
    used in machine learning, however if in the future the need arises, it
    would be nice to add them now, so that the types are next to the other
    types I8, I16, I32 in the enums, and it also reserves their type number.
    
    Furthermore, with this addition the GGUF format becomes very usable for
    most computational applications of NumPy (being compatible with the most
    common NumPy dtypes: i8, i16, i32, i64, f32, f64), providing a faster,
    and more versatile alternative to the `npz` format, and a simpler
    alternative to the `hdf5` format.
    
    The change in this PR seems small, not significantly increasing the
    maintenance burden. I tested this from Python using GGUFWriter/Reader
    and `gguf-dump`, as well as from C, everything seems to work.
    
    * Fix compiler warnings

commit aab606a11fc0a9740a7f297521c3eef851dfb351
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Mar 15 09:44:57 2024 +0100

    llama : add Orion chat template (#6066)

commit b0bc9f4a9da7c19f4779106ea83b23feca747566
Author: slaren <slarengh@gmail.com>
Date:   Fri Mar 15 09:22:24 2024 +0100

    llama-bench : use random tokens to improve accuracy with mixtral (#6069)

commit 4755afd1cbd40d93c017e5b98c39796f52345314
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 14 22:58:41 2024 +0200

    llama : fix integer overflow during quantization (#6063)

commit 6e0438da3cc95b89cdbf55f45fa4e324d9076792
Author: Steve Grubb <ausearch.1@gmail.com>
Date:   Thu Mar 14 14:29:32 2024 -0400

    gguf : fix resource leaks (#6061)
    
    There several places where a gguf context is allocated. A call to gguf_free
    is missing in some error paths. Also on linux, llama-bench was missing a
    fclose.

commit 727107707a73b3dc8a497cf9fc9405722c16dd2b
Author: Ondřej Čertík <ondrej@certik.us>
Date:   Thu Mar 14 11:57:31 2024 -0600

    gguf-py : bump version to 0.8.0 (#6060)

commit 69ff61397d2b7b550dcdda4a35b35128892408b0
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Thu Mar 14 17:21:56 2024 +0100

    llama : support models without vocabulary (#5798)
    
    * additional methods to read model and ctx parameters
    
    * vocab size as a part of a model metadata
    
    * models without vocabulary, convert.py part
    
    * models without vocabulary, llama.cpp part
    
    * PR clean up
    
    * converter scrypt fixes
    
    * llama_vocab_type update (renamed the new key)
    
    * pr review fixes
    
    * revert function renaming
    
    * one more NoVocab assert

commit 044ec4b2a567f649459ccd20af2f387c784faa51
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 14 15:14:14 2024 +0200

    embedding : add EOS token if not present (#899)

commit 77178eedc83d49f31bf757d8e12315d76460be78
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 14 13:32:14 2024 +0200

    gguf-py : fix dtype check (#6045)

commit 15a333260ab637a040ed0864c206a2ceaf806bb8
Author: Jian Liao <jianliao@users.noreply.github.com>
Date:   Thu Mar 14 04:18:23 2024 -0700

    readme : improve readme for Llava-1.6 example (#6044)
    
    Co-authored-by: Jian Liao <jianliao@adobe.com>

commit 43241adf22e8231ffaf3827d2c9310cc0ffd5ac5
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Thu Mar 14 12:15:39 2024 +0100

    server: disable debug release type sanitizer, simplify trigger (#6047)
    
    - increase time out for server
     - do not fail fast

commit a44bc969e4cd62ca9f4332e17fe3c51f2093e7c6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 14 13:13:06 2024 +0200

    llama : fix typo

commit 2c4fb69246834503db7b78bcbedcef506bbc60c4
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Thu Mar 14 11:56:48 2024 +0100

    llama : optimize defrag moves + fix fragmentation calculation (#6037)
    
    * attempt to reduce the impact of a worst-case scenario
    
    * fragmentation calculation fix
    
    * Update llama.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 3ca23481dd309bd51cc31c73a4cc34f922cc372f
Author: Ondřej Čertík <ondrej@certik.us>
Date:   Thu Mar 14 04:40:14 2024 -0600

    gguf-py : add support for I8, I16 and I32 (#6045)
    
    * Refactor dtype handling to be extensible
    
    This code is equivalent as before, but now it is prepared to easily add
    more NumPy dtypes.
    
    * Add support for I8, I16 and I32
    
    These types are allowed in the GGUF specification.
    
    * Add support for I8, I16 and I32 to gguf_writer
    
    * Add support for I8, I16, I32 to gguf_reader

commit 3fe8d7a17f84bd721cd4d8db35365da44b69f68b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 14 12:38:37 2024 +0200

    ggml : designate enum vals for integer types (#6050)

commit 68265ebfc6a1bed022973ea0c3145be1450b7e70
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 14 12:37:20 2024 +0200

    embedding : print all resulting embeddings (#899)

commit 381da2d9f0940d7009e3e918bed36338c8ff2fbb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 14 11:55:23 2024 +0200

    metal : build metallib + fix embed path (#6015)
    
    * metal : build metallib + fix embed path
    
    ggml-ci
    
    * metal : fix embed build + update library load logic
    
    ggml-ci
    
    * metal : fix embeded library build
    
    ggml-ci
    
    * ci : fix iOS builds to use embedded library

commit 0fd6c1f015f6cccf3b527f7dbd8386a434728126
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 14 10:12:29 2024 +0200

    embedding : print cosine similarity (#899)

commit 19885d205e768579ab090d1e99281cae58c21b54
Author: Linwei Wang <wanix1988@gmail.com>
Date:   Thu Mar 14 02:34:40 2024 +0800

    readme : update details about running llama in Termux on Android (#6039)

commit 76a936c8939c249a7c3e8e66dfefbab13eae194f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 13 20:33:56 2024 +0200

    readme : update API changes and hot topics

commit 463628372d5fe3a0c1e5864aa5fc57deb7387039
Author: Clint Herron <hanclinto@gmail.com>
Date:   Wed Mar 13 14:10:40 2024 -0400

    grammar : handle missing "root" node (#6004)

commit f30ea47a87ed4446ad55adb265755dc9102956a2
Author: slaren <slarengh@gmail.com>
Date:   Wed Mar 13 18:54:21 2024 +0100

    llama : add pipeline parallelism support (#6017)
    
    * llama : add pipeline parallelism support for batch processing with multiple CUDA GPUs
    
    ggml-ci
    
    * server : add -ub, --ubatch-size parameter
    
    * fix server embedding test
    
    * llama : fix Mamba inference for pipeline parallelism
    
    Tested to work correctly with both `main` and `parallel` examples.
    
    * llama : limit max batch size to n_batch
    
    * add LLAMA_SCHED_MAX_COPIES to configure the number of input copies for pipeline parallelism
    default increase to 4 (from 2)
    
    changing this value may improve performance for some systems, but increases memory usage
    
    * fix hip build
    
    * fix sycl build (disable cpy_tensor_async)
    
    * fix hip build
    
    * llama : limit n_batch and n_ubatch to n_ctx during context creation
    
    * llama : fix norm backend
    
    * batched-bench : sync after decode
    
    * swiftui : sync after decode
    
    * ggml : allow ggml_get_rows to use multiple threads if they are available
    
    * check n_ubatch >= n_tokens with non-casual attention
    
    * llama : do not limit n_batch to n_ctx with non-casual attn
    
    * server : construct batch with size of llama_n_batch
    
    * ggml_backend_cpu_graph_compute : fix return value when alloc fails
    
    * llama : better n_batch and n_ubatch comment
    
    * fix merge
    
    * small fix
    
    * reduce default n_batch to 2048
    
    ---------
    
    Co-authored-by: Francis Couture-Harpin <git@compilade.net>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit d8fd0ccf6ac8b07791ffd1575eed436930854ae3
Author: slaren <slarengh@gmail.com>
Date:   Wed Mar 13 14:58:30 2024 +0100

    test-backend-ops : skip CPU backend by default (#6028)

commit b3d978600f07f22e94f2e797f18a8b5f6df23c89
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Wed Mar 13 13:17:54 2024 +0000

    Update get version (#6025)

commit 99b71c068f624521ad977e08e41589e2971fa1c7
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed Mar 13 11:39:11 2024 +0100

    Server: Use multi-task for embeddings endpoint (#6001)
    
    * use multitask for embd endpoint
    
    * specify types
    
    * remove redundant {"n_predict", 0}

commit 306d34be7ad19e768975409fc80791a274ea0230
Author: slaren <slarengh@gmail.com>
Date:   Tue Mar 12 16:55:19 2024 +0100

    ci : remove tidy-review (#6021)

commit 8030da7afea2d89f997aeadbd14183d399a017b9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 12 14:27:20 2024 +0200

    ggml : reuse quantum structs across backends (#5943)
    
    * ggml : reuse quant blocks across backends
    
    ggml-ci
    
    * ggml : define helper constants only for CUDA and SYCL
    
    ggml-ci
    
    * ggml : define helper quantum constants for SYCL
    
    ggml-ci

commit 184215e783dfad76aded2c68244c327a5c507df5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 12 13:49:55 2024 +0200

    ggml : fix UB in IQ2_S and IQ3_S (#6012)

commit 48358b2e5b3983c41ba7e61a493e84d3901dc7b9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 12 11:15:05 2024 +0200

    sycl : update IQ1_S kernels (WIP - not working!) (#5995)
    
    * sycl : try to fix after IQ1_S changes
    
    * sycl : iq1s_grid -> iq1s_grid_gpu
    
    * sycl : fix grid type

commit 5cdb371731caa2c41fcca42d4d2d43f94f6883b4
Author: gliptic <gliptic@users.noreply.github.com>
Date:   Mon Mar 11 20:59:03 2024 +0100

    grammar : fix unnecessarily retained pointer to rules (#6003)

commit 44ca159faf4fbe1a7ace13a962845ba7cdfd95ec
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Mar 11 16:53:15 2024 +0100

    1.5 bit: we can do even better (#5999)
    
    * iq1_s: we can do even better
    
    Spent one of the 4 scale bits on a signs of a 0.125 shift.
    I.e., quants are now -1 + delta, delta, 1 + delta, where delta
    is +/- 0.125.
    
    CUDA works, same performance as before.
    PPL(LLaMA-v2-7B) is now 11.85!
    
    * iq1_s: make scalar and AVX2 work with the new version
    
    * iq1_s: make Neon work with new version.
    
    ~10% drop in performance, so will need some more work.
    
    * iq1_s: make Metal work with new version
    
    * iq1_s: very slightly faster dequantize on Metal
    
    * iq1_s: fix dequantize on the CPU
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 05b06210c954491cf0f12034b0a62bd4d69ce78b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 11 17:49:47 2024 +0200

    llama : more consistent names of count variables (#5994)
    
    * llama : more consistent names of count variables
    
    ggml-ci
    
    * llama : n_parallel -> n_seq_max
    
    * common : fix param name
    
    * examples : fix param name

commit 83796e62bc9f6caae6228168e359890f51e60fee
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 11 17:47:47 2024 +0200

    llama : refactor unicode stuff (#5992)
    
    * llama : refactor unicode stuff
    
    ggml-ci
    
    * unicode : names
    
    * make : fix c++ compiler
    
    * unicode : names
    
    * unicode : straighten tables
    
    * zig : fix build
    
    * unicode : put nfd normalization behind API
    
    ggml-ci
    
    * swift : fix build
    
    * unicode : add BOM
    
    * unicode : add <cstdint>
    
    ggml-ci
    
    * unicode : pass as cpts as const ref

commit 828defefb66fc8a25404f5de845897145bf34061
Author: Jakub N <jakubniemczyk97@gmail.com>
Date:   Mon Mar 11 14:40:42 2024 +0100

    Update server docker image URLs (#5997)

commit caa106d4e05a0ab94225c220b81f9e2cd522339b
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Mar 11 10:56:41 2024 +0100

    Server: format error to json (#5961)
    
    * server: format error to json
    
    * server: do not crash on grammar error
    
    * fix api key test case
    
    * revert limit max n_predict
    
    * small fix
    
    * correct coding style
    
    * update completion.js
    
    * launch_slot_with_task
    
    * update docs
    
    * update_slots
    
    * update webui
    
    * update readme

commit 3202361c5b1ba15e695b31209567ef42c22c5c32
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Mon Mar 11 10:28:51 2024 +0100

    ggml, ci : Windows ARM runner and build fixes (#5979)
    
    * windows arm ci
    
    * fix `error C2078: too many initializers` with ggml_vld1q_u32 macro for MSVC ARM64
    
    * fix `warning C4146: unary minus operator applied to unsigned type, result still unsigned`
    
    * fix `error C2065: '__fp16': undeclared identifier`

commit 332bdfd7980718abf664bfa5460f2288a3314984
Author: Minsoo Cheong <54794500+mscheong01@users.noreply.github.com>
Date:   Mon Mar 11 17:09:32 2024 +0900

    server : maintain chat completion id for streaming responses (#5988)
    
    * server: maintain chat completion id for streaming responses
    
    * Update examples/server/utils.hpp
    
    * Update examples/server/utils.hpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit ecab1c75de68de7c41c254e2ae170d3b07bee6d4
Author: Gilad S <giladgd@users.noreply.github.com>
Date:   Mon Mar 11 10:00:08 2024 +0200

    cmake : fix subdir for `LLAMA_METAL_EMBED_LIBRARY` (#5985)

commit ee35600b9061b1ea0c4ea87fce6844297632b2a8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 11 09:56:47 2024 +0200

    llama : fix F16/F32 downcast + improve names (#5980)

commit be858f620508385ad12d0e5e862010e666ca729c
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Mar 11 07:51:49 2024 +0100

    Better 1.5 bit quantization  (#5971)
    
    * Trying blocvks of 16 for IQ1_S - seems slightly better
    
    * iq1s_blocks16: Adjust scale fudge factor to 1.125
    
    * iq1s_blocks16: going to blocks of 32
    
    with 2048 lattice points, so same bpw.
    This is even better than blocks of 16.
    Should I try blocks of 64? But to keep the same
    bpw, when I go to 4096 lattice points, I need to
    remove blocks alltogether and just have superblocks of
    256 weights.
    
    * iq1s_blocks16: Use 2*<x^2> as sigma2 in weight adjustment
    
    * iq1s_blocks16: scalar and AVX2 dot products
    
    * iq1s_blocks16: CUDA dot product
    
    * iq1s_blocks16: Metal works, Neon does not
    
    Metal works but TG is dog slow (35 t/s). PP is OKish (493 t/s).
    Not seeing the bug in the Neon implementation for now.
    
    * iq1s_blocks16: fixed Neon
    
    * iq1s_blocks16: very slightly faster TG on Metal
    
    Still pathetic at 37 t/s
    
    * iq1s_blocks16: speedup Metal by packing codebook into uint32_t's
    
    * Formatting
    
    * iq1s_blocks16: uint32_t codebook is also better in CUDA
    
    TG-128 is now 204 t/s up from 194 t/s.
    PP-512 is 5890 t/s, so significantly better than other quants
    
    * iq1s_blocks16: slightly faster Neon dot product
    
    * iq1s_blocks16: faster AVX2 dot product
    
    * iq1s_blocks16: adjust to ggml-common.h
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit ef3ced26a3817d92890b97b83acaeb018ade02d0
Author: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
Date:   Mon Mar 11 10:27:56 2024 +0530

    [SYCL] Add q3_s and q1_s (#5886)
    
    * Add q3_s and q1_s
    
    * fix compilation
    
    * fix build
    
    * fix build
    
    * fix build
    
    * enable ops
    
    * rm macro
    
    * increase grid space

commit 3814a07392d2bdc22911652bc7c2f9bdb0ce042e
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Mon Mar 11 01:13:57 2024 +0000

    [SYCL] Add support for SYCL Nvidia target (#5738)
    
    * Add support for nvidia target in CMake
    
    * Update sycl read-me for Nvidia target
    
    * Fix errors

commit bb6d00bbf98476215596b9df3870b5504ef5a29a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 10 23:12:48 2024 +0200

    metal : move mm_id indices to shared mem (#5982)

commit 7ab7b733bb48250b2df26c12b00256ef42c76932
Author: Dean <Dean.Sinaean@gmail.com>
Date:   Mon Mar 11 04:03:17 2024 +0800

    android : fix utf8 decoding error (#5935)
    
    * examples: fix utf8 decoding error
    
    some models have a tokenizer that decodes an id into an incomplete utf8 sequence, need to validate and wait for next token
    one example would be: https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat-GGUF/resolve/main/qwen1_5-1_8b-chat-q4_0.gguf and and an example of the token is 18137
    
    * android : minor
    
    ---------
    
    Co-authored-by: zhangfuwen <zhangfuwen@foxmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit d9f65c97c3dc3aa6fa27470b8c6e69b437ec1a27
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 10 20:58:26 2024 +0200

    readme : update hot topics

commit b838b53ad6de2e53f23ddf8f3ad5e6891cc3dd05
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 10 20:10:46 2024 +0200

    sync : ggml

commit df4dc3e7cb43162862f339525638ba4febcb6158
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 8 23:45:07 2024 +0200

    ggml : try fix 32-bit arm compat (whisper/1938)
    
    * ggml : try fix 32-bit arm compat
    
    * ggml : fix cont

commit bf47a5eefc669fdba71af096942af999bc1167d4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 10 20:09:24 2024 +0200

    ggml : remove __constant__ specifier for CUDA tables (#5940)

commit fa8a809a9119411bc9c3f00026550d0343f4d9b7
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Mar 10 18:17:47 2024 +0100

    server: ci: windows build and tests (#5968)
    
    * server: ci: windows build and tests
    
    * server: ci: remove tmp push branch
    
    * server: ci: EOF EOL
    
    * Use builti
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * server: tests: server graceful shutdown, then kill, then hard kill
    
    * server: tests: remove python2 unicode string
    
    * server: tests: remove wrong comment on server starting,  close_fds is always true
    
    * server: tests: server kill, if pid exists
    
    * server: tests: remove dependency to killall
    
    * server: tests: ci windows: pid exists better handling
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

commit bcebd7dbf62fd7b293d5ed089023e4e733269c71
Author: DAN™ <dranger003@gmail.com>
Date:   Sun Mar 10 11:56:30 2024 -0400

    llama : add support for GritLM (#5959)
    
    * add gritlm example
    
    * gritlm results match
    
    * tabs to spaces
    
    * comment out debug printing
    
    * rebase to new embed
    
    * gritlm embeddings are back babeee
    
    * add to gitignore
    
    * allow to toggle embedding mode
    
    * Clean-up GritLM sample code.
    
    * Fix types.
    
    * Flush stdout and output ending newline if streaming.
    
    * mostly style fixes; correct KQ_mask comment
    
    * add causal_attn flag to llama_cparams
    
    * gritml : minor
    
    * llama : minor
    
    ---------
    
    Co-authored-by: Douglas Hanley <thesecretaryofwar@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 2960eae847f8dbde23be6d170a61bcf44ebf32de
Author: Clint Herron <hanclinto@gmail.com>
Date:   Sun Mar 10 11:17:43 2024 -0400

    grammar : verify parsed state (#5950)

commit c78541479cf835dd9eb568ccd9a2083198a7203d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 10 16:43:08 2024 +0200

    nix: update flake.lock (#5969)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/1536926ef5621b09bba54035ae2bb6d806d72ac8' (2024-02-29)
      → 'github:NixOS/nixpkgs/9df3e30ce24fd28c7b3e2de0d986769db5d6225d' (2024-03-06)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit 621e86b331f8b0e71f79fd82a4ae1cd54c3e4396
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Mar 9 23:41:49 2024 +0100

    server: benchmark: chat/completions scenario and other llm servers comparison (#5941)
    
    * server: bench: Init a bench scenario with K6
    See #5827
    
    * server: bench: EOL EOF
    
    * server: bench: PR feedback and improved k6 script configuration
    
    * server: bench: remove llamacpp_completions_tokens_seconds as it include prompt processing time and it's misleading
    
    server: bench: add max_tokens from SERVER_BENCH_MAX_TOKENS
    
    server: bench: increase truncated rate to 80% before failing
    
    * server: bench: fix doc
    
    * server: bench: change gauge custom metrics to trend
    
    * server: bench: change gauge custom metrics to trend
    server: bench: add trend custom metrics for total tokens per second average
    
    * server: bench: doc add an option to debug http request
    
    * server: bench: filter dataset too short and too long sequences
    
    * server: bench: allow to filter out conversation in the dataset based on env variable
    
    * server: bench: fix assistant message sent instead of user message
    
    * server: bench: fix assistant message sent instead of user message
    
    * server : add defrag thold parameter
    
    * server: bench: select prompts based on the current iteration id not randomly to make the bench more reproducible
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 77d1ac7e00bf049b9f2bba1b5a310a78318c49c4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 9 22:04:00 2024 +0200

    server : print chat template info

commit d894f352bf433157232dc8dc54eacd50014e898e
Author: slaren <slarengh@gmail.com>
Date:   Sat Mar 9 19:55:54 2024 +0100

    perplexity : support using multiple sequences to allow larger batch sizes (#5946)
    
    * perplexity : support using multiple sequences to allow larger batch sizes
    
    ggml-ci
    
    * set cparams.n_parallel to the number of sequences
    
    * print tested n_ctx, add assert

commit 098dbaab449f5309a54871ba7e5acef72ae696de
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 9 18:14:13 2024 +0200

    readme : update hot topics

commit 8380ecfb219c2e73cc706fcc83935b7c806cc7c3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 9 17:36:20 2024 +0200

    ggml : fix unnecessary f32 -> f16 -> f32 casts (mmla) (#5951)

commit 58308a0ecce7cc261b802f4803c38d420063db21
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 9 17:34:15 2024 +0200

    server : fix metrics init (#5964)

commit 5b09797321430f08caf0473143a962916ab2ea89
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 9 15:53:59 2024 +0200

    ggml : remove old quantization functions (#5942)
    
    * ggml : remove old quantization functions
    
    ggml-ci
    
    * ggml : simplify ggml_quantize_chunk
    
    ggml-ci
    
    * ggml : restrict correctness
    
    ggml-ci
    
    * ggml : remove hist data from the quantization API
    
    ggml-ci
    
    * tests : remove hist usage in test-backend-ops
    
    ggml-ci
    
    * vulkan : remove hist and fix typo

commit 97c09585d65a95864773b4d25d66d0f708baf38d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 9 15:47:47 2024 +0200

    server : clarify some items in the readme (#5957)
    
    * server : clarify some items in the readme
    
    * server : fix typo

commit fb215c3832236fec7380c4fb618bd7154cb196ef
Author: SeungWon Jeong <65549245+redlion0929@users.noreply.github.com>
Date:   Sat Mar 9 21:27:58 2024 +0900

    server : normalize embeddings (#5956)
    
    * output normalize embedding in '/v1/embeddings'
    
    * common : reuse llama_embd_normalize
    
    * common : better normalize impl
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 2c4f566c88322ebf2f9bd11b01b5ebdaa0130b89
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 9 14:17:11 2024 +0200

    tests : gitignore ggml-common.h

commit 0db32beaf09d90b8959d3d0cc493ed1e45685353
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Sat Mar 9 11:16:53 2024 +0000

    server : fix passing prompt as tokens (#5955)
    
    * server: fix passing prompt as tokens
    
    * Update examples/server/server.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 8a3012a4ad08112bb3dc3f1399afec4e93780c44
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 9 12:47:57 2024 +0200

    ggml : add ggml-common.h to deduplicate shared code (#5940)
    
    * ggml : add ggml-common.h to shared code
    
    ggml-ci
    
    * scripts : update sync scripts
    
    * sycl : reuse quantum tables
    
    ggml-ci
    
    * ggml : minor
    
    * ggml : minor
    
    * sycl : try to fix build

commit 9674aaf35cb81478eb38c3f3ebde713ec72fbb79
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 9 12:34:18 2024 +0200

    server : simplify logic for empty prompts (#5953)

commit 950ba1ab84db199f0bbdecdb2bb911f35261b321
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat Mar 9 11:27:53 2024 +0100

    Server: reorganize some http logic (#5939)
    
    * refactor static file handler
    
    * use set_pre_routing_handler for validate_api_key
    
    * merge embedding handlers
    
    * correct http verb for endpoints
    
    * fix embedding response
    
    * fix test case CORS Options
    
    * fix code style

commit e1fa9569ba8ce276bc7801a3cebdcf8b1aa116ea
Author: Gabe Goodhart <gabe.l.hart@gmail.com>
Date:   Sat Mar 9 02:57:09 2024 -0700

    server : add SSL support (#5926)
    
    * add cmake build toggle to enable ssl support in server
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * add flags for ssl key/cert files and use SSLServer if set
    
    All SSL setup is hidden behind CPPHTTPLIB_OPENSSL_SUPPORT in the same
    way that the base httlib hides the SSL support
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * Update readme for SSL support in server
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * Add LLAMA_SERVER_SSL variable setup to top-level Makefile
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    ---------
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>

commit fd72d2d2a5e79d61ccef6af3d15f16e5e5cbc352
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Mar 9 10:30:04 2024 +0100

    server: tests: add truncated prompt tests, better kv cache size (#5933)
    
    * server: tests: add truncated prompt tests, better size
    
    * server, tests : update regex
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit c2101a2e909ac7c08976d414e64e96c90ee5fa9e
Author: compilade <113953597+compilade@users.noreply.github.com>
Date:   Fri Mar 8 17:31:00 2024 -0500

    llama : support Mamba Selective State Space Models (#5328)
    
    * mamba : begin working on support for Mamba SSM
    
    * mamba : begin figuring out how to (ab)use the kv cache for Mamba
    
    * mamba : recurrent inference almost works, but incoherent
    
    * mamba : recurrent inference WORKS!!!
    
    * convert : optionally use d_conv and d_state from config.json for Mamba
    
    * mamba : refactor recurrent conv, resulting in 20% perf increase
    
    It's still slower than I'd like, but I did not really optimize `ggml_exp` yet.
    
    I also refactored `ggml_exp` to work with tensors with more than 2 dimensions.
    
    * ggml : parallelize ggml_exp
    
    This results in 8% faster token generation for Mamba-130M.
    
    * mamba : simplify the conv step with a self-overlapping view
    
    Turns out the conv_state can be made smaller by one column.
    Note that this breaks existing GGUFs of Mamba,
    because the key_value_length field is tied to the conv_state size.
    
    Convolution with a self-overlapping view is cool!
    And it's much simpler than what I initially thought would be necessary
    to make the convolution step work with more than 1 token at a time.
    
    Next step is to make the SSM step work on batches of tokens too,
    and thus I need to figure out a way to make a parallel selective scan
    which will keep the ssm_state small and won't make it bigger
    by a factor of (n_layer * batch_size).
    
    * llama : fix Mamba KV self size wrongly displaying as f16 instead of f32
    
    Relatedly, I also tried to see if other types than f32 worked for the states,
    but they don't, because of the operators used.
    It's probably better anyway to keep lots of precision there,
    since the states are small anyway.
    
    * mamba : fix self-overlapping view depth stride
    
    * mamba : handle batches of more than 1 token
    
    This means running Mamba no longer crashes when using the default settings!
    And probably also slightly faster prompt processing.
    Both batched and non-batched processing yield the same output.
    
    Previously, the state was not cleared when starting a sequence.
    Next step is to make the KV cache API work as expected for Mamba models.
    
    * ggml: add ggml_ssm_scan to help with parallel selective scan
    
    If the selective scan was implemented without a custom operator,
    there would be waaay too many nodes in the graph. For example,
    for Mamba-130M, with a batch size of 512 (the default),
    a naive selective scan could add at least 24*512=12288 nodes,
    which is more than LLAMA_MAX_NODES (8192),
    and that's only for the smallest Mamba model.
    So it's much cleaner with a custom operator.
    Not sure about the name, though.
    
    * ggml : in ggml_ssm_scan, merge multiple rows in the same vec operation
    
    This will help with performance on CPU if ggml_vec_mul_f32
    and ggml_vec_add_f32 are ever optimized with SIMD.
    
    * mamba : very basic quantization support
    
    Mostly works, but there is currently no difference
    between the variants of a k-quant (e.g. Q4_K_S and Q4_K_M are the same).
    Most of the SSM-specific weights can be kept in f32 without affecting
    the size that much, since they are relatively small.
    (the linear projection weights are responsible for most of Mamba's size)
    
    Too much quantization seems to make the state degrade quite fast, and
    the model begins to output gibberish.
    It seems to affect bigger models to a lesser extent than small models,
    but I'm not sure by how much.
    
    Experimentation will be needed to figure out which weights are more important
    for the _M (and _L?) variants of k-quants for Mamba.
    
    * convert : fix wrong name for layer norm weight of offical Mamba models
    
    I was using Q-bert/Mamba-* models before, which have a slighlty different
    naming scheme for the weights.
    (they start with "model.layers" instead of "backbone.layers")
    
    * mamba : fuse more steps of the SSM scan in the ggml_ssm_scan operator
    
    This increases performance on CPU by around 30% for prompt processing,
    and by around 20% for text generation.
    
    However, it also makes the ggml_exp and ggml_soft_plus operators unused.
    Whether or not they should be kept will be decided later.
    
    * convert : for Mamba, also consider the "MambaLMHeadModel" arch name
    
    It's the name of the class of the official implementation,
    though they don't use it (yet) in the "architectures" field of config.json
    
    * mamba : fix vocab size problems with official models
    
    The perplexity was waaaay to high for models with a non-round vocab size.
    Not sure why, but it needed to be fixed in the metadata.
    
    Note that this breaks existing GGUF-converted Mamba models,
    but **only if** the vocab size was not already rounded.
    
    * ggml : remove ggml_exp and ggml_soft_plus
    
    They did not exist anyway outside of this branch,
    and since ggml_ssm_scan fused operations together, they are unused.
    It's always possible to bring them back if needed.
    
    * mamba : remove some useless comments
    
    No code change.
    
    * convert : fix flake8 linter errors
    
    * mamba : apply suggestions from code review
    
    * mamba : remove unecessary branch for row-wise ssm_state and C multiplication
    
    It was previously done to avoid permuting when only one token is processed
    at a time (like when generating text), but permuting is cheap,
    and dynamically changing the compute graph is not future-proof.
    
    * ggml : in ggml_ssm_scan, use more appropriate asserts
    
    * ggml : rename the destination pointer in ggml_compute_forward_ssm_scan_f32
    
    * mamba : multiple sequences, but one at a time
    
    This is a step towards making this Mamba implementation usable
    with the server example (the way the system prompt is kept when clearing
    the client slots will need to be changed before this can work, though).
    
    The KV cache size for this kind of model is tied to the maximum number
    of sequences kept at any single time.
    For now, this number is obtained from n_parallel (plus one,
    to have an extra sequence to dedicate to the system prompt),
    but there might be a better way to do this which won't also
    make the main example use 2 cells even if only 1 is really used.
    (for this specific case, --parallel 0 helps)
    
    Simultaneous sequence processing will probably require changes to
    ggml_ssm_scan, and possibly a new operator for the conv step.
    
    * mamba : support llama_kv_cache_seq_cp
    
    This (mis)uses the logic around K shifts, because tokens in a state
    can't be shifted anyway, and because inp_K_shift has the right shape and type.
    Using ggml_get_rows is a nice way to do copies, but copy chains can't work.
    Fortunately, copy chains don't really seem to be used in the examples.
    
    Each KV cell is dedicated to the sequence ID corresponding to its own index.
    
    * mamba : use a state mask
    
    It's cleaner than the previous heuristic of
    checking for the pos of the first token in the batch.
    
    inp_KQ_mask could not be re-used for this, because it has the wrong shape
    and because it seems more suited to the next step of
    simultaneous sequence processing (helping with the problem of
    remembering which token belongs to which sequence(s)/state(s)).
    
    * llama : replace the usage of n_ctx with kv_self.size in many places
    
    * mamba : use n_tokens directly instead of n_tok
    
    * mamba : in comments, properly refer to KV cells instead of slots
    
    * mamba : reduce memory usage of ggml_ssm_scan
    
    From 290.37 MiB to 140.68 MiB of CPU compute buffer size
    with Mamba 3B with a batch size of 512.
    
    The result tensor of ggml_ssm_scan was previously a big part
    of the CPU compute buffer size. To make it smaller,
    it does not contain the intermediate ssm states anymore.
    Both y and the last ssm state are combined in the result tensor,
    because it seems only a single tensor can be returned by an operator
    with the way the graph is built.
    
    * mamba : simultaneous sequence processing
    
    A batch can now contain tokens from multiple sequences.
    
    This is necessary for at least the parallel example, the server example,
    and the HellaSwag test in the perplexity example.
    
    However, for this to be useful, uses of llama_kv_cache_seq_rm/cp
    will need to be changed to work on whole sequences.
    
    * ggml : add ggml_ssm_conv as a new operator for the conv step of Mamba
    
    This operator makes it possible to use and update the correct states
    for each token of the batch in the same way as ggml_ssm_scan.
    Other solutions which use existing operators would need loops which would
    add too many nodes to the graph (at least the ones I thought of).
    
    Using this operator further reduces the size of the CPU compute buffer
    from 140.68 MiB to 103.20 MiB with Mamba 3B with a batch size of 512.
    And (at least on CPU), it's a bit faster than before.
    
    Note that "ggml_ssm_conv" is probably not the most appropriate name,
    and it could be changed if a better one is found.
    
    * llama : add inp_s_seq as a new input tensor
    
    The most convenient implementation to select the correct state (for Mamba)
    for each token is to directly get the correct index from a tensor.
    This is why inp_s_seq is storing int32_t and not floats.
    
    The other, less convenient way to select the correct state would be
    to have inp_KQ_mask contain 1.0f for each state used by a token
    and 0.0f otherwise. This complicates quickly fetching the first used
    state of a token, and is also less efficient because a whole row
    of the mask would always need to be read for each token.
    
    Using indexes makes it easy to stop searching when there are
    no more sequences for a token, and the first sequence assigned
    is always very quickly available (it's the first element of each row).
    
    * mamba : support llama_kv_cache_seq_cp copy chains
    
    * mamba : support shifting and dividing the kv cache pos
    
    * mamba : make the server and parallel examples work with whole sequences
    
    A seq_id is dedicated to the system prompt in both cases.
    
    * llama : make llama_kv_cache_seq_rm return whether it succeeded or not
    
    * mamba : dedicate an input tensor for state copy indices
    
    This is cleaner and makes it easier to adapt when/if token positions
    (and by extension, inp_K_shift) are no longer integers.
    
    * mamba : adapt perplexity, batched, and batched-bench examples
    
    * perplexity : limit the max number of sequences
    
    This adapts to what the loaded model can provide.
    
    * llama : add llama_n_max_seq to get the upper limit for seq_ids
    
    Used by the perplexity example.
    
    * batched : pass n_parallel to the model's context params
    
    This should have been there already, but it wasn't.
    
    * batched-bench : reserve sequences to support Mamba
    
    * batched-bench : fix tokens being put in wrong sequences
    
    Generation quality isn't what's measured in there anyway,
    but at least using the correct sequences avoids using non-consecutive
    token positions.
    
    * mamba : stop abusing attention metadata
    
    This breaks existing converted-to-GGUF Mamba models,
    but will allow supporting mixed architectures like MambaFormer
    without needing to break Mamba models.
    
    This will also allow changing the size of Mamba's states
    without having to reconvert models in the future.
    (e.g. using something else than d_conv - 1 columns for the conv_states
     will not require breaking existing converted Mamba models again)
    
    * gguf-py : add new KV metadata key-value pairs for Mamba
    
    * llama : add new metadata key-value pairs for Mamba
    
    * llama : guard against divisions by zero when n_head is 0
    
    * mamba : rename "unlimited" KV cache property to "recurrent"
    
    * mamba : more correctly update the "used" field of the KV cache
    
    * ggml : in ggml_ssm_scan, use a threshold for soft_plus
    
    This is how the official Mamba implementation does it,
    and it's also what torch.nn.Softplus does.
    
    * convert : for Mamba, fallback to internal NeoX tokenizer
    
    The resulting models are exactly the same
    as if the tokenizer.json and tokenizer_config.json of GPT-NeoX were there.
    
    * mamba : support state saving and restoring
    
    * ggml : implicitly pass src tensors through dst for Mamba-related ops
    
    * mamba : clarify some comments
    
    * server : fix cache_tokens not getting correctly resized
    
    Otherwise, when the "we have to evaluate at least 1 token" special case
    was triggered, an extra token was kept in cache_tokens even if it was
    removed from the KV cache.
    
    For Mamba, this caused useless prompt reprocessing when the previous
    request triggered the above case.
    
    * convert-hf : support new metadata keys for Mamba
    
    For the models available at
    https://huggingface.co/collections/state-spaces/transformers-compatible-mamba-65e7b40ab87e5297e45ae406
    
    * mamba : rename metadata to be more similar to transformers library
    
    This breaks existing converted-to-GGUF models,
    but the metadata names are more "standard".
    
    * mamba : support mamba-*-hf models
    
    These models share their token_embd.weight with their output.weight
    
    * mamba : add missing spaces
    
    This is purely a formatting change.
    
    * convert-hf : omit output.weight when identical with token_embd.weight
    
    Only for Mamba for now, but it might be relevant for other models eventually.
    Most Mamba models actually share these two tensors, albeit implicitly.
    
    * readme : add Mamba to supported models, and add recent API changes
    
    * mamba : move state_seq and state_mask views outside layer loop
    
    A few tensors were also missing `struct` in front of `ggml_tensor`.

commit 515f7d0d4fce41c752fc253acf30707c3be2531e
Author: compilade <113953597+compilade@users.noreply.github.com>
Date:   Fri Mar 8 10:53:37 2024 -0500

    llama : fix quantization of shared token_embd (#5944)

commit 76e868821a94072fbc87cb1fcca291694319eae8
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Mar 8 12:25:04 2024 +0100

    server: metrics: add llamacpp:prompt_seconds_total and llamacpp:tokens_predicted_seconds_total, reset bucket only on /metrics. Fix values cast to int. Add Process-Start-Time-Unix header. (#5937)
    
    Closes #5850

commit e457fb3540e0aaec47cfde0abf784c213f9216ee
Author: Don Mahurin <dmahurin@users.noreply.github.com>
Date:   Fri Mar 8 02:41:50 2024 -0800

    llama : assume tied weights if lm_head/output weights is missing (#5824)
    
    This is to support model configurations with "tie_word_embeddings" set to true.
    
    Co-authored-by: Don Mahurin <2797413+dmahurin@users.noreply.github.com>

commit af37fd8b30e37ccbffdd82e6f48559e2fb7ce7dd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 8 12:40:02 2024 +0200

    server : fix EOS token detection with disabled cache (#5938)

commit 581ed5c4fe3a8909aaa8313633ac443f471ba755
Author: UEXTM.com <84163508+uextm@users.noreply.github.com>
Date:   Fri Mar 8 04:35:04 2024 -0500

    log : fix MSVC compile errors (#5643)
    
    MSVC gives the following error with the existing macros:
    `Error C2059 : syntax error: ','`
    
    This patch adds `##` as a prefix to `__VA_ARGS__` to address this error.

commit 6cdabe652695167263c8b447520987b11856f7ca
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 7 16:32:38 2024 +0200

    llama-bench : add embeddings option (#5924)
    
    * llama-bench : add embeddings option
    
    * llama-bench : do not hard code embd default value
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 89fb735fcfd21781a8194b211cf32824beb3f71f
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Thu Mar 7 19:14:49 2024 +0800

    Revert "[SYCL] fix error when set main gpu to non-zero (#5901)" (#5918)
    
    This reverts commit ceca1aef0738b57951cd12c603c3477e75312dec.

commit 55a2a900ff4a02fc33708ac7858d595d289a3f2a
Author: Minsoo Cheong <54794500+mscheong01@users.noreply.github.com>
Date:   Thu Mar 7 19:42:39 2024 +0900

    server : add `/v1/completions` endpoint (#5914)
    
    * add-`/v1/completions`-endpoint
    
    * add legacy comment to `/completion` endpoint

commit 2002bc96bf2cbf5ab981a17d7e994d817c9801f5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 7 11:41:53 2024 +0200

    server : refactor (#5882)
    
    * server : refactoring (wip)
    
    * server : remove llava/clip objects from build
    
    * server : fix empty prompt handling + all slots idle logic
    
    * server : normalize id vars
    
    * server : code style
    
    * server : simplify model chat template validation
    
    * server : code style
    
    * server : minor
    
    * llama : llama_chat_apply_template support null buf
    
    * server : do not process embedding requests when disabled
    
    * server : reorganize structs and enums + naming fixes
    
    * server : merge oai.hpp in utils.hpp
    
    * server : refactor system prompt update at start
    
    * server : disable cached prompts with self-extend
    
    * server : do not process more than n_batch tokens per iter
    
    * server: tests: embeddings use a real embeddings model (#5908)
    
    * server, tests : bump batch to fit 1 embedding prompt
    
    * server: tests: embeddings fix build type Debug is randomly failing (#5911)
    
    * server: tests: embeddings, use different KV Cache size
    
    * server: tests: embeddings, fixed prompt do not exceed n_batch, increase embedding timeout, reduce number of concurrent embeddings
    
    * server: tests: embeddings, no need to wait for server idle as it can timout
    
    * server: refactor: clean up http code (#5912)
    
    * server : avoid n_available var
    
    ggml-ci
    
    * server: refactor: better http codes
    
    * server : simplify json parsing + add comment about t_last
    
    * server : rename server structs
    
    * server : allow to override FQDN in tests
    
    ggml-ci
    
    * server : add comments
    
    ---------
    
    Co-authored-by: Pierrick Hymbert <pierrick.hymbert@gmail.com>

commit ceca1aef0738b57951cd12c603c3477e75312dec
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Thu Mar 7 16:34:31 2024 +0800

    [SYCL] fix error when set main gpu to non-zero (#5901)
    
    * fix error when set main gpu to non-zero
    
    * fix delete condition

commit e04e04f8fad549bb0b3ec1c91f0413aeb08baf29
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Wed Mar 6 15:42:23 2024 -0500

    ggml : use SYS_get_cpu if SYS_getcpu is not defined (#5906)
    
    Fixes #5694
    Fixes ggerganov/whisper.cpp#1894

commit e25fb4b18fcedb9bed6be4585cf842e9a669b28b
Author: bobqianic <129547291+bobqianic@users.noreply.github.com>
Date:   Wed Mar 6 07:35:07 2024 +0000

    ggml : use `uint8x16_t` return type for `ggml_vqtbl1q_u8` (#5894)
    
    * use uint8x16_t
    
    * Update ggml-quants.c

commit 1e35d619a6fb0b9c5e3dc955345980ff056ddbaf
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 6 09:12:25 2024 +0200

    convert : remove AWQ remnants (#5768)

commit 8ced9f7e3225adb8501e9821ed1bbd92e3a5c7ae
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Wed Mar 6 12:08:32 2024 +0800

    add wait() to make code stable (#5895)

commit 652ca2bded3c818320d92c70d2b67f64bdbff5e5
Author: slaren <slarengh@gmail.com>
Date:   Tue Mar 5 22:27:29 2024 +0100

    compare-llama-bench.py : remove mul_mat_q (#5892)

commit bd836944f826f07e19b7edcf994a78728da49c1c
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Tue Mar 5 11:56:37 2024 -0500

    quants : use MM256_SET_M128I consistently to fix gcc 7 build (#5889)

commit 3de31677d36aa4f82d4d99898902d7bcf398e666
Author: ExtReMLapin <3909752+ExtReMLapin@users.noreply.github.com>
Date:   Tue Mar 5 17:33:08 2024 +0100

    grammars : blacklists character control set (#5888)
    
    * Prevent control characters from being served in json string
    
    * Prevent control characters from being served in json string (array)

commit 82cb31eb93fd19b74115e0f0133225d1dfdbfdbc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 5 15:56:24 2024 +0200

    Revert "grammars : don't allow to output unescaped new line in string (#5885)"
    
    This reverts commit b1a4e994fde929300d4aeb1deb8320c59cb6edec.

commit b1a4e994fde929300d4aeb1deb8320c59cb6edec
Author: ExtReMLapin <3909752+ExtReMLapin@users.noreply.github.com>
Date:   Tue Mar 5 14:44:29 2024 +0100

    grammars : don't allow to output unescaped new line in string (#5885)
    
    * Don't allow grammar json array to output unescaped new line in string
    
    * Don't allow new line in json object string

commit 61d1c88e155515dd03940913a5707ea84a8b119b
Author: 0cc4m <picard12@live.de>
Date:   Tue Mar 5 13:33:42 2024 +0100

    Vulkan Improvements (#5835)
    
    * Improve dequant shaders, add fast q4_0 dequant
    
    * Optimize dmmv non-kquants for GCN
    
    Remove unnecessary SPIR-V shader duplication
    
    * Fix q4_0 dequant dispatch sizes
    
    Fix backend free bug
    
    * Optimize dequant shaders for q4_1, q5_0, q5_1 and q8_0
    
    * Add unary and binary op shader templates
    
    * Fix Vulkan check results
    
    * Enable non-contiguous support for simple ops
    
    * Add argsort
    
    Basic q4_0 mmq shader and unit test
    
    * Speed up q4_0 dequant code, enable mmq for q4_0
    
    * Rework matmul pipeline selection
    
    * Add soft_max alibi support
    
    * Add q4_1, q5_0, q5_1 and q8_0 dequant mat mat mul shaders
    
    * Add environment variable GGML_VK_FORCE_MAX_ALLOCATION_SIZE to limit max buffer size
    
    Rename GGML_VULKAN_DISABLE_F16 to GGML_VK_DISABLE_F16 for consistency

commit 21b08674331e1ea1b599f17c5ca91f0ed173be31
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Tue Mar 5 16:08:35 2024 +0800

    [SYCL] fix mul_mat fault in CI/unit-test (#5862)
    
    * fix mul_mat fault in cpy_f32_f16
    
    * rm unused function
    
    * add wait() for memcpy
    
    * restore ci/run.sh, rename struct defination, fix bug in ggml_sycl_op_mul_mat_sycl
    
    * fix format issue
    
    * llama : fix segfault from unknown model arch name (#5820)
    
    * llama : fix segfault from unknown model arch name
    
    * llama : make all LLM maps const
    
    This also requires using `std::map::at` instead of its `operator[]`
    which does not exist for const maps.
    
    * llama : name LLM_ARCH_UNKNOWN to "(unknown)"
    
    This avoids errors from `std::map::at` when
    getting the general name of the model architecture.
    Using "(unknown)" instead of an empty string as per suggestion
    https://github.com/ggerganov/llama.cpp/pull/5820#issuecomment-1973735284
    
    * llama : remove redundant inner const for LLM_TENSOR_NAMES
    
    The extra const won't do anything here as const maps
    return const references to values.
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * llama : remove redundant nullptr check in llm_arch_from_string
    
    Since LLM_ARCH_NAMES is a const map, no spurious elements
    with a NULL name are inserted anymore, so this check is dead code.
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * llama : refactor internal quantization functions (#5830)
    
    * scripts : add pod-llama.sh
    
    * ggml : IQ3_S improvements (#5829)
    
    * iq3_s: somewhat faster AVX2 dot product
    
    On Ryzen a 7950X TG-128 increases to 16 t/s from 15.5 t/s using
    16 threads. For 8 threads it is 13.85 t/s vs 11.75 t/s.
    PP-512 increases to 28.5 t/s from 23.8 t/s.
    
    * iq3_s: somewhat faster ARM_NEON dot product
    
    Still dog slow - 10.7 t/s up from 9.9 t/s.
    
    * iq3_s: another small ARM_NEON improvement
    
    10.7 -> 11.0 t/s. Using vmulq_s8 is faster than the xor - sub trick
    that works best on AVX2.
    
    * iq3_s: minor improvement on Metal
    
    49.4 t/s -> 50.3 t/s
    
    * iq3_s: PPL improvement
    
    E.g., for a context of 4096 LLaMA-v2-7B goes to 5.1340 from 5.1653.
    
    * iq3_s: use new grid everywhere
    
    * Fix ARM_NEON
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    
    * convert-hf : make model class definitions self-contained (#5825)
    
    * convert : automatically fall back to HfVocab if tokenizer.model doesn't exist (#5821)
    
    * ggml : fix IQ3_S AVX implementation (#5834)
    
    ggml-ci
    
    * llama : add abort_callback to interrupt computation (#5409)
    
    * using abort_callback from ggml to stop llama computation
    
    * format fix
    
    * a brief explaining comment
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * server: tests: passkey challenge /  self-extend with context shift demo (#5832)
    
    * server: tests: add models endpoint scenario
    
    * server: /v1/models add some metadata
    
    * server: tests: add debug field in context before scenario
    
    * server: tests: download model from HF, add batch size
    
    * server: tests: add passkey test
    
    * server: tests: add group attention params
    
    * server: do not truncate prompt tokens if self-extend through group attention is enabled
    
    * server: logs: do not truncate log values
    
    * server: tests - passkey - first good working value of nga
    
    * server: tests: fix server timeout
    
    * server: tests: fix passkey, add doc, fix regex content matching, fix timeout
    
    * server: tests: fix regex content matching
    
    * server: tests: schedule slow tests on master
    
    * server: metrics: fix when no prompt processed
    
    * server: tests: self-extend add llama-2-7B and Mixtral-8x7B-v0.1
    
    * server: tests: increase timeout for completion
    
    * server: tests: keep only the PHI-2 test
    
    * server: tests: passkey add a negative test
    
    * flake.lock: Update (#5842)
    
    Flake lock file updates:
    
    • Updated input 'flake-parts':
        'github:hercules-ci/flake-parts/b253292d9c0a5ead9bc98c4e9a26c6312e27d69f' (2024-02-01)
      → 'github:hercules-ci/flake-parts/f7b3c975cf067e56e7cda6cb098ebe3fb4d74ca2' (2024-03-01)
    • Updated input 'flake-parts/nixpkgs-lib':
        'github:NixOS/nixpkgs/97b17f32362e475016f942bbdfda4a4a72a8a652?dir=lib' (2024-01-29)
      → 'github:NixOS/nixpkgs/1536926ef5621b09bba54035ae2bb6d806d72ac8?dir=lib' (2024-02-29)
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/cbc4211f0afffe6dfd2478a62615dd5175a13f9a' (2024-02-23)
      → 'github:NixOS/nixpkgs/1536926ef5621b09bba54035ae2bb6d806d72ac8' (2024-02-29)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
    
    * server : init http requests thread pool with --parallel if set (#5836)
    
    * ci : schedule slow server tests only on Release or on demand (#5839)
    
    * llama : fix llama_copy_state_data with fragmented KV cache (#5840)
    
    The row size of the saved states was based on kv_self.head while
    it should be based on llama_kv_cache_cell_max.
    
    Existing session files should still work.
    
    * llama : fix llama_kv_cache_cell_max inability to return 1
    
    I've also changed its return type to uint32_t,
    because this function is always used to set the value of uint32_t variables,
    and because the index already has this type.
    
    * llama : fix state size calculation
    
    Some bytes in the state were unaccounted for in llama_get_state_size.
    Since the logits reserve so much space, it did not cause problems.
    
    * gguf-dump : support i-quants (#5841)
    
    Co-authored-by: Black_Fox <radekliska@gmail.com>
    
    * llama : allow for user specified embedding pooling type (#5849)
    
    * allow for user specified pooling type
    
    * llama : use enum types over int
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * readme : add API changes section
    
    * cuda : fix data race in soft max (#5853)
    
    * main : support special tokens as reverse/anti prompt (#5847)
    
    * Support special tokens as reverse/anti prompt.
    
    * Tokenize antiprompts only once.
    
    * main : minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * common : use LLAMA_DEFAULT_SEED (#5855)
    
    * add some new ops, fix some operators and add batch operations to certain operators. (ggml/747)
    
    * cuda: fix group_norm
    
    * cuda: add batch inference support for ggml_pad/ggml_upscale
    
    * add ggml_arrange
    
    * add ggml_timestep_embedding
    
    * update ggml_arange/ggml_timestep_embedding tests
    
    * cuda: fix im2col
    
    * add ggml_arange/ggml_timestep_embbeding support for metal backend
    
    * fix some bugs
    
    * fix some bugs
    
    * Update ggml.h
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml-cuda.cu
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml-metal.m
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml-metal.m
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml-metal.metal
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * modify according to the review comments
    
    * ggml : fix compile warnings + code style
    
    * ggml : normalize compute_forward calls + fix seg fault in debug
    
    * minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * sync : ggml
    
    * add alias for chat template (#5858)
    
    * speculative : implement stochastic speculative sampling (#5625)
    
    * (WIP) Implement stochastic speculative decoding
    
    * sample from residual distribution on draft accept failure
    
    * fix #5657: force greedy sampling with probs when temp is 0
    
    * remove p_accept parameter
    
    * fix style
    
    * remove unused variables
    
    * add srand() in speculative.cpp
    
    * replace use of rand() with mt19937 sampling
    
    * fixes based on review (@JohannesGaessler)
    
    * fix r random generation
    
    * randomly select next sequence to verify + fix bug in memory freeing
    
    * fix bug in active_seqs sync
    
    * fix uniform int distribution initialization
    
    * remove warnings from comparison between int and size_t
    
    * check grammar in `llama_sample_probability_distribution_impl`
    
    * remove malloc code by utilizing vectors
    
    * add PR link to README
    
    * cmake : handle cases where git index is not found in .git (#5844)
    
    * Update CMakeLists.txt
    
    * Update CMakeLists.txt
    
    * ggml : introduce ggml_status (ggml/750)
    
    * using enum as an exit code instead of macros
    
    * update return type from enum to unsigned int
    
    * indentation fix
    
    * compound update
    ggml_compute_exit_code -> ggml_status
    changed ggml_status from a bit-field type to simple codes
    ggml_status to string cast
    
    * ggml_status to string cast
    
    * GGML_CALL was removed
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * sync : ggml
    
    ggml-ci
    
    * ggml : fix unknown status (#0)
    
    * flake : fix
    
    * llama : fix embeddings (#5796)
    
    * llama : fix embeddings
    
    ggml-ci
    
    * llama : do not use KV cache for non-causal models
    
    ggml-ci
    
    * embeddings : fix llama_batch_init arg
    
    * llama : add pooling switch
    
    * llama : distinguish token vs sequence embeddings
    
    ggml-ci
    
    * llama : assert pooling tensor
    
    * llama : simplify causal mask condition
    
    ggml-ci
    
    * llama : assert input batch with pooling enabled
    
    * readme : update API changes list
    
    * nix: static build (#5814)
    
    * fix speculative decoding build on windows (#5874)
    
    * rebase and rm tailing space
    
    ---------
    
    Co-authored-by: LiangtaoJin <liang-tao.jin@intel.com>
    Co-authored-by: compilade <113953597+compilade@users.noreply.github.com>
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>
    Co-authored-by: Michael Podvitskiy <podvitskiymichael@gmail.com>
    Co-authored-by: Pierrick Hymbert <pierrick.hymbert@gmail.com>
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
    Co-authored-by: Nindaleth <Nindaleth@users.noreply.github.com>
    Co-authored-by: Black_Fox <radekliska@gmail.com>
    Co-authored-by: Douglas Hanley <thesecretaryofwar@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: DAN™ <dranger003@gmail.com>
    Co-authored-by: leejet <leejet714@gmail.com>
    Co-authored-by: Minsoo Cheong <54794500+mscheong01@users.noreply.github.com>
    Co-authored-by: Dane Madsen <dane_madsen@hotmail.com>
    Co-authored-by: hutli <6594598+hutli@users.noreply.github.com>
    Co-authored-by: Jeffrey Quesnelle <emozilla@nousresearch.com>

commit 6a87ac3a52668e117d97bcea07b529c93188b303
Author: Minsoo Cheong <54794500+mscheong01@users.noreply.github.com>
Date:   Tue Mar 5 15:12:23 2024 +0900

    fix editorconfig check break (#5879)

commit 29eee404746e4696143a4f3a642660a4793a15d8
Author: Jeffrey Quesnelle <emozilla@nousresearch.com>
Date:   Mon Mar 4 19:23:06 2024 -0800

    fix speculative decoding build on windows (#5874)

commit 1d41d6f7c2a666eb9c18a686a4684c4b03289bf3
Author: hutli <6594598+hutli@users.noreply.github.com>
Date:   Tue Mar 5 02:33:08 2024 +0100

    nix: static build (#5814)

commit 29ae62d2ae163e2b68aa0ad3bf2ab4636de0c957
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 4 22:31:20 2024 +0200

    llama : fix embeddings (#5796)
    
    * llama : fix embeddings
    
    ggml-ci
    
    * llama : do not use KV cache for non-causal models
    
    ggml-ci
    
    * embeddings : fix llama_batch_init arg
    
    * llama : add pooling switch
    
    * llama : distinguish token vs sequence embeddings
    
    ggml-ci
    
    * llama : assert pooling tensor
    
    * llama : simplify causal mask condition
    
    ggml-ci
    
    * llama : assert input batch with pooling enabled
    
    * readme : update API changes list

commit e0843afe1b37890b631bc7d3d2da2ed36c862b91
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 4 21:50:50 2024 +0200

    flake : fix

commit a1c6d96ed8f906aa1cda439f7386b1171a22bf9f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 4 20:53:27 2024 +0200

    ggml : fix unknown status (#0)

commit efd8533ef8d0752cef7119eb5dbee412c4dba270
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 4 11:06:39 2024 +0200

    sync : ggml
    
    ggml-ci

commit 9fa262734733573fa629ffc97dfcb971fe3f4832
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Mon Mar 4 10:05:42 2024 +0100

    ggml : introduce ggml_status (ggml/750)
    
    * using enum as an exit code instead of macros
    
    * update return type from enum to unsigned int
    
    * indentation fix
    
    * compound update
    ggml_compute_exit_code -> ggml_status
    changed ggml_status from a bit-field type to simple codes
    ggml_status to string cast
    
    * ggml_status to string cast
    
    * GGML_CALL was removed
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit fe52be11e35358d2fd249f19d7ef5b6f9c08b16b
Author: Dane Madsen <dane_madsen@hotmail.com>
Date:   Tue Mar 5 05:26:55 2024 +1100

    cmake : handle cases where git index is not found in .git (#5844)
    
    * Update CMakeLists.txt
    
    * Update CMakeLists.txt

commit 6d341ab6c53cd51f2921d986d0090cc8b049b39a
Author: Minsoo Cheong <54794500+mscheong01@users.noreply.github.com>
Date:   Tue Mar 5 03:24:00 2024 +0900

    speculative : implement stochastic speculative sampling (#5625)
    
    * (WIP) Implement stochastic speculative decoding
    
    * sample from residual distribution on draft accept failure
    
    * fix #5657: force greedy sampling with probs when temp is 0
    
    * remove p_accept parameter
    
    * fix style
    
    * remove unused variables
    
    * add srand() in speculative.cpp
    
    * replace use of rand() with mt19937 sampling
    
    * fixes based on review (@JohannesGaessler)
    
    * fix r random generation
    
    * randomly select next sequence to verify + fix bug in memory freeing
    
    * fix bug in active_seqs sync
    
    * fix uniform int distribution initialization
    
    * remove warnings from comparison between int and size_t
    
    * check grammar in `llama_sample_probability_distribution_impl`
    
    * remove malloc code by utilizing vectors
    
    * add PR link to README

commit 4ffcdce2ff877ebb683cd217ea38faf20faa5ffe
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Mar 4 12:22:08 2024 +0100

    add alias for chat template (#5858)

commit a0fc62661f0fd2a9edd10ae5617345bbbf972f42
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 4 10:40:04 2024 +0200

    sync : ggml

commit 7d43c585dc174bb586775c22c15e5db9242b5b4b
Author: leejet <leejet714@gmail.com>
Date:   Sun Mar 3 20:23:52 2024 +0800

    add some new ops, fix some operators and add batch operations to certain operators. (ggml/747)
    
    * cuda: fix group_norm
    
    * cuda: add batch inference support for ggml_pad/ggml_upscale
    
    * add ggml_arrange
    
    * add ggml_timestep_embedding
    
    * update ggml_arange/ggml_timestep_embedding tests
    
    * cuda: fix im2col
    
    * add ggml_arange/ggml_timestep_embbeding support for metal backend
    
    * fix some bugs
    
    * fix some bugs
    
    * Update ggml.h
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml-cuda.cu
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml-metal.m
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml-metal.m
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml-metal.metal
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * modify according to the review comments
    
    * ggml : fix compile warnings + code style
    
    * ggml : normalize compute_forward calls + fix seg fault in debug
    
    * minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>

commit 82f3e668adafba647de703f835991e91a96b5ac4
Author: DAN™ <dranger003@gmail.com>
Date:   Mon Mar 4 03:08:19 2024 -0500

    common : use LLAMA_DEFAULT_SEED (#5855)

commit 5a51cc1bb4592f0d71f9af89cd08b11a066ba447
Author: DAN™ <dranger003@gmail.com>
Date:   Mon Mar 4 02:57:20 2024 -0500

    main : support special tokens as reverse/anti prompt (#5847)
    
    * Support special tokens as reverse/anti prompt.
    
    * Tokenize antiprompts only once.
    
    * main : minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 67be2ce1015d070b3b2cd488bcb041eefb61de72
Author: slaren <slarengh@gmail.com>
Date:   Sun Mar 3 14:26:18 2024 +0100

    cuda : fix data race in soft max (#5853)

commit 231ae28f078c3148d097b301f2145f1e3e816cc1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 3 12:44:03 2024 +0200

    readme : add API changes section

commit 475df1d6cf817060028d3ff763cb8097d4ec40d6
Author: Douglas Hanley <thesecretaryofwar@gmail.com>
Date:   Sun Mar 3 04:40:27 2024 -0600

    llama : allow for user specified embedding pooling type (#5849)
    
    * allow for user specified pooling type
    
    * llama : use enum types over int
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 87c2e8b2797860a06af3d6c06b8488a8ff1a09ab
Author: Nindaleth <Nindaleth@users.noreply.github.com>
Date:   Sun Mar 3 09:43:42 2024 +0100

    gguf-dump : support i-quants (#5841)
    
    Co-authored-by: Black_Fox <radekliska@gmail.com>

commit de9692a7d2db66e29e5cb373c6551acc49145ccd
Author: compilade <113953597+compilade@users.noreply.github.com>
Date:   Sun Mar 3 03:41:55 2024 -0500

    llama : fix llama_copy_state_data with fragmented KV cache (#5840)
    
    The row size of the saved states was based on kv_self.head while
    it should be based on llama_kv_cache_cell_max.
    
    Existing session files should still work.
    
    * llama : fix llama_kv_cache_cell_max inability to return 1
    
    I've also changed its return type to uint32_t,
    because this function is always used to set the value of uint32_t variables,
    and because the index already has this type.
    
    * llama : fix state size calculation
    
    Some bytes in the state were unaccounted for in llama_get_state_size.
    Since the logits reserve so much space, it did not cause problems.

commit e6029348e86c3810d4435faee54ba822cb43e2ef
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Mar 3 09:35:23 2024 +0100

    ci : schedule slow server tests only on Release or on demand (#5839)

commit 8ef969afcec1645d2d9c3ab1fc82263bba968989
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Mar 3 08:48:36 2024 +0100

    server : init http requests thread pool with --parallel if set (#5836)

commit fa974646e1a2024fc7dc9e6f27cf1f2f5d4a3763
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 3 06:11:31 2024 +0200

    flake.lock: Update (#5842)
    
    Flake lock file updates:
    
    • Updated input 'flake-parts':
        'github:hercules-ci/flake-parts/b253292d9c0a5ead9bc98c4e9a26c6312e27d69f' (2024-02-01)
      → 'github:hercules-ci/flake-parts/f7b3c975cf067e56e7cda6cb098ebe3fb4d74ca2' (2024-03-01)
    • Updated input 'flake-parts/nixpkgs-lib':
        'github:NixOS/nixpkgs/97b17f32362e475016f942bbdfda4a4a72a8a652?dir=lib' (2024-01-29)
      → 'github:NixOS/nixpkgs/1536926ef5621b09bba54035ae2bb6d806d72ac8?dir=lib' (2024-02-29)
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/cbc4211f0afffe6dfd2478a62615dd5175a13f9a' (2024-02-23)
      → 'github:NixOS/nixpkgs/1536926ef5621b09bba54035ae2bb6d806d72ac8' (2024-02-29)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit 9731134296af3a6839cd682e51d9c2109a871de5
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Mar 2 22:00:14 2024 +0100

    server: tests: passkey challenge /  self-extend with context shift demo (#5832)
    
    * server: tests: add models endpoint scenario
    
    * server: /v1/models add some metadata
    
    * server: tests: add debug field in context before scenario
    
    * server: tests: download model from HF, add batch size
    
    * server: tests: add passkey test
    
    * server: tests: add group attention params
    
    * server: do not truncate prompt tokens if self-extend through group attention is enabled
    
    * server: logs: do not truncate log values
    
    * server: tests - passkey - first good working value of nga
    
    * server: tests: fix server timeout
    
    * server: tests: fix passkey, add doc, fix regex content matching, fix timeout
    
    * server: tests: fix regex content matching
    
    * server: tests: schedule slow tests on master
    
    * server: metrics: fix when no prompt processed
    
    * server: tests: self-extend add llama-2-7B and Mixtral-8x7B-v0.1
    
    * server: tests: increase timeout for completion
    
    * server: tests: keep only the PHI-2 test
    
    * server: tests: passkey add a negative test

commit 4a6e2d6142ab815c964924896891e9ab3e050632
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Sat Mar 2 20:52:25 2024 +0100

    llama : add abort_callback to interrupt computation (#5409)
    
    * using abort_callback from ggml to stop llama computation
    
    * format fix
    
    * a brief explaining comment
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 494c87032613e31c0be99b2735e732871f2c4e4d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 2 20:00:49 2024 +0200

    ggml : fix IQ3_S AVX implementation (#5834)
    
    ggml-ci

commit 4d4d2366fc9c54d4a275065cfe9299c6cf7c5b78
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Sat Mar 2 12:27:26 2024 -0500

    convert : automatically fall back to HfVocab if tokenizer.model doesn't exist (#5821)

commit c7a0ad8ec9ebb5ddb1c1c80c82f2ee041c525d47
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Sat Mar 2 12:21:47 2024 -0500

    convert-hf : make model class definitions self-contained (#5825)

commit bbde6eb2561153aabbdfac5001c690fe00cad639
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sat Mar 2 17:00:51 2024 +0200

    ggml : IQ3_S improvements (#5829)
    
    * iq3_s: somewhat faster AVX2 dot product
    
    On Ryzen a 7950X TG-128 increases to 16 t/s from 15.5 t/s using
    16 threads. For 8 threads it is 13.85 t/s vs 11.75 t/s.
    PP-512 increases to 28.5 t/s from 23.8 t/s.
    
    * iq3_s: somewhat faster ARM_NEON dot product
    
    Still dog slow - 10.7 t/s up from 9.9 t/s.
    
    * iq3_s: another small ARM_NEON improvement
    
    10.7 -> 11.0 t/s. Using vmulq_s8 is faster than the xor - sub trick
    that works best on AVX2.
    
    * iq3_s: minor improvement on Metal
    
    49.4 t/s -> 50.3 t/s
    
    * iq3_s: PPL improvement
    
    E.g., for a context of 4096 LLaMA-v2-7B goes to 5.1340 from 5.1653.
    
    * iq3_s: use new grid everywhere
    
    * Fix ARM_NEON
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit ef2cd694c4155fbf25bae61c5178c47eb3676dba
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 2 16:54:08 2024 +0200

    scripts : add pod-llama.sh

commit 6c32d8c7ad8ba7b6ad2a162e929a21dd04fcdca0
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat Mar 2 15:19:09 2024 +0100

    llama : refactor internal quantization functions (#5830)

commit 802da0091ba646ecf02e1a8fae2da0b8e76409bd
Author: compilade <113953597+compilade@users.noreply.github.com>
Date:   Sat Mar 2 08:42:56 2024 -0500

    llama : fix segfault from unknown model arch name (#5820)
    
    * llama : fix segfault from unknown model arch name
    
    * llama : make all LLM maps const
    
    This also requires using `std::map::at` instead of its `operator[]`
    which does not exist for const maps.
    
    * llama : name LLM_ARCH_UNKNOWN to "(unknown)"
    
    This avoids errors from `std::map::at` when
    getting the general name of the model architecture.
    Using "(unknown)" instead of an empty string as per suggestion
    https://github.com/ggerganov/llama.cpp/pull/5820#issuecomment-1973735284
    
    * llama : remove redundant inner const for LLM_TENSOR_NAMES
    
    The extra const won't do anything here as const maps
    return const references to values.
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * llama : remove redundant nullptr check in llm_arch_from_string
    
    Since LLM_ARCH_NAMES is a const map, no spurious elements
    with a NULL name are inserted anymore, so this check is dead code.
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

commit 715641391dda1ff9762dc5d99d9a30acce99f2c6
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Sat Mar 2 19:49:30 2024 +0800

    Support multiple GPUs (split mode) on SYCL backend (#5806)
    
    * suport multiple cards: split-mode - layer|row
    
    * rm warning
    
    * rebase with master, support tow new OPs, close feature for -sm=row, fix for unit test
    
    * update news
    
    * fix merge error
    
    * update according to review comments

commit 9bf297a02bfbd474e51912409a470dd797e2fe13
Author: crasm <crasm@git.vczf.net>
Date:   Sat Mar 2 00:11:06 2024 -0500

    workflows : remove nocleanup arg for check-requirements.sh (#5826)
    
    Reduces peak tmpfs usage and should prevent the check from failing from
    running out of space.
    
    Fixes the 'No space left on device' issue mentioned in #5703.

commit cb5e8f7fc4ee57d4bcccafbe04a82cededd35486
Author: Tushar <ditsuke@protonmail.com>
Date:   Sat Mar 2 04:48:26 2024 +0530

    build(nix): Introduce flake.formatter for `nix fmt` (#5687)
    
    * build(nix): Introduce flake.formatter for `nix fmt`
    * chore: Switch to pkgs.nixfmt-rfc-style

commit da3b9ba2b710c0f8b44398a0eb9e5a7ae2ad967a
Author: nold <Nold360@users.noreply.github.com>
Date:   Fri Mar 1 22:51:12 2024 +0100

    convert-hf-to-gguf : require einops for InternLM2ForCausalLM (#5792)

commit c29af7e2252d288f2ea58a7d437c1cb7c0abf160
Author: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>
Date:   Sat Mar 2 01:00:46 2024 +0530

    llama : add StarCoder2 support (#5795)
    
    * Add support for starcoder2
    
    * handle rope type
    
    * skip rope freq and rotary embeddings from being serialized
    
    * resolve comments
    
    * Update llama.cpp
    
    * remove redundant changes
    
    * handle `rope-theta`
    
    * llama : change starcoder2 rope type
    
    * address comment
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 38d16b142624bdd7c41d9955752b7f7b59c5e048
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 1 20:00:58 2024 +0200

    server : remove api_like_OAI.py proxy script (#5808)

commit c2224f003bf9cf558b1a3c57033563e11a4de9a5
Author: ddpasa <112642920+ddpasa@users.noreply.github.com>
Date:   Fri Mar 1 18:00:00 2024 +0100

    ggml-vulkan: fix VULKAN_CHECK_RESULTS flag, which was previously broken (#5813)

commit e7433867288d2f142cffe596f3751bda5d7ee2c7
Author: kunal-vaishnavi <115581922+kunal-vaishnavi@users.noreply.github.com>
Date:   Fri Mar 1 06:08:08 2024 -0800

    gemma : fix bfloat16 -> float16 conversion issue (#5810)

commit f49a5356865ced0eca1df9f9d84631dfef71b9dc
Author: Miwa / Ensan <63481257+ensan-hcl@users.noreply.github.com>
Date:   Fri Mar 1 22:48:56 2024 +0900

    common : fix flag `--logits-all` to `--all-logits` (#5805)

commit 3ab8b3a92ede46df88bc5a2dfca3777de4a2b2b6
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Mar 1 12:39:06 2024 +0100

    llama : cleanup unused mmq flags (#5772)
    
    * cleanup unused --no-mul-mat-q,-nommq, -mmq, --mul-mat-q, mul_mat_q
    
    * remove: mul_mat_q in compare llama bench and usage
    
    * update llama-bench
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 9600d59e010c18f5872580a21734ea1bf1968d04
Author: Douglas Hanley <thesecretaryofwar@gmail.com>
Date:   Fri Mar 1 03:15:36 2024 -0600

    unicode : switch to multimap based nfd_map (#5799)
    
    * switch to multimap based nfd_map due to compile time issues
    
    * simplify multimap keys
    
    * dont construct new locale every time

commit 5cb02b4a012bb16c6c699c0c62c05ffa653eee0f
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Mar 1 10:08:08 2024 +0100

    server: allow to override threads server pool with --threads-http (#5794)

commit 6ea0f010ff6967034528d9e0b8330b9b0f0b7c13
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Fri Mar 1 08:54:53 2024 +0000

    ci : add Ubuntu 22 Vulkan CI run (#5789)

commit f105471ef6aa4727afac8240da398590d7277f45
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 1 09:59:43 2024 +0200

    server : fix newlines in help (#5785)

commit 38d152160898b0173ffe4dc7df5daadcbd2eceb0
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Fri Mar 1 07:36:47 2024 +0000

    [SYCL] Use batched mul_mat pathway (#5591)
    
    * Use batched mul_mat pathway
    
    * rm extra line
    
    * Explicitly state scaled data type
    
    ---------
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>

commit 052051d8ae4639a1c3c61e7da3237bcc572469d4
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Feb 29 21:42:11 2024 +0100

    Server: normalize naming (#5779)
    
    * server: normalize naming
    
    * fix spacing

commit d5ab29757ebc59a30f03e408294ec20628a6374e
Author: Marcus Dunn <51931484+MarcusDunn@users.noreply.github.com>
Date:   Thu Feb 29 00:17:23 2024 -0800

    llama : constified `llama_set_state_data`'s `src` (#5774)

commit 87c91c07663b707e831c59ec373b5e665ff9d64a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Feb 28 21:44:21 2024 +0200

    ci : reduce 3b ppl chunks to 1 to avoid timeout (#5771)
    
    ggml-ci

commit 317709b2a81dbaf87850202686ec5bb2602a504e
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Wed Feb 28 19:33:37 2024 +0000

    make portability_enumeration_ext apple only (#5757)

commit 08c5ee87e4cceb603ecceac90734fcdade57311b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Feb 28 18:43:38 2024 +0200

    llama : remove deprecated API (#5770)
    
    ggml-ci

commit 78aacf36344df724cdca9f1e1af849b2d2519cb8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Feb 28 17:36:53 2024 +0200

    awq-py : remove (#5768)

commit 8c0e8f4e73e275756ad69f9c99b26ead085ca9f0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Feb 28 11:17:32 2024 +0200

    sync : ggml

commit 2774b0c97427ee3ad3e2ee121354d078794e89d9
Author: slaren <slarengh@gmail.com>
Date:   Sun Feb 25 20:41:35 2024 +0100

    add google magika inference example (ggml/748)
    
    * add magika inference example
    
    * ggml : fix unaligned accesses in custom ops
    
    * ggml : fix FP32 GELU for values that exceed the FP16 range
    
    * use ggml_pool_1d
    
    * add README
    
    * Update README.md
    
    * pad inputs if the files are too small
    
    * cleanup
    
    ggml-ci

commit 5f706718566e3a5147916dc381f3b99de0ffad47
Author: UEXTM.com <84163508+uextm@users.noreply.github.com>
Date:   Sat Feb 24 11:27:36 2024 -0500

    Introduce backend GUIDs (ggml/743)
    
    * Introduce backend GUIDs
    
    Initial proposed implementation of backend GUIDs
    (Discussed in https://github.com/ggerganov/ggml/pull/741)
    
    Hardcoded CPU backend GUID (for now)
    Change ggml_backend_is_cpu logic to use GUID
    
    * Remove redundant functions
    
    Remove redundant functions `ggml_backend_i::get_name` and `ggml_backend_guid` which are not desired for future expansion
    
    * Add spaces to match style
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Fix brace style to match
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Add void to () in function signature
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Add back ggml_backend_guid and make CPU_GUID a local static in ggml_backend_cpu_guid
    
    * add guids to all backends
    
    ggml-ci
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit a693bea1e6762a17b78b6ddf4611e54136941ea2
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed Feb 28 09:55:37 2024 +0100

    server : hit Ctrl+C twice to exit (#5734)
    
    * server: twice ctrl+C to exit
    
    * std::atomic_flag
    
    * sigint: message
    
    * sigint: stderr
    
    * Update examples/server/server.cpp
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

commit adcb12a9bad87bc96f2f158c95892b3d04aa7ffb
Author: compilade <113953597+compilade@users.noreply.github.com>
Date:   Wed Feb 28 03:52:56 2024 -0500

    llama : fix non-quantization of expert gating tensors (#5754)
    
    This reverts a single line from #5475

commit 177628bfd85565070916ad66a5ac4071ee0527d8
Author: Douglas Hanley <thesecretaryofwar@gmail.com>
Date:   Wed Feb 28 02:51:11 2024 -0600

    llama : improve BERT tokenization (#5740)
    
    * implement nfd for stripping accents in wpm tokenizer
    
    * sort nfd map; reuse iterator
    
    * use builtin tolower
    
    * add locale include
    
    * Simplify to_lower cases
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

commit 6c4416868df2e5455da7d20547f62bcf9735ba8e
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed Feb 28 09:39:39 2024 +0100

    readme : add link to LLaVA 1.6 models (#5758)
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit efc72253f7987ed7bdc8bde9d9fa5c7cac2f6292
Author: Jorge A <161275481+jorgealias@users.noreply.github.com>
Date:   Wed Feb 28 01:39:15 2024 -0700

    server : add "/chat/completions" alias for "/v1/...` (#5722)
    
    * Add "/chat/completions" as alias for "/v1/chat/completions"
    
    * merge to upstream master
    
    * minor : fix trailing whitespace
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 7c4263d4261d6ee6f0539d53eb9e1b4d120ba8af
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Wed Feb 28 10:37:02 2024 +0200

    ggml : make i-quants work with super-blocks of 64 (CPU,Metal) (#5760)
    
    * WIP: make i-quants work for QK_K = 64
    
    * iq2_xs: attempt to fix AVX dot product for QK_K = 64
    
    Tests pass, but I get gibberish.
    
    * QK_K = 64 tests pass on ARM_NEON and Metal
    
    Sadly, that does not mean it actually works.
    
    * Make CUDA compile with QK_K = 64
    
    Tests don't pass, plus we get misaligned access
    
    * Q2_K: fixed bug in imatrix quantization for QK_K = 64
    
    * iq1_s: turn off SIMD implementation for QK_K = 64 (it does not work)
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit cb49e0f8c906e5da49e9f6d64a57742a9a241c6a
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Feb 27 19:16:49 2024 +0200

    Attempt to fix android build (#5752)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 0becb22ac05b6542bd9d5f2235691aa1d3d4d307
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Feb 27 16:34:24 2024 +0200

    IQ4_XS: a 4.25 bpw quantization (#5747)
    
    * Try IQ4_NL with blocks of 64 - does not look good
    
    * iq4_xs: go to super-blocks of 256 and 6-bit scales for blocks of 32
    
    * iq4_xs: CUDA works - 133.2 t/s
    
    * iq4_xs: AVX2 dot product
    
    * iq4_xs: ARM_NEON dot product
    
    * iq4_nl: Metal implementation
    
    As usual, Metal / Apple Silicon don't like my quants.
    
    * iq3_xs: minor fix
    
    * iq4_xs: shrink by using IQ3_S for attn_k and attn_q
    
    * iq4_xs: revert using IQ3_S for attn_k and attn_v
    
    PPL vs size is good, but CPU performance suffers: on M2 Max
    TG-128 drops to 21.7 t/s from 28.8, and on a Ryzen-7950X
    to 14.5 t/s from 15.8 t/s. On CUDA we have 135 t/s when
    using IQ3_S vs 133 t/s with pure IQ4_XS.
    
    * Fix CI
    
    * iq4_xs: Added forgotten check for 256 divisibility
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit c24a2a6e6005e5d424301525a42ba45a4a362d30
Author: Engininja2 <139037756+Engininja2@users.noreply.github.com>
Date:   Tue Feb 27 07:22:45 2024 -0600

    cuda : replace remaining shfl_xor with calls to warp_reduce functions (#5744)

commit 1f30b7a9f1b86baa455072d3182b9ebeee0cd845
Author: Engininja2 <139037756+Engininja2@users.noreply.github.com>
Date:   Tue Feb 27 06:50:18 2024 -0600

    ggml-quants : fix avx2 iq1_s vec_dot when compiled with gcc (#5742)

commit 9d533a77d0c3850ce09d736bc1baa67fd6ad27b3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Feb 27 14:35:51 2024 +0200

    llama : fix defrag bugs + add parameter (#5735)
    
    * llama : fix defrag bugs + enable by default
    
    ggml-ci
    
    * llama : add defrag_thold parameter
    
    ggml-ci
    
    * llama : cont
    
    * llama : disable log message
    
    ggml-ci
    
    * llama : fix graph size check during defrag

commit cbbd1efa06f8c09f9dff58ff9d9af509cc4c152b
Author: le.chang <cljs118@126.com>
Date:   Tue Feb 27 10:03:06 2024 +0800

    Makefile: use variables for cublas (#5689)
    
    * make: use arch variable for cublas
    
    * fix UNAME_M
    
    * check opt first
    
    ---------
    
    Co-authored-by: lindeer <le.chang118@gmail.com>

commit b11a93df41921846a10628a7c306d5c82a549939
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Feb 26 23:15:48 2024 +0100

    fix server hangs on empty prompt (#5733)

commit a33e6a0d2a66104ea9a906bdbf8a94d050189d91
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Feb 26 18:28:38 2024 +0200

    Adding IQ2_S and IQ2_M to complete coverage of the 2-3 bit quantization range (#5721)
    
    * Adding IQ2_S and IQ2_M as a single cumulative commit
    
    * Update examples/quantize/quantize.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 47bb7b48c7cec9d8f57d56812ce811ec130b89a3
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Feb 26 15:36:38 2024 +0100

    CUDA: fix DEBUG_CUDA_MALLOC (#5729)

commit c4d7f8178608440506e5489bae0109e4ca12e44a
Author: Artem <guinmoon@gmail.com>
Date:   Mon Feb 26 17:15:28 2024 +0300

    readme : update ui list (#5731)
    
    * Add LLMFarm (ui for iOS) to list

commit e849078c6e09e72fdd2c95ba61f5fba9a7b2d9ef
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Mon Feb 26 14:02:11 2024 +0000

    [SYCL] Add support for soft_max ALiBi (#5639)
    
    * Add support for bias
    
    * Update pre-processor
    
    * rm commented code
    
    * fix format
    
    * fix CI
    
    ---------
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>

commit 67fd33132fab93e6c2087bd6fa656a8a57419efa
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 26 14:02:12 2024 +0200

    unicode : reuse iterator (#5726)

commit 4804215cb833841ffb15a710a16b77ca0a29eb4b
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Mon Feb 26 11:41:34 2024 +0100

    server: CI fix trailing space (#5728)

commit 8a533f0d9078396ebaee9ba213038a1322976dee
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Mon Feb 26 09:56:10 2024 +0100

    server: CI tests reduce build matrix (#5725)

commit 269de86ba073b5dc9ce687c11a3bc4d7d873b962
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 26 08:30:17 2024 +0200

    llama : fix Gemma rope type (#5691)

commit c39373398803c669056304090050fe3f44b41bf9
Author: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
Date:   Sun Feb 25 00:17:11 2024 +0000

    flake.lock: Update
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/5863c27340ba4de8f83e7e3c023b9599c3cb3c80' (2024-02-16)
      → 'github:NixOS/nixpkgs/cbc4211f0afffe6dfd2478a62615dd5175a13f9a' (2024-02-23)

commit e3965cf35aac00d4e24998c8a3d0093ae1d98bd3
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Feb 25 22:48:33 2024 +0100

    server: tests - slow inference causes timeout on the CI (#5715)
    
    * server: tests - longer inference timeout for CI

commit 8b350356b28f782deab63d8b0e9ae103ceb25fcd
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Feb 25 21:46:29 2024 +0100

    server: docs - refresh and tease a little bit more the http server (#5718)
    
    * server: docs - refresh and tease a little bit more the http server
    
    * Rephrase README.md server doc
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update examples/server/README.md
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update examples/server/README.md
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update README.md
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit bf08e00643fd529f748f0a858fd79f3061e3fa18
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 25 22:12:24 2024 +0200

    llama : refactor k-shift implementation + KV defragmentation (#5691)
    
    * llama : refactor k-shift implementation
    
    ggml-ci
    
    * llama : rename llama_kv_cache_seq_shift to llama_kv_cache_seq_add
    
    * llama : cont k-shift refactoring + normalize type names
    
    ggml-ci
    
    * minor : fix MPI builds
    
    * llama : reuse n_rot from the build context
    
    ggml-ci
    
    * llama : revert enum name changes from this PR
    
    ggml-ci
    
    * llama : update llama_rope_type
    
    * llama : add comment about rope values
    
    * llama : fix build
    
    * passkey : apply kv cache updates explicitly
    
    ggml-ci
    
    * llama : change name to llama_kv_cache_update()
    
    * llama : add llama_kv_cache_seq_pos_max()
    
    * passkey : fix llama_kv_cache_seq_pos_max() usage
    
    * llama : some llama_kv_cell simplifications
    
    * llama : add llama_kv_cache_compress (EXPERIMENTAL)
    
    * llama : add alternative KV cache merging (EXPERIMENTAL)
    
    * llama : add llama_kv_cache_defrag
    
    * llama : comments
    
    * llama : remove llama_kv_cache_compress
    
    will add in a separate PR
    
    ggml-ci
    
    * llama : defragment via non-overlapping moves
    
    * llama : ggml_graph based defrag implementation
    
    ggml-ci
    
    * llama : switch the loop order in build_defrag
    
    * llama : add comments

commit f7625019c51ca437a5840576d92362cfa710e4a2
Author: compilade <113953597+compilade@users.noreply.github.com>
Date:   Sun Feb 25 13:43:50 2024 -0500

    server : fix crash when system prompt is bigger than batch size (#5714)
    
    The system prompt is now decoded in batches.
    
    * server : fix off-by-one n_past when start of prompt matches whole cache
    
    The tokens right after the matching part would otherwise skip a pos value.

commit abbabc5e51d0d4656b438aec10b7fae9479ef37d
Author: Radosław Gryta <radek.gryta@gmail.com>
Date:   Sun Feb 25 19:43:00 2024 +0100

    ggml-quants : provide ggml_vqtbl1q_u8 for 64bit compatibility (#5711)
    
    * [ggml-quants] Provide ggml_vqtbl1q_u8 for 64bit compatibility
    
    vqtbl1q_u8 is not part of arm v7 neon library
    
    * [android-example] Remove abi filter after arm v7a fix
    
    * [github-workflows] Do not skip Android armeabi-v7a build

commit f1a98c52546d009f742bdec2154c2a314ea950a6
Author: kwin1412 <42286931+kwin1412@users.noreply.github.com>
Date:   Mon Feb 26 00:46:49 2024 +0800

    make : fix nvcc version is empty (#5713)
    
    fix nvcc version is empty

commit 7d548a1827f6fc6aece6db74c9d112da42c40d68
Author: Ashok Gelal <401055+ashokgelal@users.noreply.github.com>
Date:   Sun Feb 25 10:57:34 2024 -0500

    readme : add Msty to UI list (#5618)

commit 930b1780269a69948d106e2d1b838ab7661f679a
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Feb 25 13:50:32 2024 +0100

    server: logs - unified format and --log-format option (#5700)
    
    * server: logs - always use JSON logger, add add thread_id in message, log task_id and slot_id
    
    * server : skip GH copilot requests from logging
    
    * server : change message format of server_log()
    
    * server : no need to repeat log in comment
    
    * server : log style consistency
    
    * server : fix compile warning
    
    * server : fix tests regex patterns on M2 Ultra
    
    * server: logs: PR feedback on log level
    
    * server: logs: allow to choose log format in json or plain text
    
    * server: tests: output server logs in text
    
    * server: logs switch init logs to server logs macro
    
    * server: logs ensure value json value does not raised error
    
    * server: logs reduce level VERBOSE to VERB to max 4 chars
    
    * server: logs lower case as other log messages
    
    * server: logs avoid static in general
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * server: logs PR feedback: change text log format to: LEVEL [function_name] message | additional=data
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit d52d7819b8ced70c642a88a59da8c78208dc58ec
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Feb 25 13:49:43 2024 +0100

    server: concurrency fix + monitoring - add /metrics prometheus compatible endpoint (#5708)
    
    * server: monitoring - add /metrics prometheus compatible endpoint
    
    * server: concurrency issue, when 2 task are waiting for results, only one call thread is notified
    
    * server: metrics - move to a dedicated struct

commit 12894088170f62e4cad4f8d6a3043c185b414bab
Author: Radosław Gryta <radek.gryta@gmail.com>
Date:   Sun Feb 25 11:53:11 2024 +0100

    cmake : fix compilation for Android armeabi-v7a (#5702)

commit ab336a9d5e5352ecdcdf4c12d2d54cf4ef82ce31
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 25 12:09:09 2024 +0200

    code : normalize enum names (#5697)
    
    * coda : normalize enum names
    
    ggml-ci
    
    * code : cont
    
    * code : cont

commit 69917dfa55674c608360638bb4d6a12a315e2810
Author: Anas Ahouzi <112881240+aahouzi@users.noreply.github.com>
Date:   Sun Feb 25 10:54:04 2024 +0100

    py : fix StableLM conversion after config.json changes (#5703)
    
    * Fix issues during StableLM models conversion
    
    * Fix hard coded layer_norm_eps
    
    * Support layer_norm_eps for LlavaStableLM
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Add missing parenthesis
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Support rotary_factor for LlavaStableLM
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * fix typo
    
    * Add StableLMEpochForCausalLM for safety
    
    Co-authored-by: compilade <113953597+compilade@users.noreply.github.com>
    
    * Add StableLMEpochForCausalLM for safety 2
    
    Co-authored-by: compilade <113953597+compilade@users.noreply.github.com>
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>
    Co-authored-by: compilade <113953597+compilade@users.noreply.github.com>

commit 9e359a4f47c1b2dceb99e29706c9f7403d32ab5e
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Feb 24 19:16:04 2024 +0100

    server: continue to update other slots on embedding concurrent request (#5699)
    
    * server: #5655 - continue to update other slots on embedding concurrent request.
    
    * server: tests: add multi users embeddings as fixed
    
    * server: tests: adding OAI compatible embedding concurrent endpoint
    
    * server: tests: adding OAI compatible embedding with multiple inputs

commit 4c4cb30736582cacb1a164a9d4bc8e17b1014be7
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sat Feb 24 16:23:52 2024 +0200

    IQ3_S: a much better alternative to Q3_K (#5676)
    
    * iq4_nl: squash commits for easier rebase
    
    * Basics (quantize, dequantize)
    * CUDA dequantize and dot product
    * Slightly faster CUDA dot product (120 t/s)
    * Switch to 6-bit scales
    * Scalar dot product
    * AVX2 dot product
    * ARM_NEON dot product
    * Works on metal, but still slow
    * Slightly better Metal dot product
    * Another small Metal improvement
    * Metal dot product is getting there
    * Faster CUDA dot product
    * Add 1/8 ffn_down layers as Q5_K when no imatrix has been provided
    * Report the actual bpw
    * Add _xs mix that is 4.05 bpw for non-MoE models
    * Remove IQ4_XS for now, slightly adjust kvalues_iq4nl
    * AVX2 dot product uses Q8_0 instead of Q8_K
    * Add to test-backend-ops
    * Minor fix
    * Also use use Q5_K for attn_output in MoE models
    * Fixes after merging latest master
    * Switching to blocks of 32
    * AVX2 for blocks of 32
    * Scaler dot product for blocks of 32
    * ARM_NEON dot product for blocks of 32
    * Metal kernels for blocks of 32
    * Slightly faster Metal kernels
    
    * Resurrecting iq3_xs
    
    After all the experimentation, nothing was better than this.
    
    * Minor PPL improvement via a block scale fudge factor
    
    * Minor improvement via 3 neighbours
    
    * iq3_xs: working scalar and AVX2 dot products
    
    * iq3_xs: ARM_NEON dot product - works but extremely slow (10 t/s)
    
    * iq3_xs: working Metal implementation
    
    * Adding IQ3_M - IQ3_XS mix with mostly Q4_K
    
    * iiq3_xs: a 3.4375 bpw variant
    
    * iq3_xs: make CUDA work for new version
    
    * iq3_xs: make scalar and AVX2 work for new version
    
    * iq3_s: make ARM_NEON work with new version
    
    * iq3_xs: make new version work on metal
    
    Performance is very similar to Q3_K_S
    
    * iq3_xs: tiny Metal speed improvement
    
    * iq3_xs: tiny Metal speed improvement
    
    * Fix stupid warning
    
    * Q3_K_XS now uses a mix of IQ3_XS and IQ3_XXS
    
    * iq3_xs: rename to iq3_s
    
    * iq3_s: make tests pass
    
    * Move Q3_K_XS mix to 3.25 bpw
    
    * Attempt to fix failing tests
    
    * Another attempt to fix the Windows builds
    
    * Attempt to fix ROCm
    
    * ROCm again
    
    * iq3_s: partial fix for QK_K = 64
    
    * iq3_s: make it work on metal for QK_K = 64
    
    Pleasent surprise: the coding was super-block size independent,
    so all it took was to delete some QK_K == 256 guards.
    
    * Will this fix ROCm?
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 525213d2f5da1eaf4b922b6b792cb52b2c613368
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Feb 24 12:28:55 2024 +0100

    server: init functional tests (#5566)
    
    * server: tests: init scenarios
     - health and slots endpoints
     - completion endpoint
     - OAI compatible chat completion requests w/ and without streaming
     - completion multi users scenario
     - multi users scenario on OAI compatible endpoint with streaming
     - multi users with total number of tokens to predict exceeds the KV Cache size
     - server wrong usage scenario, like in Infinite loop of "context shift" #3969
     - slots shifting
     - continuous batching
     - embeddings endpoint
     - multi users embedding endpoint: Segmentation fault #5655
     - OpenAI-compatible embeddings API
     - tokenize endpoint
     - CORS and api key scenario
    
    * server: CI GitHub workflow
    
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit fd43d66f46ee3b5345fb8a74a252d86ccd34a409
Author: AlpinDale <52078762+AlpinDale@users.noreply.github.com>
Date:   Fri Feb 23 19:31:54 2024 +0000

    server : add KV cache quantization options (#5684)

commit 54fbcd2ce6c48c9e22eca6fbf9e53fb68c3e72ea
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Fri Feb 23 13:39:14 2024 -0500

    convert : fix missing ftype for gemma (#5690)

commit 15499eb94227401bdc8875da6eb85c15d37068f7
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Thu Feb 22 17:05:23 2024 -0500

    mpt : do not duplicate token_embd.weight on disk (#5670)

commit 96633eeca1265ed03e57230de54032041c58f9cd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 22 23:23:46 2024 +0200

    gemma : use more bits for the token_embd.weight tensor (#5650)
    
    * gemma : use Q8_0 for the token_embd.weight tensor
    
    * llama : quantize token_embd.weight using output type

commit 847eedbdb2d1ebf14ef56eb507d4b4b975510908
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 22 23:22:48 2024 +0200

    py : add Gemma conversion from HF models (#5647)
    
    * py : add gemma conversion from HF models
    
    * Update convert-hf-to-gguf.py
    
    Co-authored-by: Aarni Koskela <akx@iki.fi>
    
    * Update convert-hf-to-gguf.py
    
    Co-authored-by: Aarni Koskela <akx@iki.fi>
    
    * Update convert-hf-to-gguf.py
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>
    
    ---------
    
    Co-authored-by: Aarni Koskela <akx@iki.fi>
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>

commit 7e4f339c404dbe029d4a117c03b37a9bf646cf0e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 22 23:21:39 2024 +0200

    ggml : always define ggml_fp16_t as uint16_t (#5666)
    
    * ggml : always define ggml_fp16_t as uint16_t
    
    ggml-ci
    
    * ggml : cont
    
    ggml-ci
    
    * ggml : cont
    
    * ggml : cont
    
    ggml-ci
    
    * ggml : cont
    
    ggml-ci
    
    * cuda : no longer ggml headers last
    
    ggml-ci
    
    * ggml : fix q6_K FP16 -> FP32 conversion
    
    ggml-ci
    
    * ggml : more FP16 -> FP32 conversion fixes
    
    ggml-ci

commit 334f76fa385ed81095165e5ae068756214893901
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 22 23:21:05 2024 +0200

    sync : ggml

commit efd56b1c2139d50b9b4381a212feb75d69598fda
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 22 18:31:40 2024 +0200

    ggml : 32-bit arm compat (whisper/1891)
    
    * ggml : 32-bit arm compat
    
    * ggml : add ggml_vqtbl1q_s8 impl
    
    * ggml : cont

commit 201294ae177b308fb3a99dc504dd6d27e8afa907
Author: Someone <sergei.kozlukov@aalto.fi>
Date:   Thu Feb 22 19:44:10 2024 +0000

    nix: init singularity and docker images (#5056)
    
    Exposes a few attributes demonstrating how to build [singularity](https://docs.sylabs.io/guides/latest/user-guide/)/[apptainer](https://apptainer.org/) and Docker images re-using llama.cpp's Nix expression.
    
    Built locally on `x86_64-linux` with `nix build github:someoneserge/llama.cpp/feat/nix/images#llamaPackages.{docker,docker-min,sif,llama-cpp}` and it's fast and effective.

commit 5a9e2f60ba3d8362ba17c77ac3092906d49b813f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 22 20:13:25 2024 +0200

    py : minor fixes (#5668)

commit 373ee3fbbabc4c1508eed4f5c3795b23a20939a3
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Feb 22 19:10:21 2024 +0100

    Add Gemma chat template (#5665)
    
    * add gemma chat template
    
    * gemma: only apply system_prompt on non-model message

commit 4cb4d8b22d4fda971621a68c570ce84d66897c37
Author: Someone <sergei.kozlukov@aalto.fi>
Date:   Thu Feb 22 16:32:09 2024 +0000

    workflows: nix: hardcode cachix ids, build unconditionally (#5663)
    
    GitHub does not expose environment and repository variables to PRs coming from forks implies that we've been disabling the Nix CI actions for most PRs.
    
    The `if:` also didn't make much sense, because we can always pull from cachix, and there's no point (albeit no risk either) in pushing cache for the untrusted code.

commit 3a03541cedea474fa9d41214484cc3fbcf468a9e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 22 13:54:03 2024 +0200

    minor : fix trailing whitespace (#5638)

commit 56d03d92be57f5880b9ed94542d87bb6effae31f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 22 10:35:54 2024 +0200

    readme : update hot topics

commit a46f50747b2028f7f9c9883b26bfba12bf92556e
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Feb 22 09:33:24 2024 +0100

    server : fallback to chatml, add AlphaMonarch chat template (#5628)
    
    * server: fallback to chatml
    
    * add new chat template
    
    * server: add AlphaMonarch to test chat template
    
    * server: only check model template if there is no custom tmpl
    
    * remove TODO

commit c5688c6250430d2b8e0259efcf26c16dfa4c1f46
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Thu Feb 22 08:27:32 2024 +0000

    server : clarify some params in the docs (#5640)

commit 4ef245a92a968ba0f18a5adfd41e51980ce4fdf5
Author: Dat Quoc Nguyen <2412555+datquocnguyen@users.noreply.github.com>
Date:   Thu Feb 22 18:15:13 2024 +1000

    mpt : add optional bias tensors (#5638)
    
    Update for MPT with optional bias parameters: to work with PhoGPT and SEA-LION models that were pre-trained with 'bias'.

commit 973053d8b0d04809836b3339a50f68d9c842de90
Author: slaren <slarengh@gmail.com>
Date:   Thu Feb 22 00:42:09 2024 +0100

    llama : fix loading models with shared tok_embd and output (#5651)
    
    ggml-ci

commit 7c8bcc11dc61cf5930b70cd0168b84afcebe12a9
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Feb 22 00:31:00 2024 +0100

    Add docs for llama_chat_apply_template (#5645)
    
    * add docs for llama_chat_apply_template
    
    * fix typo

commit 7fe4678b0244ba7b03eae66ebeaa947e2770bb1a
Author: slaren <slarengh@gmail.com>
Date:   Wed Feb 21 22:52:39 2024 +0100

    llama : fix session save/load with quantized KV (#5649)

commit ba2135ccae7462470b3865c6e41d2e1d734eac05
Author: slaren <slarengh@gmail.com>
Date:   Wed Feb 21 22:18:23 2024 +0100

    gemma : allow offloading the output tensor (#5646)

commit 89febfed9322c8849520dc63c93ee4f5fd72556e
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Wed Feb 21 10:33:54 2024 -0500

    examples : do not assume BOS when shifting context (#5622)

commit 5022cf242d689e15defd133f96c4345ad30c5d19
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Feb 21 16:52:39 2024 +0200

    sync : ggml

commit 1ecea255ebb70750b52688393f37a63606b90e3f
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Wed Feb 21 15:47:48 2024 +0100

    server: health: fix race condition on slots data using tasks queue (#5634)
    
    * server: health: fix race condition on slots data using tasks queue
    
    * server: health:
        * include_slots only if slots_endpoint
        * fix compile warning task.target_id not initialized.

commit a00a35cef93e057eace8351a667d14d152a91ebc
Author: Ettore Di Giacinto <mudler@users.noreply.github.com>
Date:   Wed Feb 21 15:39:10 2024 +0100

    readme : add LocalAI to the availables UI (#5629)

commit eccd7a26ddbff19e4b8805648f5f14c501957859
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Feb 21 16:17:10 2024 +0200

    sync : ggml (#5633)
    
    * ggml : fix conv_2d batch mode (ggml/737)
    
    Co-authored-by: bssrdf <bssrdf@gmail.com>
    
    * ggml : compute forward no longer pass src tensors (ggml/729)
    
    * sync : ggml
    
    ggml-ci
    
    ---------
    
    Co-authored-by: bssrdf <merlintiger@hotmail.com>
    Co-authored-by: bssrdf <bssrdf@gmail.com>

commit c14f72db9c62d71d35eb1c141745c0bd0cb27b49
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Feb 21 15:39:54 2024 +0200

    readme : update hot topics

commit cc6cac08e38e32bf40bbe07e9e8f8f0130b5fd94
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed Feb 21 14:36:57 2024 +0100

    llava : add --skip-unknown to 1.6 convert.py (#5632)
    
    This commit adds the `--skip-unknown` option to the convert.py script
    and removes the saving of the updated checkpoints to avoid updating
    possibly checked out files.
    
    The motivation for this change is that this was done for 1.5
    in Commit fc0c8d286a533363a9a663510b62af85ffad58b3 ("llava :
    update surgery script to not remove tensors") and makes the examples
    more consistent.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 580111d42b3b6ad0a390bfb267d6e3077506eb31
Author: postmasters <namnguyen@google.com>
Date:   Wed Feb 21 05:08:22 2024 -0800

    llama : add `gemma` model (#5631)
    
    There are couple things in this architecture:
    
    1. Shared input and output embedding parameters.
    2. Key length and value length are not derived from `n_embd`.
    
    More information about the models can be found at
    https://ai.google.dev/gemma. GGUFs can be downloaded from
    https://huggingface.co/google.

commit 88c46cbdac05cebd936511b1d3c74112e721615f
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Wed Feb 21 17:52:06 2024 +0800

    [SYCL] conext add name (#5624)
    
    * [SYCL] conext add name
    
    * name should start with SYCL*

commit a14679cc30c785e75d38028bae6ec39c6209ddef
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Wed Feb 21 11:39:52 2024 +0200

    IQ4_NL: 4-bit non-linear quants with blocks of 32 (#5590)
    
    * iq4_nl: squash commits for easier rebase
    
    * Basics (quantize, dequantize)
    * CUDA dequantize and dot product
    * Slightly faster CUDA dot product (120 t/s)
    * Switch to 6-bit scales
    * Scalar dot product
    * AVX2 dot product
    * ARM_NEON dot product
    * Works on metal, but still slow
    * Slightly better Metal dot product
    * Another small Metal improvement
    * Metal dot product is getting there
    * Faster CUDA dot product
    * Add 1/8 ffn_down layers as Q5_K when no imatrix has been provided
    * Report the actual bpw
    * Add _xs mix that is 4.05 bpw for non-MoE models
    * Remove IQ4_XS for now, slightly adjust kvalues_iq4nl
    * AVX2 dot product uses Q8_0 instead of Q8_K
    * Add to test-backend-ops
    * Minor fix
    * Also use use Q5_K for attn_output in MoE models
    * Fixes after merging latest master
    * Switching to blocks of 32
    * AVX2 for blocks of 32
    * Scaler dot product for blocks of 32
    * ARM_NEON dot product for blocks of 32
    * Metal kernels for blocks of 32
    * Slightly faster Metal kernels
    
    * iq4_nl: Fix after merging with master
    
    * iq4_nl: another fix after merging with master
    
    * Use IQ4_NL instead of Q4_K when using k-quants is not possible
    
    * Fix typo that makes several tests fail
    
    * It was the ggml_vdotq thing missed inside the brackets
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 6560bed3f066c876682464762cad90f1e28e3f1b
Author: CJ Pais <cj@cjpais.com>
Date:   Tue Feb 20 11:07:22 2024 -0800

    server : support llava 1.6 (#5553)
    
    * server: init working 1.6
    
    * move clip_image to header
    
    * remove commented code
    
    * remove c++ style from header
    
    * remove todo
    
    * expose llava_image_embed_make_with_clip_img
    
    * fix zig build

commit 06bf2cf8c406e6b70dbf9b431a02fa0ad845b9df
Author: slaren <slarengh@gmail.com>
Date:   Tue Feb 20 20:06:17 2024 +0100

    make : fix debug build with CUDA (#5616)

commit 4ed8e4fbef6a15afd993bfcd9ffa279841e18ef1
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Feb 20 18:30:27 2024 +0100

    llava : add explicit instructions for llava-1.6 (#5611)
    
    This commit contains a suggestion for the README.md in the llava
    example. The suggestion adds explicit instructions for how to convert
    a llava-1.6 model and run it using llava-cli.
    
    The motivation for this is that having explicit instructions similar to
    the 1.5 instructions will make it easier for users to try this out.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 9c405c9f9a7cfd23511fd6b2de05dc72481119b4
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Feb 20 15:58:27 2024 +0100

    Server: use llama_chat_apply_template (#5593)
    
    * server: use llama_chat_apply_template
    
    * server: remove trailing space
    
    * server: fix format_chat
    
    * server: fix help message
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * server: fix formatted_chat
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 5207b3fbc500f89dfe528693e96540956dbaed96
Author: Dane Madsen <dane_madsen@hotmail.com>
Date:   Tue Feb 20 21:00:23 2024 +1100

    readme : update UI list (#5605)
    
    * Add maid to ui list
    
    * Specify licence

commit 8dbbd75754d43ec7b4bbe42fb287cc2553fdf0e9
Author: Haoxiang Fei <tonyfettes@tonyfettes.com>
Date:   Mon Feb 19 22:58:36 2024 -1100

    metal : add build system support for embedded metal library (#5604)
    
    * add build support for embedded metal library
    
    * Update Makefile
    
    ---------
    
    Co-authored-by: Haoxiang Fei <feihaoxiang@idea.edu.cn>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit c0a8c6db371cb3e4379900867b948879f5842201
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Tue Feb 20 08:48:19 2024 +0100

    server : health endpoint configurable failure on no slot (#5594)

commit b9111bd209c7b11b0592450a6ed2e0ca545b2c84
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Tue Feb 20 07:01:25 2024 +0000

    Update ggml_sycl_op_mul_mat_vec_q (#5502)
    
    * Update ggml_sycl_op_mul_mat_vec_q
    
    * Apply suggestions from code review
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
    
    * revert suggestion on macro
    
    * fix bug
    
    * Add quant type GGML_TYPE_IQ1_S to unsupported
    
    * fix format
    
    ---------
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>

commit 633782b8d949f24b619e6c68ee37b5cc79167173
Author: Mathijs de Bruin <mathijs@mathijsfietst.nl>
Date:   Tue Feb 13 20:28:02 2024 +0000

    nix: now that we can do so, allow MacOS to build Vulkan binaries
    
    Author:    Philip Taron <philip.taron@gmail.com>
    Date:      Tue Feb 13 20:28:02 2024 +0000

commit 22f83f0c383e12106692b8afc224d61b8993a52c
Author: 0cc4m <picard12@live.de>
Date:   Sat Feb 10 22:18:33 2024 +0100

    Enable Vulkan MacOS CI

commit bb9dcd560a7e81265398b0d463c40f3e467daf19
Author: 0cc4m <picard12@live.de>
Date:   Wed Feb 14 20:57:17 2024 +0100

    Refactor validation and enumeration platform checks into functions to clean up ggml_vk_instance_init()

commit f50db6ae0bdcb5f8593ca6ca46dfa03b177faa2f
Author: 0cc4m <picard12@live.de>
Date:   Sat Feb 10 22:14:52 2024 +0100

    Add check for VK_KHR_portability_enumeration for MoltenVK support

commit d8c054517dc24f1316f3be12a98fff383e1e93e3
Author: Mathijs de Bruin <mathijs@mathijsfietst.nl>
Date:   Tue Feb 6 14:39:22 2024 +0000

    Add preprocessor checks for Apple devices.
    
    Based on work by @rbourgeat in https://github.com/ggerganov/llama.cpp/pull/5322/files

commit 42f664a3825dfde13a32c3577ab66d10c56f3aa6
Author: Mathijs de Bruin <mathijs@mathijsfietst.nl>
Date:   Sat Feb 3 18:00:11 2024 +0000

    Resolve ErrorIncompatibleDriver with Vulkan on MacOS.
    
    Refs:
    - https://chat.openai.com/share/7020ce72-65fc-45ec-b7be-9d9d798a5f3f
    - https://github.com/SaschaWillems/Vulkan/issues/954
    - https://github.com/haasn/libplacebo/issues/128
    - https://github.com/KhronosGroup/Vulkan-Samples/issues/476

commit 5dde5408978eda22242b87e22e306d1c2d1a5834
Author: Mathijs de Bruin <mathijs@mathijsfietst.nl>
Date:   Sat Feb 3 17:56:46 2024 +0000

    Allow for Vulkan build with Accelerate.
    
    Closes #5304

commit 40c3a6c1e11040088b4a1ce0abc4651cb3011dd4
Author: slaren <slarengh@gmail.com>
Date:   Mon Feb 19 23:40:26 2024 +0100

    cuda : ignore peer access already enabled errors (#5597)
    
    * cuda : ignore peer access already enabled errors
    
    * fix hip

commit f24ed14ee0ce28dfe98115c378b37da144912016
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Mon Feb 19 15:54:12 2024 -0500

    make : pass CPPFLAGS directly to nvcc, not via -Xcompiler (#5598)

commit 9d679f0fccd4030779ed3c7684a40122fe41806c
Author: nopperl <54780682+nopperl@users.noreply.github.com>
Date:   Mon Feb 19 14:14:07 2024 +0000

    examples : support minItems/maxItems in JSON grammar converter (#5039)
    
    * support minLength and maxLength in JSON schema grammar converter
    
    * Update examples/json-schema-to-grammar.py
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 1387cf60f758efb218fa06b670182c38ff149b7b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 19 15:23:17 2024 +0200

    llava : remove extra cont (#5587)

commit 6fd413791a754598a54a366145960f2e27eec015
Author: slaren <slarengh@gmail.com>
Date:   Mon Feb 19 14:02:36 2024 +0100

    llava : replace ggml_cpy with ggml_cont

commit 337c9cbd52918ae5fb9a9d9e25d7fae4e238c9f1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 19 14:54:21 2024 +0200

    sync : ggml
    
    ggml-ci

commit a3145bdc305422973e25f0b066da6f469ed5dc45
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 19 14:53:48 2024 +0200

    ggml-alloc : apply ggml/731

commit 890559ab28e354052e16e770155ad007fd0856e8
Author: Didzis Gosko <didzis@users.noreply.github.com>
Date:   Sun Feb 11 16:41:41 2024 +0200

    metal : option to embed MSL source into compiled binary (whisper/1842)
    
    * ggml : embed Metal library source (ggml-metal.metal) into binary
    
    enable by setting WHISPER_EMBED_METAL_LIBRARY
    
    * rename the build option
    
    * rename the preprocessor directive
    
    * generate Metal library embedding assembly on-fly during build process

commit d0e3ce51f45bd6a646da1952d7e5d143a087db3e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 19 14:45:41 2024 +0200

    ci : enable -Werror for CUDA builds (#5579)
    
    * cmake : pass -Werror through -Xcompiler
    
    ggml-ci
    
    * make, cmake : enable CUDA errors on warnings
    
    ggml-ci

commit 68a6b98b3c8af7e5baade3ee45fe1d2c7b9323a9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 19 13:41:51 2024 +0200

    make : fix CUDA build (#5580)

commit 70d45af0efce9ed360e1858b827989d971dd9caf
Author: valiray <133289098+valiray@users.noreply.github.com>
Date:   Mon Feb 19 02:37:10 2024 -0800

    readme : fix typo in README-sycl.md (#5353)

commit 13e2c771aa4212cd5405cf310203848d50f7f859
Author: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
Date:   Mon Feb 19 14:45:18 2024 +0530

    cmake : remove obsolete sycl compile flags (#5581)
    
    * rm unwanted sycl compile options
    
    * fix bug
    
    * fix bug
    
    * format fix

commit f53119cec4f073b6d214195ecbe1fad3abdf2b34
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 19 10:34:10 2024 +0200

    minor : fix trailing whitespace (#5538)

commit 70847553963c85e86051d06df848236829f5f951
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Mon Feb 19 09:31:59 2024 +0100

    llava : avoid changing the original BakLLaVA model (#5577)
    
    This is a follup of Commit fc0c8d286a533363a9a663510b62af85ffad58b3
    ("llava : update surgery script to not remove tensors") but this time
    the change is to the BakLLaVA specific part of the surgery script.
    
    I've been able to test this using SkunkworksAI/BakLLaVA-1 and it works
    as expected using the instructions in README.md.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 4480542b2271ba1438f0daff8e5f3a74b1dc8609
Author: NawafAlansari <72708095+NawafAlansari@users.noreply.github.com>
Date:   Mon Feb 19 03:25:38 2024 -0500

    baby-llama : allocate graphs in ggml_context (#5573)
    
    * Fixed the baby-llama issue (see issue #4830)
    
    * minor : fix whitespaces
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 11b12de39bd787c0494da0cd405958fdfedc29c4
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Feb 19 09:23:37 2024 +0100

    llama : add llama_chat_apply_template() (#5538)
    
    * llama: add llama_chat_apply_template
    
    * test-chat-template: remove dedundant vector
    
    * chat_template: do not use std::string for buffer
    
    * add clarification for llama_chat_apply_template
    
    * llama_chat_apply_template: add zephyr template
    
    * llama_chat_apply_template: correct docs
    
    * llama_chat_apply_template: use term "chat" everywhere
    
    * llama_chat_apply_template: change variable name to "tmpl"

commit 3a9cb4ca6408c29423373dd6cd7aa78a58286c00
Author: slaren <slarengh@gmail.com>
Date:   Mon Feb 19 09:04:45 2024 +0100

    cuda, metal : fix nans in soft_max (#5574)
    
    * cuda : fix nans in soft_max
    
    * metal : fix nans in soft_max
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 769a716e30ba1da46f709df1c00727d6869d30e7
Author: Mirko185 <mirkosig@gmail.com>
Date:   Mon Feb 19 08:39:31 2024 +0100

    readme : update (#5572)
    
    Added 1.5-bit on README.md

commit f0d1fafc029a056cd765bdae58dcaa12312e9879
Author: bmwl <brian.marshall@tolko.com>
Date:   Sun Feb 18 23:38:32 2024 -0800

    ggml : android and old glibc NUMA incompatibility bugfixes (#5557)
    
    * #ifdef out some code NUMA blocks for Android due to lack of support
    
    * added in some __ANDROID__ if def gates around numa code and forced GLIBC prior to 2.29 to use a syscall for getcpu instead of the wrapper
    
    * Changed gates on numa platform specific stuff to __gnu_linux__ to skip any platforms without glibc
    
    * harmonizing #if defined blocks for numa code to __gnu_linux__ since that's the only model that's being followed anyways
    
    ---------
    
    Co-authored-by: root <root@nenya.lothlorien.ca>

commit a0c2dad9d43456c677e205c6240a5f8afb0121ac
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Sun Feb 18 16:21:52 2024 -0500

    build : pass all warning flags to nvcc via -Xcompiler (#5570)
    
    * build : pass all warning flags to nvcc via -Xcompiler
    * make : fix apparent mis-merge from #3952
    * make : fix incorrect GF_CC_VER for CUDA host compiler

commit 14278f55d2e2c6a53022075c7f2719b71e1cd61d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 18 22:58:57 2024 +0200

    ggml : restore vec dot stride arg names (#5453)

commit b1de96824bdbeb91ea458abcb3e5478690ad0727
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 18 22:39:30 2024 +0200

    ci : fix wikitext url + compile warnings (#5569)
    
    ggml-ci

commit 7ad554f90e735cf2a0f612ce44f9aa4fad6ae46a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 18 21:39:58 2024 +0200

    metal : fix unused warnings (#0)

commit 5ee99c32f5e47c8d32634eff9a47fb32a24c276b
Author: Robey Holderith <robey@flaminglunchbox.net>
Date:   Sun Feb 18 11:11:16 2024 -0800

    common, server : surface min_keep as its own parameter (#5567)
    
    * Feature - surface min_keep as its own parameter
    
    * Updated README with min_keep param

commit c145f8a132b2fe1d1e65987faddbd9a40bef7a12
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Feb 18 18:39:57 2024 +0100

    server : slots monitoring endpoint (#5550)

commit 689a091bbe0537ee9abff3e15a1d74f5f3561165
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 18 19:38:06 2024 +0200

    sampling : do not set min_keep to n_probs (#5564)

commit f3f28c5395cd25b371617981b341616dbdd31e85
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 18 19:17:00 2024 +0200

    cmake : fix GGML_USE_SYCL typo (#5555)

commit e75c6279d1c8e7abb82a331f5de7124eed402de2
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Feb 18 17:31:28 2024 +0100

    server : enhanced health endpoint (#5548)
    
    * server: enrich health endpoint with available slots, return 503 if not slots are available
    
    * server: document new status no slot available in the README.md

commit 36376abe05a12a8cb3af548a4af9b8d0e2e69597
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Feb 18 17:30:09 2024 +0100

    server : --n-predict option document and cap to max value (#5549)
    
    * server: document --n-predict
    
    * server: ensure client request cannot override n_predict if set
    
    * server: fix print usage LF in new --n-predict option

commit 66c1968f7a2e895675425e875b6589f1233a1b52
Author: Daniel Hiltgen <dhiltgen@users.noreply.github.com>
Date:   Sun Feb 18 08:23:16 2024 -0800

    server : graceful server shutdown (#5244)
    
    This updates the server queue to support graceful shutdown of the server on signals.

commit 1dcc3fde004787e6fc4d84c9de0bb34cd2901a3e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 18 18:21:52 2024 +0200

    common : fix ub (#5530)

commit 5d3de51f972055702a1859186fe7acb8f0b43dc4
Author: Herman Semenov <GermanAizek@yandex.ru>
Date:   Sun Feb 18 16:20:12 2024 +0000

    ggml, common, examples, tests : fixed type arguments in printf (#5528)

commit fc0c8d286a533363a9a663510b62af85ffad58b3
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Sun Feb 18 17:19:23 2024 +0100

    llava : update surgery script to not remove tensors (#5536)
    
    This commit updates the surgery script to not remove the tensors from the
    model file. For this to work the `--skip-unknown` flag is added as an
    argument to the convert.py script in README.md.
    
    The motivation for this change is that the surgery script currently
    removes the projector tensors from the model file. If the model was
    checked out from a repository, the model file will have been updated
    and have to be checked out again to reset this effect. If this can be
    avoided I think it would be preferable.
    
    I did not perform this change for BakLLaVA models as I am not sure
    how that part works.

commit bd2d4e393b2b7d2a1b2e201058e26017c9728ead
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Feb 18 18:16:55 2024 +0200

    1.5 bit quantization (#5453)
    
    * iq1_s: WIP basics
    
    * iq1_s: CUDA is working
    
    * iq1_s: scalar CPU dot product
    
    * iq1_s: WIP AVX2 dot product - something is not right
    
    * Fix tests
    
    * Fix shadow warnings
    
    * Fix after merge with latest master
    
    * iq1_s: AVX2 finally works
    
    * iq1_s: ARM_NEON dot product. Works, but not very fast
    
    * iq1_s: better grid
    
    * iq1_s: use IQ2_XXS for attn_output
    
    At a cost of 0.04 extra bpw this gives a big improvement in PPL.
    
    * iq1_s: Metal basics
    
    Dequantize works, but not dot product
    
    * iq1_s: Metal works, but quite slow
    
    As usual, Apple Silicon does not like the code I write.
    
    * iq1_s: Tests
    
    * iq1_s: slightly faster dot product
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit c8e0d7efeb7634ecc2e9832e879ab9fca4510e71
Author: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
Date:   Sun Feb 18 00:17:07 2024 +0000

    flake.lock: Update
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/f8e2ebd66d097614d51a56a755450d4ae1632df1' (2024-02-07)
      → 'github:NixOS/nixpkgs/5863c27340ba4de8f83e7e3c023b9599c3cb3c80' (2024-02-16)

commit 8f1be0d42f23016cb6819dbae01126699c4bd9bc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Feb 17 23:04:16 2024 +0200

    ggml : add ALiBi support for ggml_soft_max_ext (#5488)
    
    * ggml : avoid recomputing alibi slopes (CPU)
    
    * llama : reuse hparams.f_max_alibi_bias in all cases
    
    ggml-ci
    
    * ggml : support alibi bias in ggml_soft_max_ext (CPU + Metal)
    
    ggml-ci
    
    * ggml : handle all SRCs (do not break on first null)
    
    ggml-ci
    
    * tests : do not use slope for large soft_max
    
    accumulates too much error
    
    ggml-ci
    
    * ggml : alternative ALiBi without extra tensor
    
    We compute the slopes in the kernel
    
    ggml-ci
    
    * cuda : add ALiBi support in ggml_soft_max_ext
    
    ggml-ci
    
    * ggml : deprecate ggml_alibi
    
    * ggml : support multi-sequence ALiBi (Metal)
    
    ggml-ci
    
    * cuda : add multi-seq ALiBi + remote F16 soft_max
    
    ggml-ci
    
    * ggml : update deprecation message
    
    * ggml : fix pos ptr when no ALiBi
    
    ggml-ci
    
    * cuda : fix performance (pow -> powf)
    
    * cuda : precompute ALiBi constants
    
    * metal : pre-compute ALiBi slopes
    
    ggml-ci
    
    * llama : init kq_pos only if needed
    
    ggml-ci
    
    * test-backend-ops : add null pos test to soft_max
    
    test-backend-ops : replace soft_max tests
    
    ggml-ci
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 6e4e973b2615f8d390b1c4f4a7e05a119078bb0f
Author: Ananta Bastola <anantarajbastola@gmail.com>
Date:   Sat Feb 17 16:03:14 2024 -0500

    ci : add an option to fail on compile warning (#3952)
    
    * feat(ci): add an option to fail on compile warning
    
    * Update CMakeLists.txt
    
    * minor : fix compile warnings
    
    ggml-ci
    
    * ggml : fix unreachable code warnings
    
    ggml-ci
    
    * ci : disable fatal warnings for windows, ios and tvos
    
    * ggml : fix strncpy warning
    
    * ci : disable fatal warnings for MPI build
    
    * ci : add fatal warnings to ggml-ci
    
    ggml-ci
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit d250c9d61d4d9f7346930814cc4aef3f3673dc3e
Author: clibdev <52199778+clibdev@users.noreply.github.com>
Date:   Sat Feb 17 18:28:37 2024 +0200

    gitignore : update for CLion IDE (#5544)

commit 5bf2b94dd4fb74378b78604023b31512fec55f8f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Feb 16 19:05:56 2024 +0200

    cmake : fix VULKAN and ROCm builds (#5525)
    
    * cmake : fix VULKAN and ROCm builds
    
    * cmake : fix (cont)
    
    * vulkan : fix compile warnings
    
    ggml-ci
    
    * cmake : fix
    
    ggml-ci
    
    * cmake : minor
    
    ggml-ci

commit d2819d5577b35507be83d0c3f4d2d3c0ab1488ca
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Feb 16 15:14:40 2024 +0200

    scripts : add helpers script for bench comparing commits (#5521)
    
    * scripts : add helpers script for bench comparing commits
    
    * scripts : detect CUDA
    
    * set flags after checking the command line
    
    * fix make flags
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 4cb072769804c77ab466bc8351c76ede9d5ba49d
Author: Herman Semenov <GermanAizek@yandex.ru>
Date:   Fri Feb 16 12:43:23 2024 +0000

    llava : removed excess free(NULL) operation (#5531)

commit 65085c713e14f78cdda6abc275b1a5d8c2b8ca15
Author: Herman Semenov <GermanAizek@yandex.ru>
Date:   Fri Feb 16 11:45:48 2024 +0000

    llama : minor fixed return int value (#5529)

commit 6dcc02d2444c779c18d49c364c5d5c5728b6b484
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Fri Feb 16 11:33:25 2024 +0000

    server : add "samplers" param to control the samplers order (#5494)

commit 5f5808ca7b7f23a1fa7a77241842bb84a0e55108
Author: Rőczey Barnabás <31726601+An0nie@users.noreply.github.com>
Date:   Fri Feb 16 11:00:56 2024 +0100

    server : fix system prompt cli (#5516)

commit f486f6e1e5e9d01603d9325ab3e05f1edb362a95
Author: bmwl <brian.marshall@tolko.com>
Date:   Fri Feb 16 01:31:07 2024 -0800

    ggml : add numa options (#5377)
    
    * Added numa options to allow finer grained control as well as plumbing for a new mirror mode that will require numa.h
    
    * Reverted Makefile
    
    * Fixed include
    
    * Removed sched.h from ggml.h, moved ggml_get_numa_affinity into ggml.c, removed trailing whitespace and fixed up a few inconsistent variables
    
    * removed trailing whitespace
    
    * Added numa options to allow finer grained control as well as plumbing for a new mirror mode that will require numa.h
    
    * Reverting Makefile
    
    * Fixed a number of issues with the move from BOOL to ggml_numa_strategies. Added a note about mirror mode note being implemented yet
    
    * Removing MIRROR_MODE code for this PR
    
    * Removing last bit of MIRROR_MODE code for this PR
    
    * Removing unneeded branch in server.cpp example and moving get_numa_affinity and making it static
    
    * Fixed lingering init_llama_backend() bool calls in tests and examples
    
    * Remote enum llama_numa_strategies
    
    * Revert bad merge with dynatemp flags
    
    * add missing enum ggml_numa_strategies declaration and revert sync problem with master
    
    * add missing enum ggml_numa_strategies declaration
    
    * fixed ggml_init_numa variable
    
    * Update ggml.h
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Update READMEs with info about numa flags, change INTERLEAVE strategy name to DISTRIBUTE everywhere, implement the improved distribution strategy from @rankaiyx, fix a spelling mistake and un-merge some bad merges
    
    * split numa init out from llama_backend_init and created llama_numa_init. Updated all code paths and samples
    
    * Fix up some boolean vs enum comparisons
    
    * Added #ifdefs for non-Linux OS that don't have cpu_set_t datatype
    
    * Update ggml.h
    
    Align enum values
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml.c
    
    Remove whitespace
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml.c
    
    align paremeters
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update examples/server/server.cpp
    
    remove whitespace and align brace
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update common/common.cpp
    
    Remove whitespace and align brace
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * unified ggml_numa_strategy enum and fixed text alignment in server.cpp example
    
    * Update ggml.c
    
    simplified return for platforms without NUMA support
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * removed redundant else from cli argument processing of --numa
    
    * whitespace
    
    ---------
    
    Co-authored-by: root <root@nenya.lothlorien.ca>
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>

commit 60ed04cf82dc91ade725dd7ad53f0ee81f76eccf
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Feb 16 10:24:39 2024 +0100

    llava : fix clip-model-is-vision flag in README.md (#5509)
    
    * llava: fix clip-model-is-vision flag in README.md
    
    This commit fixes the flag `--clip_model_is_vision` in README.md which
    is does not match the actual flag:
    ```console
    $ python convert-image-encoder-to-gguf.py --help
    ...
      --clip-model-is-vision
                            The clip model is a pure vision model
                            (ShareGPT4V vision extract for example)
    ```
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * llava: update link to vit config in README.md
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 594845aab1c6775877f6d9545a51dc0f8d0b3d77
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Feb 16 09:57:55 2024 +0200

    ci : fix BERT model download and convert

commit 4524290e87b8e107cc2b56e1251751546f4b9051
Author: Douglas Hanley <thesecretaryofwar@gmail.com>
Date:   Thu Feb 15 11:21:49 2024 -0600

    Use correct type of pooling for embedding models (#5500)
    
    Use correct type of pooling for embedding models

commit c06e45d72983d9ace7b1535f7e7ea258d212169e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 15 18:49:08 2024 +0200

    clip : fix wrong loop condition

commit 9060a1e9dfca6038906e819be5fa42217f49028c
Author: slaren <slarengh@gmail.com>
Date:   Thu Feb 15 16:49:01 2024 +0100

    cuda : print message when initialization fails (#5512)
    
    * cuda : print message when initialization fails
    
    * use CUDA_NAME both times

commit 9350a1cf21b1492c69b20175b73a419b897d6a3a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 15 15:41:15 2024 +0200

    scripts : add hf.sh helper script (#5501)
    
    * scripts : add hf.sh helper scripts
    
    * hf : add error logs
    
    * hf : add support for --repo and --file

commit 73122473ffd73030146276dbb85da7c8021a3ee4
Author: Michaël de Vries <vriesdemichael@gmail.com>
Date:   Thu Feb 15 14:14:37 2024 +0100

    fix(gguf-py): special tokens are no longer skipped when add_<token>_token is set to false (#5487)
    
    * fix(gguf-py): special tokens are no longer skipped when add_<token>_token is set to false
    
    * fix(gguf-py): added missing cls and mask token ids to the gguf metadata

commit 0d4177126b0556e202efb85bf3f768be81076400
Author: Elbios <141279586+Elbios@users.noreply.github.com>
Date:   Thu Feb 15 09:01:57 2024 +0100

    llava : fix memory management bug (#5491)
    
    * Fix memory management in llava and server code
    
    Fixes this error:
    
    llama_new_context_with_model: graph splits (measure): 3
    Available slots:
     -> Slot 0 - max context: 6000
    {"timestamp":1707926446,"level":"INFO","function":"main","line":2623,"message":"model loaded"}
    all slots are idle and system prompt is empty, clear the KV cache
    slot 0 - loaded image
    slot 0 is processing [task id: 0]
    slot 0 : kv cache rm - [0, end)
    slot 0 - encoding image [id: 1]
    munmap_chunk(): invalid pointer
    Aborted
    
    * Make it cleaner by checking size in batch free wrapper

commit 7930a8a6e89a04c77c51e3ae5dc1cd8e845b6b8f
Author: John <78893154+cmp-nct@users.noreply.github.com>
Date:   Thu Feb 15 08:59:18 2024 +0100

    llaba : hotfix for llava-1.6 image number (#5495)
    
    Co-authored-by: John <cmt-nct@users.noreply.github.com>

commit 704359e29985a06a389337a2617b7f3fa8eff908
Author: Neuman Vong <neuman.vong@gmail.com>
Date:   Thu Feb 15 17:11:15 2024 +1100

    vulkan: Find optimal memory type but with fallback (#5381)
    
    * @0cc4m feedback
    
    * More feedback @0cc4m

commit 594fca3fefe27b8e95cfb1656eb0e160ad15a793
Author: Rune <43761327+Rune-AI@users.noreply.github.com>
Date:   Wed Feb 14 16:15:49 2024 +0100

    readme : fix typo (#5490)
    
    executabhle -> executable

commit ccbb277f4642fc0d84c72dbc0d51ed2df418d6ce
Author: John <78893154+cmp-nct@users.noreply.github.com>
Date:   Wed Feb 14 15:49:42 2024 +0100

    llava : update README.md (#5489)
    
    * Update README.md
    
    * Update README.md
    
    * Update examples/llava/README.md
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 8084d554406b767d36b3250b3b787462d5dd626f
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Wed Feb 14 11:49:01 2024 +0300

    cmake : ARM intrinsics detection for MSVC (#5401)

commit aa2341298924ac89778252015efcb792f2df1e20
Author: John <78893154+cmp-nct@users.noreply.github.com>
Date:   Wed Feb 14 08:38:35 2024 +0100

    llava : support v1.6 (#5267)
    
    * Create llava-survery-v2.py
    
    * Update convert-image-encoder-to-gguf.py
    
    * Update convert-image-encoder-to-gguf.py
    
    * Rename llava-survery-v2.py to llava-surgery-v2.py
    
    * Update convert-image-encoder-to-gguf.py
    
    will now search for projector
    
    * Update convert-image-encoder-to-gguf.py
    
    whoops
    
    * Update llava-surgery-v2.py
    
    * Clip: Bugfix for normalization (it did not loat the 3 std and mean values)
    Clip: bicubic resize function
    Clip: added save-to-bmp/pil for debugging and conversion from/to 32/8 images
    Clip: added normalization with FP16 precision simulation (image tensors match HF implementation, can be switched off, only used for llava-1.6)
    Clip: added newline tensor, mergetype kv, image-grid kv, new resize-pad function with resolution from gridpoints
    Clip: clip_image_preprocess now returns a float * vector instead of float, this way llava 1.5 and 1.6 is supported
    llava: added ggml cpu graph for embedding patching, added spatial_unpad preliminary support, added a lot of comments that need to be cleaned when all is final
    convert-image-encoder: fixed image-grid flattening
    
    * whitespace corrections
    
    * ws
    
    * Tensors are now properly permuted.
    Before the embeddings were inserted 1:1, now they are split into the 24x24 patches as in reference.
    
    * ws
    
    * added verbose_prompt support into cli
    added stopwords for llava-1.6 into cli
    
    * moved llava functions to llava.cpp, made clip.h C compatible API, replaced vector style functions with pointers, added a debug define to remove functions from compilation while not needed
    
    * ws
    
    * convert : skip unknown tensors (need for LLaVA)
    
    * llava : update readme
    
    * llava : fix compile warnings
    
    * llava : style
    
    * convert : add --skip-unknown CLI arg
    
    * server : remove clip structs
    
    * bugfix for non llava-1.6
    
    It should now work with llava-1.5 as well
    
    * clip : minor code rearrange
    
    * llava : update readme a bit
    
    ---------
    
    Co-authored-by: John <cmt-nct@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit f5ca054855dea83f424003162f26de376e5643f6
Author: AT <manyoso@users.noreply.github.com>
Date:   Tue Feb 13 15:44:25 2024 -0600

    Early return for zero size calls to get_tensor. (#5482)
    
    * Early return for zero size calls to get_tensor.
    
    Signed-off-by: Adam Treat <treat.adam@gmail.com>
    
    * Update ggml-kompute.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml-kompute.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Add an early return to the get/set tensor when the size is null.
    
    Signed-off-by: Adam Treat <treat.adam@gmail.com>
    
    * Early return after the assertions.
    
    Signed-off-by: Adam Treat <treat.adam@gmail.com>
    
    * Since we do the early return in the generic backend now no reason to do so here as well.
    
    Signed-off-by: Adam Treat <treat.adam@gmail.com>
    
    ---------
    
    Signed-off-by: Adam Treat <treat.adam@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 6c00a066928b0475b865a2e3e709e2166e02d548
Author: John <78893154+cmp-nct@users.noreply.github.com>
Date:   Tue Feb 13 18:56:38 2024 +0100

    gguf : add python reader example (#5216)
    
    * Update CMakeLists.txt
    
    * Create reader.py
    
    * Update reader.py
    
    * Update reader.py
    
    another whitespace :|
    
    * Update reader.py
    
    * lintlintlint

commit ea9c8e11436ad50719987fa23a289c74b7b40d40
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Tue Feb 13 12:03:53 2024 -0500

    llama : add support for Nomic Embed (#5468)

commit c4e6dd59e45ef7b14f7763fb073b517395dc176c
Author: Aarni Koskela <akx@iki.fi>
Date:   Tue Feb 13 18:18:16 2024 +0200

    llama : allow raw byte in SPM vocabs; don't crash on nl 404 (#5478)
    
    * common : don't crash if newline token is not found
    
    * common : llama_byte_to_token: allow falling back to finding just the token byte in SPM vocabs

commit 037259be689353081e7bae3c1ab4ab18e7fbe8c9
Author: Aarni Koskela <akx@iki.fi>
Date:   Tue Feb 13 15:24:50 2024 +0200

    llama : make load error reporting more granular (#5477)
    
    Makes it easier to pinpoint where e.g. `unordered_map::at: key not found` comes from.

commit 263978904c7472db1865409a7ff1129599f6a40b
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Feb 13 14:15:42 2024 +0100

    finetune : rename feed-forward tensors (w1/w2/w3) (#4839)
    
    * finetune: rename feed-forward tensors (w1/w2/w3)
    
    This commit renames the feed-forward tensors w1, w2 and w3 to ffn_gate,
    ffn_down and ffn_up respectively.
    
    The motivation for this change is to make it easier to understand the
    purpose of the tensors. This also seems to be inline with the names
    used in the llama_layer struct in llama.cpp.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * train-text-from-scratch: rename ff tensors
    
    This commit renames the feed-forward tensors w1, w2 and w3 to ffn_gate,
    ffn_down and ffn_up respectively.
    
    The motivation for this change is to make it easier to understand the
    purpose of the tensors. This also seems to be inline with the names
    used in the llama_layer struct in llama.cpp
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit cf45252a7cfcb998bade46a886e20477cecc538a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Feb 13 15:14:22 2024 +0200

    tests : multi-thread the tokenizer tests (#5474)
    
    * tests : multi-thread the tokenizer tests
    
    ggml-ci
    
    * unicode : fix data race for unidentified codepoints
    
    ggml-ci
    
    * unicode : minor style fixes
    
    ggml-ci

commit 03bf161eb6dea6400ee49c6dc6b69bdcfa9fd3fc
Author: Douglas Hanley <thesecretaryofwar@gmail.com>
Date:   Tue Feb 13 06:06:58 2024 -0600

    llama : support batched embeddings (#5466)
    
    * batched embedding: pool outputs by sequence id. updated embedding example
    
    * bring back non-causal attention
    
    * embd : minor improvements
    
    * llama : minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit ad014bba97ef6ef6c3e2f78b2fc463e91ae94579
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Feb 13 12:38:37 2024 +0100

    make: add error message for bad CUDA version (#5444)
    
    * make: add error message for bad CUDA version
    
    * Update Makefile
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

commit 49cc1f7d67de2da99f3ac185f9ff1319b7bf35f8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Feb 13 13:01:29 2024 +0200

    bert : add tests + fix quantization (#5475)
    
    * llama : do not quantize pos embd and token type tensors
    
    * ci : add BERT tests
    
    ggml-ci
    
    * ci : do not do BERT tests on low-perf nodes
    
    ggml-ci

commit 99b8b43d7b185a6483f28cf798a2d968b2e16ca7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Feb 13 11:20:24 2024 +0200

    tests : disable moe test (#5473)

commit 895407f31b358e3d9335e847d13f033491ec8a5b
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Feb 13 09:07:57 2024 +0200

    ggml-quants : fix compiler warnings (shadow variable) (#5472)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 099afc6274c859ca67146e725839f2d97a5ef313
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 12 20:14:39 2024 +0200

    llama : fix quantization when tensors are missing (#5423)

commit df334a11251b81fd0b6a0e51e7146e0ba9e973f2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 12 19:54:29 2024 +0200

    swift : package no longer use ggml dependency (#5465)
    
    * Revert "swift : update Package.swift to use ggml as dependency (#4691)"
    
    This reverts commit ece9a45e8ffb73ad461c792720c2fec28b0137bc.
    
    * spm : add ggml headers

commit dbd8828eb03b9aa8d0af7e4c533d3c2f5b38aba6
Author: Lee <44310445+lx200916@users.noreply.github.com>
Date:   Tue Feb 13 01:29:57 2024 +0800

    py : fix persimmon `n_rot` conversion (#5460)
    
    * convert : fix persimmon offical weight conversion to write correct n_rot.
    
    * Update convert-persimmon-to-gguf.py
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 43fe07c1a4f3a58612e1d9543f7c6b556710f5d0
Author: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
Date:   Mon Feb 12 20:22:05 2024 +0530

    ggml-sycl: Replace 3d ops with macro  (#5458)
    
    * use macro
    
    * use macro
    
    * fix format

commit 4a46d2b7923be83d6019251671ee63aa1fa0d6bc
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Mon Feb 12 09:38:44 2024 +0100

    llava : remove prog parameter from ArgumentParser (#5457)
    
    * llava: remove prog parameter from ArgumentParser
    
    This commit removes the `prog` parameter from `ArgumentParser`
    so that it uses the default value which is the name of the script.
    
    The motivation for this change is that currently the usage output looks
    like this:
    ```console
    $ python examples/llava/convert-image-encoder-to-gguf.py --help
    usage: convert_hf_to_gguf.py [-h] ...
    ```
    And with this change it will look like this:
    ```console
    $ python examples/llava/convert-image-encoder-to-gguf.py --help
    usage: convert-image-encoder-to-gguf.py [-h] ...
    ```
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * ci: add W503 to flake8 ignore list
    
    This commit adds W503 to the ignore list for flake8. This is done to
    avoid the following error:
    W503 line break before binary operator
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 3b169441dfe8e420f88d1592708cc2a871daadb9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 12 09:16:06 2024 +0200

    sync : ggml (#5452)
    
    * ggml-alloc : v3 (ggml/727)
    
    * ggml-alloc v3
    
    ggml-ci
    
    * fix ci
    
    ggml-ci
    
    * whisper : check for backend buffer allocation failures
    
    * whisper : avoid leaks when initialization fails
    
    * cleanup
    
    ggml-ci
    
    * style fixes
    
    ggml-ci
    
    * sync : ggml
    
    * update llama.cpp, clip.cpp, export-lora.cpp
    
    * update finetune.cpp, train-text-from-scratch.cpp
    
    ggml-ci
    
    * ggml-backend : reduce alignment to 32 to match gguf and fix mmap
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 3bdc4cd0f595a6096cca4a64aa75ffa8a3503465
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Feb 11 19:08:39 2024 +0100

    CUDA: mul_mat_vec_q tiling, refactor mul mat logic (#5434)
    
    * CUDA: mul_mat_vec_q tiling, refactor mul mat logic
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 2891c8aa9af17f4ff636ff3868bc34ff72b56e25
Author: Douglas Hanley <thesecretaryofwar@gmail.com>
Date:   Sun Feb 11 10:21:38 2024 -0600

    Add support for BERT embedding models (#5423)
    
    * BERT model graph construction (build_bert)
    * WordPiece tokenizer (llm_tokenize_wpm)
    * Add flag for non-causal attention models
    * Allow for models that only output embeddings
    * Support conversion of BERT models to GGUF
    * Based on prior work by @xyzhang626 and @skeskinen
    
    ---------
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 97a336507ed9b971d72262bec7e2b8b7016a054a
Author: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
Date:   Sun Feb 11 00:17:31 2024 +0000

    flake.lock: Update
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/b8b232ae7b8b144397fdb12d20f592e5e7c1a64d' (2024-01-31)
      → 'github:NixOS/nixpkgs/f8e2ebd66d097614d51a56a755450d4ae1632df1' (2024-02-07)

commit c88c74f967028ae3d5ebade40ae586d20a961abc
Author: Sergio López <slp@sinrega.org>
Date:   Sun Feb 11 15:12:00 2024 +0100

    vulkan: only use M-sized matmul on Apple GPUs (#5412)
    
    * vulkan: refactor guess_matmul_pipeline for vendor
    
    Refactor ggml_vk_guess_matmul_pipeline to simplify adding per-vendor
    conditionals.
    
    Signed-off-by: Sergio Lopez <slp@redhat.com>
    
    * vulkan: only use M-sized matmul on Apple GPUs
    
    L-sized and S-sized matmuls are broken on Apple GPUs, force using
    M-size with this vendor.
    
    Signed-off-by: Sergio Lopez <slp@redhat.com>
    
    ---------
    
    Signed-off-by: Sergio Lopez <slp@redhat.com>

commit a803333a4e6fc534c93afe90d741bc2388bdec87
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Sun Feb 11 13:43:31 2024 +0000

    common : use enums for sampler types (#5418)
    
    * common: use enums for sampler types
    
    * Apply suggestions from code review
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * minor : spaces
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 684780141a08200ec98eba3e982dbafd1d0b5000
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Sun Feb 11 13:38:14 2024 +0000

    server : allow to specify tokens as strings in logit_bias (#5003)
    
    * server: allow to specify tokens as strings in logit_bias
    
    * Apply suggestions from code review
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 85910c5b30f6e268321be8df044f5528a6efac52
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 11 15:35:50 2024 +0200

    main : ctrl+C print timing in non-interactive mode (#3873)

commit 139b62a839825ef20084ed75ed624db7a5ad554a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 11 15:33:43 2024 +0200

    common : fix compile warning

commit 0f2411f154db46780d3aaa3a0664691b2170c83f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 11 15:33:01 2024 +0200

    ggml : fix compile warnings (unused vars) (#4966)

commit a07d0fee1f05c5c1dc49948ae1a3293db017275f
Author: snadampal <87143774+snadampal@users.noreply.github.com>
Date:   Sun Feb 11 07:22:33 2024 -0600

    ggml : add mmla kernels for quantized GEMM (#4966)
    
    * ggml: aarch64: implement smmla kernel for q8_0_q8_0 quantized gemm
    
    armv8.2-a and above supports MMLA instructions that have higher
    throughput than DOT. this commit adds mmla kernel for
    q8_0_q8_0 gemm. The feature is enabled if the platform supports
    "__ARM_FEATURE_MATMUL_INT8"
    
    On AWS Graviton3 processors this kernel resulted up to 1.5x
    improvement for prompt evaluation throughput compared to the
    default sdot kernel.
    
    * ggml: aarch64: implement smmla kernel for q4_0_q8_0 quantized gemm
    
    armv8.2-a and above supports MMLA instructions that have higher
    throughput than DOT. this commit adds mmla kernel for
    q4_0_q8_0 gemm. The feature is enabled if the platform supports
    "__ARM_FEATURE_MATMUL_INT8"
    
    On AWS Graviton3 processors this kernel resulted up to 1.5x
    improvement for prompt evaluation throughput compared to the
    default sdot kernel.
    
    * ggml: aarch64: implement smmla kernel for q4_1_q8_1 quantized gemm
    
    armv8.2-a and above supports MMLA instructions that have higher
    throughput than DOT. this commit adds mmla kernel for
    q4_1_q8_1 gemm. The feature is enabled if the platform supports
    "__ARM_FEATURE_MATMUL_INT8"
    
    On AWS Graviton3 processors this kernel resulted up to 1.5x
    improvement for prompt evaluation throughput compared to the
    default sdot kernel.
    
    * ggml: update unit tests for the new vec_dot interface
    
    * llama.cpp: add MATMUL_INT8 capability to system_info

commit e4640d8fdf56f14a6db3d092bcd3d2d315cb5d04
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Feb 11 12:44:51 2024 +0100

    lookup: add print for drafting performance (#5450)

commit 907e08c1109f498b01036367804cff3082c44524
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sun Feb 11 11:16:22 2024 +0100

    server : add llama2 chat template (#5425)
    
    * server: add mistral chat template
    
    * server: fix typo
    
    * server: rename template mistral to llama2
    
    * server: format_llama2: remove BOS
    
    * server: validate "--chat-template" argument
    
    * server: clean up using_chatml variable
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

commit f026f8120f97090d34a52b3dc023c82e0ede3f7d
Author: Ian Bull <irbull@eclipsesource.com>
Date:   Sat Feb 10 02:53:28 2024 -0800

    metal : use autoreleasepool to avoid memory leaks (#5437)
    
    There appears to be a known memory leak when using the
    `MLTCommandBuffer`. It is suggested to use `@autoreleasepool` in
    [1,2]
    
    [1] https://developer.apple.com/forums/thread/662721
    [2] https://forums.developer.apple.com/forums/thread/120931
    
    This change-set wraps the `ggml_metal_graph_compute` in a
    `@autoreleasepool`.
    
    This commit addresses https://github.com/ggerganov/llama.cpp/issues/5436

commit cd9aea63b577a83def84dbd6dcd90a6fa02af745
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Feb 10 09:53:05 2024 +0200

    scripts : update sync scripts with new backends

commit 43b65f5eb85e8741aba573a8f65bb8efad245d31
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Feb 10 09:30:36 2024 +0200

    sync : ggml

commit 4633d93af08d890ecd00fa6e4f61d76f21cded4c
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Fri Feb 9 10:42:27 2024 +0100

    ggml : add abort_callback for cpu backend (ggml/725)
    
    * a way to use abort_callback with the cpu backend
    
    * whisper update

commit 4b7b38bef5addbd31f453871d79647fbae6bec8a
Author: Neuman Vong <neuman.vong@gmail.com>
Date:   Sat Feb 10 05:30:19 2024 +1100

    vulkan: Set limit for task concurrency (#5427)
    
    A common default for the maximum number of open files is 256, which can
    lead to `asyncio.gather(*tasks)` failing with Too many open files.
    
        $ python ggml_vk_generate_shaders.py --glslc=$ANDROID_NDK_PATH/shader-tools/darwin-x86_64/glslc
        ggml_vulkan: Generating and compiling shaders to SPIR-V
        Traceback (most recent call last):
          File "/Users/neuman/Code.noindex/github/llama.cpp/ggml_vk_generate_shaders.py", line 2326, in <module>
            asyncio.run(main())
          File "/Users/neuman/Code.noindex/miniforge3/lib/python3.10/asyncio/runners.py", line 44, in run
            return loop.run_until_complete(main)
          File "/Users/neuman/Code.noindex/miniforge3/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
            return future.result()
          File "/Users/neuman/Code.noindex/github/llama.cpp/ggml_vk_generate_shaders.py", line 2294, in main
            await asyncio.gather(*tasks)
        [...snip...]
        OSError: [Errno 24] Too many open files
    
    This change sets a reasonable concurrency limit for tasks (and therefore
    open files), without significant impact on run time.

commit e00d2a62dd1441e3b089570ec06d05c18800d368
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Feb 9 14:00:59 2024 +0100

    llava : add requirements.txt and update README.md (#5428)
    
    * llava: add requirements.txt and update README.md
    
    This commit adds a `requirements.txt` file to the `examples/llava`
    directory. This file contains the required Python packages to run the
    scripts in the `examples/llava` directory.
    
    The motivation of this to make it easier for users to run the scripts in
    `examples/llava`. This will avoid users from having to possibly run into
    missing package issues if the packages are not installed on their system.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * llava: fix typo in llava-surgery.py output
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 7c777fcd5dd4af7079e33390cf6a19c328a2666f
Author: Riley Stewart <ristew@users.noreply.github.com>
Date:   Fri Feb 9 02:49:49 2024 -0800

    server : fix prompt caching for repeated prompts (#5420)

commit e5ca3937c685d6e012ac4db40555d6ec100ff03c
Author: Paul Tsochantaris <ptsochantaris@icloud.com>
Date:   Fri Feb 9 10:48:06 2024 +0000

    llama : do not cap thread count when MoE on CPU (#5419)
    
    * Not capping thread count when MoE inference is running on CPU
    
    * Whitespace

commit e4124c24775f2cb5b3d7acc93bf9dc5471c172ef
Author: Marko Tasic <mtasic85@gmail.com>
Date:   Fri Feb 9 11:17:00 2024 +0100

    readme : add JavaScript/Wasm repo (#5415)

commit b2f87cb64db47d799b6f3656855c9caf9792ab2a
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Fri Feb 9 10:56:43 2024 +0100

    ggml : fix `error C2078: too many initializers` for MSVC ARM64 (#5404)

commit 44fbe34360dd760f9e68b4271f21533436397f84
Author: 0cc4m <picard12@live.de>
Date:   Fri Feb 9 06:52:33 2024 +0100

    Fix Vulkan crash on APUs with very little device memory (#5424)
    
    * Fix Vulkan crash on APUs with very little device memory
    
    * Fix debug output function names

commit 8e6a9d2de0096af7120606c74ee2f26684e87b41
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Feb 8 21:56:40 2024 +0100

    CUDA: more warps for mmvq on NVIDIA (#5394)

commit 41f308f58edc2a04bcf9e245100b0a9b10e9a0fb
Author: slaren <slarengh@gmail.com>
Date:   Thu Feb 8 21:33:03 2024 +0100

    llama : do not print "offloading layers" message in CPU-only builds (#5416)

commit 6e99f2a04f1871d637dd77eb4d81de31a5510253
Author: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
Date:   Thu Feb 8 22:39:10 2024 +0530

    Fix f16_sycl cpy call from Arc (#5411)
    
    * fix f16_sycl cpy call
    
    * rm old logic
    
    * add fp16 build CI
    
    * use macro
    
    * format fix

commit ff4ff05c5ff4311c05a8ce1f984c7d8def4f07a5
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Feb 8 15:20:03 2024 +0100

    llava : add missing .py, and fix paths in README.md (#5414)
    
    This commit adds the missing .py extension to the convert-image-encoder-to-gguf
    script. It also fixes the paths for the `model` and `mmproj` options in the
    example llava-cli command.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit b7b74cef36a93ae01e0b9af8986d131761742d0e
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Feb 8 11:36:54 2024 +0100

    fix trailing whitespace (#5407)

commit 4aa43fab569215a13495a7f1a0f8afc541b16d03
Author: runfuture <runfuture@users.noreply.github.com>
Date:   Thu Feb 8 18:36:19 2024 +0800

    llama : fix MiniCPM (#5392)
    
    * fix bug for norm_rms_eps missing
    
    * to align with the same order as convert.py for model write
    
    * fix: undo HF models permute tensor
    
    * update for flake8 lint

commit a6e514a85f0fda38ff78ec91782877ea3d19ed98
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Feb 8 09:58:19 2024 +0100

    llava: fix typo/formatting in README.md (#5405)
    
    This commit fixes a typo in the README.md file for the llava example
    which is causing the formatting to look a little off:
    
    Clone llava-v15-7b`` and clip-vit-large-patch14-336`` locally
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 26d4efd11e48908e14e2ee9471a7fc4c57079a1d
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Feb 8 09:46:30 2024 +0100

    sampling: fix top_k <= 0 (#5388)
    
    * sampling: fix top_k <= 0
    
    * Update llama.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 8504d2d0da8cc7a1f2eee0e9e56949f960510b75
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 8 09:46:47 2024 +0200

    tests : .gitignore obj files

commit c4fbb6717c684196bd13b72d21747557130914e8
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Wed Feb 7 22:39:23 2024 +0100

    CMAKE_OSX_ARCHITECTURES for MacOS cross compilation (#5393)
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>

commit 8c933b70c21e05b685d476d0a1f36b34cbda7365
Author: Ebey Abraham <ebey97@gmail.com>
Date:   Wed Feb 7 21:11:30 2024 +0000

    fix typo in readme (#5399)
    
    Co-authored-by: Ebey Abraham <ebeyabraham@microsoft.com>

commit b906596bb775b17656c2e51d5ab1b347faab6860
Author: Kamil Tomšík <info@tomsik.cz>
Date:   Wed Feb 7 19:44:52 2024 +0100

    Add Ava in the list of llama.cpp UIs (#4362)

commit aa7ab99be29b633263803f2e185265734c2d9427
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Feb 7 12:40:26 2024 +0100

    CUDA: fixed mmvq kernel for bs 2,3,4 and -sm row (#5386)

commit 10afa6f1d11ebc9fcc1085f468170002cbf6e2b5
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Wed Feb 7 18:16:55 2024 +0800

    [SYCL] update install make by w64devkit (#5297)

commit 0ef46da632c32faa1a538e5dc180994e8bbb46e1
Author: Xiao-Yong Jin <jinxiaoyong@gmail.com>
Date:   Wed Feb 7 02:17:25 2024 -0600

    llava-cli : always tokenize special tokens (#5382)
    
    * llava-cli: tokenize special tokens in prompt
    
    * llava-cli: use the escape CLI argument, remove incomplete separate escaping process

commit ee1628bdfea8b0079fed0140ac2f00ef1b465b57
Author: 0cc4m <picard12@live.de>
Date:   Wed Feb 7 07:54:50 2024 +0100

    Basic Vulkan Multi-GPU implementation (#5321)
    
    * Initial Vulkan multi-gpu implementation
    
    Move most global variables into backend context
    
    * Add names to backend device functions
    
    * Add further missing cleanup code
    
    * Reduce code duplication in tensor split layer assignment
    
    * generalize LLAMA_SPLIT_LAYER for all backends, do not expose device count and memory in llama.h
    
    * Only do device info print in the beginning and initialize one backend for cpu assist
    
    Add missing cleanup code
    
    * Rework backend memory management to make sure devices and buffers get properly allocated and freed
    
    * Rename cpu assist free function
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit ed0bf32290ee5b30ffad5becd99cbecef74aedd7
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Wed Feb 7 06:21:30 2024 +0000

    readme : modernize (#5379)
    
    * first cleanup, update everything to Llama 2 and remove outdated content
    
    * Delete SHA256SUMS
    
    * make build instructions generic
    
    * recommend Q4_K_M quantization method
    
    * Update README.md

commit 9a697d842bc0cfce8268ebd2ba703ffc1c904f98
Author: Ben Williams <ben@719ben.com>
Date:   Tue Feb 6 22:16:48 2024 -0800

    readme : update ui list (#5354)

commit 316c7faf7740fa98ea68f1445f4505810f706b9e
Author: runfuture <runfuture@users.noreply.github.com>
Date:   Wed Feb 7 14:15:56 2024 +0800

    llama : add MiniCPM support (#5346)
    
    * support minicpm arch.
    
    * fix tab/space typo.
    
    * convert minicpm model via convert-hf-gguf.py
    
    * try to make tokenizer work
    
    * fix bug for quantize minicpm
    
    * fix for flake8 lint
    
    * remove convert-minicpm.py
    
    * fix for editorconfig
    
    * correct minicpm model type (size)
    
    * constants expanded for minicpm
    
    * Minor change of the constant names for minicpm

commit f3e2b4fa3f81a410ecb7dec929c259ef8d8dbb7d
Author: Justin Parker <jparkerweb@gmail.com>
Date:   Wed Feb 7 01:15:19 2024 -0500

    server : update `/props` with "total_slots" value (#5373)
    
    * include total "num_slots" in default_generation_settings_for_props
    
    * cleanup total_slots return value in /props endpoint
    
    * update /props endpoint docs with total_slots
    
    * remove num_slots from default_generation_settings_for_props
    
    * update /props endpoint section

commit f68664ac241a6b5c233d8f1051eef20929b06008
Author: Sang-Kil Park <sang.park@42dot.ai>
Date:   Wed Feb 7 13:28:00 2024 +0900

    convert : fix TypeError on GPT-2 vocab.json (#5288)

commit 213d1439fadefe182f69c5f7e8dd3b4b6572ebcb
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Tue Feb 6 18:08:38 2024 +0000

    server : remove model.json endpoint (#5371)

commit 17c97fb0620448b37516a3f53fea6c482b0a30a4
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Feb 6 18:43:06 2024 +0100

    CUDA: mul_mat_vec_q max. batch size 8 -> 4 (#5370)

commit b08f22c882a1443e6b97081f3ce718a4d1a741f8
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Feb 6 19:00:16 2024 +0200

    Update README.md (#5366)
    
    Add some links to quantization related PRs

commit f57fadc009cbff741a1961cb7896c47d73978d2c
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Feb 6 17:28:02 2024 +0200

    Slight quantization improvement for Q4_K and Q5_K (#5361)
    
    * Q4_K: slightly better quantization
    
    * Q5_K: slightly better quantization
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 2e9c0bd6b301155ce749e162527fc55e9fb5b832
Author: BarfingLemurs <128182951+BarfingLemurs@users.noreply.github.com>
Date:   Tue Feb 6 09:06:48 2024 -0500

    readme : add phi, orion 14b, internlm2, and yi-VL to readme (#5362)

commit 2c516611f1d0f1e5e9754f8ea1cf97cb1b17bf2c
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Feb 6 14:44:06 2024 +0100

    CUDA: mul_mat_vec_q for batch sizes > 1 (#5351)

commit 8a79c591de9b7ff3242a94f68b7fb5a17ed8c2be
Author: Justin Parker <jparkerweb@gmail.com>
Date:   Tue Feb 6 04:20:59 2024 -0500

    server : include total "num_slots" in props endpoint (#5349)

commit 31e790322133a4b1d0684527ea446e765e8a96cf
Author: Michael Coppola <m18coppola@gmail.com>
Date:   Tue Feb 6 04:20:00 2024 -0500

    server : add `dynatemp_range` and `dynatemp_exponent` (#5352)
    
    * server: added `dynatemp_range` and `dynatemp_exponent`
    
    * Update README.md
    
    ---------
    
    Co-authored-by: Michael Coppola <info@michaeljcoppola.com>

commit 4ffc7a17d4e80c5f3f905139cb570ed9b6934fcb
Author: Niall Coates <1349685+Niall-@users.noreply.github.com>
Date:   Tue Feb 6 08:16:23 2024 +0000

    server : various fixes for the prompt field in /completion (#5300)
    
    server : fix deadlock when prompt array contains strings and numbers
    
    server : removed an unnecessary generation when generating multi-prompts
    
    server : removed an unnecessary assert

commit 906cff55c2848fda091d888a1585915ec0c9ea9e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Feb 6 07:47:22 2024 +0200

    py : handle byte tokens in `get_token_type` (#5341)
    
    * py : handle byte tokens in `get_token_type`
    
    * py : fix empty bytes arg

commit 098f6d737b65134cf220d12b9b706e8cfc5e4610
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Feb 5 19:33:00 2024 +0100

    make: Use ccache for faster compilation (#5318)
    
    * make: Use ccache for faster compilation

commit 78b00dda6c0d62c34f5371d47718defff6ed2b22
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Feb 5 15:55:10 2024 +0100

    README: updated introduction (#5343)
    
    * README: updated introduction
    
    * readme : update
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit c6b395535a6874d749ef47c33eacd466cb252cd5
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Feb 5 14:09:47 2024 +0200

    ggml : make use of ggml-quants.h possible in C++ code (#5338)
    
    * Make use of ggml-quants.h possible in C++ code
    
    * One cannot possibly be defining static_assert in a C++ compilation
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit abb61944a5f64dec62c893ed0db10790169b672a
Author: Dr. Tom Murphy VII Ph.D <499244+tom7@users.noreply.github.com>
Date:   Mon Feb 5 06:13:57 2024 -0500

    ggml : avoid duplicating function calls using MIN/MAX macros (#5325)
    
    * Avoid duplicating function calls when using MIN/MAX macros.
    
    Since these copy "a" and "b" they ask the compiler to evaluate one of them twice. The compiler doesn't have a problem with removing the duplication in something like MAX(0, x + 2), but in some cases we're calling functions, and those calls just happen twice.
    By explicitly evaluating at the expression we get smaller and faster code without duplicate calls. See ggml_rope_yarn_corr_dims in Compiler Explorer:
    
    https://godbolt.org/z/Ee4KMrvKh
    
    Code behaves exactly the same.
    
    * Update ggml.c
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 89503dcb5f764a5cc7093db1f395f5121876a2cc
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Feb 5 12:32:27 2024 +0200

    iq3_xxs: quards for the no-imatrix situation (#5334)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 7e1ae372f36d98fa66b1d778c5862904b4d80c88
Author: Guoteng <32697156+SolenoidWGT@users.noreply.github.com>
Date:   Mon Feb 5 17:04:06 2024 +0800

    py : fix internlm2-hf convert to gguf (#5305)
    
    * py : fix internlm2-hf convert to gguf
    
    * ggml-ci

commit 6fdfa2ecc684000a25a4ad91823bc82a6652b645
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Feb 5 10:46:06 2024 +0200

    iq2_xxs: tune quantization (#5320)
    
    We get slightly better PPL, and we cut quantization time in
    nearly half.
    
    The trick is to 1st quantize without forcing points onto the E8-lattice.
    We can then use a narrower search range around the block scale that we
    got that way.
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit a2d60c9158435ae9a6f14632f07f1acf7a3becef
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Mon Feb 5 08:10:22 2024 +0000

    server : allow to get default generation settings for completion (#5307)

commit e6f81775323f6f4e4a30abf022a6028fa86b79ac
Author: l3utterfly <gc.pthzfoldr@gmail.com>
Date:   Mon Feb 5 17:00:47 2024 +0900

    common : add dynamic temperature parameters to main example cli (#5295)
    
    * added dynamic temp params in main
    
    * added help text

commit 30679d438d5225b3aecf5cec6482cbc9f8f87ba5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 5 09:48:03 2024 +0200

    scripts : fix typos, cleanup (#5303)

commit 4be04c8965578edc09194fab769b4b922b8444f5
Author: Нияз Гарифзянов <112617865+garrnizon@users.noreply.github.com>
Date:   Mon Feb 5 10:43:57 2024 +0300

    scripts : add non-interactive server-llm.sh (#5303)
    
    * Update server-llm.sh
    
    Add flag --non-interactive that allows run script without asking a permission
    
    * Update scripts/server-llm.sh
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 5d55b0cd827bb0fcfedfa329a82bd5d6ef2c93ca
Author: chiranko <96988916+chiranko@users.noreply.github.com>
Date:   Mon Feb 5 15:41:38 2024 +0800

    readme : add CodeShell models to the supported models list (#5330)

commit 4833ac209da6a427de64f97e8f403dcdc5de6bc3
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Mon Feb 5 07:08:24 2024 +0000

    [SYCL] Fix cpy with dims of 3 (#5289)
    
    * Fix cpy with dims of 3
    
    * rm asserts
    
    ---------
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>

commit 9392ebd49ea5ae236a55b47cbf6a13247e8a3b8c
Author: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
Date:   Sun Feb 4 00:17:24 2024 +0000

    flake.lock: Update
    
    Flake lock file updates:
    
    • Updated input 'flake-parts':
        'github:hercules-ci/flake-parts/07f6395285469419cf9d078f59b5b49993198c00' (2024-01-11)
      → 'github:hercules-ci/flake-parts/b253292d9c0a5ead9bc98c4e9a26c6312e27d69f' (2024-02-01)
    • Updated input 'flake-parts/nixpkgs-lib':
        'github:NixOS/nixpkgs/b0d36bd0a420ecee3bc916c91886caca87c894e9?dir=lib' (2023-12-30)
      → 'github:NixOS/nixpkgs/97b17f32362e475016f942bbdfda4a4a72a8a652?dir=lib' (2024-01-29)
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/ae5c332cbb5827f6b1f02572496b141021de335f' (2024-01-25)
      → 'github:NixOS/nixpkgs/b8b232ae7b8b144397fdb12d20f592e5e7c1a64d' (2024-01-31)

commit 5ed26e1fc9fab4ce96ecf2d84183fe45bdcab0d4
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Feb 4 10:39:58 2024 +0200

    Adding some imatrix tools (#5302)
    
    * imatrix: adding --combine and --continue-from
    
    * imatrix: be able to start from a specific chunk
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 277fad30c60ef3559dc2d01b19d05e659d40a824
Author: Welby Seely <welbyseely@gmail.com>
Date:   Sat Feb 3 23:18:51 2024 -0500

    cmake : use set() for LLAMA_WIN_VER (#5298)
    
    option() is specifically for booleans.
    
    Fixes #5158

commit 3c0d25c4756742ebf15ad44700fabc0700c638bd
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Feb 3 20:15:13 2024 +0100

    make: add nvcc info print (#5310)

commit 3cc5ed353c07201d8d5b98b0a4713ab633da6d04
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Feb 3 20:14:59 2024 +0100

    make: fix nvcc optimization flags for host code (#5309)

commit 60ecf099eddfe70fec797ef6790572e452054add
Author: Martin Schwaighofer <mschwaig@users.noreply.github.com>
Date:   Sun Jan 28 12:59:43 2024 +0100

    add Vulkan support to Nix flake

commit e920ed393d989ed35625ddaf182ebb52cda07fcd
Author: 0cc4m <picard12@live.de>
Date:   Sat Feb 3 18:15:00 2024 +0100

    Vulkan Intel Fixes, Optimizations and Debugging Flags (#5301)
    
    * Fix Vulkan on Intel ARC
    
    Optimize matmul for Intel ARC
    
    Add Vulkan dequant test
    
    * Add Vulkan debug and validate flags to Make and CMakeLists.txt
    
    * Enable asynchronous transfers in Vulkan backend
    
    * Fix flake8
    
    * Disable Vulkan async backend functions for now
    
    * Also add Vulkan run tests command to Makefile and CMakeLists.txt

commit 52bb63c7082c859c3f1dfc527227e6a95b299c7c
Author: Michael Klimenko <mklimenko29@gmail.com>
Date:   Sat Feb 3 12:23:37 2024 +0100

    refactor : switch to emplace_back to avoid extra object (#5291)

commit 1ec3332ade60aeb1494ace2211cf1a966db6d770
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Sat Feb 3 06:22:06 2024 -0500

    YaRN : store rope scaling type as int32_t in memory (#5285)
    
    * YaRN : store rope scaling type as int32_t in memory
    
    * llama : store mapped names as const char *

commit 6a66c5071a74a96c4f52cf1015a092acd18c3714
Author: BADR <contact@pythops.com>
Date:   Sat Feb 3 12:20:26 2024 +0100

    readme : add tenere in the ui tools list (#5284)

commit a305dba8ff642e57f538f42010868fe0bc5262a1
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Sat Feb 3 08:11:37 2024 +0000

    Fix im2col with 32fp (#5286)

commit 191221178f51b6e81122c5bda0fd79620e547d07
Author: kalomaze <66376113+kalomaze@users.noreply.github.com>
Date:   Fri Feb 2 08:15:30 2024 -0600

    perplexity : fix KL divergence calculations on Windows (#5273)

commit e437b37fd0b2b97e6c6ff1045ec7f901faa6498a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Feb 2 14:23:40 2024 +0200

    scripts : parse wtype in server-llm.sh (#5167)
    
    * scripts : parse wtype in server-llm.sh
    
    * scripts : fix check for wfile

commit 2d40085c26794e29c434480b9e06738e89e5686f
Author: Mirror Azure <54669636+MirrorAzure@users.noreply.github.com>
Date:   Fri Feb 2 14:39:09 2024 +0300

    py : add check for '.attn.masked_bias' layers to GPT2model (#5281)

commit b05102fe8cfa9893851c6bf6efd15cdc20b6afa2
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Fri Feb 2 08:39:48 2024 +0000

    Tidy ggml-sycl (#5261)
    
    * Tidy some code in ggml-sycl
    
    * Remove blank space
    
    * Remove std::printf comments
    
    ---------
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>

commit 6b91b1e0a92ac2e4e269eec6361ca53a61ced6c6
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Feb 2 08:56:31 2024 +0100

    docker : add build for SYCL, Vulkan + update readme (#5228)
    
    * add vulkan dockerfile
    
    * intel dockerfile: compile sycl by default
    
    * fix vulkan dockerfile
    
    * add docs for vulkan
    
    * docs: sycl build in docker
    
    * docs: remove trailing spaces
    
    * docs: sycl: add docker section
    
    * docs: clarify install vulkan SDK outside docker
    
    * sycl: use intel/oneapi-basekit docker image
    
    * docs: correct TOC
    
    * docs: correct docker image for Intel oneMKL

commit e805f0fa9951081ce0a86378a7aa52b6f636b82d
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Fri Feb 2 15:54:14 2024 +0800

    [SYCL] get MAX_MEM_ALLOC from device property (#5270)
    
    * get max alloc size from device prop
    
    * fix macro typo

commit af3ba5d94627d337e32a95129e31a3064c459f6b
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Fri Feb 2 15:53:27 2024 +0800

    [SYCL] update guide of SYCL backend (#5254)
    
    * update guide for make installation, memory, gguf model link,  rm todo for windows build
    
    * add vs install requirement
    
    * update for gpu device check
    
    * update help of llama-bench
    
    * fix grammer issues

commit e1e721094d8169636d55f68efe37f222cd3f0677
Author: Ian Bull <irbull@gmail.com>
Date:   Thu Feb 1 23:20:13 2024 -0800

    llama : fix memory leak in llama_batch_free (#5252)
    
    The llama_batch_init allocates memory for a fixed number of tokens.
    However, the llama_batch_free only frees memory for the number of
    tokens that were added to the batch.
    
    This change-set uses a null terminated array for the batch seq_id, and
    frees all the elements until the nullptr is reached. This change-set
    also changes the name of the first parameter from `n_tokens` to
    `n_tokens_alloc` to more clearly indicate that this value is the number
    of tokens allocated to the batch, not the number of tokens in the batch.

commit 128dcbd3c9c4b12f42b560a4430427d7b2828628
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Fri Feb 2 03:48:53 2024 +0800

    add --no-mmap in llama-bench (#5257)
    
    * add --no-mmap, show sycl backend
    
    * fix conflict
    
    * fix code format, change print for --no-mmap
    
    * ren no_mmap to mmap, show mmap when not default value in printer
    
    * update guide for mmap
    
    * mv position to reduce model reload

commit 4d0924a8902010d31bd737b6f1f594943d120d0f
Author: 0cc4m <picard12@live.de>
Date:   Thu Feb 1 19:25:24 2024 +0100

    Vulkan Phi Fix for AMD Proprietary Drivers (#5260)
    
    * Replace tanh to avoid NaN in gelu shader on AMD proprietary driver
    
    * Fix another Vulkan CPY buffer size bug

commit 8ca511cadee2c67f0bd8c7034a2513778ee9a1b7
Author: slaren <slarengh@gmail.com>
Date:   Thu Feb 1 18:30:17 2024 +0100

    cuda : fix LLAMA_CUDA_F16 (#5262)

commit d71ac90985854b0905e1abba778e407e17f9f887
Author: Ali Nehzat <ali.nehzat@thanks.dev>
Date:   Fri Feb 2 02:18:53 2024 +1100

    make : generate .a library for static linking (#5205)

commit ce32060198b7e2d6a13a9b8e1e1369e3c295ae2a
Author: Guoteng <32697156+SolenoidWGT@users.noreply.github.com>
Date:   Thu Feb 1 17:19:51 2024 +0800

    llama : support InternLM2 (#5184)
    
    * support InternLM2 inference
      * add add_space_prefix KV pair

commit 1cfb5372cf5707c8ec6dde7c874f4a44a6c4c915
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Wed Jan 31 19:21:55 2024 +0000

    Fix broken Vulkan Cmake (properly) (#5230)
    
    * build vulkan as object
    
    * vulkan ci

commit d3bac7d58408c602ec1f1e423695f1df8410bb03
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 31 18:47:10 2024 +0200

    llama : reorder build_orion() at correct place (#5118)

commit 5cb04dbc16d1da38c8fdcc0111b40e67d00dd1c3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 31 17:30:17 2024 +0200

    llama : remove LLAMA_MAX_DEVICES and LLAMA_SUPPORTS_GPU_OFFLOAD (#5240)
    
    * llama : remove LLAMA_MAX_DEVICES from llama.h
    
    ggml-ci
    
    * Update llama.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * server : remove LLAMA_MAX_DEVICES
    
    ggml-ci
    
    * llama : remove LLAMA_SUPPORTS_GPU_OFFLOAD
    
    ggml-ci
    
    * train : remove LLAMA_SUPPORTS_GPU_OFFLOAD
    
    * readme : add deprecation notice
    
    * readme : change deprecation notice to "remove" and fix url
    
    * llama : remove gpu includes from llama.h
    
    ggml-ci
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit efb7bdbbd061d087c788598b97992c653f992ddd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 31 15:35:41 2024 +0200

    metal : add im2col F32 dst support (#5132)

commit 15606309a05ccf7fadbaad5538cb7c32acb1e06b
Author: JidongZhang-THU <1119708529@qq.com>
Date:   Wed Jan 31 21:10:15 2024 +0800

    llava : add MobileVLM support (#5132)
    
    * New Feature:
        1. Sum_Rows:
            fix cuda kernel overflow
            fix block shape error when nrows too big
        2. Im2Col:
            Support Batch in cuda
            Support f32 to f32 both in cpu && cuda
        3. DepthWiseConv:
            Support by Im2Col && MulMat
        4. Pool_2d:
            Supoort avg pooling in cuda
        5. HardSigmoid:
            Imp in cuda
        6. HardSwish:
            Imp in cuda
    
    * fix tabs instead of spaces
    
    * code clean
    
    * CUDA POOL2D
    
    * ADD POOL2D test case in test-backend-ops.cpp
    
    * code clean
    
    * fix pool2d_kernel
    
    nits
    
    * fix bug in pool2d kernel
    
    * fix avg pooling, count_include_pad
    
    nits
    
    * test-backend-ops : add more pool_2d tests
    
    * cuda : fix warnings and formatting
    
    * ggml : check types in release builds too in pool_2d
    
    * test-backend-ops : remove f16 pool_2d tests
    
    * cuda : more style fixes
    
    * Add assert in ggml_cuda_op_pool2d
    
    * pool2d float padding fallback
    
    * test-backend-ops : add dst_type to im2col
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit b2b9f025e7821e78bd501d75d01838c26de07a57
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Wed Jan 31 21:04:46 2024 +0800

    format license text, restore apache license by legal suggestion (#5233)

commit dabcc5b471348e4ae03ddacc41e19ad75fb2f041
Author: slaren <slarengh@gmail.com>
Date:   Wed Jan 31 13:43:03 2024 +0100

    ggml : limit n_threads to the max n_tasks (#5238)

commit f8e9140cb46eebaa867e1184a9946e4840eec772
Author: 0cc4m <picard12@live.de>
Date:   Wed Jan 31 11:44:19 2024 +0100

    Vulkan Fixes (#5223)
    
    * Fix Vulkan F16 models
    
    * Fix Vulkan context shift crash
    
    * Add Vulkan to common.cpp dump_non_result_info_yaml function
    
    * Fix bug in Vulkan CPY op
    
    * Fix small matrix multiplication errors in AMD GPUs on Windows or with amdvlk
    
    Co-authored-by: Engininja2 <139037756+Engininja2@users.noreply.github.com>
    
    ---------
    
    Co-authored-by: Engininja2 <139037756+Engininja2@users.noreply.github.com>

commit d62520eb2cc1d7168a30edec6110e1daefbd959f
Author: Yiming Cui <conandiy@vip.qq.com>
Date:   Wed Jan 31 11:04:21 2024 +0800

    Fix typos of IQ2_XXS and IQ3_XXS in llama.cpp (#5231)

commit 01684139c352561840ae55ec627ab58abc3e06ab
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Wed Jan 31 10:38:07 2024 +0800

    support SYCL backend windows build (#5208)
    
    * support SYCL backend windows build
    
    * add windows build in CI
    
    * add for win build CI
    
    * correct install oneMKL
    
    * fix install issue
    
    * fix ci
    
    * fix install cmd
    
    * fix install cmd
    
    * fix install cmd
    
    * fix install cmd
    
    * fix install cmd
    
    * fix win build
    
    * fix win build
    
    * fix win build
    
    * restore other CI part
    
    * restore as base
    
    * rm no new line
    
    * fix no new line issue, add -j
    
    * fix grammer issue
    
    * allow to trigger manually, fix format issue
    
    * fix format
    
    * add newline
    
    * fix format
    
    * fix format
    
    * fix format issuse
    
    ---------
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>

commit e8dc55d0065d076d4c20f3c4bfca562701b4edfe
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Tue Jan 30 19:04:37 2024 -0500

    kompute : llama-bench support and ggml_cpu_has_kompute() (#5226)

commit e0085fdf7c758f0bc2746fc106fb29dd9df959de
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 30 21:19:26 2024 +0200

    Revert "server : change deps.sh xxd files to string literals (#5221)"
    
    This reverts commit 4003be0e5feef320f3707786f22722b73cff9356.

commit e6f291d15844398f8326940fe5ad7f2e02b5aa56
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 30 20:17:30 2024 +0200

    server : fix context shift (#5195)
    
    * server : fix context shift + simplify self-extend
    
    * server : take system_tokens into account
    
    * server : more n_past fixes
    
    * server : rever n_past_se changes

commit 4003be0e5feef320f3707786f22722b73cff9356
Author: JohnnyB <jboero@users.noreply.github.com>
Date:   Tue Jan 30 12:15:05 2024 -0600

    server : change deps.sh xxd files to string literals (#5221)
    
    * Changed ugly xxd to literals.
    
    HPP files are much more readable as multiline literals rather than hex arrays.
    
    * Dashes in literal variable names.
    
    Replace . and - with _ in file names -> variable names.
    
    * Comment on removing xxd.
    
    XXD-> string literals
    
    * XXD to string literals.
    
    Replaced these unreadable headers with string literal versions using new deps.sh.

commit fea4fd4ba7f6b754ac795387b275e1a014a77bde
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Jan 30 19:15:28 2024 +0200

    ggml : fix IQ3_XXS on Metal (#5219)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 8f8ddfcfadc830b936318c3ea9fe2e8e3365aa85
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 30 16:21:57 2024 +0200

    sync : ggml (#0)

commit 6fb50ebbf036ac57a20fe8d8da31731a543582d5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 29 21:08:18 2024 +0200

    gguf : fix comparison (ggml/715)
    
    ggml-ci

commit 625a699b5456994bc32a8093d53818f60ceda6d1
Author: John Balis <phobossystems@gmail.com>
Date:   Mon Jan 29 06:37:33 2024 -0600

    `ggml_cuda_cpy` support for 4d tensors and float16->float32 upcasting (ggml/686)
    
    * added cuda float16->float32 upcasting to ggml_cuda_cpy
    
    * added ability to copy 4d tensors with the cuda backend
    
    * added tests for float16_>float32 upcast and 4d tensor cuda copys
    
    * added 4d copy test for float32->float16 copy
    
    * applied patch suggested by @iamlemec
    
    * simplify cpy tests
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit a4b07c057a553b1ac253051efc3f040351e2eae1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 29 14:00:10 2024 +0200

    gguf : add input validation, prevent integer overflows (ggml/709)
    
    * gguf : add input validation, prevent integer overflows
    
    ggml-ci
    
    * gguf : fix switch default case
    
    * gguf : sanitize info->n_dims and info->type
    
    ggml-ci
    
    * gguf : assert GGUF_TYPE_SIZE access
    
    ggml-ci
    
    * ggml : assert mallocs are successful
    
    ggml-ci
    
    * gguf : prevent integer overflow
    
    * gguf : sanitize tensor info
    
    ggml-ci
    
    * gguf : stricter limit on the number of items
    
    ggml-ci

commit 549a1e6cd5b39fe0dc3d4ea5515c65f17797a31e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 29 13:29:46 2024 +0200

    ci : fix yolo URLs + fix metal capture (ggml/712)

commit 5f14ee0b0cd06f1c4790e6123df4b38ace637e88
Author: Jack Mousseau <jmousseau@users.noreply.github.com>
Date:   Mon Jan 29 01:22:23 2024 -0800

    metal : add debug capture backend function (ggml/694)
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 8e14e3ddb3744566aef7bc0fa734180e47ae6bdf
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Jan 30 15:15:07 2024 +0200

    Faster AVX2 dot product for IQ2_XS (#5187)
    
    * iq2xs: faster AVX2 dot product
    
    * iq2xs: small AVX2 imrovement
    
    * Speed up computing sign bits in AVX2 iq2_xs dot product
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Peter Reid <peter@peterreid.net>

commit f4d7e5497485ce6ce0e322533930b7da4657dd2d
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Jan 30 15:14:12 2024 +0200

    SOTA 3-bit quants  (#5196)
    
    * iq3_xxs: quantize/dequantize
    
    RMSE seems a bit high-ish at about half-way between q2_K and
    q3_K, so need to check more.
    
    * iq3_xxs: CUDA dequantize works
    
    * iq2_xxs: tuning quantization
    
    * iq3_xxs: starting to look better
    
    PPL on wiki.test.raw
    LLaMA-v1-7B: 6.4218
    LLaMA-v2-7B: 6.3560
    Mistral-7B : 6.0717
    
    This is better than Q3_K_XS, with a 5% reduction in quantized model
    size.
    
    * iq3_xxs: CUDA dot product
    
    We have
    PP-512: 5891 t/s
    TG-128: 143.9 t/s
    
    * iq3_xxs: scalar and AVX2 dot products
    
    * iq3_xxs: ARM_NEON and Metal
    
    Metal performance is decent, ARM_NEON is pathetic
    
    * iq3_xxs: slightly better grid points
    
    * Faster iq3_xxs and iq2_xs dot products on CUDA
    
    * iq3_xxs: add some quant mix
    
    * iq3_xxs: fix failing quantization test
    
    Dot product still fails. Is this real?
    
    * iq3_xxs: hopefully fix ROCm
    
    * iq3_xxs: failing tests
    
    This time the dot product accuracy did find an actual bug
    in the AVX2 implementation.
    
    * Add IQ3_XXS to test-backend-ops
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 2256f36b79a932a478d4dcdf02c1e5a60056e5f3
Author: 0cc4m <picard12@live.de>
Date:   Tue Jan 30 13:59:30 2024 +0100

    Vulkan Windows APU Memory Handling (#5199)
    
    * Add basic UMA memory handling
    
    Improve memory OOM behavior
    
    Fix tests
    
    * Fix UMA handling
    
    * Also fix UMA handling for prealloc buffers
    
    * Remove unnecessary warning message
    
    * Remove outdated comment

commit 7359016c7c0f65dc30cf79791212b06f15866450
Author: Vladimir Malyutin <first-leon@yandex.ru>
Date:   Tue Jan 30 17:57:07 2024 +0700

    quantize : fix typo (#5211)
    
    Fix misprint in quantize help

commit 813416991ab0d1caa0d12f93ac4e8a24a2add0a3
Author: divinity76 <divinity76@gmail.com>
Date:   Tue Jan 30 10:18:02 2024 +0100

    main : allow empty --prompt-cache file (#5176)
    
    * allow empty --prompt-cache file
    
    This allows the use of std::tmpnam(), std::tmpfile(), Python's tempfile.NamedTemporaryFile(), and similar create-empty-file API's for the user.
    
    I switched from the C fopen API to the C++ filesystem api to get around the fact that, to the best of my knowledge, C has no portable way to get the file size above LONG_MAX, with std::ftell() returning long? fallback to std::ifstream for c++  < 17
    (the project is currently targeting C++11 it seems - file_exists() and file_size() can be removed when we upgrade to c++17)
    
    * formatting
    
    (requested in codereview)
    
    * remove c++17, file_is_empty

commit 5589921ef84a4fb1c6d1c9c34d626a5a83033db6
Author: Romain Neutron <romain@neutron.io>
Date:   Tue Jan 30 10:16:38 2024 +0100

    readme : minor (#5204)
    
    This is about tuning the code formatting of the README file

commit 49f44b5c55d801e3d51ddcf409d866047d718905
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 30 11:14:44 2024 +0200

    readme : update hot topics

commit 6685cc41c237544623b4b5651eceb9c4280728cc
Author: Wu Jian Ping <wujp@greatld.com>
Date:   Tue Jan 30 17:11:46 2024 +0800

    server : improve README (#5209)

commit ceebbb5b21b971941b2533210b74bf359981006c
Author: Paul Tsochantaris <ptsochantaris@icloud.com>
Date:   Mon Jan 29 22:19:29 2024 +0000

    ggml alloc: Fix for null dereference on alloc failure (#5200)
    
    * Fix for a null pointer dereference if a metal GGML buffer fails to be allocated
    
    * Freeing the allocated buffers rather than the pointer in ggml-alloc.c
    
    * Fixed the fix of the fix

commit 6daa69ee81851ab26ca8aefca1a4202941fc0262
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Mon Jan 29 17:11:27 2024 -0500

    kompute : fix fallback to CPU (#5201)

commit fbf1ddec69f7001cc707de17fa74d7200813bbac
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Mon Jan 29 15:50:50 2024 -0500

    Nomic Vulkan backend (#4456)
    
    Signed-off-by: Jared Van Bortel <jared@nomic.ai>
    Co-authored-by: niansa <anton-sa@web.de>
    Co-authored-by: Adam Treat <treat.adam@gmail.com>
    Co-authored-by: Aaron Miller <apage43@ninjawhale.com>
    Co-authored-by: ToKiNoBug <tokinobug@163.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>

commit 2aed77eb06a329f0d82bb1c467f4244904d4073f
Author: divinity76 <divinity76@gmail.com>
Date:   Mon Jan 29 15:45:41 2024 +0100

    fix typo "RLIMIT_MLOCK" (#5175)

commit c82d18e863fcde91b4b1109b1d0c73ea4470c405
Author: Wu Jian Ping <wujjpp@hotmail.com>
Date:   Mon Jan 29 21:48:10 2024 +0800

    server : embeddings compatibility for OpenAI (#5190)

commit 14fef85e2d5361e6b4dd7a9ecf22bb817362997d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 29 15:35:54 2024 +0200

    py : fix except (#5194)
    
    ggml-ci

commit e76627bcce9f77adb6034cb127b7ec93d4287b69
Author: Sang-Kil Park <sang.park@42dot.ai>
Date:   Mon Jan 29 18:24:19 2024 +0900

    py : improve BPE tokenizer support (#5189)

commit fbe7dfa53caff0a7e830b676e6e949917a5c71b4
Author: slaren <slarengh@gmail.com>
Date:   Mon Jan 29 09:05:13 2024 +0100

    ggml : add max buffer sizes to opencl and metal backends (#5181)

commit 172ac82629815d038702b049070f4c8c02662da5
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Mon Jan 29 08:04:47 2024 +0000

    cmake : fix Vulkan build (#5182)

commit d2f650cb5b04ee2726663e79b47da5efe196ce00
Author: Paul Tsochantaris <ptsochantaris@icloud.com>
Date:   Sun Jan 28 19:50:16 2024 +0000

    metal : free metal objects (#5161)
    
    * Releasing MTLFunction references after Metal pipeline construction
    
    * Keeping the `ggml_metal_kernel` structure
    
    * Spacing fix
    
    * Whitespace fix

commit 35dec26cc25a9ff7d8c3ed52326b94f772b911ce
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 28 19:48:05 2024 +0200

    sync : ggml

commit d460510c7222d43a458a17e01d4bbe72437cdd3c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 28 18:44:58 2024 +0200

    ggml : minor type fix (int64_t -> size_t)

commit 2307523d322af762ae06648b29ec5a9eb1c73032
Author: 0cc4m <picard12@live.de>
Date:   Sun Jan 28 18:03:59 2024 +0100

    ggml : add Vulkan backend (#2059)
    
    * Vulkan loader code
    
    * Fix matmul kernel, continue implementation
    
    * Continue implementation
    
    * Vulkan memory management
    
    * Vulkan development
    
    * Matmul call
    
    * Add aligned malloc and free for VMA
    
    * Continue implementation
    
    * First matmul success
    
    * GEMM Kernel optimization
    
    * 1D Blocktiling
    
    * 2D Blocktiling
    
    * Write coalescing
    
    * Continue vulkan implementation and optimization
    
    * First FP16 attempt, disabled for now
    
    * Code abstraction, FP16 implementation, fix kernel, add FP16 to FP32 kernel
    
    * Enable device extensions properly, restore fp16 matmul op
    
    * Fix mulmat_f16
    
    * Output FP32 in fp16 matmul shader
    
    * Fix f16_to_f32 kernel
    
    * dequant_q4_0 kernel
    
    * Add VMA library
    
    * Avoid requesting dedicated memory, VMA can decide that by itself
    
    * Add bounds checking to matmul kernels, improve implementation, fix command buffers not freed properly
    
    * add cmake commands
    
    * Add 2d write operation, profiling code
    
    * Fix 2d write
    
    * Fix queue selection for AMD RADV
    
    * Fix trailing whitespace in vk_mem_alloc.h
    
    * Add WIP warp tile mat mul shaders
    
    * Disable glslc optimization
    
    * Disable glslc optimization for CMake
    
    * Optimize warptile matmul shader, replace blocktile with it
    
    * Add split-k optimization for small matrix multiplication
    
    Use semaphores for synchronization instead of fences or waitidle
    
    Rework async write/read for synchronization
    
    * Fix validation errors, improve compatibility with AMD GPUs
    
    * Rework command buffer handling
    
    * Variable matmul kernel using specialization constants
    
    * Fix synchronization on AMD, add barriers for buffer ownership transfer, add debug flag and prints
    
    * Reuse semaphores
    
    * Handle stage flags during command buffer submission properly
    
    * Increase matmul test runs for consistent results
    
    * Fix F32 matmul
    
    * Add vectorized loading and zeropadding for matrix multiplication
    
    * Use pinned memory for f16 preprocessing
    
    * Don't force aligned matmul
    
    * Don't free before queue done
    
    * Replace VMA library with native Vulkan buffer management
    
    * Basic offloading support with mul_f32 and dmmv for q4_0
    
    * Run glslc commands in parallel
    
    * Unroll loops in dmmv shader
    
    * Reduce usage of waitIdle
    
    * Reuse pinned allocation for f16 conversion
    
    * Handle devices with only a single queue
    
    * Fix trailing whitespace in CMakeLists.txt
    
    * Allow parallel execution of kernels, parallelize third and fourth dimension calls
    
    * Add fallback for devices only supporting one DescriptorSet per DescriptorPool
    
    * Move to graph function similar to CUDA implementation
    
    * Use F16 kernel for most things, replace q_f32 with mul_mat_q_f16 function
    
    * Add F32 dmmv shaders
    
    * Batch submissions
    
    * Add .spv to gitignore
    
    * Split off matrix vector multiplication for separate optimization
    
    * Use single command buffer for matrix vector multiplication ops
    
    * Reduce overhead of mul_f32 calls by using a single command buffer
    
    * Add submission batching to mul_f32
    
    * Fix tests
    
    * Add missing barrier
    
    * Add further missing barrier
    
    * Add further ops
    
    * Replace vk::QueueFamilyIgnored with VK_QUEUE_FAMILY_IGNORED to support more Vulkan header versions
    
    * Remove unnecessary cblas link
    
    * Fix descriptor set pre-allocation assert
    
    * Add runtime shader compilation, start transferring shaders to this approach
    
    * Transfer remaining shaders to header and compile on runtime
    
    * Fix fp32 fallback if device doesn't support fp16, add force disable env var GGML_VULKAN_DISABLE_F16
    
    * Add support for q4_1, q5_0, q5_1 and q8_0
    
    * Remove unnecessary scalar layout extension
    
    * Parse graph early to pre-record command buffers
    
    * Add q6_k support
    
    * Add multi-submit for command buffers
    
    * Fix q6_k dequant shader for AMD
    
    * Fix q6_k for GPUs without fp16 support
    
    * Simplify q6_k fp16 fix
    
    * Minor fixes
    
    * Fix wg_denom of m-mulmat shaders
    
    * Add Python-based Vulkan shader generator
    
    * Replace shaderc dependency with precompiled shaders
    
    Fix python script to generate shaders
    
    * Clean up code
    
    * Fix shader generator script Windows compatibility
    
    Co-authored-by: Concedo <39025047+LostRuins@users.noreply.github.com>
    
    * Close file before deletion
    
    * Fix vulkan shader fp32 name
    
    * Add q2_k and q3_k support
    
    Add validation check to compare shader results to cpu results
    
    * Add q4_k support
    
    * Add q5_k support
    
    * Bake SPIR-V bytecode into the library instead of loading shaders from file
    
    * Switch to signal semaphores for flexibility
    
    Prepare broadcasting support for mul mat
    
    * Finish broadcasting mul mat support for GQA
    
    * Clean up unused functions
    
    Add repeat op
    
    * Add further ops, not yet enabled. Improve semaphore code
    
    * Reduce number of used semaphores by utilizing timelines more properly
    
    * Remove queue information
    
    * Reuse timeline semaphores, allow parallel operation with binary semaphores to work around nvidia driver limitations
    
    * Add Vulkan to llama-bench
    
    * Remove cblas dependency
    
    * Fix matmul k-split bug
    
    * Fix q4_k dmmv K_QUANTS_PER_ITERATION 1 shader
    
    * Add RMS Norm shader, rework op_f32 shader setup, fix matmul bug
    
    * Fix issues with float16 overflows in shaders
    
    * Fix issues with older Vulkan headers on Ubuntu 22.04
    
    * Allow multi-op partial offloading by parsing the graph to preallocate enough between-op buffers
    
    * Implement further ops, rework op_f32 calls, fix bugs
    
    * Finish full offloading support, add last remaining ops, fix bugs, remove redundant code
    
    * Upload generated file ggml-vulkan-shaders.hpp, remove redundant shaders
    
    * Merge upstream changes, fix conflicts, adapt soft_max op
    
    * Fix Python and shader header format
    
    * Free model gpu buffers on exit
    
    * Use single queue per device to simplify code
    
    * Add matmul shader support for running multiple calculations in parallel
    
    * Switch from semaphore-synchronized multiple command buffers per op to single command buffer for multiple ops, whole graph if possible
    
    * Fix missing event cast
    
    * Replace uint64_t(-1) with UINT64_MAX, rename function for clarity
    
    * Fix warning about empty C function parameters
    
    * Fix compiler warnings
    
    * Properly implement Vulkan backend buffer handling
    
    * Fix oversized host staging buffers
    
    * Simplify barrier synchronization calls
    
    * Fix gcc warnings
    
    * Implement max_size for backend buffer types to limit the size of a single allocation
    
    * Use min of maxMemoryAllocationSize and maxBufferSize for device max allocation size
    
    * refactor multi buf
    
    * Disable unsupported ops to fix tests
    
    * Check for maintenance4 support before using it
    
    * Handle devices with only a single queue
    
    * Fix single queue logic
    
    * propagate buffer usage in multi buffers
    
    * Implement rope_neox op
    
    * Cleanup header and other files
    
    * Simplify gpu_extras by removing events and putting staging memcpys into contexts
    
    * Move queue into context
    
    Add not-yet-enabled async backend ops
    
    * Simplify context use, optimize matmul shader for warp size 64 (AMD GCN), fix split_k matmul shader optimization
    
    * Add get_max_size to SYCL backend.
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * llama : fix trailing whitespace
    
    ---------
    
    Co-authored-by: Henri Vasserman <henv@hot.ee>
    Co-authored-by: Concedo <39025047+LostRuins@users.noreply.github.com>
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 0f648573dde61c510560f68244f70ece7e60d8c1
Author: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
Date:   Sun Jan 28 21:26:23 2024 +0530

    ggml : add unified SYCL backend for Intel GPUs (#2690)
    
    * first update for migration
    
    * update init_cublas
    
    * add debug functio, commit all help code
    
    * step 1
    
    * step 2
    
    * step3 add fp16, slower 31->28
    
    * add GGML_LIST_DEVICE function
    
    * step 5 format device and print
    
    * step6, enhance error check, remove CUDA macro, enhance device id to fix none-zero id issue
    
    * support main device is non-zero
    
    * step7 add debug for code path, rm log
    
    * step 8, rename all macro & func from cuda by sycl
    
    * fix error of select non-zero device, format device list
    
    * ren ggml-sycl.hpp -> ggml-sycl.h
    
    * clear CMAKE to rm unused lib and options
    
    * correct queue: rm dtct:get_queue
    
    * add print tensor function to debug
    
    * fix error: wrong result in 658746bb26702e50f2c59c0e4ada8e9da6010481
    
    * summary dpct definition in one header file to replace folder:dpct
    
    * refactor device log
    
    * mv dpct definition from folder dpct to ggml-sycl.h
    
    * update readme, refactor build script
    
    * fix build with sycl
    
    * set nthread=1 when sycl, increase performance
    
    * add run script, comment debug code
    
    * add ls-sycl-device tool
    
    * add ls-sycl-device, rm unused files
    
    * rm rear space
    
    * dos2unix
    
    * Update README_sycl.md
    
    * fix return type
    
    * remove sycl version from include path
    
    * restore rm code to fix hang issue
    
    * add syc and link for sycl readme
    
    * rm original sycl code before refactor
    
    * fix code err
    
    * add know issue for pvc hang issue
    
    * enable SYCL_F16 support
    
    * align pr4766
    
    * check for sycl blas, better performance
    
    * cleanup 1
    
    * remove extra endif
    
    * add build&run script, clean CMakefile, update guide by review comments
    
    * rename macro to intel hardware
    
    * editor config format
    
    * format fixes
    
    * format fixes
    
    * editor format fix
    
    * Remove unused headers
    
    * skip build sycl tool for other code path
    
    * replace tab by space
    
    * fix blas matmul function
    
    * fix mac build
    
    * restore hip dependency
    
    * fix conflict
    
    * ren as review comments
    
    * mv internal function to .cpp file
    
    * export funciton print_sycl_devices(), mv class dpct definition to source file
    
    * update CI/action for sycl code, fix CI error of repeat/dup
    
    * fix action ID format issue
    
    * rm unused strategy
    
    * enable llama_f16 in ci
    
    * fix conflict
    
    * fix build break on MacOS, due to CI of MacOS depend on external ggml, instead of internal ggml
    
    * fix ci cases for unsupported data type
    
    * revert unrelated changed in cuda cmake
    remove useless nommq
    fix typo of GGML_USE_CLBLAS_SYCL
    
    * revert hip cmake changes
    
    * fix indent
    
    * add prefix in func name
    
    * revert no mmq
    
    * rm cpu blas duplicate
    
    * fix no_new_line
    
    * fix src1->type==F16 bug.
    
    * pass batch offset for F16 src1
    
    * fix batch error
    
    * fix wrong code
    
    * revert sycl checking in test-sampling
    
    * pass void as arguments of ggml_backend_sycl_print_sycl_devices
    
    * remove extra blank line in test-sampling
    
    * revert setting n_threads in sycl
    
    * implement std::isinf for icpx with fast math.
    
    * Update ci/run.sh
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update examples/sycl/run-llama2.sh
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update examples/sycl/run-llama2.sh
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update CMakeLists.txt
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update CMakeLists.txt
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update CMakeLists.txt
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update CMakeLists.txt
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * add copyright and MIT license declare
    
    * update the cmd example
    
    ---------
    
    Co-authored-by: jianyuzh <jianyu.zhang@intel.com>
    Co-authored-by: luoyu-intel <yu.luo@intel.com>
    Co-authored-by: Meng, Hengyu <hengyu.meng@intel.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit b764b8f1d079ba44d912801ce6d29bd0d94d51cf
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 28 16:54:54 2024 +0200

    flake.lock: Update (#5162)

commit 9241c3a2ace544aef708334e54bbdddb90208ee8
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Jan 28 09:59:49 2024 +0100

    Apply min_p to unsorted tokens (#5115)

commit b2b2bf988c098851b4f3831f0cf38394bff75121
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Jan 28 09:35:14 2024 +0100

    Tests for min_p, sampling queue (#5147)

commit af4980bfedfd8df43b9e4cd1442895e85fee37bc
Author: Marcus Dunn <51931484+MarcusDunn@users.noreply.github.com>
Date:   Sun Jan 28 00:30:44 2024 -0800

    readme : add link to rust bindings (#5148)
    
    * added link to another set of rust bindings with brief note on differences.
    
    * fixed link name

commit f2e69d28c01303ca9dc79907f89ef120a6ac4a92
Author: sharpHL <132747147+sharpHL@users.noreply.github.com>
Date:   Sun Jan 28 16:00:30 2024 +0800

    llama : add support for Orion-14B (#5118)
    
    * add support for Orion-14B(https://huggingface.co/OrionStarAI/Orion-14B-Chat)
    
    * flake8 support
    
    * Update llama.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update llama.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update llama.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update llama.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update llama.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Update llama.cpp
    
    * Update llama.cpp
    
    ---------
    
    Co-authored-by: lixiaopu <lixiaopu@cmcm.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>

commit 39baaf55a160909bb9428bd981014218761a20cb
Author: Kyle Mistele <kyle@mistele.com>
Date:   Sun Jan 28 01:55:31 2024 -0600

    docker : add server-first container images (#5157)
    
    * feat: add Dockerfiles for each platform that user ./server instead of ./main
    
    * feat: update .github/workflows/docker.yml to build server-first docker containers
    
    * doc: add information about running the server with Docker to README.md
    
    * doc: add information about running with docker to the server README
    
    * doc: update n-gpu-layers to show correct GPU usage
    
    * fix(doc): update container tag from `server` to `server-cuda` for README example on running server container with CUDA

commit 6db2b41a76ee78d5efdd5c3cddd5d7ad3f646855
Author: John <78893154+cmp-nct@users.noreply.github.com>
Date:   Sat Jan 27 16:09:18 2024 +0100

    llava : support for Yi-VL and fix for mobileVLM (#5093)
    
    * Support for Yi-VL, templating fix for mobileVLM
    
    * ws
    
    * Update examples/llava/clip.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update llava-cli.cpp
    
    * Update clip.cpp
    
    bugfix for new conversions
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 753eafed0ebd07af6903771327a1786a7c02cf98
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jan 27 16:59:20 2024 +0200

    sync : ggml

commit e9764230054e01553bdead6f2bfd8e001869599d
Author: Judd <foldl@users.noreply.github.com>
Date:   Fri Jan 26 21:04:01 2024 +0800

    ggml : check ggml_add src1 type (ggml/708)
    
    Co-authored-by: Judd <foldl@boxvest.com>

commit 35a2ee914308c85ab5cb576467381443ad23f0ac
Author: Michael Klimenko <mklimenko29@gmail.com>
Date:   Sat Jan 27 15:25:55 2024 +0100

    Remove unused data and add fixes (#5154)
    
    * Remove unused data and add fixes
    
    * Add missing file
    
    * Address review comments
    
    * Replace the scope of vq allocation

commit ec903c034131848da9222536ff18da07ec0882a0
Author: Maximilian Winter <maximilian.winter.91@gmail.com>
Date:   Sat Jan 27 14:38:05 2024 +0100

    server : add self-extend support (#5104)
    
    * Ported self extension to server example
    
    * Update server.cpp
    
    * Fixed prompt caching without self extend
    
    * Update server.cpp
    
    * Added description to server readme.
    
    * Update server.cpp
    
    * Update server.cpp
    
    * Update server.cpp
    
    * Update server.cpp
    
    * Update README.md
    
    * Changed descriptions
    
    * server : formatting
    
    * Update examples/server/server.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update examples/server/server.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update server.cpp
    
    * Update server.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit a1d6df129bcd3d42cda38c09217d8d4ec4ea3bdd
Author: 0cc4m <picard12@live.de>
Date:   Fri Jan 26 23:07:32 2024 +0100

    Add OpenCL add kernel (#5151)
    
    * Add OpenCL add kernel
    
    * Put add kernel into different string to stay within MSVC string length limit, disable float16 support due to bad results

commit bbe7c56c9993af86aa2d84cbe1fd69e1b4300cea
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Fri Jan 26 15:34:06 2024 -0500

    cmake : pass CPU architecture flags to nvcc (#5146)

commit 62fead3ea0a30c8d424f4a8373fa14165c7c707f
Author: slaren <slarengh@gmail.com>
Date:   Fri Jan 26 18:59:43 2024 +0100

    cuda : fix tensor size calculation for non-split buffer (#5145)

commit 15b4538ff29b280a395a1406d711497d8eaa2564
Author: slaren <slarengh@gmail.com>
Date:   Fri Jan 26 18:18:26 2024 +0100

    ggml-alloc : add 10% margin to the buffer sizes (#5149)

commit 7032f4f6349c17a8352f9f93f7d2122f45469e59
Author: snadampal <87143774+snadampal@users.noreply.github.com>
Date:   Fri Jan 26 11:17:59 2024 -0600

    ggml : update softmax n_task calculation (#5126)
    
    updated the n_task calculation to use max number of
    threads possible. This has improved the prompt eval
    performance by around 5% for DOT kernels and by
    around 10% for MMLA kernels on AWS Graviton3.

commit 5f1925a8cef81eb9b372faaae34b0dd76d5361d4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 26 17:09:44 2024 +0200

    scripts : move run-with-preset.py from root to scripts folder

commit 3b7c914de25c6851396d7f9178249f1ed278120e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 26 14:48:15 2024 +0200

    tests : gitignore test-c.o

commit 48c857aa10aea73210a4a72da3f1a6f99269e75d
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Jan 26 13:42:20 2024 +0100

    server : refactored the task processing logic (#5065)
    
    * server: add llama_server_queue struct
    
    * server: add llama_server_response_event
    
    * server: add comments
    
    * server: move all mutexes away from server.cpp
    
    * server: correct multitask response
    
    * server: only add back deferred tasks when one slot is available
    
    * server: fix a race condition cause by "request_completion"

commit 413e7b0559f922bd4de5e9eec548829d111651b1
Author: crasm <crasm@git.vczf.net>
Date:   Fri Jan 26 07:18:00 2024 -0500

    ci : add model tests + script wrapper (#4586)
    
    * scripts : add lib.sh and lib_test.sh
    
    * scripts : stub out new ci-run.sh script
    
    * scripts : switch to PascalCase for functions
    
    This looks a little odd at first, but I find it very useful as a
    convention to know if a command is part of our code vs a builtin.
    
    * scripts : add some fancy conversion from snake_case to PascalCase
    
    * Add venv to ci/run.sh
    
    * Revert scripts work
    
    * scripts : add wrapper script for local use of ci/run.sh
    
    * Simplify .gitignore for tests, clang-tidy fixes
    
    * Label all ctest tests
    
    * ci : ctest uses -L main
    
    * Attempt at writing ctest_with_model
    
    * Update test-model-load-cancel
    
    * ci : add ctest_with_model for debug and release
    
    ggml-ci
    
    * Fix gg_get_model function
    
    ggml-ci
    
    * got stuck on CMake
    
    * Add get_model.cpp to tests/CMakeLists.txt
    
    ggml-ci
    
    * Fix README.md output for ctest_with_model
    
    ggml-ci
    
    * workflows : use `-L main` for all ctest
    
    ggml-ci
    
    * Fixes
    
    * GG_RUN_CTEST_MODELFILE => LLAMACPP_TESTMODELFILE
    * Always show warning rather than failing if model file variable is not
      set
    
    * scripts : update usage text for ci-run.sh

commit 6dd3c28c9cd1ef74b49d79f47d668759346a3c6c
Author: Paul Tsochantaris <ptsochantaris@icloud.com>
Date:   Fri Jan 26 12:16:07 2024 +0000

    metal : remove unused `n_buffers` and `buffers` (#5129)

commit 38b431de232d1b736b5af19b8c7d72f7075a70bc
Author: Riceball LEE <snowyu.lee@gmail.com>
Date:   Fri Jan 26 17:10:28 2024 +0800

    gguf : fix "general.alignment" type in gguf_reader.py (#5136)

commit aad0b01d7380a7cdfe0dd42307b18c7b6bac9575
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 26 10:52:33 2024 +0200

    readme : update hot topics

commit 1182cf4d4f6ee383b92695c2e3fe438086dcdba7
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Jan 26 09:14:39 2024 +0200

    Another bucket sort (#5109)
    
    * Initial bucket sort
    
    * Bucket sort: slightly better version
    
    * Bucket sort: another minor improvement
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit fe54033b69b83164cabb5f3ed92dc0ff7ea47605
Author: XiaotaoChen <chenxiaotao1234@gmail.com>
Date:   Fri Jan 26 04:14:32 2024 +0800

    readme : add MobileVLM 1.7B/3B to the supported models list (#5107)
    
    Co-authored-by: Chenxiaotao03 <chenxiaotao03@meituan.com>

commit 5eaf9964fc797d4585c214db32a463d557f3ed33
Author: l3utterfly <gc.pthzfoldr@gmail.com>
Date:   Fri Jan 26 05:06:22 2024 +0900

    llama : dynamic temperature sampling (#4972)
    
    * implemented dynamic temperature sampling from koboldcpp
    
    * removed trailing whitespace
    
    * removed unused temp parameter in llama_sample_entropy
    
    * exposed exponent_val in dynamic temp sampler
    
    * added debug check for printf statements
    
    * use nullptr in llama_sample_softmax call during llama_sample_entropy
    
    this avoids counting the time taken stats twice
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * return earlier if there is only 1 candiate (i.e. max_entropy == 0)
    
    * reformat 't' case in llama_sample_queue
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * check for one or zero candidates case in llama_sample_entropy
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

commit d292f4f2047963f558dd516f1baaa71793e9acf2
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Thu Jan 25 14:51:24 2024 -0500

    examples : make pydantic scripts pass mypy and support py3.8 (#5099)

commit 256d1bb0ddce6a0a21f5a7503019bdd5c1933cba
Author: Valentin Konovalov <valle.ketsujin@gmail.com>
Date:   Thu Jan 25 12:05:51 2024 -0500

    android : use release cmake build type by default (#5123)

commit faa3526a1eba458120987ed8269e5616385a76f4
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Jan 25 17:58:53 2024 +0200

    Fix Q3_K_XS for MoE models (#5113)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit ddc5a5033f948dc7ab0a3a6ec2d914d13c274077
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 25 11:26:17 2024 +0200

    metal : show compile log messages

commit cd4fddb29f81d6a1f6d51a0c016bc6b486d68def
Author: Engininja2 <139037756+Engininja2@users.noreply.github.com>
Date:   Wed Jan 24 16:18:15 2024 -0600

    cuda : fix 2-bit quants on amd hip (#5105)
    
    * cuda : fix 2-bit quants on amd hip
    
    * use __low2float intrinsic function for new quants

commit c9b316c78fba31e65879a2ec91cbafd341b88cce
Author: Michael Hueschen <m@mhueschen.dev>
Date:   Mon Jan 22 16:44:10 2024 -0700

    nix-shell: use addToSearchPath
    
    thx to @SomeoneSerge for the suggestion!

commit bf63d695b804b1c995c7ae4427a8a86936ea6d25
Author: Michael Hueschen <m@mhueschen.dev>
Date:   Mon Jan 22 03:17:05 2024 -0700

    nix: add cc to devShell LD_LIBRARY_PATH
    
    this fixes the error I encountered when trying to run the convert.py
    script in a venv:
    
    ```
    $ nix develop
    
    [...]$ source .venv/bin/activate
    (.venv)
    [...]$ pip3 install -r requirements.txt
    <... clipped ...>
    [...]$ python3 ./convert.py
    Traceback (most recent call last):
      File "/home/mhueschen/projects-reference/llama.cpp/./convert.py", line 40, in <module>
        from sentencepiece import SentencePieceProcessor
      File "/home/mhueschen/projects-reference/llama.cpp/.venv/lib/python3.11/site-packages/sentencepiece/__init__.py", line 13, in <module>
        from . import _sentencepiece
    ImportError: libstdc++.so.6: cannot open shared object file: No such file or directory
    ```
    
    however, I am not sure this is the cleanest way to address this linker
    issue...

commit 1387ea21178f9f154944013d4dd9764b54c69deb
Author: slaren <slarengh@gmail.com>
Date:   Wed Jan 24 12:48:14 2024 +0100

    llama : pre-allocate input tensors in a separate buffer (#5100)

commit 26d607608d794efa56df3bdb6043a2f94c1d632c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 23 15:50:56 2024 +0200

    metal : disable support for MUL_MAT F32 x F16

commit 44879ee885f48ecf4675dd216b373dce0a6f3690
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Jan 23 15:17:20 2024 +0200

    Additional KL-divergence statistics (#5081)
    
    * perplexity: add top-token probability
    
    * perplexity: add additional KL-divergence statistics
    
    * perplexity: a better organized KL-divergence statistics output
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 9ecdd12e95aee20d6dfaf5f5a0f0ce5ac1fb2747
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Jan 23 13:31:56 2024 +0100

    CUDA: more info when no device code (#5088)

commit 89758723c75ba594e401f6513751beeba7ca1d28
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 23 14:12:57 2024 +0200

    minor : clean-up some warnings and style (#5094)
    
    * minor : clean-up some warnings and style
    
    ggml-ci
    
    * ggml : add comment

commit 2bed4aa3f37cb4e39e16e9ec7b595a7738fd5faf
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Jan 23 08:11:39 2024 +0100

    devops : add intel oneapi dockerfile (#5068)
    
    Co-authored-by: Xuan Son Nguyen <xuanson.nguyen@snowpack.eu>

commit 125d03a5036a02a983c8e98c2cdc126e061afb8e
Author: Michael Coppola <m18coppola@gmail.com>
Date:   Tue Jan 23 01:51:27 2024 -0500

    llama.vim : added api key support (#5090)
    
    Co-authored-by: Michael Coppola <info@michaeljcoppola.com>

commit 011e8ec577fd135cbc02993d3ea9840c516d6a1c
Author: slaren <slarengh@gmail.com>
Date:   Mon Jan 22 23:42:41 2024 +0100

    llama : fix not enough space in buffer with Qwen (#5086)

commit 6f9939d119b2d004c264952eb510bd106455531e
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jan 22 16:10:14 2024 +0200

    KL-divergence (#5076)
    
    * kl-divergence: be able to save all logits to a file
    
    * Add ability to compute KL-divergence
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 780e24a22eb595b705cbe8284771e9ceff1c4dd2
Author: Reinforce-II <fate@eastal.com>
Date:   Mon Jan 22 21:15:08 2024 +0800

    ggml : parallelize FP32 conversion when using BLAS (#5045)
    
    * make GGML_TASK_INIT phase can be run in multithread
    
    * multithreaded dequantize in mul_mat when using blas library
    
    * minor fixes
    
    * update outdated comment
    * fix coding style
    
    * simplify code
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 3ce7e8f8e7ccfce07e5947ac5f1f3f4628cf68ea
Author: XiaotaoChen <chenxiaotao1234@gmail.com>
Date:   Mon Jan 22 21:09:35 2024 +0800

    llava : MobileVLM support (#4954)
    
    * MobileVLM native implementation
    
    * delete depthwise_conv_2d and permute_cpy relative code, replace the two by the existed functions, and opt ldp definition, support LLAMA_PERF option for CMake
    
    * move android script to example/llava directory
    
    * Fix the editor config checks
    
    ---------
    
    Co-authored-by: Chenxiaotao03 <chenxiaotao03@meituan.com>

commit b2d80e105a59b54822edf7ce7f3ed5f317e96e21
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sun Jan 21 03:41:37 2024 +0000

    flake.nix: add a comment about flakes vs nix

commit 28603cd2833cedd4434f398d847f87fc83546dbb
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sun Jan 21 03:29:38 2024 +0000

    nix: add a comment on the many nixpkgs-with-cuda instances

commit 5e97ec91ae3038720a5b15cde4c52d2a53ec2137
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sun Jan 21 03:15:13 2024 +0000

    nix: add a comment about makeScope

commit 7251870780e2d572dd6f239d7a0bfe438c82fa74
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sat Jan 13 17:45:01 2024 +0000

    nix: refactor the cleanSource rules

commit fe8b3c0d4b0d806e8b46660e24eaf4b90b8b385f
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sat Jan 13 17:38:32 2024 +0000

    workflows: nix-ci: drop the redundant "paths" filter

commit f4dd059259d0234913b9e9780e1662811744c09d
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sat Jan 13 17:16:54 2024 +0000

    workflows: nix-build-aarch64: rate limit

commit f7276f7500f7ea588836dd1fc6f126334c517878
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sat Jan 13 17:10:19 2024 +0000

    workflows: nix-ci: rebuild on flake.lock updates

commit 15bceec2d73d4166340b46b27677c45ac1b4cdad
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jan 22 14:18:43 2024 +0200

    imatrix : keep intermediate imatrix results (#5077)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit d6bd4d46ddb6926087c11e0f6633ab1c81da58c3
Author: compilade <113953597+compilade@users.noreply.github.com>
Date:   Mon Jan 22 06:21:52 2024 -0500

    llama : support StableLM 2 1.6B (#5052)
    
    * llama : support StableLM 2 1.6B
    
    * convert : fix Qwen's set_vocab wrongly naming all special tokens [PAD{id}]
    
    * convert : refactor Qwen's set_vocab to use it for StableLM 2 too
    
    * nix : add tiktoken to llama-python-extra
    
    * convert : use presence of tokenizer.json to determine StableLM tokenizer loader
    
    It's a less arbitrary heuristic than the vocab size.

commit 152d9d05e097e35f1cac21262946d57faec7542a
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Mon Jan 22 12:11:01 2024 +0100

    finetune : print sample-start/include-sample-start (#5072)
    
    This commit adds `--sample-start` and `--include-sample-start` to the
    output from the main function in finetune.cpp.
    
    The motivation for this is that even though these are set explicitly by
    the user via the command line, if one forgets to set them then it is
    useful to have their values printed out. Otherwise it is possible to go
    through the whole training process before realizing that the values are
    not what one expected.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 66d575c45c5a370d668f9c3283cdf348e2329fa2
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jan 22 12:43:33 2024 +0200

    llama : add Q3_K_XS (#5060)
    
    * Add Q3_K_XS - intermediate size between Q2_K and Q3_K_S
    
    * Q3_K_XS: quanize first 1/8 of ffn_down layers with Q4_K
    
    Together with an importance matrix, this brings perplexity
    for LLaMA-v2-70B below the perplexity of the former Q2_K
    with a 800 MB smaller quantized model size.
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 57744932c64266359ee905518de7e096c0295d8c
Author: bobqianic <129547291+bobqianic@users.noreply.github.com>
Date:   Mon Jan 22 08:55:05 2024 +0000

    ci : fix Windows CI by updating Intel SDE version (#5053)

commit 3466c6ebcf668cac453f891b493ead19283347a8
Author: Shijie <821898965@qq.com>
Date:   Mon Jan 22 15:33:19 2024 +0800

    llama : add more qwen2 models (#5071)

commit 504dc37be8446fb09b1ede70300250ad41be32a2
Author: iSma <ismail.senhaji@gmail.com>
Date:   Sun Jan 21 22:37:13 2024 +0100

    Revert LLAMA_NATIVE to OFF in flake.nix (#5066)

commit 05490fad7f7f60ff2bed9ad05cd81b44e82ccde3
Author: kuronekosaiko <EvanChanJ@163.com>
Date:   Mon Jan 22 00:28:14 2024 +0800

    add safetensors support to convert-lora-to-ggml.py (#5062)
    
    * add safetensors support to convert-lora-to-ggml.py
    
    * Update convert-lora-to-ggml.py
    
    Remove white space in line 69.

commit 6c5629d4d2d15557c38a0e609b30c1a42abad80d
Author: bobqianic <129547291+bobqianic@users.noreply.github.com>
Date:   Sun Jan 21 15:17:35 2024 +0000

    add `#include <string>` to unicode.h (#5051)
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>

commit 7dcbe39d36b76389f6c5cd3b151928472b7e22ff
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Jan 21 14:42:44 2024 +0200

    Add ability to evauate multiple choice tasks  (#5047)
    
    * TruthfulQA: 1st attempt, does not look like it is working
    
    The same implementation can be used for HellaSwag as well,
    so I converted a HellaSwag validation dataset to the binary
    format used here and tested with that. The score is only
    around 50, so something is not quite right.
    
    * TruthfulQA: works but the result is bad
    
    I know it works because if I convert the HellaSwag validation
    data to the binary format used in the truthful_qa_score() function
    I get the exact same result as from the hellaswag_score() function.
    But I guess, the questions are tricky and the way I have done
    the combination of question + answer is very likely not the best.
    The TruthfulQA validation dataset contains 817 questions, with
    random chance result around 19%. With this version I get
    29.1% for Mistral-7B and 55.2% for Mistral-7B-Instruct-v0.2.
    The HF leader board results for these two models are
    42.2% and 68.3%, respectively.
    
    * TruthfulQA: fix random sample
    
    * TruthfulQA: prepare tasks in parallel for large test datasets
    
    * Rename truthful_qa to multiple_choice
    
    * Make MSVC happy
    
    I had forgotten that MSVC does not make constexpr's available
    inside a lambda.
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 726c0fa9a2da976e9c5d5c51e185d9dd453fc9e5
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Jan 21 08:01:20 2024 +0200

    Slightly faster imatrix (#5050)
    
    * imatrix: speedup by avoiding unnecessary allocations and copies
    
    * imatrix: add --no-ppl option to skip PPL calculations altogether
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 942c0107a7301434c0a5e7da46bc4cf2393aa556
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 21 05:17:27 2024 +0200

    flake.lock: Update (#5054)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/9b19f5e77dd906cb52dade0b7bd280339d2a1f3d' (2024-01-13)
      → 'github:NixOS/nixpkgs/bbe7d8f876fbbe7c959c90ba2ae2852220573261' (2024-01-19)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit b43ebde3b0ccbc42d9dd782b32e2fd8eb35b43b5
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Sat Jan 20 18:14:18 2024 -0500

    convert : partially revert PR #4818 (#5041)

commit 97c1549808d2742d37584a3c9df28154bdf34417
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Sat Jan 20 10:08:08 2024 -0500

    perplexity : fix MSVC build after #5020 (#5043)
    
    * perplexity : fix MSVC build after #5020
    
    * try a differerent fix

commit 6df465a91d402370bcba6676b19fad85b06ce7e0
Author: slaren <slarengh@gmail.com>
Date:   Sat Jan 20 16:05:49 2024 +0100

    llama : run all KQV ops on the CPU with no KV offload (#5049)
    
    ggml-ci

commit 77bc1bbd05f0c31cb45773eb5eb59b9ff2b07e1b
Author: Herman Semenov <GermanAizek@yandex.ru>
Date:   Sat Jan 20 08:11:31 2024 +0000

    cmake : add support for ccache (#5002)
    
    * Added support ccache for speedup recompilation
    
    * cmake : option to disable ccache
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 48e2b1337257bb77573bf76cfffcff3a5efa8704
Author: adel boussaken <netdur@gmail.com>
Date:   Sat Jan 20 09:05:43 2024 +0100

    Add a dart/flutter binding to README.md (#4882)

commit cca894f16a5eade15afd07b015e4cb3d8658943f
Author: Kylin <56434533+KyL0N@users.noreply.github.com>
Date:   Sat Jan 20 15:01:46 2024 +0800

    cuda : fix compile error in jetson platform (#4975)
    
    * cuda: fix compile error in jetson platform
    
    * cuda: update comment in ggml-cuda.cu
    
    * cuda: update ggml-cuda.cu comment

commit 381ee195721d8e747ee31a60c0751822b3072f02
Author: Uzo Nweke <uzoechi@gmail.com>
Date:   Fri Jan 19 13:20:50 2024 -0500

    finetune : fix ggml_allocr lifetimes (tmp workaround) (#5033)
    
    * Fix issue with alloc causing max_compute_size to be calculated
    
    * remove ggml_allocr_free as suggested in issue #4791

commit a5cacb22b2114fd9adf61c00cbb237384d86bced
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 19 15:24:47 2024 +0200

    imatrix : add README.md

commit 9b75cb2b3ccbed3df2e14c1202168db3e5145095
Author: Shijie <821898965@qq.com>
Date:   Fri Jan 19 19:53:13 2024 +0800

    llama : support upcoming Qwen2 (#5037)

commit de9a147df14e62f54f879d2d15e6c4793107f4fc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 19 13:52:22 2024 +0200

    py : fix flake8 lint

commit 7051aacfac0057fa5fac9ea46c55bffc3892d810
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Jan 19 11:39:11 2024 +0200

    winogrande: evaluate log-probs in parallel (#5036)
    
    This is a relatively minor performance tweak resulting in
    ~10% speedup on my system.
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 2b3b999cacc7ad1207c32fbdf3479a19c06e1a34
Author: chiranko <96988916+chiranko@users.noreply.github.com>
Date:   Fri Jan 19 17:07:27 2024 +0800

    llama : add CodeShell support (#5016)
    
    * llama: add codeshell support
    
    * llama.cpp: fix codeshell with NeoX rope
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 993fba81807e55d27b570945af8e416d535eced1
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Jan 19 11:02:39 2024 +0200

    perplexity: avoid unnecessary alloocations and logit copies (#5035)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 8b20858e5e9c44b99b4b31ae9c40b8f20d01d94f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 19 10:45:06 2024 +0200

    perplexity : faster Winogrande via batching (#5024)
    
    * perplexity : faster Winogrande via batching
    
    ggml-ci
    
    * perplexity : remove unused function
    
    * perplexity : only tokenize selected tasks for Winogrande

commit 57e2a7a52a819883f40dada8a2edc24ecf48186b
Author: John <78893154+cmp-nct@users.noreply.github.com>
Date:   Thu Jan 18 23:12:15 2024 +0100

    llama : fix falcon arch for tied output embeddings (#4978)
    
    * falcon arch fix for tied output embeddings
    
    * Update llama.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update llama.cpp
    
    * Update llama.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update llama.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 9b6ea4263ab45e02ff905bf7a29dc143ca1facc3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 18 23:36:07 2024 +0200

    cmake : add ggml public headers (#5011)

commit 821f0a271e7c9ee737945245dd7abfa22cc9b5b0
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Jan 18 21:33:05 2024 +0100

    server : defer tasks when "slot unavailable" (#5018)
    
    * server: defer task when no slot is available
    
    * remove unnecessary log
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <xuanson.nguyen@snowpack.eu>

commit 96d7f56d2918ffde1995dbb32392571deb76d7fc
Author: slaren <slarengh@gmail.com>
Date:   Thu Jan 18 21:12:15 2024 +0100

    llama : fix mlock with no-mmap with Metal (#5025)

commit 2d5419d08ab1131623e6a1d554607b7663435e87
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 18 21:45:51 2024 +0200

    imatrix : fix assert for src0 non-cont check

commit d391ae9b4919e24624cc963d82162450848beaf4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 18 20:49:00 2024 +0200

    perplexity : fix winogrande N tasks option

commit e9240cdfa06a50c1b5dbafa367cb8cd698e65103
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 18 20:45:39 2024 +0200

    scripts : add get-winogrande.sh

commit b46757735d30f5c6ed4f20ebeccc684e02d4f3bf
Author: David Sommers <12738+databyte@users.noreply.github.com>
Date:   Thu Jan 18 12:20:59 2024 -0500

    convert.py : fix llama/llama2 conversion due to vocab_size=-1 (#5019)
    
    PR #4818 (merged last week) reintroduced a config check for vocab_size that was addressed in PR #4258 (merged 2023-11-30).
    
    Without the fix, llama2 models can't be converted. The error is:
    
    `ValueError: The model's vocab size is set to -1 in params.json. Please update it manually. Maybe 32000?`

commit 3e945cc1e9c06d2001031360e4e303e9548fb02c
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Jan 18 19:18:21 2024 +0200

    HellaSwag: speed up by parallelizing log-prob evaluation (#5020)
    
    For Mistral-7B and fp16, time on my system goes down from 536 seconds
    to 423 seconds for the full evaluation dataset (10042 tasks).
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit ad19812cda4062c9f154ef16315df41fbe6a770a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 18 15:33:01 2024 +0200

    perplexity : faster HellaSwag via batching (#5017)
    
    * perplexity : faster HellaSwag
    
    ggml-ci
    
    * perplexity : clean-up
    
    ggml-ci
    
    * perplexity : no need for decode_helper
    
    ggml-ci
    
    * perplexity : add comments
    
    * perplexity : option to specify max batched tasks via `n_parallel`
    
    * perplexity : remove HellaSwag restruction for n_batch

commit 682986a08eb5cb04865d2e713449f17304d266d8
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Jan 18 13:46:27 2024 +0200

    Add Winogrande evaluation (#5015)
    
    * winogrande: simple implementation
    
    It doesn't look like it is working - why?
    For Mistral-7B it is barely better than
    random chance (score ~60% for 1267 tasks), while I see
    Mistral-7B scoring 78.4% on the HF leader board.
    1-sigma statistical uncertainty for 1267 tasks is ~1.4,
    so no way the difference is due to statistics.
    
    * winogrande: somewhat better
    
    Score for Mistrali7-B is now 68.9 on the validation set of
    winogrande_debiased. Still far from the reported 78.4, but
    better than what I had before.
    
    * winogrande: improving
    
    Mistral-7B score is now 73.56.
    Still not quite 78.4 but getting there.
    We are also getting a lower score on HellaSwag
    compared to HF leader board, so I'm not expecting
    we will get up to 78.4 anyway.
    
    It looks like it is better to skip the choice word(s)
    when evaluating the average log-likelihood. This kind of
    makes sense because a more common word (in Winogrande this is
    often a name) will have a higher probability without knowing
    about the follow up context, and this will skew the log-likelihood
    towards the more common word. We can only do this if the
    choice words are not last in the sentence.
    
    It also looks like it is better to skip the punctuation at the
    end of the sentence, provided the choice words are not last.
    
    * winogrande: add dataset instructions
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit dcad445d0c83ad49bca1b58cf9c139cfcebee5d4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 18 11:44:49 2024 +0200

    scritps : add helper script to get hellaswag data in txt format

commit 1e605f4102c7ea8dc0dff82f5eaa6a71973d549f
Author: Paul Tsochantaris <ptsochantaris@icloud.com>
Date:   Thu Jan 18 08:47:24 2024 +0000

    metal : fix memory leak, dangling pointer and unused autorel (#5007)
    
    * Metal memory: Small memory leak on init, dangling pointer, and unused autorelease pool in graph compute
    
    * SPM header potential fix
    
    * Reverting symlinks

commit 6b6916b215251e09bd57cdbf870dc8a73345edc2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 17 20:54:50 2024 +0200

    sync : ggml

commit 38566680cdfe982a495562332c25b9227de9cf8d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 17 18:54:56 2024 +0200

    ggml : add IQ2 to test-backend-ops + refactoring (#4990)
    
    * ggml : add IQ2 to test-backend-ops + refactoring
    
    ggml-ci
    
    * cuda : update supports_op for IQ2
    
    ggml-ci
    
    * ci : enable LLAMA_CUBLAS=1 for CUDA nodes
    
    ggml-ci
    
    * cuda : fix out-of-bounds-access in `mul_mat_vec_q`
    
    ggml-ci
    
    * tests : avoid creating RNGs for each Q tensor
    
    ggml-ci
    
    * tests : avoid creating RNGs for each tensor
    
    ggml-ci

commit ba69bbc84ced580fe4fdb0713ca2d95634325b7a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 17 18:46:30 2024 +0200

    imatrix : offload to GPU support (#4957)
    
    * backend : add eval callback
    
    ggml-ci
    
    * backend : group nodes in a single compute when user don't need them
    
    * backend : clean-up the implementation
    
    ggml-ci
    
    * simple : do not perform tensor data copy if not needed
    
    * simple : fix
    
    * imatrix : offload to GPU support
    
    * imatrix : fix ggml_mul_mat_id hanlding
    
    ggml-ci
    
    * ci : add imatrix test
    
    ggml-ci
    
    * ci : rearrange output
    
    ggml-ci

commit 44a1a4a41a4c0b03afaa7d9e06bcbc7cf95aa1e6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 17 18:39:41 2024 +0200

    backend : add eval callback (#4935)
    
    * backend : add eval callback
    
    ggml-ci
    
    * backend : group nodes in a single compute when user don't need them
    
    * backend : clean-up the implementation
    
    ggml-ci
    
    * simple : do not perform tensor data copy if not needed
    
    * simple : fix
    
    * simple : no need for ggml_is_contiguous + fix bool parse
    
    * llama : fix callback placement in llama_context_params
    
    * backend : avoid double-ask callback calls
    
    * simple : restore examples, imatrix will serve as a demo

commit c918fe8dca8fa1c4602427e0a4b88e20046f6c34
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 17 18:38:39 2024 +0200

    metal : create autorelease pool during library build (#4970)
    
    * metal : create autorelease pool during library build
    
    ggml-ci
    
    * test : simplify
    
    ggml-ci

commit 0f83e727af0a7cadf90b7ecc1f8e35de1d0880bc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 17 18:37:36 2024 +0200

    py : fix whitespace

commit 4f4bf35f46600441dec2f941e667291eeb9a18d8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 17 15:45:03 2024 +0200

    py : fix missing added_tokens_dict for SPM and BPE vocabs (#4971)
    
    * py : fix missing added_tokens_dict for SPM vocab
    
    * py : pad with unknown tokens when data is missing
    
    ggml-ci
    
    * py : fix BPE vocab conversion
    
    ggml-ci
    
    * py : fix padded dummy tokens (I hope)

commit 2b3a665d3917edf393761a24c4835447894df74a
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Wed Jan 17 12:36:37 2024 +0200

    llama : use Q4_K for attn_v for Q2_K_S when n_gqa >= 4 (#4996)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 75632936659772d5b2ce54b0b65319fecbaac2e6
Author: Paul Tsochantaris <ptsochantaris@icloud.com>
Date:   Wed Jan 17 08:07:24 2024 +0000

    metal : remove unnecessary nil check (#4986)

commit f46c0c1b0ea0bc67e24e4bf026a7e898c1af22a9
Author: David Renshaw <dwrenshaw@gmail.com>
Date:   Wed Jan 17 02:17:50 2024 -0500

    llama : fix copy/paste error in llama_sampling_params comment (#4994)

commit 5c999609013a30c06e6fd28be8db5c2074bcc196
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 16 20:59:31 2024 +0200

    py : remove unnecessary hasattr (#4903)

commit bee938da74c33f42242c3a1058ac0a0a6eeef531
Author: Philip Taron <philip.taron@gmail.com>
Date:   Tue Jan 16 09:56:21 2024 -0800

    nix: remove nixConfig from flake.nix (#4984)

commit cec8a4847062fbd76253e3b085683f39d91e80d3
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Jan 16 18:54:24 2024 +0100

    finetune : add training data file to log message (#4979)
    
    This commit adds the name of the training data file to the log message
    printed when the training data is tokenized.
    
    The motivation for this change is that it can be useful to show which
    file is being tokenized when running the finetune example.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 334a835a1ccc8106a5fa355683a965efb1bfa24b
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Jan 16 19:51:26 2024 +0200

    ggml : importance matrix support for legacy quants (#4969)
    
    * imatrix: adding support for legacy quants
    
    * imatrix: guard Q4_0/Q5_0 against ffn_down craziness
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 4feb4b33eeb1756e46084a4db9230b279af1a480
Author: Maximilian Winter <maximilian.winter.91@gmail.com>
Date:   Tue Jan 16 18:41:42 2024 +0100

    examples : add complete parallel function calling example (#4974)

commit 959ef0c0df725c013c7f712eaa7790b8e38a8e20
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 16 19:34:54 2024 +0200

    perplexity : fix kv cache handling for hellaswag (#4981)
    
    ggml-ci

commit c37b3474e61d609d43cccc3bde5d559e80e4f5d1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 16 19:13:54 2024 +0200

    flake.lock: update flake-parts, flake-parts/nixpkgs-lib, and nixpkgs (#4920)
    
    Flake lock file updates:
    
    • Updated input 'flake-parts':
        'github:hercules-ci/flake-parts/34fed993f1674c8d06d58b37ce1e0fe5eebcb9f5' (2023-12-01)
      → 'github:hercules-ci/flake-parts/07f6395285469419cf9d078f59b5b49993198c00' (2024-01-11)
    • Updated input 'flake-parts/nixpkgs-lib':
        'github:NixOS/nixpkgs/e92039b55bcd58469325ded85d4f58dd5a4eaf58?dir=lib' (2023-11-29)
      → 'github:NixOS/nixpkgs/b0d36bd0a420ecee3bc916c91886caca87c894e9?dir=lib' (2023-12-30)
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/cfc3698c31b1fb9cdcf10f36c9643460264d0ca8' (2023-12-27)
      → 'github:NixOS/nixpkgs/317484b1ead87b9c1b8ac5261a8d2dd748a0492d' (2024-01-08)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

commit 158f8c9e21302114bac3c646f80ea85b52ffa0bd
Author: Paul Tsochantaris <ptsochantaris@icloud.com>
Date:   Tue Jan 16 17:05:19 2024 +0000

    metal : localized logic in `ggml_metal_graph_compute` (#4924)
    
    * Metal: Localized logic in `ggml_metal_graph_compute`, minor performance improvement
    
    * Whitespace
    
    * Collecting command buffer completions on single thread
    
    * Whitespace
    
    * Reduce diff noise

commit 862f5e41ab1fdf12d6f59455aad3f5dd8258f805
Author: Neuman Vong <neuman.vong@gmail.com>
Date:   Wed Jan 17 00:47:34 2024 +1100

    android : introduce starter project example (#4926)
    
    * Introduce starter project for Android
    
    Based on examples/llama.swiftui.
    
    * Add github workflow
    
    * Set NDK version
    
    * Only build arm64-v8a in CI
    
    * Sync bench code
    
    * Rename CI prop to skip-armeabi-v7a
    
    * Remove unused tests

commit 3a48d558a69c88ac17efcaa5900cd9eb19596ac4
Author: Alex Azarov <alex@azarov.by>
Date:   Tue Jan 16 14:41:27 2024 +0100

    metal : replace loop of dispatch_async with dispatch_apply (#4934)
    
    * Replace loop of dispatch_async with dispatch_apply
    
    * Update ggml-metal.m
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 7c8d3abd1a17c28fc56b1a4814bc4b29f91d7454
Author: Alex Azarov <alex@azarov.by>
Date:   Tue Jan 16 14:33:02 2024 +0100

    metal : log `recommendedMaxWorkingSetSize` on iOS 16+ (#4936)
    
    * metal: Log `recommendedMaxWorkingSetSize` on iOS 16+
    
    * Only log on iOS and macOS, ignoring tvOS and other platforms
    
    * Check for Xcode version before using recommendedMaxWorkingSetSize
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 122ed4840cc6d209df6043e027f9f8a03aee01da
Author: Maximilian Winter <maximilian.winter.91@gmail.com>
Date:   Tue Jan 16 13:10:48 2024 +0100

    examples : fix and improv docs for the grammar generator (#4909)
    
    * Create pydantic-models-to-grammar.py
    
    * Added some comments for usage
    
    * Refactored Grammar Generator
    
    Added example and usage instruction.
    
    * Update pydantic_models_to_grammar.py
    
    * Update pydantic-models-to-grammar-examples.py
    
    * Renamed module and imported it.
    
    * Update pydantic-models-to-grammar.py
    
    * Renamed file and fixed grammar generator issue.
    
    * Fixed some issues and bugs of the grammar generator. Imporved Documentation
    
    * Update pydantic_models_to_grammar.py

commit a0b3ac8c48b66206b9c5921ce57bd5c0ea6557c3
Author: Justine Tunney <jtunney@gmail.com>
Date:   Tue Jan 16 03:16:33 2024 -0800

    ggml : introduce GGML_CALL function annotation (#4850)
    
    This change makes it possible to build ggml-cuda.cu and ggml-metal.m as
    independent dynamic shared objects, that may be conditionally linked at
    runtime in a multiplatform binary. It introduces a GGML_CALL annotation
    that documents which functions have a cyclic call relationship, between
    the application code and GPU modules.
    
    This change does nothing, unless the build defines -DGGML_MULTIPLATFORM
    which causes back-references and function pointers to conform to MS ABI
    which is supported by NVCC, ROCm, XCode, GCC and Clang across platforms

commit d75c232e1da56f19ac4d2530dadbe0ab3a11fde5
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Jan 16 12:14:19 2024 +0100

    finetune : use LLAMA_FILE_MAGIC_GGLA (#4961)
    
    This commit replaces the magic number LLAMA_FILE_MAGIC_LORA used in
    finetune.cpp with LLAMA_FILE_MAGIC_GGLA defined in llama.h.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit e0324285a569d0583cf2f4a07a2402221ee25f58
Author: stduhpf <stephduh@live.fr>
Date:   Tue Jan 16 12:04:32 2024 +0100

    speculative : threading options (#4959)
    
    * speculative: expose draft threading
    
    * fix usage format
    
    * accept -td and -tbd args
    
    * speculative: revert default behavior when -td is unspecified
    
    * fix trailing whitespace

commit 3e5ca7931c68152e4ec18d126e9c832dd84914c8
Author: ngc92 <7938269+ngc92@users.noreply.github.com>
Date:   Mon Jan 15 20:40:48 2024 +0200

    pass cpu-architecture arguments only to host code (C;C++) (#4943)

commit 4483396751c79dea540808b9cb9238245d06da2b
Author: David Friehs <david@friehs.info>
Date:   Mon Jan 15 14:06:52 2024 +0100

    llama : apply classifier-free guidance to logits directly (#4951)

commit d9aa4ffa6e0296d42f1f676dd85de97c8491eb73
Author: Victor Z. Peng <ziliangdotme@gmail.com>
Date:   Mon Jan 15 04:41:46 2024 -0800

    awq-py : fix typo in awq-py/README.md (#4947)

commit ddb008d845cd50bb090bf051f570130524042936
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 15 13:27:00 2024 +0200

    cuda : fix dequantize kernel names (#4938)

commit 2faaef39799c97a53bec3898141478700da25757
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jan 15 10:09:38 2024 +0200

    llama : check for 256 divisibility for IQ2_XS, IQ2_XXS (#4950)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 4a3156de2fac9a8ee4279de7804d4e352dcfe121
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jan 15 07:48:06 2024 +0200

    CUDA: faster dequantize kernels for Q4_0 and Q4_1 (#4938)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit a836c8f534ab789b02da149fbdaf7735500bff74
Author: David Pflug <david@pflug.email>
Date:   Sun Jan 14 10:46:00 2024 -0500

    llama : fix missing quotes (#4937)

commit 467a882fd2e5b6172897b49aa45aa29bd3f27685
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Jan 14 16:21:12 2024 +0200

    Add ability to use importance matrix for all k-quants (#4930)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit bb0c1392479398f9aba86d9ec98db0b95ede6e6d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 14 13:26:53 2024 +0200

    llama : check LLAMA_TRACE env for extra logging (#4929)
    
    * llama : minor fix indent
    
    * llama : check LLAMA_TRACE env for extra logging
    
    ggml-ci

commit 9408cfdad6b1c090a7e1419d4434edc260b7e47e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 14 11:08:09 2024 +0200

    scripts : sync-ggml-am.sh option to skip commits

commit 03c526749041c863b0cd842b26b8907e1ea0e0b1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 14 11:03:19 2024 +0200

    llama : use LLAMA_LOG_ macros for logging

commit a128c38de862431f1aae9ccc40b792fbc1b8b682
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Jan 14 10:53:39 2024 +0200

    Fix ffn_down quantization mix for MoE models (#4927)
    
    * Fix ffn_down quantization mix for MoE models
    
    In #4872 I did not consider the part where every third
    tensor is quantized with more bits. Fir MoE this leads to tensors
    of the same layer being quantized with different number of bits,
    which is not considered as a possibility in the inference implementation
    (it is assumed all experts use the same quantization).
    
    * Fix the fix
    
    * Review suggestion
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 5f5fe1bd608fa2ed42af97b5f2ea31be6625fc48
Author: Alex Azarov <alex@azarov.by>
Date:   Sun Jan 14 09:44:39 2024 +0100

    metal : correctly set SIMD support flags on iOS (#4923)
    
    * Correctly set support_simdgroup_reduction and support_simdgroup_mm on iPhone/iPad
    
    * log a little bit more info on iOS

commit ac32902a87147f78d63c931aa8a23dee762660e7
Author: Karthik Kumar Viswanathan <195178+guilt@users.noreply.github.com>
Date:   Sun Jan 14 00:41:44 2024 -0800

    llama : support WinXP build with MinGW 8.1.0 (#3419)

commit 147b17ac94a24d524e367cda26a9ff6245689f34
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Jan 14 09:45:56 2024 +0200

    2-bit quantizations (#4897)
    
    * imatrix: load
    
    * imatrix: WIP
    
    * imatrix: Add Q2_K quantization
    
    * imatrix: also guard against Q2_K_S quantization without importance matrix
    
    * imatrix: guard even more against low-bit quantization misuse
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 807179ec583dcb882f97d9704577c06beb2c5ec9
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Jan 14 09:44:30 2024 +0200

    Make Q3_K_S be the same as olf Q3_K_L for Mixtral-8x7B (#4906)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 76484fbfd355df388f71d6edaa98e1692a74de7e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 14 00:14:46 2024 +0200

    sync : ggml

commit c71d608ce7a1584bf5072f197919dd24f3a6163f
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jan 13 21:41:37 2024 +0100

    ggml: cache sin/cos for RoPE (#4908)

commit 4be5ef556de830c5c4f6e45c05ef4427823fe607
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jan 13 20:45:45 2024 +0200

    metal : remove old API (#4919)
    
    ggml-ci

commit 0ea069b87bd296c556824e57455433b6c0357340
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jan 13 19:31:26 2024 +0200

    server : fix prompt caching with system prompt (#4914)

commit f172de03f11465dc6c5a0fc3a22f8ec254c6832c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jan 13 18:47:38 2024 +0200

    llama : fix detokenization of non-special added-tokens (#4916)
    
    Co-authored-by: goerch <jhr.walter@t-online.de>

commit 2d57de525541247132e354f561ff48775fba5d85
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jan 13 18:46:37 2024 +0200

    metal : disable log for loaded kernels (#4794)

commit df845cc982e7e2ea7b9900e29d55b15338faa78d
Author: David Friehs <david@friehs.info>
Date:   Sat Jan 13 17:29:43 2024 +0100

    llama : minimize size used for state save/load (#4820)
    
    * examples : save-load-state: save only required state
    
    * llama : only reserve n_vocab * n_batch at most for logits
    
    llama_decode asserts that only n_batch tokens are passed each call, and
    n_ctx is expected to be bigger than n_batch.
    
    * llama : always reserve n_vocab * n_batch for logits
    
    llama_context de-serialization breaks if the contexts have differing
    capacity for logits and llama_decode will at maximum resize to
    n_vocab * n_batch.
    
    * llama : only save and restore used logits
    
    for batch sizes of 512 this reduces save state in the best case by
    around 62 MB, which can be a lot if planning to save on each message
    to allow regenerating messages.
    
    * llama : use ostringstream and istringstream for save and load
    
    * llama : serialize rng into minimum amount of space required
    
    * llama : break session version due to serialization changes

commit 6b48ed089377330cdb362970a51c1c89b6d857a8
Author: Someone <sergei.kozlukov@aalto.fi>
Date:   Sat Jan 13 16:29:16 2024 +0000

    workflows: unbreak nix-build-aarch64, and split it out (#4915)
    
    The fix should be just the `sudo apt-get update`

commit 722d33f34ec74c6f7046109f936d0928ffe171bc
Author: Yann Follet <131855179+YannFollet@users.noreply.github.com>
Date:   Sun Jan 14 00:09:08 2024 +0800

    main : add parameter --no-display-prompt (#4541)
    
    * add the parameter : --no-display-prompt , combine with --log-disable it will display only the generated tokens
    
    * remove empty line
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit c30b1ef39aeba497a943416d2897d69fee055b96
Author: texmex76 <40733439+texmex76@users.noreply.github.com>
Date:   Sat Jan 13 17:06:20 2024 +0100

    gguf : fix potential infinite for-loop (#4600)
    
    Co-authored-by: Bernhard Gstrein <gstrein@informatik.uni-freiburg.de>

commit b38b5e93ae31019e87f692b69d27124eae6aac02
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jan 13 18:03:45 2024 +0200

    metal : refactor kernel loading code (#4794)
    
    * metal : detect more GPU families
    
    * metal : refactor kernel loading
    
    * metal : set kernel family requirements
    
    * metal : fix kernel init + fix compile options
    
    * metal : take into account simdgroup reduction support
    
    * metal : print only skipped kernels
    
    * metal : fix check for simdgroup reduction support
    
    * metal : check for Metal 3
    
    * metal : free allocations
    
    * metal : normalize encoder:setComputePipelineStatus calls
    
    ggml-ci
    
    * metal : fix Metal3 family check
    
    ggml-ci
    
    * metal : check for simdgroup matrix mul. feature
    
    ggml-ci

commit 7dc78764e2ff86512e6e31cb0fcb8087df4b4708
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jan 13 15:52:53 2024 +0100

    compare-llama-bench: tweak output format (#4910)

commit 356327feb3f66980ab687040495d722696d98970
Author: Ziad Ben Hadj-Alouane <zied.benhadjalouane@gmail.com>
Date:   Sat Jan 13 09:20:46 2024 -0500

    server : fix deadlock that occurs in multi-prompt scenarios (#4905)
    
    * * fix deadlock
    
    * * dont ruint all whitespace

commit ee8243adaa9a9f51ff449213383874e49efe368f
Author: makomk <makosoft@googlemail.com>
Date:   Sat Jan 13 14:16:11 2024 +0000

    server : fix crash with multimodal models without BOS token (#4904)

commit 15ebe59210e7fd9817ff67f51fa1a5ee2d004294
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jan 13 13:44:37 2024 +0200

    convert : update phi-2 to latest HF repo (#4903)
    
    * convert : update phi-2 to latest HF repo
    
    ggml-ci
    
    * py : try to fix flake stuff

commit de473f5f8e19ba5e659cdf5af65fb9251dce16c5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 12 22:02:43 2024 +0200

    sync : ggml

commit f238461236f4e0e18cac1a554af23c7deadc9b01
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 12 14:02:30 2024 +0200

    ggml : fix 32-bit ARM compat for IQ2_XS (whisper/1758)
    
    * ggml : fix 32-bit ARM compat
    
    * ggml : fix fix
    
    * ggml : fix fix fix

commit fa5c1fb44a2724292da545d6b7cf2a1ac0e0b989
Author: slaren <slarengh@gmail.com>
Date:   Fri Jan 12 20:38:34 2024 +0100

    backend_sched : fix assignments
    
    ggml-ci

commit 52ee4540c0f2e11d52c839db6eb51d014ce060e1
Author: Maximilian Winter <maximilian.winter.91@gmail.com>
Date:   Fri Jan 12 20:46:45 2024 +0100

    examples : add pydantic models to GBNF grammar generator (#4883)
    
    * Create pydantic-models-to-grammar.py
    
    * Added some comments for usage
    
    * Refactored Grammar Generator
    
    Added example and usage instruction.
    
    * Update pydantic_models_to_grammar.py
    
    * Update pydantic-models-to-grammar-examples.py
    
    * Renamed module and imported it.
    
    * Update pydantic-models-to-grammar.py
    
    * Renamed file and fixed grammar generator issue.

commit 3fe81781e3bf98b8e44946240a19f3a6ad08a11a
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Jan 12 20:38:54 2024 +0100

    CUDA: faster q8_0 -> f16 dequantization (#4895)

commit e7e4df031b9e29d4b55a4e0b0295187f6b213db1
Author: slaren <slarengh@gmail.com>
Date:   Fri Jan 12 20:07:38 2024 +0100

    llama : ggml-backend integration (#4766)
    
    * llama : ggml-backend integration
    
    * ggml-backend : add names to buffers
    
    * fix unmap after loading
    
    * batched-bench : add tensor_split param
    
    * llama : check for null tensor_split
    
    * ggml-backend : increase GGML_MAX_BACKENDS
    
    * improve graph splitting, partial fix for --no-kv-offload
    
    * cuda : add ggml-backend split buffer support
    
    * cuda : do not create buffer types for devices that don't exist (fixes usage without CUDA devices available)
    
    * ggml : fix null backend dereference (#4807)
    
    * ggml : fix null backend dereference
    
    * ggml : also check ggml_backend_is_cpu
    
    * test-backend-ops : check buffer allocation failures
    
    * llama : add cparam (split_mode) and command line argument (--split-mode, -sm) to configure the split mode (none, layer or row)
    
    * ggml : fix mul_mat_id work size
    
    * llama : rewrite session kv load/set without graphs
    
    * minor
    
    * llama : only initialize used backends, free backends on context free
    
    * llama : abort ctx if cuda backend init fails
    
    * llama : rewrite lora with ggml-backend and compute on CPU
    
    ggml-ci
    
    * llama : only map to a backend buffer the region of the file mapping containing the tensors used in the buffer
    
    * opencl : add ggml-backend buffer type
    
    * cuda : only use batched_cublas with batched mat muls (fixes fp16 tg perf)
    
    * llama : on Metal, by default offload the full model
    
    ggml-ci
    
    * metal : page align the data ptr (#4854)
    
    * Apply suggestions from code review
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    * cuda : fix split buffer free
    
    * address review comments
    
    * llama-bench : add split-mode parameter
    
    * fix whitespace
    
    * opencl : fix double initialization
    
    * server : add --split-mode parameter
    
    * use async copy and compute to improve multi-gpu performance
    
    ggml-ci
    
    * use async memcpys to copy the graph outputs to the CPU
    
    * fix opencl
    
    * use a host buffer for the cpu compute buffer for faster copies to the gpu
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

commit 584d674be622fbf1578694ada6e62eebedbfd377
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 12 20:54:12 2024 +0200

    llama : remove redundant assert for StableLM (#4901)

commit 930f907d3ece1eb5b0a1ec5e209983a66dcbfa68
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Jan 12 18:54:53 2024 +0100

    export-lora : use LLAMA_FILE_MAGIC_GGLA (#4894)
    
    This commit replaces the magic number used in export-lora.cpp with
    the one defined in llama.h, which is indirectly included via common.h.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit e790eef21ce659f5c16d59f8a5c8dcf6cde0692a
Author: Zay <95888118+isaiahbjork@users.noreply.github.com>
Date:   Fri Jan 12 05:48:00 2024 -0700

    llama.swiftui : update models layout (#4826)
    
    * Updated Models Layout
    
    - Added a models drawer
    - Added downloading directly from Hugging Face
    - Load custom models from local folder
    - Delete models by swiping left
    
    * trimmed trailing white space
    
    * Updated Models Layout

commit 5537d9d36bfdb4379555431f574d3d78ce6e7955
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 12 14:33:21 2024 +0200

    gitignore : imatrix

commit 1b280c9fffd682b6924010a4437f0275f2921fa9
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Jan 12 12:30:41 2024 +0100

    CUDA: fix softmax compile for old CUDA versions (#4862)

commit 3cabe80630c7eeb57713cd02249053a8cf6894fa
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 12 13:10:19 2024 +0200

    llama : fix typo "imp_embd" -> "inp_embd"

commit 4315a94366708828f949f9db89d2a8d99b634459
Author: howlger <eclipse@voormann.de>
Date:   Fri Jan 12 12:05:32 2024 +0100

    common : streamline the formatting of help (#4890)
    
    * common : streamline the formatting of help
    
    - Separate alternative parameters by a comma
    
    - Do not indent `--version` differently
    
    * Update common/common.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 2d00741e12c5db4a33dfccd1125f5de4adec9a5b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 12 13:03:38 2024 +0200

    py : fix lint (#4889)

commit f445c0e68cf8e1faca0b2aa8dfb9d48231cec301
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 12 13:01:56 2024 +0200

    llama : fix llm_build_k_shift to use correct n_rot (#4889)
    
    * llama : fix llm_build_k_shift to use correct n_rot
    
    ggml-ci
    
    * llama : always use hparams.n_rot for ggml_rope_custom
    
    ggml-ci
    
    * convert : fix persimmon conversion to write correct n_rot

commit 326b418b59b6d48d854c4461a2303e8ac0a311e6
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Jan 12 06:59:57 2024 +0100

    Importance Matrix calculation (#4861)
    
    * imatrix: 1st version
    
    * imatrix: WIP
    
    * Cleanup
    
    * Update examples/imatrix/imatrix.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 1d118386fea031f01550f8cd47a5c86296e5333f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 11 23:23:49 2024 +0200

    server : fix infill when prompt is empty (#4833)

commit 7edefbd79cc6dea96640edc54c6b94b2b2496d8b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 11 22:46:26 2024 +0200

    main : better name for variable n_print (#4874)

commit 3ca63b4538dfc78aaec88cd2c3e3f8417c1924e3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 11 22:43:05 2024 +0200

    main : disable token count by default (#4874)

commit b0377875488b33f7114138687d828da1de61775d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 11 21:58:28 2024 +0200

    swift : track ggml release branch (#4867)

commit 469e75d0a35b08de549a4fd87f082ca7a8a539ba
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Jan 11 20:43:15 2024 +0100

    llama : restore intended k-quants mixes for MoE models (#4872)
    
    * Restore intended k-quants quantization mixes for MoE models
    
    * Update Q2_K_S values in the quantize tool
    
    Still using LLaMA-v1 PPL values in the quant description
    today does not make much sense. But let's leave this update
    for another PR.
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 49662cbed3e95f5976c070b85b9fd53fd577038d
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Jan 11 20:39:39 2024 +0100

    ggml : SOTA 2-bit quants (add IQ2_XS) (#4856)
    
    * iq2_xs: basics
    
    * iq2_xs: this should have been in the basics
    
    * iq2_xs: CUDA and scalar CPU works
    
    * iq2_xs: WIP Metal
    
    * iq2_xs: Metal now works
    
    * iq2_xs: working, but dog slow, ARM_NEON dot product
    
    * iq2_xs: better ARM_NEON dot product
    
    We are now at 19.5 t/s for TG-128 and 61 t/s for PP-512 when
    running on the CPU.
    
    * iq2_xs: AVX2 dot product - 19.5 t/s
    
    * iq2_xs: faster AVX2 dit product
    
    21.4 t/s for TG-128, 59.2 t/s for PP-512.
    The latter is 2x compared to the previous version.
    
    * iq2_xs: had forgotten to delete iq2-data.h
    
    * Add llama enum for IQ2_XS
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 3ba5b8ca8e6181a5c712c5b77595a29f1d3e2b97
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 11 21:31:31 2024 +0200

    swift : pin ggml commit + remove ggml.h from spm-headers (#4878)
    
    ggml-ci

commit 4330bd83feb39683de4bd7a34cfcf672ff8ac3e4
Author: Laura <Tijntje_7@msn.com>
Date:   Thu Jan 11 19:02:48 2024 +0100

    server : implement credentialed CORS (#4514)
    
    * Implement credentialed CORS according to MDN
    
    * Fix syntax error
    
    * Move validate_api_key up so it is defined before its first usage

commit 27379455c38cb13f24de92dbd6fcdd04eeb1b9d9
Author: Michael Coppola <m18coppola@gmail.com>
Date:   Thu Jan 11 12:51:17 2024 -0500

    server : support for multiple api keys (#4864)
    
    * server: added support for multiple api keys, added loading api keys from file
    
    * minor: fix whitespace
    
    * added file error handling to --api-key-file, changed code to better
    reflect current style
    
    * server: update README.md for --api-key-file
    
    ---------
    
    Co-authored-by: Michael Coppola <info@michaeljcoppola.com>

commit eab67950068e4b125007d027232c47d2a5831cd0
Author: Behnam M <58621210+ibehnam@users.noreply.github.com>
Date:   Thu Jan 11 12:41:39 2024 -0500

    server : add `LOG_INFO` when model is successfully loaded (#4881)
    
    * added /health endpoint to the server
    
    * added comments on the additional /health endpoint
    
    * Better handling of server state
    
    When the model is being loaded, the server state is `LOADING_MODEL`. If model-loading fails, the server state becomes `ERROR`, otherwise it becomes `READY`. The `/health` endpoint provides more granular messages now according to the server_state value.
    
    * initialized server_state
    
    * fixed a typo
    
    * starting http server before initializing the model
    
    * Update server.cpp
    
    * Update server.cpp
    
    * fixes
    
    * fixes
    
    * fixes
    
    * made ServerState atomic and turned two-line spaces into one-line
    
    * updated `server` readme to document the `/health` endpoint too
    
    * used LOG_INFO after successful model loading

commit d8d90aa343c22fe01429d3540e47ded87e9dcb9d
Author: Someone <sergei.kozlukov@aalto.fi>
Date:   Thu Jan 11 17:22:34 2024 +0000

    ci: nix-flake-update: new token with pr permissions (#4879)
    
    * ci: nix-flake-update: new token with pr permissions
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 43f76bf1c362c067fce46bb8dcda0b64af8a9533
Author: pudepiedj <pudepiedj@gmail.com>
Date:   Thu Jan 11 16:14:52 2024 +0000

    main : print total token count and tokens consumed so far (#4874)
    
    * Token count changes
    
    * Add show token count
    
    * Updating before PR
    
    * Two requested changes
    
    * Move param def posn

commit 2f043328e3116724d15b915b5c6078e2df860a69
Author: Isaac McFadyen <isaac@imcf.me>
Date:   Thu Jan 11 09:33:26 2024 -0500

    server : fix typo in model name (#4876)

commit 2a7c94db5fb67b2f8882d2d16a11bf5d8d12d397
Author: Paul Tsochantaris <ptsochantaris@icloud.com>
Date:   Thu Jan 11 14:31:52 2024 +0000

    metal : put encoder debug group behind a define (#4873)

commit 64802ec00d6383784a9dacf616095eaced16c3c3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 11 09:39:08 2024 +0200

    sync : ggml

commit 3267c2abc72e34608224408ace3c048831050f97
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 11 09:34:59 2024 +0200

    metal : fix deprecation warning (ggml/690)

commit f85a973aa139ae6f37e8b8e1966f1d278b5e0372
Author: Timothy Cronin <40186632+4imothy@users.noreply.github.com>
Date:   Thu Jan 11 02:27:48 2024 -0500

    ggml : remove ggml_cpy_inplace and ggml_cont_inplace (ggml/693)

commit 5362e43962e84d61e20b91f34991d7ccaef4a7d5
Author: Jack Mousseau <jmousseau@users.noreply.github.com>
Date:   Wed Jan 10 06:19:19 2024 -0800

    metal : wrap each operation in debug group (ggml/690)

commit e739de790921e6abbc8c70398303cacd74913f61
Author: leejet <leejet714@gmail.com>
Date:   Wed Jan 10 21:13:42 2024 +0800

    ggml : change GGML_MAX_NAME at compile time (ggml/682)
    
    * change GGML_MAX_NAME to 128
    
    * allow controlling the value of GGML_MAX_NAME through external macro definitions

commit c910e3c28a1caee8cb1398143d582dd9ab697e68
Author: Halalaluyafail3 <55773281+Halalaluyafail3@users.noreply.github.com>
Date:   Tue Jan 9 11:16:37 2024 -0500

    Fix execlp call (ggml/689)
    
    NULL can be an integer constant expression with the value zero, in this case the behavior would be undefined because of an incorrect type being passed to the variable arguments.

commit f34432ca1e0b288129390c1db8296a82aaf1e632
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Fri Jan 5 16:00:00 2024 +0100

    fix : cuda order of synchronization when setting a buffer (ggml/679)
    
    * fix : cuda order of synchronization when setting a buffer
    
    * also sync before memcpy
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 7a9f75c38b5e62fe27b8a5a3ed823b4a3714024b
Author: Behnam M <58621210+ibehnam@users.noreply.github.com>
Date:   Thu Jan 11 02:12:05 2024 -0500

    server : update readme to document the new `/health` endpoint (#4866)
    
    * added /health endpoint to the server
    
    * added comments on the additional /health endpoint
    
    * Better handling of server state
    
    When the model is being loaded, the server state is `LOADING_MODEL`. If model-loading fails, the server state becomes `ERROR`, otherwise it becomes `READY`. The `/health` endpoint provides more granular messages now according to the server_state value.
    
    * initialized server_state
    
    * fixed a typo
    
    * starting http server before initializing the model
    
    * Update server.cpp
    
    * Update server.cpp
    
    * fixes
    
    * fixes
    
    * fixes
    
    * made ServerState atomic and turned two-line spaces into one-line
    
    * updated `server` readme to document the `/health` endpoint too

commit 5c1980d8d4c4e0c0af77359f81cc44d90b3f250b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 11 09:10:34 2024 +0200

    server : fix build + rename enums (#4870)

commit cd108e641dbdedd8c5641c4cec1762f751f38136
Author: Behnam M <58621210+ibehnam@users.noreply.github.com>
Date:   Wed Jan 10 14:56:05 2024 -0500

    server : add a `/health` endpoint (#4860)
    
    * added /health endpoint to the server
    
    * added comments on the additional /health endpoint
    
    * Better handling of server state
    
    When the model is being loaded, the server state is `LOADING_MODEL`. If model-loading fails, the server state becomes `ERROR`, otherwise it becomes `READY`. The `/health` endpoint provides more granular messages now according to the server_state value.
    
    * initialized server_state
    
    * fixed a typo
    
    * starting http server before initializing the model
    
    * Update server.cpp
    
    * Update server.cpp
    
    * fixes
    
    * fixes
    
    * fixes
    
    * made ServerState atomic and turned two-line spaces into one-line

commit 57d016ba2d46a6e22517a31a75cebb48f9e234b6
Author: Brian <mofosyne@gmail.com>
Date:   Thu Jan 11 01:09:53 2024 +1100

    llama : add additional suffixes for model params (#4834)
    
    * llm_load_print_meta: Add additional suffixs for model params
    
    * Update llama.cpp model param log
    
    remove unneeded comments and convert from > to >=

commit 329ff615699d32f596d4ebf8baba654c30064e0d
Author: Austin <77757836+teleprint-me@users.noreply.github.com>
Date:   Wed Jan 10 08:39:09 2024 -0500

    llama : recognize 1B phi models (#4847)
    
    This update categorizes models with 24 layers as MODEL_1B, ensuring compatibility with different Phi model variants without impacting existing Phi-2 model functionality.

commit d34633d8db6c2e400355de4862cd699154ecc73f
Author: John <78893154+cmp-nct@users.noreply.github.com>
Date:   Wed Jan 10 14:37:09 2024 +0100

    clip : support more quantization types (#4846)
    
    Uses ggml functions instead of hardcoded names and adds support to quantize into the modern Q-K variants.
    This is just the bare minimum to get k-types working - a more refined choice of types would be needed to get best quality on low quantizations.
    
    I ran a few tests, it doesn't break anything I could notice and a Q6_K ViT works almost as well as Q8_0 but 3 times the inference speed.

commit 4f56458d34cb13dcbf69aca650e9bf77d5497e6f
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jan 10 01:04:33 2024 +0100

    Python script to compare commits with llama-bench (#4844)

commit 6efb8eb30e7025b168f3fda3ff83b9b386428ad6
Author: Austin <77757836+teleprint-me@users.noreply.github.com>
Date:   Tue Jan 9 13:46:46 2024 -0500

    convert.py : fix vanilla LLaMA model conversion (#4818)
    
    * Update Imports and Add Notes for Future Reference
    
    - Updated import statements in `convert.py`.
    - Added import for `AutoTokenizer` from `transformers` module.
    - Added conditional import for `gguf` from the local directory.
    - Added comments and notes for future reference.
    
    Additional Notes:
    
    - Noted removal of a redundant `TypeAlias` import.
    - Noted the removal of a `gguf` debug statement.
    - Commented on the presence of `ARCH` and `NDArray` definitions.
    - Commented on cleaning up and refactoring data type definitions.
    
    * Refine Model Hyperparameters and Params Class
    
    - Updated type annotations to use `Optional` for clarity.
    - Improved method names and attribute consistency.
    - Removed unnecessary variables for better code readability.
    
    Additional Notes:
    
    - Highlighted the use of `Optional` for clearer intent.
    - Ensured backward and forward compatibility.
    
    * Restore BpeVocab and SentencePieceVocab classes
    
    - Restored the BpeVocab class for handling BPE tokenization.
    - Restored the SentencePieceVocab class for SentencePiece tokenization.
    
    These classes are essential for maintaining the original behavior of the codebase.
    
    * refactor: Standardize vocabulary handling with HfVocab
    
    - Replaced VocabLoader with HfVocab, aligning vocabulary handling across classes.
    - Updated initialization of HfVocab with local_files_only=True for AutoTokenizer.
    - Introduced optional parameter fname_added_tokens for flexible added token management.
    - Streamlined added token handling for clarity and conciseness.
    - Maintained special tokens and IDs, enhancing token management.
    - Simplified token processing methods for improved readability.
    - Added a placeholder for score computation with a default value of -1000.0.
    - Optimized newline token check for efficiency.
    - Updated __repr__ function for clarity in representation.
    - Adjusted type alias Vocab to include BpeVocab, SentencePieceVocab, and HfVocab.
    - Removed redundant code related to special token handling, reverse vocabulary mapping, and vocabulary file detection.
    
    This refactoring promotes a standardized and modular approach to vocabulary management, facilitating future integration with a VocabFactory and improving code maintainability and scalability.
    
    * refactor: Enhance readability, functionality, and code quality
    
    - Improved code formatting and readability for better maintainability.
    - Refactored LazyUnpickler's CLASSES dictionary for clarity.
    - Added print statements and warnings in check_vocab_size for user feedback.
    - Removed find_vocab_file_path, as it's superseded by VocabFactory.
    - Preparatory changes for upcoming classes: OutputFile and VocabFactory.
    - Overall focus on code quality, error handling, and consistency.
    
    These changes reflect a continuous effort to refine the codebase, ensuring it meets best practices and prepares for future enhancements, such as the VocabFactory.
    
    * refactor: Update OutputFile class for enhanced model vocabulary management
    
    - Restructured the constructor for improved readability.
    - Updated `add_meta_arch` method for flexible model name determination.
    - Introduced `handle_tokenizer_model` for mapping vocab types to supported tokenizer models.
    - Streamlined vocabulary extraction with `extract_vocabulary_from_model`.
    - Simplified vocabulary metadata addition using `add_meta_vocab`.
    - Refactored `add_tensor_info` for clarity and consistency.
    - Improved error handling for better user feedback.
    
    These changes signify the development of a versatile and comprehensive `OutputFile` class, enabling efficient management of model conversion output, metadata, vocabulary, and tensor information.
    
    * feat: Introduce VocabFactory for flexible vocabulary management in model conversion
    
    - The VocabFactory class is added to facilitate modular vocabulary handling.
    - The constructor initializes a directory path and detects vocabulary-related files.
    - The _select_file method provides file paths based on vocabulary type (e.g., BPE, SentencePiece).
    - _create_special_vocab generates special vocabularies, accommodating different types.
    - The load_vocab method loads vocabularies, handling BPE, SentencePiece, and Hugging Face Fast Tokenizer.
    - Error handling and logging enhance debugging and user feedback.
    - The modular and flexible design simplifies vocabulary management and supports future extensions.
    
    The VocabFactory class enhances code modularity and maintainability, allowing versatile vocabulary handling in the model conversion process.
    
    * refactor: Improve code organization, argument parsing, and user interface
    
    - Renamed 'default_outfile' to 'default_output_file' for clarity.
    - Refactored argument parser setup into 'get_argument_parser' function.
    - Introduced descriptive comments for each argument in the parser.
    - Added '--vocab-type' argument with choices ["spm", "bpe", "hfft"] for vocabulary processing.
    - Improved flag naming consistency: '--outfile' to '--out-file' and '--bigendian' to '--big-endian'.
    - Enhanced error handling to prevent overwriting input data in 'default_output_file'.
    - Made 'argv' in 'main' an optional parameter for flexibility.
    - Introduced dynamic import for 'awq.apply_awq' based on 'args.awq_path' for conditional dependency.
    
    These changes enhance code clarity, organization, and the user interface of the script, aligning it with Python best practices and improving maintainability.
    
    * refactor: Further refine functionality, improve user interaction, and streamline vocabulary handling
    
    - Renamed command-line arguments for clarity and consistency.
    - Improved path resolution and import adjustments for robustness.
    - Thoughtfully handled 'awq-path' and conditional logic for the weighted model.
    - Enhanced model and vocabulary loading with the 'VocabFactory' class for structured and adaptable loading.
    - Strengthened error handling and user feedback for a more user-friendly experience.
    - Structured output file handling with clear conditions and defaults.
    - Streamlined and organized the 'main' function for better logic flow.
    - Passed 'sys.argv[1:]' to 'main' for adaptability and testability.
    
    These changes solidify the script's functionality, making it more robust, user-friendly, and adaptable. The use of the 'VocabFactory' class is a notable enhancement in efficient vocabulary handling, reflecting a thoughtful and iterative approach to script development.
    
    * chore: Apply ruff formatting to convert.py
    
    Signed-off-by: teleprint-me <77757836+teleprint-me@users.noreply.github.com>
    
    * Revert to commit 0614c33
    
    * chore: Apply flake8 formatting rules
    
    Signed-off-by: teleprint-me <77757836+teleprint-me@users.noreply.github.com>
    
    * refactor: Revise `check_vocab_size` for Enhanced Clarity and Correctness
    
    - Resolved an unreachable branch issue by reorganizing the conditional structure.
    - Moved the special case check for `params.n_vocab == -1` to the top for immediate assertion.
    - Flattened the conditional logic for improved clarity and predictability of the function's behavior.
    
    These changes enhance the readability and functional correctness of the `check_vocab_size` function without altering its intended functionality.
    
    * py : fix outfile and outtype
    
    * py : suggest hint for missing vocab size
    
    ---------
    
    Signed-off-by: teleprint-me <77757836+teleprint-me@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 36e5a08b203542dca53cca4eaf172c5dc4bbc991
Author: Justine Tunney <jtunney@gmail.com>
Date:   Tue Jan 9 09:59:14 2024 -0800

    llava-cli : don't crash if --image flag is invalid (#4835)
    
    This change fixes an issue where supplying `--image missing-file` would
    result in a segfault due to a null pointer being dereferenced. This can
    result in distracting info being printed if robust crash analysis tools
    are being used.

commit 4dccb38d9abab7f9f2d1f9a6977df4185d490132
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 9 19:37:08 2024 +0200

    metal : improve dequantize precision to match CPU (#4836)
    
    ggml-ci

commit 9a818f7c42761984ac99e08e613cc20634f8410e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 9 19:20:45 2024 +0200

    scripts : improve get-pg.sh (#4838)

commit 18adb4e9bb340b7b4565d8b6715b4449283e7641
Author: iohub <rickyang.pro@gmail.com>
Date:   Wed Jan 10 00:45:54 2024 +0800

    readme : add 3rd party collama reference to UI list (#4840)
    
    Add a VSCode extension for llama.cpp reference to UI list

commit d9653894dffbfd3a58616f31b0967b34faf6f611
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 9 16:23:05 2024 +0200

    scripts : script to get Paul Graham essays in txt format (#4838)

commit 128de3585b0f58b1e562733448fc00109f23a95d
Author: Behnam M <58621210+ibehnam@users.noreply.github.com>
Date:   Tue Jan 9 05:02:05 2024 -0500

    server : update readme about token probs (#4777)
    
    * updated server readme to reflect the gg/server-token-probs-4088 commit
    
    added explanation for the API's completion result which now includes `completion_probabilities`. Also added a JSON schema that shows the type/structure of `completion_probabilities`.
    
    * simplified the `completion_probabilities` JSON schema
    
    It's now easier to understand what the structure of `completion_probabilities` looks like.
    
    * minor : fix trailing whitespace
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 8c5833031857c9e9ada61948bae894ab9c785f86
Author: Zsapi <martin1.zsapka@gmail.com>
Date:   Tue Jan 9 10:12:43 2024 +0100

    server : add api-key flag to documentation (#4832)
    
    Document the api-key flag added to server in https://github.com/ggerganov/llama.cpp/pull/4441

commit 18c2e1752c3b387689e9e73d7d8a1a3b1511ce23
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 9 10:42:06 2024 +0200

    ggml : fix vld1q_s8_x4 32-bit compat (#4828)
    
    * ggml : fix vld1q_s8_x4 32-bit compat
    
    ggml-ci
    
    * ggml : fix 32-bit ARM compat (cont)
    
    ggml-ci

commit 8f900abfc09851e281bc9027e0ab2f16bf079b29
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Jan 9 08:58:55 2024 +0100

    CUDA: faster softmax via shared memory + fp16 math (#4742)

commit 1fc2f265ff9377a37fd2c61eae9cd813a3491bea
Author: howlger <eclipse@voormann.de>
Date:   Mon Jan 8 20:05:53 2024 +0100

    common : fix the short form of `--grp-attn-w`, not `-gat` (#4825)
    
    See https://github.com/ggerganov/llama.cpp/blob/master/common/common.cpp#L230C53-L230C57

commit a9a8c5de3d2028701c239d821b220214fcaefbf1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 8 20:25:17 2024 +0200

    readme : add link to SOTA models

commit dd5ae06405c5565b99889bdb3f168f4351252cfb
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jan 8 16:02:32 2024 +0100

    SOTA 2-bit quants (#4773)
    
    * iq2_xxs: basics
    
    * iq2_xxs: scalar and AVX2 dot products
    
    Needed to change Q8_K to have quants in the -127...127 range,
    else the IQ2_XXS AVX implementation becomes very awkward.
    The alternative would have been to use Q8_0 instead. Perhaps
    I'll change later, for now this is what we have.
    
    * iq2_xxs: ARM_NEON dot product
    
    Somehow strangely slow (112 ms/token).
    
    * iq2_xxs: WIP Metal
    
    Dequantize works, something is still wrong with the
    dot product.
    
    * iq2_xxs: Metal dot product now works
    
    We have
    PP-512 = 475 t/s
    TG-128 = 47.3 t/s
    
    Not the greatest performance, but not complete garbage either.
    
    * iq2_xxs: slighty faster dot product
    
    TG-128 is now 48.4 t/s
    
    * iq2_xxs: slighty faster dot product
    
    TG-128 is now 50.9 t/s
    
    * iq2_xxs: even faster Metal dot product
    
    TG-128 is now 54.1 t/s.
    
    Strangely enough, putting the signs lookup table
    into shared memory has a bigger impact than the
    grid values being in shared memory.
    
    * iq2_xxs: dequantize CUDA kernel - fix conflict with master
    
    * iq2_xxs: quantized CUDA dot product (MMVQ)
    
    We get TG-128 = 153.1 t/s
    
    * iq2_xxs: slightly faster CUDA dot product
    
    TG-128 is now at 155.1 t/s.
    
    * iq2_xxs: add to llama ftype enum
    
    * iq2_xxs: fix MoE on Metal
    
    * Fix missing MMQ ops when on hipBLAS
    
    I had put the ggml_supports_mmq call at the wrong place.
    
    * Fix bug in qequantize_row_iq2_xxs
    
    The 0.25f factor was missing.
    Great detective work by @ggerganov!
    
    * Fixing tests
    
    * PR suggestion
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 668b31fc7d86245435ad6574e0e1126e734049e2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 8 16:40:51 2024 +0200

    swift : exclude ggml-metal.metal from the package (#4822)

commit 42ea63c5a3da01d4a94e906d8565868012c79f4f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 8 15:57:36 2024 +0200

    llama.swiftui : update readme

commit 52531fdff88764282c1b233174721aab8347252d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 8 11:18:32 2024 +0200

    main : add self-extend support (#4815)
    
    * examples : add passkey test
    
    * passkey : better prints
    
    * passkey : select pass key pos from CLI
    
    * passkey : simplify n_past logic
    
    * llama : "self-extend"-like context extension
    
    * passkey : add comment
    
    * main : add Self-Extend support
    
    * llama : add comment about llama_kv_cache_seq_div

commit b0034d93ce2949ce7d9c098ca02e56f66cd484e2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 8 11:14:04 2024 +0200

    examples : add passkey test (#3856)
    
    * examples : add passkey test
    
    * passkey : better prints
    
    * passkey : select pass key pos from CLI
    
    * passkey : simplify n_past logic
    
    * make : add passkey target
    
    * passkey : add "self-extend"-like context extension (#4810)
    
    * llama : "self-extend"-like context extension
    
    * passkey : add comment
    
    * passkey : add readme

commit b7e7982953f80a656e03feb5cfb17a17a173eb26
Author: Lars Grammel <lars.grammel@gmail.com>
Date:   Sun Jan 7 21:24:11 2024 +0100

    readme : add lgrammel/modelfusion JS/TS client for llama.cpp (#4814)

commit 226460cc0d5b185bc6685fb76f418fd9418d7add
Author: slaren <slarengh@gmail.com>
Date:   Sun Jan 7 17:59:01 2024 +0100

    llama-bench : add no-kv-offload parameter (#4812)

commit d5a410e8556191672465f7ff58682ea2474038b0
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Jan 7 17:24:08 2024 +0100

    CUDA: fixed redundant value dequantization (#4809)

commit 9dede37d812604897496dd9d276ae9fbe13d1042
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 7 14:29:36 2024 +0200

    llama : remove unused vars (#4796)

commit 3c36213df850a2353e95572b3636797c79b7c815
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 7 11:21:53 2024 +0200

    llama : remove redundant GQA check (#4796)

commit 72d8407b3696dd1293bd233b6db392be108bc377
Author: Alex Azarov <alexander.azarov@mapbox.com>
Date:   Sun Jan 7 09:20:50 2024 +0100

    llama.swiftui : use llama.cpp as SPM package (#4804)

commit d117d4dc5dadb46831036bfa4d6e5e8c86babaf1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 7 09:50:31 2024 +0200

    llama : print tensor meta for debugging

commit 3418c03ecc149fd657527ebb06776239b60a3f3b
Author: Alex Azarov <alexander.azarov@mapbox.com>
Date:   Sun Jan 7 08:46:55 2024 +0100

    llama.swiftui : add visionOS target (#4805)

commit 63ee677efd92060b14894b984597c62e3742b8da
Author: Konstantin Zhuravlyov <konstantin.zhuravlyov@amd.com>
Date:   Sun Jan 7 01:52:42 2024 -0500

    ggml : use __builtin_amdgcn_sudot4 in __dp4a for gfx11 (#4787)

commit 67984921a70a7e680a24494aeb7575a66e90685d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 7 08:45:26 2024 +0200

    server : fix n_predict check (#4798)

commit c75ca5d96f902564cbbbdd7f5cade80d53c288bb
Author: Daniel Illescas Romero <illescas.daniel@protonmail.com>
Date:   Sat Jan 6 16:12:59 2024 +0100

    llama.swiftui : use correct pointer for llama_token_eos (#4797)

commit 96e80dabc6e73ff68b09b68947b1fc25883c5094
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jan 6 11:40:24 2024 +0200

    examples : improve base-translate.sh script (#4783)

commit eec22a1c6378d9a013943cbddb4330c0da621442
Author: a-n-n-a-l-e-e <150648636+a-n-n-a-l-e-e@users.noreply.github.com>
Date:   Fri Jan 5 08:04:40 2024 -0800

    cmake : check for openblas64 (#4134)
    
    openblas v0.3.22 64-bit pkg-config file is named openblas64.pc
    https://github.com/OpenMathLib/OpenBLAS/issues/3790

commit be36bb946a6336238e92706464de6a30495fe825
Author: Ikko Eltociear Ashimine <eltociear@gmail.com>
Date:   Sat Jan 6 01:02:44 2024 +0900

    flake.nix : fix typo (#4700)
    
    betwen -> between

commit 91d38876dfa10332359ac671b62353aeceb448d3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 5 16:30:52 2024 +0200

    metal : switch back to default.metallib (ggml/681)
    
    ggml-ci

commit d061bf9405cc5fd50792fb2dbdff9c9ea53d6bf9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 5 15:36:04 2024 +0200

    ggml : fix q2_k bpw in comments (ggml/680)

commit 1bf681f90ef4cf37b36e6d604d3e30fc57eda650
Author: Finn Voorhees <finnvoorhees@gmail.com>
Date:   Wed Jan 3 08:39:43 2024 -0500

    ggml : add error handling to graph_compute (whisper/1714)

commit c1d7cb28d3fed97fbc95fc3c43f0c5e2113e546c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 5 15:18:21 2024 +0200

    ggml : do not sched_yield when calling BLAS (#4761)
    
    * ggml : do not sched_yield when calling BLAS
    
    ggml-ci
    
    * ggml : fix do_yield logic
    
    ggml-ci
    
    * ggml : simplify do_yield logic
    
    ggml-ci

commit 3681f22443d917e7328456b69c276d6927dafeec
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 5 15:11:10 2024 +0200

    examples : add few-shot translation example (#4783)

commit b3a7c20b5c035250257d2b62851c379b159c899a
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Jan 4 20:45:37 2024 +0100

    finetune : remove unused includes (#4756)
    
    This commit removes unused includes from finetune.cpp.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 012cf349aec8ffb47c9def5dc018240fa3721e8b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 4 19:56:33 2024 +0200

    server : send token probs for "stream == false" (#4714)

commit a91928014fcf51fe297823fcff0788de4f14206e
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Jan 4 09:43:23 2024 +0100

    Print backend name on test-backend-ops failure (#4751)

commit 3c0b585561d74a56977cf3a3844535ecc9e37972
Author: singularity <12184989+singularity-s0@users.noreply.github.com>
Date:   Thu Jan 4 16:22:38 2024 +0800

    llama.swiftui : support loading custom model from file picker (#4767)
    
    * swiftui: support load model from file picker
    
    * swiftui: remove trailing whitespace

commit e5804313a1edaf00726ed0b96ecced07accbf50c
Author: Michael Coppola <m18coppola@gmail.com>
Date:   Thu Jan 4 03:17:09 2024 -0500

    server : fix options in README.md (#4765)
    
    * fix examples/server/README.md
    
    * minor : fix whitespace
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit dc891b7f7a23158d54f9383790b92c79cc5906c1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 4 10:12:26 2024 +0200

    ggml : include stdlib.h before intrin.h (#4736)

commit 46cea79e1f32499bb24b9fab12123cd386e96728
Author: singularity <12184989+singularity-s0@users.noreply.github.com>
Date:   Thu Jan 4 15:58:16 2024 +0800

    llama.swiftui : fix build of ggml.metallib (#4754)
    
    * metal: fix metal backend init failure in swiftui
    
    * metal: build ggml.metallib instead of copy src
    
    * llama.swift : remove debug flags from metallib build
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit cb1e2818e0e12ec99f7236ec5d4f3ffd8bcc2f4a
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed Jan 3 18:53:40 2024 +0100

    train : fix typo in overlapping-samples help msg (#4758)
    
    This commit fixes a typo in the help message for the
    --overlapping-samples option.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit ece9a45e8ffb73ad461c792720c2fec28b0137bc
Author: Ashraful Islam <ashraful.meche@gmail.com>
Date:   Wed Jan 3 11:30:02 2024 -0600

    swift : update Package.swift to use ggml as dependency (#4691)
    
    * updates the package.swift to use ggml as dependency
    
    * changes the ggml package url src to ggerganov

commit 7bed7eba359b0fa8e575345dc5467a46b4ba509f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 3 14:18:46 2024 +0200

    cuda : simplify expression
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit d55356d3baa58a6c3a9171cb67a67094b9aa9dff
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 3 13:01:44 2024 +0200

    cuda : mark I16 and I32 ops as unsupported
    
    ggml-ci

commit 75e3fd85814c367b55aea11e7bb38cb7b82c6aa0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 3 11:37:44 2024 +0200

    sync : ggml
    
    ggml-ci

commit 289313716ff7ccf6aee284f686a0fe8cbc7714af
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 3 11:35:46 2024 +0200

    metal : add kernel_get_rows_i32
    
    ggml-ci

commit ab62fc3e5520f5a143c36cb23c269f11aa4dafd6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 3 11:25:54 2024 +0200

    scripts : fix sync order + metal sed

commit 5f66ebca9c41a17385341da4b553a8eb5f07edee
Author: Guillaume Wenzek <gwenzek@users.noreply.github.com>
Date:   Fri Dec 29 18:07:03 2023 +0100

    ggml : extend ggml_get_rows, ggml_repeat, ggml_concat (ggml/639)
    
    * add more int ops
    
    * ggml_compute_forward_dup_bytes
    
    * add tests
    
    * PR comments
    
    * tests : minor indentations
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit f2eb19bd8bc9f5730d6e05d7a52a9e19bf5ac099
Author: Justin Parker <jparkerweb@gmail.com>
Date:   Wed Jan 3 03:43:19 2024 -0500

    server : throw an error when `slot unavailable` (#4741)

commit f3f62f0d835d559e80714bbeb05d03125574e3dd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 2 21:07:47 2024 +0200

    metal : optimize ggml_mul_mat_id (faster Mixtral PP) (#4725)
    
    * ggml : disable fast-math for Metal (cmake build only)
    
    ggml-ci
    
    * metal : fix Metal API debug warnings
    
    * cmake : add -fno-inline for Metal build (#4545)
    
    * metal : fix API debug warnings
    
    * metal : fix compile warnings
    
    * metal : use uint64_t for strides
    
    * cmake : rename option to LLAMA_METAL_SHADER_DEBUG
    
    * metal : fix mat-vec Q8_0 kernel for BS > 1
    
    * metal : normalize mat-vec kernel signatures
    
    * cmake : respect LLAMA_QKK_64 option
    
    * metal : fix mat-vec Q4_K kernel for QK_K == 64
    
    * metal : optimizing ggml_mul_mat_id (wip)
    
    * metal : minor fix
    
    * metal : opt mul_mm_id

commit 0ef3ca2ac62016c0c545de1c89dc2e3e130f4a99
Author: Phil H <5756783+phiharri@users.noreply.github.com>
Date:   Tue Jan 2 15:48:49 2024 +0000

    server : add token counts to html footer (#4738)
    
    * server: add token counts to stats
    
    * server: generate hpp
    
    ---------
    
    Co-authored-by: phiharri <ph@got-root.co.uk>

commit 540938f8904707dd74cb3be18495f853b312e72f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 2 16:26:45 2024 +0200

    llama : llama_model_desc print number of experts

commit 0040d42eeb237197054cc7790df5776eacfa608e
Author: Marcus Dunn <51931484+MarcusDunn@users.noreply.github.com>
Date:   Tue Jan 2 06:15:16 2024 -0800

    llama : replace all API facing `int`'s with `int32_t` (#4577)
    
    * replaced all API facing `int`'s with `int32_t`
    
    * formatting and missed `int` in `llama_token_to_piece`

commit 83e633c27efdf0eb0ba54249e784b0ea760b1007
Author: postmasters <namnguyen@google.com>
Date:   Tue Jan 2 03:51:28 2024 -0800

    llama : differentiate the KV dims in the attention (#4657)
    
    * Add n_key_dim and n_value_dim
    
    Some models use values that are not derived from `n_embd`.
    Also remove `n_embd_head` and `n_embd_gqa` because it is not clear
    which "head" is referred to (key or value).
    
    Fix issue #4648.
    
    * Fix `llm_build_kqv` to use `n_value_gqa`
    
    * Rebase
    
    * Rename variables
    
    * Fix llm_build_kqv to be more generic wrt n_embd_head_k
    
    * Update default values for n_embd_head_k and n_embd_head_v
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Fix llm_load_tensors: the asserts were not backcompat
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 32866c5edde402f42ff4233bb89dcfcede34fd22
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 2 13:28:15 2024 +0200

    editorconfig : fix whitespace and indentation #4710

commit 5d7002d4372ebf107cfaf46fcd90df27b204f330
Author: minarchist <minarchist@users.noreply.github.com>
Date:   Tue Jan 2 04:38:15 2024 -0600

    server : add --override-kv parameter (#4710)
    
    * Changes to server to allow metadata override
    
    * documentation
    
    * flake.nix: expose full scope in legacyPackages
    
    * flake.nix: rocm not yet supported on aarch64, so hide the output
    
    * flake.nix: expose checks
    
    * workflows: nix-ci: init; build flake outputs
    
    * workflows: nix-ci: add a job for eval
    
    * workflows: weekly `nix flake update`
    
    * workflows: nix-flakestry: drop tag filters
    
    ...and add a job for flakehub.com
    
    * workflows: nix-ci: add a qemu job for jetsons
    
    * flake.nix: suggest the binary caches
    
    * flake.lock: update
    
    to a commit recently cached by nixpkgs-cuda-ci
    
    ---------
    
    Co-authored-by: John <john@jLap.lan>
    Co-authored-by: Someone Serge <sergei.kozlukov@aalto.fi>

commit 26f3071d714f0b27ad7f021a46a66a1085480258
Author: Nam D. Tran <42194884+namtranase@users.noreply.github.com>
Date:   Tue Jan 2 16:23:38 2024 +0700

    py : re-enable mmap in convert hf (#4732)
    
    * update: awq support llama-7b model
    
    * update: change order
    
    * update: benchmark results for llama2-7b
    
    * update: mistral 7b v1 benchmark
    
    * update: support 4 models
    
    * fix: Readme
    
    * update: ready for PR
    
    * update: readme
    
    * fix: readme
    
    * update: change order import
    
    * black
    
    * format code
    
    * update: work for bot mpt and awqmpt
    
    * update: readme
    
    * Rename to llm_build_ffn_mpt_awq
    
    * Formatted other files
    
    * Fixed params count
    
    * fix: remove code
    
    * update: more detail for mpt
    
    * fix: readme
    
    * fix: readme
    
    * update: change folder architecture
    
    * fix: common.cpp
    
    * fix: readme
    
    * fix: remove ggml_repeat
    
    * update: cicd
    
    * update: cicd
    
    * uppdate: remove use_awq arg
    
    * update: readme
    
    * llama : adapt plamo to new ffn
    
    ggml-ci
    
    * fix: update torch version
    
    ---------
    
    Co-authored-by: Trần Đức Nam <v.namtd12@vinai.io>
    Co-authored-by: Le Hoang Anh <v.anhlh33@vinai.io>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 775ac8712a7b42cfead2585f42cec0dfd56644ab
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Jan 2 10:16:55 2024 +0100

    finetune: fix typo in README.md (#4733)
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 58ba655af054715c0516ee270ad028ad9e74f357
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 2 10:57:44 2024 +0200

    metal : enable shader debugging (cmake option) (#4705)
    
    * ggml : disable fast-math for Metal (cmake build only)
    
    ggml-ci
    
    * metal : fix Metal API debug warnings
    
    * cmake : add -fno-inline for Metal build (#4545)
    
    * metal : fix API debug warnings
    
    * metal : fix compile warnings
    
    * metal : use uint64_t for strides
    
    * cmake : rename option to LLAMA_METAL_SHADER_DEBUG
    
    * metal : fix mat-vec Q8_0 kernel for BS > 1
    
    * metal : normalize mat-vec kernel signatures
    
    * cmake : respect LLAMA_QKK_64 option
    
    * metal : fix mat-vec Q4_K kernel for QK_K == 64
    
    ggml-ci

commit edd1ab7bc34c10a780ee7f9a4499f7689cdad36d
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sun Dec 31 17:42:22 2023 +0000

    flake.lock: update
    
    to a commit recently cached by nixpkgs-cuda-ci

commit 198ed7ebfc89b8f2b35a8b1655d57bfb57530c1a
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sat Dec 30 18:25:25 2023 +0000

    flake.nix: suggest the binary caches

commit d8361747317c5cb2e00e7fb3b59ff4dce5a176a5
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sat Dec 30 18:01:07 2023 +0000

    workflows: nix-ci: add a qemu job for jetsons

commit 06f2a5d1909a1385b1a16dab4ade68377e121bdd
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sat Dec 30 17:36:08 2023 +0000

    workflows: nix-flakestry: drop tag filters
    
    ...and add a job for flakehub.com

commit c5239944bab0ff71915df8f2dc7e42fc2c138ff6
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sat Dec 30 16:38:36 2023 +0000

    workflows: weekly `nix flake update`

commit 1e9ae54cf24d27afe3900d1250634a2a33423db1
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sat Dec 30 17:19:11 2023 +0000

    workflows: nix-ci: add a job for eval

commit 7adedecbe39bd552bc14142f496246d55a43ac4e
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Tue Dec 26 19:17:26 2023 +0000

    workflows: nix-ci: init; build flake outputs

commit 356ea17e0f92bfbbf28a4f69261bed48eff68d9c
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Fri Dec 29 16:21:50 2023 +0000

    flake.nix: expose checks

commit a5c088d8c698299b973d2709153e5d95295606d9
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Tue Dec 26 23:34:40 2023 +0000

    flake.nix: rocm not yet supported on aarch64, so hide the output

commit 1e3900ebacb3a0b385271389686403c97ad76d88
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Fri Dec 29 16:15:37 2023 +0000

    flake.nix: expose full scope in legacyPackages

commit e39106c0554cbd0e9310e08fb3b2a577ea4b6273
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Dec 31 11:43:31 2023 +0200

    ggml : add ggml_vdotq_s32 alias (#4715)
    
    ggml-ci

commit 9fbda719de18a9400a064c28759c39d55d687d3e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Dec 30 23:24:42 2023 +0200

    clip : refactor + bug fixes (#4696)
    
    * clip : refactor + bug fixes
    
    ggml-ci
    
    * server : add log message

commit 39d8bc71edcb8b6f99d46fa4216af7a15232e218
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Dec 30 13:52:01 2023 +0100

    CUDA: fixed tensor cores not being used on RDNA3 (#4697)

commit 24a447e20af425fa44cf10feaa632b6bb596c80f
Author: automaticcat <daogiatuank54@gmail.com>
Date:   Sat Dec 30 15:07:48 2023 +0700

    ggml : add ggml_cpu_has_avx_vnni() (#4589)
    
    * feat: add avx_vnni based on intel documents
    
    * ggml: add avx vnni based on intel document
    
    * llama: add avx vnni information display
    
    * docs: add more details about using oneMKL and oneAPI for intel processors
    
    * docs: add more details about using oneMKL and oneAPI for intel processors
    
    * docs: add more details about using oneMKL and oneAPI for intel processors
    
    * docs: add more details about using oneMKL and oneAPI for intel processors
    
    * docs: add more details about using oneMKL and oneAPI for intel processors
    
    * Update ggml.c
    
    Fix indentation upgate
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit a20f3c7465d6d1b33767757c2760643b799a81bf
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Dec 29 23:12:53 2023 +0100

    CUDA: fix tensor core logic for Pascal and HIP (#4682)

commit 0235b9b571f3cc7d2b8836409a5404b41ce1379c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Dec 29 18:53:34 2023 +0200

    clip : use ggml_backend_buffer_is_host (#4205)

commit ce18d727a47f2473ca863a6f78bf3ad480008f72
Author: Steward Garcia <57494570+FSSRepo@users.noreply.github.com>
Date:   Fri Dec 29 11:52:15 2023 -0500

    clip : enable gpu backend (#4205)
    
    * clip: enable CUDA backend
    
    * add missing kernels
    
    * add enough padding for alignment
    
    * remove ggml_repeat of clip.cpp
    
    * add metal backend
    
    * llava : fixes
    
    - avoid ggml_repeat
    - use GGML_USE_ instead of CLIP_USE_ macros
    - remove unused vars
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 91bb39cec7e4dfb9e2293509ef60298a67f0b1b7
Author: hydai <z54981220@gmail.com>
Date:   Sat Dec 30 00:31:19 2023 +0800

    cuda: fix vmm oom issue on NVIDIA AGX Orin (#4687)
    
    Signed-off-by: hydai <hydai@secondstate.io>

commit 04ac0607e913ab91234dfb240e12a76509e30982
Author: crasm <crasm@git.vczf.us>
Date:   Fri Dec 29 09:50:29 2023 -0500

    python : add check-requirements.sh and GitHub workflow (#4585)
    
    * python: add check-requirements.sh and GitHub workflow
    
    This script and workflow forces package versions to remain compatible
    across all convert*.py scripts, while allowing secondary convert scripts
    to import dependencies not wanted in convert.py.
    
    * Move requirements into ./requirements
    
    * Fail on "==" being used for package requirements (but can be suppressed)
    
    * Enforce "compatible release" syntax instead of ==
    
    * Update workflow
    
    * Add upper version bound for transformers and protobuf
    
    * improve check-requirements.sh
    
    * small syntax change
    
    * don't remove venvs if nocleanup is passed
    
    * See if this fixes docker workflow
    
    * Move check-requirements.sh into ./scripts/
    
    ---------
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>

commit 68eccbdc5b56f2a2450f9a8463f9934388cafabf
Author: Philip Taron <philip.taron@gmail.com>
Date:   Fri Dec 29 06:42:26 2023 -0800

    flake.nix : rewrite (#4605)
    
    * flake.lock: update to hotfix CUDA::cuda_driver
    
    Required to support https://github.com/ggerganov/llama.cpp/pull/4606
    
    * flake.nix: rewrite
    
    1. Split into separate files per output.
    
    2. Added overlays, so that this flake can be integrated into others.
       The names in the overlay are `llama-cpp`, `llama-cpp-opencl`,
       `llama-cpp-cuda`, and `llama-cpp-rocm` so that they fit into the
       broader set of Nix packages from [nixpkgs](https://github.com/nixos/nixpkgs).
    
    3. Use [callPackage](https://summer.nixos.org/blog/callpackage-a-tool-for-the-lazy/)
       rather than `with pkgs;` so that there's dependency injection rather
       than dependency lookup.
    
    4. Add a description and meta information for each package.
       The description includes a bit about what's trying to accelerate each one.
    
    5. Use specific CUDA packages instead of cudatoolkit on the advice of SomeoneSerge.
    
    6. Format with `serokell/nixfmt` for a consistent style.
    
    7. Update `flake.lock` with the latest goods.
    
    * flake.nix: use finalPackage instead of passing it manually
    
    * nix: unclutter darwin support
    
    * nix: pass most darwin frameworks unconditionally
    
    ...for simplicity
    
    * *.nix: nixfmt
    
    nix shell github:piegamesde/nixfmt/rfc101-style --command \
        nixfmt flake.nix .devops/nix/*.nix
    
    * flake.nix: add maintainers
    
    * nix: move meta down to follow Nixpkgs style more closely
    
    * nix: add missing meta attributes
    
    nix: clarify the interpretation of meta.maintainers
    
    nix: clarify the meaning of "broken" and "badPlatforms"
    
    nix: passthru: expose the use* flags for inspection
    
    E.g.:
    
    ```
    ❯ nix eval .#cuda.useCuda
    true
    ```
    
    * flake.nix: avoid re-evaluating nixpkgs too many times
    
    * flake.nix: use flake-parts
    
    * nix: migrate to pname+version
    
    * flake.nix: overlay: expose both the namespace and the default attribute
    
    * ci: add the (Nix) flakestry workflow
    
    * nix: cmakeFlags: explicit OFF bools
    
    * nix: cuda: reduce runtime closure
    
    * nix: fewer rebuilds
    
    * nix: respect config.cudaCapabilities
    
    * nix: add the impure driver's location to the DT_RUNPATHs
    
    * nix: clean sources more thoroughly
    
    ...this way outPaths change less frequently,
    and so there are fewer rebuilds
    
    * nix: explicit mpi support
    
    * nix: explicit jetson support
    
    * flake.nix: darwin: only expose the default
    
    ---------
    
    Co-authored-by: Someone Serge <sergei.kozlukov@aalto.fi>

commit 97bbca6e8522d18041fcde6c3d0907a52ce36446
Author: Cuong Trinh Manh <nguoithichkhampha@gmail.com>
Date:   Fri Dec 29 21:39:15 2023 +0700

    cmake : fix ld warning duplicate libraries libllama.a (#4671)
    
    * fix "ld: warning: ignoring duplicate libraries: '../libllama.a'"
    
    * fix warning in example.

commit 4af4801566bc262a38fb77f51edf278ac323c2bd
Author: Justine Tunney <jtunney@gmail.com>
Date:   Fri Dec 29 06:38:38 2023 -0800

    llava-cli : refactor to use sampling library (#4669)
    
    This change makes it possible to use flags like `--grammar` when using
    the `llava-cli` program. The rest is just code cleanup deleting a long
    standing TODO comment.
    
    This change also ensures that logging information is emitted to stderr
    which helps the `llava-cli` command be more friendly to shell scripts.
    
    See Mozilla-Ocho/llamafile@1cd334f

commit db49ff8ed7f0bb201176703441cc02911b08ef2a
Author: Justine Tunney <jtunney@gmail.com>
Date:   Fri Dec 29 06:24:12 2023 -0800

    server : replace sleep with condition variables (#4673)
    
    The server currently schedules tasks using a sleep(5ms) busy loop. This
    adds unnecessary latency since most sleep implementations do a round up
    to the system scheduling quantum (usually 10ms). Other libc sleep impls
    spin for smaller time intervals which results in the server's busy loop
    consuming all available cpu. Having the explicit notify() / wait() code
    also helps aid in the readability of the server code.
    
    See mozilla-Ocho/llamafile@711344b

commit 60f55e888c29cbd87c4238dd19e85d0eef87245d
Author: SakuraUmi <yukinon244@gmail.com>
Date:   Fri Dec 29 22:22:44 2023 +0800

    server : fix OpenAI server sampling w.r.t. penalty. (#4675)

commit b93edd22f55d3e5268263c3edcdae1818505c078
Author: Karthik Sethuraman <k.seth1993@gmail.com>
Date:   Fri Dec 29 06:22:10 2023 -0800

    server : allow to generate multimodal embeddings (#4681)

commit 82d6eab224862a7044069fb9211dc4b29124264b
Author: andrijdavid <david@geek.mg>
Date:   Fri Dec 29 15:18:20 2023 +0100

    main-cmake-pkg : fix build issue (#4665)
    
    * Fix main-cmake-pkg compilation
    
    * Use glob to load common files
    
    * cmake : fix trailing whitespace
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit afd997ab6011dfefe9e917425b04ef4d83614841
Author: Peter Sugihara <peter@campsh.com>
Date:   Fri Dec 29 05:58:56 2023 -0800

    llama.swiftui : fix infinite loop, ouput timings, buff UI (#4674)
    
    * fix infinite loop
    
    * slight UI simplification, clearer UX
    
    * clearer UI text, add timings to completion log

commit c8255f8a6b2a3b3ebc6cb340cc2487f39fc95ffc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Dec 29 15:12:35 2023 +0200

    scripts : print list of sync commits

commit 441f51dca004debf8b275f1bdc08e0f1af7fd8f8
Author: Tamotsu Takahashi <ttakah+github@gmail.com>
Date:   Fri Dec 29 19:23:27 2023 +0900

    ci : build with CLBlast + ggml-opencl use GGML_API (whisper/1576)
    
    * Build with CLBlast
    
    * Declare GGML_API
    
    After rebasing, examples/talk-llama failed:
    
    "D:\a\whisper.cpp\whisper.cpp\build\ALL_BUILD.vcxproj" (build target) (1) ->
    "D:\a\whisper.cpp\whisper.cpp\build\examples\talk-llama\talk-llama.vcxproj" (default target) (14) ->
    (Link target) ->
      llama.obj : error LNK2019: unresolved external symbol ggml_cl_free_data referenced in function "public: __cdecl llama_model::~llama_model(void)" (??1llama_model@@QEAA@XZ) [D:\a\whisper.cpp\whisper.cpp\build\examples\talk-llama\talk-llama.vcxproj]
      llama.obj : error LNK2019: unresolved external symbol ggml_cl_transform_tensor referenced in function "public: void __cdecl llama_model_loader::load_all_data(struct ggml_context *,void (__cdecl*)(float,void *),void *,struct llama_mlock *)" (?load_all_data@llama_model_loader@@QEAAXPEAUggml_context@@P6AXMPEAX@Z1PEAUllama_mlock@@@Z) [D:\a\whisper.cpp\whisper.cpp\build\examples\talk-llama\talk-llama.vcxproj]
      D:\a\whisper.cpp\whisper.cpp\build\bin\Release\talk-llama.exe : fatal error LNK1120: 2 unresolved externals [D:\a\whisper.cpp\whisper.cpp\build\examples\talk-llama\talk-llama.vcxproj]

commit 38b3de4658292582a8941a2be5c77b40ce6ac0f2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Dec 29 14:56:41 2023 +0200

    sync : ggml

commit afc8c192919f04613a92d40391bff4c8cd99856b
Author: bssrdf <merlintiger@hotmail.com>
Date:   Fri Dec 29 03:32:31 2023 -0500

    ggml : fix some mul mat cases + add tests for src1 F16 (ggml/669)
    
    * fixed mul-mat error for old GPUs
    
    * style fixes
    
    * add mul mat src1 f16 test cases, fix more cases
    
    ggml-ci
    
    ---------
    
    Co-authored-by: bssrdf <bssrdf@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>

commit ca38b8d334baa724bd6c9402470931d26427466f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Dec 29 14:41:36 2023 +0200

    scripts : do not sync commits from this repo

commit 65e5f6dadbba4b496bba27f573e473c66b446496
Author: Justine Tunney <jtunney@gmail.com>
Date:   Thu Dec 28 11:20:00 2023 -0800

    Fix OpenAI server sampling w.r.t. temp and seed (#4668)
    
    The default values for tfs_z and typical_p were being set to zero, which
    caused the token candidates array to get shrunk down to one element thus
    preventing any sampling. Note this only applies to OpenAI API compatible
    HTTP server requests.
    
    The solution is to use the default values that OpenAI documents, as well
    as ensuring we use the llama.cpp defaults for the rest. I've tested this
    change still ensures deterministic output by default. If a "temperature"
    greater than 0 is explicitly passed, then output is unique each time. If
    "seed" is specified in addition to "temperature" then the output becomes
    deterministic once more.
    
    See mozilla-Ocho/llamafile#117
    See mozilla-Ocho/llamafile@9e4bf29

commit ea5497df5d138c83b2b0ca70aefdc4b1175c1001
Author: manikbhandari <mbbhandarimanik2@gmail.com>
Date:   Thu Dec 28 09:03:57 2023 -0500

    gpt2 : Add gpt2 architecture integration (#4555)

commit f6793491b5af6da75edad34d6f503ef86d31b09f
Author: Nam D. Tran <42194884+namtranase@users.noreply.github.com>
Date:   Wed Dec 27 22:39:45 2023 +0700

    llama : add AWQ for llama, llama2, mpt, and mistral models (#4593)
    
    * update: awq support llama-7b model
    
    * update: change order
    
    * update: benchmark results for llama2-7b
    
    * update: mistral 7b v1 benchmark
    
    * update: support 4 models
    
    * fix: Readme
    
    * update: ready for PR
    
    * update: readme
    
    * fix: readme
    
    * update: change order import
    
    * black
    
    * format code
    
    * update: work for bot mpt and awqmpt
    
    * update: readme
    
    * Rename to llm_build_ffn_mpt_awq
    
    * Formatted other files
    
    * Fixed params count
    
    * fix: remove code
    
    * update: more detail for mpt
    
    * fix: readme
    
    * fix: readme
    
    * update: change folder architecture
    
    * fix: common.cpp
    
    * fix: readme
    
    * fix: remove ggml_repeat
    
    * update: cicd
    
    * update: cicd
    
    * uppdate: remove use_awq arg
    
    * update: readme
    
    * llama : adapt plamo to new ffn
    
    ggml-ci
    
    ---------
    
    Co-authored-by: Trần Đức Nam <v.namtd12@vinai.io>
    Co-authored-by: Le Hoang Anh <v.anhlh33@vinai.io>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 879b690a9e1eb1ab0a29b58236fc76978fb4d902
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed Dec 27 15:16:55 2023 +0100

    finetune : fix output formatting in print_params (#4653)
    
    This commit fixes the output formatting in the print_params function
    which currently looks like this:
    ```console
    print_params: n_vocab:   32000
    print_params: n_ctx:     128
    print_params: n_embd:    4096
    print_params: n_ff:      11008
    print_params: n_head:    32
    print_params: n_head_kv: 32
    print_params: n_layer:   32
    print_params: norm_rms_eps          : 0.000010
    print_params: rope_freq_base        : 10000.000000
    print_params: rope_freq_scale       : 1.000000
    ```
    With this comit the output will look like this:
    ```console
    print_params: n_vocab               : 32000
    print_params: n_ctx                 : 128
    print_params: n_embd                : 4096
    print_params: n_ff                  : 11008
    print_params: n_head                : 32
    print_params: n_head_kv             : 32
    print_params: n_layer               : 32
    print_params: norm_rms_eps          : 0.000010
    print_params: rope_freq_base        : 10000.000000
    print_params: rope_freq_scale       : 1.000000
    ```
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit b47879b0dda43f2d26415e88b6840295817e552a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Dec 27 11:15:31 2023 +0200

    scripts : add sync-ggml-am.sh

commit 951010fa53a0ffe81b7d2e87c4349e0d3cb3d19d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Dec 27 11:02:13 2023 +0200

    ggml : fix dot product for ARM (#4630)
    
    ggml-ci

commit f56d6077d0c37e6606ac0a4fa3169de70593acfe
Author: wonjun Jang <strutive07@gmail.com>
Date:   Wed Dec 27 17:37:25 2023 +0900

    Add byte token type when tokenizer.model is not exists (#4641)
    
    * Add byte token type to hf format
    
    * remove unused variable

commit dc68f0054cd279cddddb0cae0c9ef4f9cbaa512a
Author: slaren <slarengh@gmail.com>
Date:   Tue Dec 26 21:23:59 2023 +0100

    cuda : fix vmm pool with multi GPU (#4620)
    
    * cuda : fix vmm pool with multi GPU
    
    * hip
    
    * use recommended granularity instead of minimum
    
    * better error checking
    
    * fix mixtral
    
    * use cudaMemcpy3DPeerAsync
    
    * use cuda_pool_alloc in ggml_cuda_op_mul_mat
    
    * consolidate error checking in ggml_cuda_set_device
    
    * remove unnecessary inlines
    
    ggml-ci
    
    * style fixes
    
    * only use vmm for the main device
    
    * fix scratch buffer size, re-enable vmm pool for all devices
    
    * remove unnecessary check id != g_main_device

commit de8e496437c59e7d1cc84109e3e49a3478aee25a
Author: WillCorticesAI <150854901+WillCorticesAI@users.noreply.github.com>
Date:   Tue Dec 26 05:42:08 2023 -0500

    Update comment for AdamW implementation reference. (#4604)
    
    Co-authored-by: Will Findley <findley@gmail.com>

commit 77465dad48d7c945c367ab46b6f2ea98ae9b7b15
Author: FantasyGmm <16450052+FantasyGmm@users.noreply.github.com>
Date:   Tue Dec 26 18:38:36 2023 +0800

    Fix new CUDA10 compilation errors (#4635)

commit a206137f927daef1752753cf5e281220b449a468
Author: Paul Tsochantaris <ptsochantaris@icloud.com>
Date:   Mon Dec 25 16:09:53 2023 +0000

    Adding Emeltal reference to UI list (#4629)

commit b9f47952ffae4e0d3420905526003c23333f6c98
Author: slaren <slarengh@gmail.com>
Date:   Sun Dec 24 21:01:12 2023 +0100

    simplify bug issue template (#4623)

commit 753be377b69bda2d65a7e089f2b7f0c53ef3495e
Author: Shintarou Okada <kokuzen@gmail.com>
Date:   Sun Dec 24 22:35:49 2023 +0900

    llama : add PLaMo model (#3557)
    
    * add plamo mock
    
    * add tensor loading
    
    * plamo convert
    
    * update norm
    
    * able to compile
    
    * fix norm_rms_eps hparam
    
    * runnable
    
    * use inp_pos
    
    * seems ok
    
    * update kqv code
    
    * remove develop code
    
    * update README
    
    * shuffle attn_q.weight and attn_output.weight for broadcasting
    
    * remove plamo_llm_build_kqv and use llm_build_kqv
    
    * fix style
    
    * update
    
    * llama : remove obsolete KQ_scale
    
    * plamo : fix tensor names for correct GPU offload
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 5bf3953d7e9831ea22b0bc017ce97409b801ccf1
Author: slaren <slarengh@gmail.com>
Date:   Sun Dec 24 14:34:22 2023 +0100

    cuda : improve cuda pool efficiency using virtual memory (#4606)
    
    * cuda : improve cuda pool efficiency using virtual memory
    
    * fix mixtral
    
    * fix cmake build
    
    * check for vmm support, disable for hip
    
    ggml-ci
    
    * fix hip build
    
    * clarify granularity
    
    * move all caps to g_device_caps
    
    * refactor error checking
    
    * add cuda_pool_alloc, refactor most pool allocations
    
    ggml-ci
    
    * fix hip build
    
    * CUBLAS_TF32_TENSOR_OP_MATH is not a macro
    
    * more hip crap
    
    * llama : fix msvc warnings
    
    * ggml : fix msvc warnings
    
    * minor
    
    * minor
    
    * cuda : fallback to CPU on host buffer alloc fail
    
    * Update ggml-cuda.cu
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    * Update ggml-cuda.cu
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    * ensure allocations are always aligned
    
    * act_size -> actual_size
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

commit 708e179e8562c2604240df95a2241dea17fd808b
Author: slaren <slarengh@gmail.com>
Date:   Sat Dec 23 16:10:51 2023 +0100

    fallback to CPU buffer if host buffer alloc fails (#4610)

commit 925e5584a058afb612f9c20bc472c130f5d0f891
Author: Samuel Maynard <samwmaynard@gmail.com>
Date:   Sat Dec 23 11:35:55 2023 +0200

    ci(docker): fix tags in "Build and push docker image (tagged)" (#4603)

commit 6123979952385847d8348e295d77d6e01da8aa84
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Sat Dec 23 09:31:49 2023 +0000

    server : allow to specify custom prompt for penalty calculation (#3727)

commit b9ec82d262cb20d7f0a8a1157bfa9aace40e2625
Author: kalomaze <66376113+kalomaze@users.noreply.github.com>
Date:   Sat Dec 23 03:27:07 2023 -0600

    grammar : check the full vocab only if necessary (opt) (#4306)
    
    * Check the full vocab for grammar only if necessary
    
    * Fix missing logit restoration step (?)
    
    Does this matter, actually?
    
    * Fix whitespace / formatting
    
    * Adjust comment
    
    * Didn't mean to push test gbnf
    
    * Split sampling into the helper function (?)
    
    And also revert the changes made to the header
    
    * common : fix final newline
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit e0a4002273907b2c414b6b5442d99e08bfe2df35
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Dec 23 09:16:33 2023 +0100

    CUDA: fixed row rounding for 0 tensor splits (#4594)

commit 7082d24cec35e9ce9147535a2224dfc67ee0a78c
Author: LeonEricsson <70749762+LeonEricsson@users.noreply.github.com>
Date:   Fri Dec 22 17:05:56 2023 +0100

    lookup : add prompt lookup decoding example (#4484)
    
    * initial commit, going through initializations
    
    * main loop finished, starting to debug
    
    * BUG: generates gibberish/repeating tokens after a while
    
    * kv_cache management
    
    * Added colors to distinguish drafted tokens (--color). Updated README
    
    * lookup : fix token positions in the draft batch
    
    * lookup : use n_draft from CLI params
    
    * lookup : final touches
    
    ---------
    
    Co-authored-by: Leon Ericsson <leon.ericsson@icloud.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit ba661751322a7c201fd3bef71af077c5aebfaa2a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Dec 22 17:53:43 2023 +0200

    sync : ggml (fix im2col) (#4591)
    
    * cuda : fix im2col_f32_f16 (ggml/#658)
    
    ggml-ci
    
    * ggml-alloc : fix ggml_tallocr_is_own
    
    ---------
    
    Co-authored-by: leejet <leejet714@gmail.com>

commit a55876955b1a83464171de8d578d3ab062a7b62d
Author: FantasyGmm <16450052+FantasyGmm@users.noreply.github.com>
Date:   Fri Dec 22 23:11:12 2023 +0800

    cuda : fix jetson compile error (#4560)
    
    * fix old jetson compile error
    
    * Update Makefile
    
    * update jetson detect and cuda version detect
    
    * update cuda marco define
    
    * update makefile and cuda,fix some issue
    
    * Update README.md
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update Makefile
    
    * Update README.md
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 6724ef16573ec7ecce620be56cbbff145856b2fb
Author: Henrik Forstén <henrik.forsten@gmail.com>
Date:   Fri Dec 22 15:34:05 2023 +0200

    Fix CudaMemcpy direction (#4599)

commit 48b7ff193e64c97ab174280ba0eb8d14b47c49ba
Author: slaren <slarengh@gmail.com>
Date:   Fri Dec 22 12:12:53 2023 +0100

    llama : fix platforms without mmap (#4578)
    
    * llama : fix platforms without mmap
    
    * win32 : limit prefetch size to the file size
    
    * fix win32 error clobber, unnecessary std::string in std::runtime_error

commit 48b24b170e3b4f9dc28200306840cb07d1c123df
Author: Herman Semenov <GermanAizek@yandex.ru>
Date:   Fri Dec 22 09:26:49 2023 +0000

    ggml : add comment about backward GGML_OP_DIAG_MASK_INF (#4203)

commit 28cb35a0ecb9852adc3494aa51dde60141939d64
Author: Michael Kesper <mkesper@schokokeks.org>
Date:   Fri Dec 22 09:03:25 2023 +0100

    make : add LLAMA_HIP_UMA option (#4587)
    
    NB: LLAMA_HIP_UMA=1 (or any value) adds MK_CPPFLAG -DGGML_HIP_UMA

commit f31b98489824a86c937fa62ccf5dfd4bb0327b86
Author: rhuddleston <ryan.huddleston@percona.com>
Date:   Thu Dec 21 23:56:34 2023 -0700

    ci : tag docker image with build number (#4584)

commit 2bb98279c5a087d62949972b35cf63ff974ffe6a
Author: Deins <deinsegle@gmail.com>
Date:   Fri Dec 22 08:49:54 2023 +0200

    readme : add zig bindings (#4581)

commit 0137ef88ea9f8fd837a065700814329d24adeec3
Author: bobqianic <129547291+bobqianic@users.noreply.github.com>
Date:   Fri Dec 22 06:47:01 2023 +0000

    ggml : extend `enum ggml_log_level` with `GGML_LOG_LEVEL_DEBUG` (#4579)

commit c7e9701f86564088350209d2f9d71c96ea00527f
Author: crasm <crasm@git.vczf.us>
Date:   Fri Dec 22 01:19:36 2023 -0500

    llama : add ability to cancel model loading (#4462)
    
    * llama : Add ability to cancel model load
    
    Updated llama_progress_callback so that if it returns false, the model
    loading is aborted.
    
    * llama : Add test for model load cancellation
    
    * Fix bool return in llama_model_load, remove std::ignore use
    
    * Update llama.cpp
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Fail test if model file is missing
    
    * Revert "Fail test if model file is missing"
    
    This reverts commit 32ebd525bf7e5a87ee8a3dbaab3d92ce79fbf23d.
    
    * Add test-model-load-cancel to Makefile
    
    * Revert "Revert "Fail test if model file is missing""
    
    This reverts commit 2796953257ee5383fa7c8fe8fa8fc888c048fb0b.
    
    * Simplify .gitignore for tests, clang-tidy fixes
    
    * Label all ctest tests
    
    * ci : ctest uses -L main
    
    * Attempt at writing ctest_with_model
    
    * ci : get ci/run.sh working with test-model-load-cancel
    
    * ci : restrict .github/workflows/build.yml ctest to -L main
    
    * update requirements.txt
    
    * Disable test-model-load-cancel in make
    
    * Remove venv before creation
    
    * Restructure requirements.txt
    
    Top-level now imports the specific additional requirements for each
    python file. Using `pip install -r requirements.txt` will fail if
    versions become mismatched in the per-file requirements.
    
    * Make per-python-script requirements work alone
    
    This doesn't break the main requirements.txt.
    
    * Add comment
    
    * Add convert-persimmon-to-gguf.py to new requirements.txt scheme
    
    * Add check-requirements.sh script and GitHub workflow
    
    * Remove shellcheck installation step from workflow
    
    * Add nocleanup special arg
    
    * Fix merge
    
    see: https://github.com/ggerganov/llama.cpp/pull/4462#discussion_r1434593573
    
    * reset to upstream/master
    
    * Redo changes for cancelling model load
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

commit afefa319f1f59b002dfa0d1ef407a2c74bd9770b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Dec 21 23:20:49 2023 +0200

    ggml : change ggml_scale to take a float instead of tensor (#4573)
    
    * ggml : change ggml_scale to take a float instead of tensor
    
    * ggml : fix CPU implementation
    
    * tests : fix test-grad0
    
    ggml-ci

commit 769a7bc85eaa44e3d7eadf39abfeff7bb0b9cc2f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Dec 21 23:20:36 2023 +0200

    gguf-py : fix broken link

commit 32259b2dade6f6856739bf7ba0a4ff7b474dc760
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Dec 21 23:07:58 2023 +0200

    gguf : simplify example dependencies

commit 4a5f9d629ecfd0a53afdddbaf54a4fa02d9a9ce9
Author: Samuel Maynard <samwmaynard@gmail.com>
Date:   Thu Dec 21 22:36:26 2023 +0200

    ci : add `jlumbroso/free-disk-space` to docker workflow (#4150)
    
    * [github][workflows][docker]: removes hardcoded `ggerganov` from `ghcr` repo
    
    * [github][workflows][docker]: adds `jlumbroso/free-disk-space`

commit d232aca5a73b290e218a2e48b91023d5e994203f
Author: slaren <slarengh@gmail.com>
Date:   Thu Dec 21 21:07:46 2023 +0100

    llama : initial ggml-backend integration (#4520)
    
    * llama : initial ggml-backend integration
    
    * add ggml-metal
    
    * cuda backend can be used though ggml-backend with LLAMA_GGML_BACKEND_CUDA_TEST
    access all tensor data with ggml_backend_tensor_get/set
    
    * add ggml_backend_buffer_clear
    zero-init KV cache buffer
    
    * add ggml_backend_buffer_is_hos, used to avoid copies if possible when accesing tensor data
    
    * disable gpu backends with ngl 0
    
    * more accurate mlock
    
    * unmap offloaded part of the model
    
    * use posix_fadvise64(.., POSIX_FADV_SEQUENTIAL) to improve performance with mmap
    
    * update quantize and lora
    
    * update session copy/set to use ggml-backend
    
    ggml-ci
    
    * use posix_fadvise instead of posix_fadvise64
    
    * ggml_backend_alloc_ctx_tensors_from_buft : remove old print
    
    * llama_mmap::align_offset : use pointers instead of references for out parameters
    
    * restore progress_callback behavior
    
    * move final progress_callback call to load_all_data
    
    * cuda : fix fprintf format string (minor)
    
    * do not offload scales
    
    * llama_mmap : avoid unmapping the same fragments again in the destructor
    
    * remove unnecessary unmap
    
    * metal : add default log function that prints to stderr, cleanup code
    
    ggml-ci
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 31f27758faf4a4bd08101a57c7ec3a473f771f86
Author: Marcus Dunn <51931484+MarcusDunn@users.noreply.github.com>
Date:   Thu Dec 21 11:57:48 2023 -0800

    llama : allow getting n_batch from llama_context in c api (#4540)
    
    * allowed getting n_batch from llama_context in c api
    
    * changed to use `uint32_t` instead of `int`
    
    * changed to use `uint32_t` instead of `int` in `llama_n_ctx`
    
    * Update llama.h
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 56fa50819f7a3ca2128f63b81c17c08a4454479e
Author: Finn Voorhees <finnvoorhees@gmail.com>
Date:   Thu Dec 21 14:55:02 2023 -0500

    metal : fix `ggml_metal_log` vargs (#4373)

commit 0f630fbc924aaabeea6eaf466bb4b47d13015c3e
Author: Erik Garrison <erik.garrison@gmail.com>
Date:   Thu Dec 21 13:45:32 2023 -0600

    cuda : ROCm AMD Unified Memory Architecture (UMA) handling (#4449)
    
    * AMD ROCm: handle UMA memory VRAM expansions
    
    This resolves #2797 by allowing ROCm AMD GPU users with a UMA to
    dynamically expand the VRAM allocated to the GPU.
    
    Without this, AMD ROCm users with shared CPU/GPU memory usually are
    stuck with the BIOS-set (or fixed) framebuffer VRAM, making it
    impossible to load more than 1-2 layers.
    
    Note that the model is duplicated in RAM because it's loaded once for
    the CPU and then copied into a second set of allocations that are
    managed by the HIP UMA system. We can fix this later.
    
    * clarify build process for ROCm on linux with cmake
    
    * avoid using deprecated ROCm hipMallocHost
    
    * keep simplifying the change required for UMA
    
    * cmake: enable UMA-compatible allocation when LLAMA_HIP_UMA=ON

commit 562cf222b5129e40b312877e928eac3a02e4ec33
Author: arlo-phoenix <140345165+arlo-phoenix@users.noreply.github.com>
Date:   Thu Dec 21 20:13:25 2023 +0100

    ggml-cuda: Fix HIP build by adding define for __trap (#4569)
    
    Regression of 139882392258671ffe5acdfcadc0bc08572d6eef
    HIP doesn't have trap, only abort

commit 8fe03ffddaaa0ab5d48feaafe398151c9f22d4f6
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Thu Dec 21 12:55:34 2023 -0500

    common : remove incorrect --model-draft default (#4568)

commit 9154494808dc865475c59022c29060b4947a803b
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Dec 21 18:42:59 2023 +0100

    CUDA: mul_mat_id always on GPU for batches >= 32 (#4553)

commit c083718c895b7c8c7fb2a4660643fb78d0c64dfd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Dec 21 19:27:14 2023 +0200

    readme : update coding guidelines

commit 880e352277fc017df4d5794f0c21c44e1eae2b84
Author: howlger <github@voormann.de>
Date:   Thu Dec 21 18:07:34 2023 +0100

    py : open merges file as 'utf-8' (#4566)
    
    Otherwise, on Windows converting bling-phi-2-v0 (<https://huggingface.co/llmware/bling-phi-2-v0>) via convert-hf-to-gguf.py will fail with the following error:
    
    ```
    Traceback (most recent call last):
      File "C:\Users\User\git\gguf\convert-hf-to-gguf.py", line 1061, in <module>
        model_instance.set_vocab()
      File "C:\Users\User\git\gguf\convert-hf-to-gguf.py", line 52, in set_vocab
        self._set_vocab_gpt2()
      File "C:\Users\User\git\gguf\convert-hf-to-gguf.py", line 264, in _set_vocab_gpt2
        special_vocab = gguf.SpecialVocab(dir_model, load_merges=True)
      File "C:\Users\User\git\gguf\gguf\vocab.py", line 33, in __init__
        self._load(Path(path))
      File "C:\Users\User\git\gguf\gguf\vocab.py", line 81, in _load
        self._try_load_merges_txt(path)
      File "C:\Users\User\git\gguf\gguf\vocab.py", line 95, in _try_load_merges_txt
        for line in fp:
      File "C:\Users\User\miniconda3\envs\gguf\lib\encodings\cp1252.py", line 23, in decode
        return codecs.charmap_decode(input,self.errors,decoding_table)[0]
    UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 1415: character maps to <undefined>
    ```

commit 66f35a2f48e1965a13835a523e677223dbf148be
Author: bobqianic <129547291+bobqianic@users.noreply.github.com>
Date:   Thu Dec 21 17:06:44 2023 +0000

    cuda : better error message for ggml_get_rows (#4561)
    
    * Update ggml-cuda.cu
    
    * Update ggml-cuda.cu
    
    * Update ggml-cuda.cu
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 139882392258671ffe5acdfcadc0bc08572d6eef
Author: slaren <slarengh@gmail.com>
Date:   Thu Dec 21 18:02:30 2023 +0100

    cuda : replace asserts in wrong architecture checks with __trap (#4556)
    
    * cuda : replace asserts in wrong architecture checks with __trap
    
    * make bad_arch noreturn, remove returns

commit d3223afdad0ed2821a8ddf739c291cd410c92a11
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Dec 21 17:34:17 2023 +0100

    llama : disable per-tensor info prints on model load (#4562)

commit 1d7a1912cea2227f9a1a449758ed622c560542f9
Author: LoganDark <github@logandark.mozmail.com>
Date:   Thu Dec 21 01:59:27 2023 -0800

    Fix access violation in ggml_cuda_free_data if tensor->extra is NULL (#4554)

commit 799fc2268989482054944c902874cca76337580f
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Dec 20 15:41:22 2023 +0100

    CUDA: Faster Mixtral prompt processing (#4538)
    
    * CUDA: make MoE tensors contiguous for batch size>1
    
    * Update ggml-cuda.cu
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 328b83de23b33240e28f4e74900d1d06726f5eb1
Author: Eric Sommerlade <es0m@users.noreply.github.com>
Date:   Tue Dec 19 16:17:01 2023 +0000

    ggml : fixed check for _MSC_VER (#4535)
    
    Co-authored-by: Eric Sommerlade <ersomme@microsoft.com>

commit a7aee47b98e45539d491071b25778b833b77e387
Author: arlo-phoenix <140345165+arlo-phoenix@users.noreply.github.com>
Date:   Mon Dec 18 22:33:45 2023 +0100

    ggml-cuda: Fix HIP build (#4528)
    
    regression of #4490
    Adds defines for two new datatypes
    cublasComputeType_t, cudaDataType_t.
    
    Currently using deprecated hipblasDatatype_t since newer ones very recent.

commit 0e18b2e7d0b5c0a509ea40098def234b8d4a938a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Dec 18 20:17:43 2023 +0200

    llama.swiftui : add tinyllama 1.1B F16

commit 6ff39b129d0281d045f83d515e51b7197b44b253
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Dec 18 20:05:12 2023 +0200

    llama.swiftui : add more models

commit b9e74f9bca5fdf7d0a22ed25e7a9626335fdfa48
Author: Ebey Abraham <ebey97@gmail.com>
Date:   Mon Dec 18 17:27:47 2023 +0000

    llama : add phi-2 + fix NeoX rope + ggml_mul_mat_set_prec (#4490)
    
    * phi2 implementation
    
    * fix breaking change
    
    * phi-2 : various fixes
    
    * phi-2 : use layer norm eps
    
    * py : whitespaces
    
    * llama : fix meta KV override bug
    
    * convert : phi don't add BOS token
    
    * convert : revert "added_tokens_decoder" change
    
    * phi-2 : scale Q instead of KQ for better precision
    
    * ggml : fix NeoX rope to rotate just first n_dims
    
    * cuda : less diff in the rope_neox kernel
    
    * ggml : add ggml_mul_mat_set_prec
    
    ggml-ci
    
    * Update ggml-cuda.cu
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Update ggml-cuda.cu
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * cuda : ggml_cuda_op_mul_mat_cublas support F32 precision
    
    * cuda : remove oboslete comment
    
    ---------
    
    Co-authored-by: Ebey Abraham <ebeyabraham@microsoft.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>

commit 3c04bf6da89eaf4c7d317e0518f0687dfcbf2de7
Author: hankcs <cnhankmc@gmail.com>
Date:   Mon Dec 18 05:14:58 2023 -0800

    llama : fix try_override for bool_value which always return true (#4519)

commit 2994f0c5a2e8c96955b422dedc93ec2595d16b82
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Sun Dec 17 19:39:02 2023 -0500

    decode : fix logits_valid for legacy API (#4516)

commit b1306c439490c7fa4ec33594500d980d1e9e15e6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Dec 17 20:16:23 2023 +0200

    readme : update hot topics

commit 800a489e4a8be199122259a995b1ee9dd7fae320
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Dec 17 19:38:41 2023 +0200

    llama.swiftui : add bench functionality (#4483)
    
    * llama.swiftui : add bench button
    
    * llama.swiftui : initial bench functionality
    
    * force to use n_gpu_layers on simulator
    
    * add download buttons & expose llamaState.loadModel
    
    * update project.pbxproj
    
    * comment #Preview & fix editorconfig check
    
    * gitignore : xcode stuff
    
    * llama.swiftui : UX improvements
    
    * llama.swiftui : avoid data copy via "downloadTask"
    
    * llama.swiftui : remove model from project
    
    * llama : remove "mostly" from model infos
    
    * llama.swiftui : improve bench
    
    ---------
    
    Co-authored-by: jhen <developer@jhen.me>

commit f7f468a97dceec2f8fe8b1ed7a2091083446ebc7
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Sun Dec 17 10:45:46 2023 -0500

    gguf-py : fail fast on nonsensical special token IDs (#4489)

commit 919c40660fd27157b391b5832d2a577d5afef4cb
Author: Matheus Gabriel Alves Silva <matheusgasource@gmail.com>
Date:   Sun Dec 17 12:23:33 2023 -0300

    build : Check the ROCm installation location (#4485)
    
    * build : Check the ROCm installation location
    
    * more generic approach
    
    * fixup! It was returning the path instead of the command output
    
    * fixup! Trailing whitespace

commit 45668633fdb522a925c3dafc1ecf426f539efb27
Author: slaren <slarengh@gmail.com>
Date:   Sun Dec 17 16:05:56 2023 +0100

    finetune : keep allocs alive until all allocations are done (#4486)

commit 0ffc92d2d23a789625f018840469af045be1e3c0
Author: olexiyb <olexiyb@gmail.com>
Date:   Sun Dec 17 17:02:16 2023 +0200

    server : disable llm logs if SERVER_VERBOSE is off (#3792)

commit 8edd2b40fdbcafbf630f2cf29306b29d5cb48c42
Author: AdithyanI <adithyan.i4internet@gmail.com>
Date:   Sun Dec 17 15:57:56 2023 +0100

    server : fix grammar being ignored (#4494)
    
    Fix bug in identifying the grammar.

commit eb16dae7e70ca97396190698b29c0f9ee3388e88
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Sun Dec 17 14:56:09 2023 +0000

    server : fix possible ambiguity in content type charset (#4501)

commit 62bd52b7bf90819e75f427a95a484cd5eee0b3c7
Author: mzcu <milos.cubrilo@gmail.com>
Date:   Sun Dec 17 15:54:37 2023 +0100

    server : allow requests larger than 8K (#4500)

commit 5daa5f54fdcd2b5228add1a4c43a1897b2168f35
Author: Bach Le <bach@bullno1.com>
Date:   Sun Dec 17 18:57:33 2023 +0800

    Link to cublas dynamically on Windows even with LLAMA_STATIC (#4506)

commit c6c4fc081c1df1c60a9bfe3e6a3fd086f1a29ec7
Author: slaren <slarengh@gmail.com>
Date:   Sat Dec 16 18:58:46 2023 +0100

    lora : add support for non-llama models (#3333)
    
    * lora : add support for non-llama models
    
    ggml-ci
    
    * avoid leaking ggml_context on failure
    cleanup
    
    ggml-ci
    
    * lora : allow 1d tensors
    
    * lora : include embd and output layers in size calculation
    
    * fix style

commit 8a5be3bd5885d79ad84aadf32bb8c1a67bd43c19
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Fri Dec 15 22:16:15 2023 -0500

    llama : sanity checks for access to logits (#4274)
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 88ae8952b65cbf32eb1f5703681ea592e510e570
Author: ShadovvBeast <ShadovvBeast@gmail.com>
Date:   Fri Dec 15 13:49:01 2023 +0200

    server : add optional API Key Authentication example (#4441)
    
    * Add API key authentication for enhanced server-client security
    
    * server : to snake_case
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit ee4725a686643669a8587142fa068cbf29de3ce2
Author: slaren <slarengh@gmail.com>
Date:   Fri Dec 15 12:45:50 2023 +0100

    ggml : group mul_mat_id rows by matrix (cpu only) (#4480)
    
    * ggml : group mul_mat_id rows by matrix (cpu only)
    
    * remove mmid parameters from mm forward
    
    * store row groups in wdata and calculate only once in GGML_TASK_INIT
    
    ggml-ci

commit 6744dbe924a317e3e2a5a2a4a2037061b2223449
Author: slaren <slarengh@gmail.com>
Date:   Thu Dec 14 20:05:21 2023 +0100

    ggml : use ggml_row_size where possible (#4472)
    
    * ggml : use ggml_row_size where possible
    
    ggml-ci
    
    * ggml : move ggml_nbytes_split to ggml-cuda.cu

commit cafcd4f89500b8afef722cdb08088eceb8a22572
Author: slaren <slarengh@gmail.com>
Date:   Thu Dec 14 16:52:08 2023 +0100

    ggml : remove n_dims from ggml_tensor (#4469)
    
    ggml-ci

commit c50e40016394f124b97ce39da48148b1f6c01833
Author: wonjun Jang <strutive07@gmail.com>
Date:   Thu Dec 14 21:44:49 2023 +0900

    py : add protobuf dependency (#4466)

commit 20a68a7030ee06e8eb7eb8e24ae4ac52dc17803f
Author: LostRuins <39025047+LostRuins@users.noreply.github.com>
Date:   Thu Dec 14 20:13:33 2023 +0800

    ggml : add ggml_row_size() (fixes llama out of space) (#4461)
    
    * Fixes "Not enough space in the context's memory pool" encountered on certain models, which seems to be caused by some imprecision related to the automatic casting of floating point values
    
    * do not cast to size_t, instead just use doubles
    
    * ggml : add ggml_row_size(), deprecate ggml_type_sizef()
    
    * ggml : fix row size compute to avoid overflows
    
    * tests : fix sizey -> sizez
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 55e87c3749cb4985c3b316984d40e00e4df4a5d0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Dec 14 10:35:29 2023 +0200

    ggml : fix OpenCL broadcast requirement for ggml_mul (close #4453)

commit 873637afc7924f435ac44c067630a28e82eefa7b
Author: wonjun Jang <strutive07@gmail.com>
Date:   Thu Dec 14 17:09:34 2023 +0900

    convert : support loading vocab from fast tokenizer config (#3633)
    
    * Add HFVocab into convert.py
    
    * Update convert.py
    
    * Update convert.py
    
    * add bytes_to_unicode function
    
    * change add_meta_vocab fucntion
    
    * remove debug code
    
    * remove byte_encoder
    
    * Add newline between classes
    
    * Check tokenizer.json when tokenizer.model is not exist.
    
    * Move transformers dependency to local code
    
    * Add error context with 'raise from'
    
    * Add fast tokenizer option to BpeVocab
    
    * Update convert.py
    
    * Add VocabLoader and remove *Vocab class
    
    * Add transformers dependency
    
    * remove added tokens and check newline token to decide spm or bpe
    
    * Update convert.py
    
    * Add special token type
    
    * Update convert.py
    
    * Update convert.py
    
    * Update convert.py
    
    * Fix typo in convert.py
    
    * Fix when params.n_vocab < tokenizer vocab size
    
    * update vocab class
    
    * change funtion name
    
    * Remove unused variable/functions, add types to class variable and methods, delete blank liens
    
    * fix flake8 warnings
    
    * code style cleanup
    
    * make mypy happy
    
    * change exception
    
    ---------
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>

commit 0353a1840134b24b07ab61fd4490192f28c4db6b
Author: BarfingLemurs <128182951+BarfingLemurs@users.noreply.github.com>
Date:   Thu Dec 14 02:38:49 2023 -0500

    readme : update supported model list (#4457)

commit 948ff137ec37f1ec74c02905917fa0afc9b97514
Author: shibe2 <shibe@tuta.io>
Date:   Wed Dec 13 23:57:15 2023 +0400

    server : fix handling of characters that span multiple tokens when streaming (#4446)

commit 4d98d9a65665eee3838cef936641f640e3f5b649
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Dec 13 21:54:54 2023 +0200

    sync : ggml (SD ops, tests, kernels) (#4444)
    
    * sync : ggml (SD ops, tests, kernels)
    
    ggml-ci
    
    * cuda : restore im2col
    
    ggml-ci
    
    * metal : fix accuracy of dequantization kernels
    
    ggml-ci
    
    * cuda : restore correct im2col
    
    ggml-ci
    
    * metal : try to fix moe test by reducing expert size
    
    ggml-ci
    
    * cuda : fix bin bcast when src1 and dst have different types
    
    ggml-ci
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 70f806b821f603cafb6f634c93a6729dc21bb354
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Wed Dec 13 12:10:10 2023 -0500

    build : detect host compiler and cuda compiler separately (#4414)

commit 9fb13f95840c722ad419f390dc8a9c86080a3700
Author: Siwen Yu <yusiwen@gmail.com>
Date:   Wed Dec 13 20:50:14 2023 +0800

    common : add `--version` option to show build info in CLI (#4433)

commit 113f9942fc73a262c85e9dcf7c2ea7336250bba0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Dec 13 14:05:38 2023 +0200

    readme : update hot topics

commit 799a1cb13b0b1b560ab0ceff485caed68faa8f1f
Author: slaren <slarengh@gmail.com>
Date:   Wed Dec 13 13:04:25 2023 +0100

    llama : add Mixtral support (#4406)
    
    * convert : support Mixtral as LLAMA arch
    
    * convert : fix n_ff typo
    
    * llama : model loading
    
    * ggml : sync latest ggml_mul_mat_id
    
    * llama : update graph to support MoE
    
    * llama : fix cur -> cur_expert
    
    * llama : first working version
    
    * llama : fix expert weighting in the FFN
    
    * ggml : ggml_get_rows support 2D indexing [n_tokens, n_experts] (cpu only)
    
    * ggml : add n_as argument to ggml_mul_mat_id
    
    * ggml : fix ggml_get_rows to take into account ne02 / ne11
    
    * metal : add more general support for ggml_get_rows + tests
    
    * llama : add basic support for offloading moe with CUDA
    
    * metal : add/mul/div use general kernel when src1 not cont
    
    * metal : reduce the kernel launches for ggml_mul_mat_id
    
    * ggml : get_rows : support non-contiguos tensors with gaps, generalize up to 3D
    
    * ggml : update get_rows f16 and q
    
    * cuda : support non-contiguous src1 in get_rows
    
    * llama : offload missing ffn_moe_silu
    
    * metal : fix ggml_get_rows to work with non-cont src1
    
    * metal : add indirect mat-vec kernels for all quantization types
    
    * llama : do not quantize expert gating tensors
    
    * llama : add n_expert and n_expert_used to hparams + change quants
    
    * test-backend-ops : add moe test
    
    * cuda : fix get_rows when ncols is odd
    
    * convert : determine n_ctx correctly
    
    * metal : fix ggml_mul_mat_id for F32
    
    * test-backend-ops : make experts more evenly probable (test_moe)
    
    * test-backend-ops : cleanup, add moe test for batches
    
    * test-backend-ops : add cpy from f32 -> all types test
    
    * test-backend-ops : fix dequantize block offset
    
    * llama : fix hard-coded number of experts
    
    * test-backend-ops : simplify and disable slow tests to avoid CI timeout
    
    * test-backend-ops : disable MOE test with thread sanitizer
    
    * cuda : fix mul_mat_id with multi gpu
    
    * convert : use 1e6 rope_freq_base for mixtral
    
    * convert : fix style
    
    * convert : support safetensors format
    
    * gguf-py : bump version
    
    * metal : add cpy f16 -> f32 kernel
    
    * metal : fix binary ops for ne10 % 4 != 0
    
    * test-backend-ops : add one more sum_rows test
    
    * ggml : do not use BLAS with ggml_mul_mat_id
    
    * convert-hf : support for mixtral-instruct (#4428)
    
    * convert : typo fix, add additional hyperparameters, use LLaMA arch for Mixtral-instruct
    
    * convert : use sentencepiece tokenizer for Mixtral-instruct
    
    * convert : make flake8 happy
    
    * metal : fix soft_max kernels
    
    ref: https://github.com/ggerganov/ggml/pull/621/commits/1914017863d2f9ab8ecc0281cc2a56d683668b92
    
    * metal : limit kernels to not use more than the allowed threads
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: Radek Pilar <github@mrkva.eu>

commit fecac45658a99eddc4d6e36ba0310ca8f87a77f0
Author: kalomaze <66376113+kalomaze@users.noreply.github.com>
Date:   Tue Dec 12 04:12:35 2023 -0600

    server : tweak default sampling parameters (#4367)
    
    * Set a more typical Top P setting as the default
    
    * Update temp max

commit 9494d7c4774ab745490b5a19570ff7747a194143
Author: Richard Kiss <him@richardkiss.com>
Date:   Tue Dec 12 01:53:36 2023 -0800

    english : use `typos` to fix comments and logs (#4354)

commit 6138963fb232cbae70c9d181db0ba125708f473d
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Tue Dec 12 04:27:26 2023 -0500

    build : target Windows 8 for standard mingw-w64 (#4405)
    
    * build : target Windows 8 for standard mingw-w64
    
    * make : fix missing console.o deps
    
    This was causing a link error with `make all` on Windows.

commit 6391817cd19a4507c6c941a1fd08756268662b2d
Author: crasm <crasm@git.vczf.us>
Date:   Tue Dec 12 04:25:57 2023 -0500

    llama : document logits_all deprecation (#4418)
    
    llama_context_params.logits_all is a parameter for controlling
    llama_eval. This documents that logits_all should not be used with
    llama_decode and llama_batch.

commit d9d4cfef64ea416dd66632173787d03ffb180cc7
Author: Vladimir Zorin <vladimir@deviant.guru>
Date:   Tue Dec 12 11:25:29 2023 +0200

    server : fix local model name in server (#4420)

commit 41a11aaf99feff4901e4c8dc48ad00766c5da4e9
Author: Taikono-Himazin <kazu@po.harenet.ne.jp>
Date:   Tue Dec 12 18:24:32 2023 +0900

    ggml : increased GGML_MAX_PARAMS to allow finetuning of 70b models (#4424)

commit 8a7b2fa528f130631a5f43648481596ab320ed5a
Author: Yueh-Po Peng <94939112+y10ab1@users.noreply.github.com>
Date:   Mon Dec 11 06:27:38 2023 +0800

    Update README.md (#4388)
    
    Fix small typo.

commit e18f7345a300920e234f732077bda660cc6cda9c
Author: Xiang (Kevin) Li <kevinli020508@gmail.com>
Date:   Sat Dec 9 16:29:27 2023 -0500

    grammar : revert the replacement of llama_token_to_piece with id_to_token (#4396)

commit fe680e3d1080a765e5d3150ffd7bab189742898d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Dec 7 22:26:54 2023 +0200

    sync : ggml (new ops, tests, backend, etc.) (#4359)
    
    * sync : ggml (part 1)
    
    * sync : ggml (part 2, CUDA)
    
    * sync : ggml (part 3, Metal)
    
    * ggml : build fixes
    
    ggml-ci
    
    * cuda : restore lost changes
    
    * cuda : restore lost changes (StableLM rope)
    
    * cmake : enable separable compilation for CUDA
    
    ggml-ci
    
    * ggml-cuda : remove device side dequantize
    
    * Revert "cmake : enable separable compilation for CUDA"
    
    This reverts commit 09e35d04b1c4ca67f9685690160b35bc885a89ac.
    
    * cuda : remove assert for rope
    
    * tests : add test-backend-ops
    
    * ggml : fix bug in ggml_concat
    
    * ggml : restore `ggml_get_n_tasks()` logic in `ggml_graph_plan()`
    
    * ci : try to fix macOS
    
    * ggml-backend : remove backend self-registration
    
    * ci : disable Metal for macOS cmake build
    
    ggml-ci
    
    * metal : fix "supports family" call
    
    * metal : fix assert
    
    * metal : print resource path
    
    ggml-ci
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit bcc0eb4591bec5ec02fad3f2bdcb1b265052ea56
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Dec 7 13:03:17 2023 +0200

    llama : per-layer KV cache + quantum K cache (#4309)
    
    * per-layer KV
    
    * remove unnecessary copies
    
    * less code duplication, offload k and v separately
    
    * llama : offload KV cache per-layer
    
    * llama : offload K shift tensors
    
    * llama : offload for rest of the model arches
    
    * llama : enable offload debug temporarily
    
    * llama : keep the KV related layers on the device
    
    * llama : remove mirrors, perform Device -> Host when partial offload
    
    * common : add command-line arg to disable KV cache offloading
    
    * llama : update session save/load
    
    * llama : support quantum K cache (#4312)
    
    * llama : support quantum K cache (wip)
    
    * metal : add F32 -> Q8_0 copy kernel
    
    * cuda : add F32 -> Q8_0 copy kernel
    
    ggml-ci
    
    * cuda : use mmv kernel for quantum cache ops
    
    * llama : pass KV cache type through API
    
    * llama : fix build
    
    ggml-ci
    
    * metal : add F32 -> Q4_0 copy kernel
    
    * metal : add F32 -> Q4_1 copy kernel
    
    * cuda : wip
    
    * cuda : add F32 -> Q4_0 and F32 -> Q4_1 copy kernels
    
    * llama-bench : support type_k/type_v
    
    * metal : use mm kernel only for quantum KV cache
    
    * cuda : add comment
    
    * llama : remove memory_f16 and kv_f16 flags
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * readme : add API change notice
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 81bc9214a389362010f7a57f4cbc30e5f83a2d28
Author: Hongyu Ouyang <96765450+casavaca@users.noreply.github.com>
Date:   Thu Dec 7 02:25:22 2023 -0800

    train : fix #4227 (double free in examples/train-text-from-scratch/train-text-from-scratch.cpp) (#4351)
    
    On commit b1108 (44c117f4) xaedes added
    
        ggml_allocr * alloc = NULL;
    
        ... (many lines in between)
    
        if (alloc) {
            ggml_allocr_free(alloc);
        }
    
    Which is correct, but it's easy to lose context after many lines in between.
    
    On commit b1287 (0e76a899) xaedes made a big change. From here on, alloc is freed eagerly.
    
        alloc = ggml_allocr_new(...)
        ... (short lines of code)
        ggml_allocr_free(alloc)
    
    This happens a few times, but alloc is never set to NULL, and many lines below,
    we still have
    
        if (alloc) {
            ggml_allocr_free(alloc);
        }
    
    which causes a double-free.

commit 05cd6e5036d72d0930de4d8f6be7bce09e8dda24
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Dec 6 20:21:59 2023 +0200

    server : recognize cache_prompt parameter in OAI API (#4347)

commit caa9249217c5fd524b900add5ddcbeaa20cbcb12
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Dec 6 10:41:03 2023 +0200

    common : fix compile warning

commit da5eaef1f34d0a1f584cd4a092e7691ea46a9d91
Author: stduhpf <stephduh@live.fr>
Date:   Wed Dec 6 09:08:17 2023 +0100

    speculative : support `--color` (#4343)
    
    * speculative: add some colors
    
    * minor : add braces
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 5f6e0c0dff1e7a89331e6b25eca9a9fd71324069
Author: Marcus Dunn <51931484+MarcusDunn@users.noreply.github.com>
Date:   Tue Dec 5 10:55:12 2023 -1000

    grammar : pre-computed pieces + reserve mem + less string copies (#4330)
    
    * reserve space for codepoints
    
    * improvement for the appended 0
    
    * used precomputed token text for grammar sample
    
    * reserve canidates_decoded
    
    * reserve canidates_grammar
    
    * remove candidates_decoded
    
    * Revert "remove candidates_decoded"
    
    This reverts commit 3773328080e6a139ee83198329a13cf4ff61d707.
    
    * changed decode_utf8 to take src by ref

commit 5aa365d88fdb8fdd430ef3fc141c7a5fd37c3502
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Tue Dec 5 10:19:18 2023 -0700

    llama : allow overriding GGUF metadata when loading model (#4092)
    
    * feat: Allow overriding GGUF metadata when loading model
    
    * Fix the one time GCC is stricter than clang about something
    
    * Step1
    
    * Refactor... basically everything!
    
    * Nuke obsolete GetArrayLen struct
    
    * simplify std::string specialization
    
    * Various cleanups
    
    Add informational output when overrides are applied
    
    Warn user when an override with the wrong type is specified
    
    * Fix broken logic for parsing bool KV overrides
    Fix issue where overrides didn't apply when key missing in GGUF metadata
    Resolve merge changes
    
    * llama : rearrange model params
    
    * Update new GET_KEY call
    
    Add note that metadata KV overrides aren't reflected in initial metadata KV info dump
    
    ---------
    
    Co-authored-by: cebtenzzre <cebtenzzre@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 52c8bc3cf312e1caf02d37bfb9d9d865cbe33594
Author: MaggotHATE <clay1326@gmail.com>
Date:   Tue Dec 5 15:05:51 2023 +0500

    sampling : custom samplers order (#4285)
    
    * Samplers sequence order w parameter
    
    * Cleaned commented code
    
    * Fixed formatting
    
    * Rewrote with unordered_map
    
    * Revert and rewrite, too many problems and safeguards would be needed
    
    * Fixed code style
    
    * Code style fixes according to review
    
    * More readable samplers input string, fixed help
    
    * Style fix in sampler_queue
    
    * Formatting fixes
    
    * Fixing whitespaces

commit e4b76bbe316ee50fb17d9ac29e654c0edf830eba
Author: kchro3 <62481661+kchro3@users.noreply.github.com>
Date:   Mon Dec 4 23:29:46 2023 -0800

    swift : revert compiler checks for swift package (#4332)

commit 23b5e12eb5a76489b4c3ee22213a081da68b1809
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Mon Dec 4 17:04:21 2023 +0100

    simple : update error message for KV cache check (#4324)
    
    This commit updates the error message that is printed when the
    KV cache is not big enough to hold all the prompt and generated
    tokens. Specifically it removes the reference to n_parallel and
    replaces it with n_len.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit d208995c6da66f252d4054c1c5a90eb8ccb7a2f7
Author: Miwa / Ensan <63481257+ensan-hcl@users.noreply.github.com>
Date:   Tue Dec 5 01:03:49 2023 +0900

    swift : fix concatenation method to avoid invalid UTF8 stringfication (#4325)

commit 5c9f90cba1cc6b0a2a7d19ee5dcb73cad6331d30
Author: Miwa / Ensan <63481257+ensan-hcl@users.noreply.github.com>
Date:   Mon Dec 4 22:43:45 2023 +0900

    swift : fix prompt tokenization logic (#4321)

commit 4fa44e84adb4c78e1885694cc3513982d4af2b08
Author: Ikko Eltociear Ashimine <eltociear@gmail.com>
Date:   Mon Dec 4 16:57:35 2023 +0900

    grammar-parser : fix typo (#4318)
    
    preceeding -> preceding

commit fbbc42827b2949b95bcde23ce47bb47d006c895d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Dec 3 15:56:35 2023 +0200

    ggml : reuse ggml_get_n_tasks() in ggml_graph_plan() (#4308)
    
    * ggml : fix soft max out-of-bounds access
    
    ggml-ci
    
    * ggml : reuse ggml_get_n_tasks() in ggml_graph_plan()
    
    ggml-ci

commit adf3de4f69ff7e44131222f05f9c7447ac0be3cb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Dec 3 15:56:22 2023 +0200

    ggml : fix soft max out-of-bounds access (#4307)
    
    ggml-ci

commit 33e171d1e9fc4903f9314b490d77fb8d58331b63
Author: Ed Lee <edilee@mozilla.com>
Date:   Sun Dec 3 01:10:43 2023 -0800

    server : fix OpenAI API `stop` field to be optional (#4299)
    
    (cherry picked from commit Mozilla-Ocho/llamafile@e8c92bcb84ae3bcbf0d617b7ee6a5413bcbd58af)

commit 6949b50df56ee58a2d76d45487942cb211c08629
Author: Rickard Edén <rickardeden@gmail.com>
Date:   Sun Dec 3 10:03:25 2023 +0100

    py : add grammar to oai like api (#4294)

commit d7b800b8bc490a221acbd83c575206a907f2f6e2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Dec 3 10:58:16 2023 +0200

    llama : pad KV cache size (#4280)
    
    * llama : pad KV cache size to 32
    
    * metal : try to improve batched decoding

commit 5a7d3125e7c24f223659b7f0b7aa7736986e92c0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Dec 1 20:39:12 2023 +0200

    llama : avoid using "optional" keyword (#4283)

commit d5a1cbde60531d02ac74da27ea355182e3a4d516
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Dec 1 20:35:03 2023 +0200

    llama : support optional tensors (#4283)

commit b220222a64ce760bfbec9c770f11db3ec6a6abb6
Author: Miwa / Ensan <63481257+ensan-hcl@users.noreply.github.com>
Date:   Sat Dec 2 03:19:45 2023 +0900

    swift : fix token_to_piece implementation (#4278)
    
    * Fix token_to_piece implementation in Swift
    
    * Fix errors

commit 511f52c334e37033f9c9de07b98fca4abc9470bd
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Fri Dec 1 13:18:35 2023 -0500

    build : enable libstdc++ assertions for debug builds (#4275)

commit 03562f3a86d6706eea9f4fc09b532946c191b34e
Author: CausalLM <148736309+CausalLM@users.noreply.github.com>
Date:   Sat Dec 2 02:17:06 2023 +0800

    llama : support attention bias on LLaMA architecture (#4283)
    
    * Support attention_bias on LLaMA architecture
    
    QKVO bias, should fix InternLM (https://github.com/ggerganov/llama.cpp/issues/3133) and works for LLaMAfied Qwen models (https://github.com/ggerganov/llama.cpp/pull/3743#issuecomment-1825923608).
    
    * check existence of qkvo bias while loading llama models
    
    Tested on LLaMA2, CUDA and CPU.
    
    * Update llama.cpp

commit 37c746d687d877bc11803e96b4dc5f378b83c0a0
Author: Shijie <821898965@qq.com>
Date:   Sat Dec 2 02:16:31 2023 +0800

    llama : add Qwen support (#4281)
    
    * enable qwen to llama.cpp
    
    * llama : do not GPU split bias tensors
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 880f57973b8e0091d0f9f50eb5ab4cd4e31582ca
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Dec 1 18:42:11 2023 +0200

    llama : fix integer overflow during quantization (#4284)
    
    happens with multi-threaded quantization of Qwen-72B
    
    ggml-ci

commit 8d6d9f033b8101f929e445cf45b39e1557ca7934
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Dec 1 10:41:56 2023 +0100

    py : add requirements file for convert-hf-to-gguf.py (#4277)
    
    This commit adds a requirements file for the convert-hf-to-gguf.py
    script, and also add the torch and transformers packages to it.
    
    The motivation for this is that currently running convert-hf-to-gguf.py
    will produce the following error:
    ```console
    $ python3 -m venv venv
    $ source venv/bin/activate
    (venv) $ pip install -r requirements.txt
    Collecting numpy==1.24.4
    Collecting sentencepiece==0.1.98
    Collecting gguf>=0.1.0
    Installing collected packages: sentencepiece, numpy, gguf
    Successfully installed gguf-0.5.1 numpy-1.24.4 sentencepiece-0.1.98
    
    (venv) $ python convert-hf-to-gguf.py --help
    Traceback (most recent call last):
      File "llama.cpp/convert-hf-to-gguf.py", line 16, in <module>
        import torch
    ModuleNotFoundError: No module named 'torch'
    ```
    With this commit, and using requirements-hf-to-gguf.txt instead of
    requirements.txt, the script can be run and shows the help output.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit ef47ec18da469423c276b683dd9b5741cee7023e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Dec 1 10:51:24 2023 +0200

    ggml : add ggml_soft_max_ext (#4256)
    
    * metal : implement soft_max_ext
    
    * cuda : implement soft_max_ext
    
    * ggml : implement soft_max_ext (CPU)
    
    * batched-bench : print threads
    
    ggml-ci
    
    * metal : simplify soft_max encoding
    
    ggml-ci
    
    * cuda : use 512 threads for soft_max instead of 32
    
    * ggml : update soft max cpu
    
    * cuda : do warp-based block reduce
    
    * cuda : increase max block size to 1024
    
    * cuda : fix warp reduction initialization of shared mem
    
    * metal : warp-based reduction for soft max kernel
    
    * metal : warp-based reduce for rms_norm
    
    * metal : simplify soft max kernel
    
    ggml-ci
    
    * alloc : fix build with debug

commit 1d144112c0fbbb4ecc07dbcf4f05a380148bd6de
Author: Ziad Ben Hadj-Alouane <zied.benhadjalouane@gmail.com>
Date:   Thu Nov 30 17:25:49 2023 -0500

    server : add --log-disable to disable logging to file (#4260)
    
    * * add --log-disable to disable logging to file in the server example
    
    * * typo fix

commit f43f09366dfd018e4568e23a232aaa8c4f7cfc78
Author: Ziad Ben Hadj-Alouane <zied.benhadjalouane@gmail.com>
Date:   Thu Nov 30 17:25:04 2023 -0500

    server : add single-client multi-prompt support (#4232)
    
    * * add multiprompt support
    
    * * cleanup
    
    * * more cleanup
    
    * * remove atomicity of id_gen, and change lock_guard to unique_lock on completion requests
    
    * * remove all references to mutex_multitasks
    
    * Update examples/server/server.cpp
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Update examples/server/server.cpp
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Update examples/server/server.cpp
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Update examples/server/server.cpp
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * * change to set
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

commit d2809a3ba2780e00fce5a6149a7eda09f1c0e906
Author: WillCorticesAI <150854901+WillCorticesAI@users.noreply.github.com>
Date:   Thu Nov 30 17:23:44 2023 -0500

    make : fix Apple clang determination bug (#4272)
    
    Co-authored-by: Will Findley <findley@gmail.com>

commit 15f5d96037e597523b721aa39c874d69de2acf85
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Thu Nov 30 17:23:08 2023 -0500

    build : fix build info generation and cleanup Makefile (#3920)
    
    * cmake : fix joining of REAL_GIT_DIR
    
    * fix includes with help from include-what-you-use
    
    * make : remove unneeded deps and add test-rope target
    
    * fix C includes in C++ source files
    
    * Revert "fix includes with help from include-what-you-use"
    
    This reverts commit 635e9fadfd516d4604a0fecf4a854bfb25ad17ae.

commit 33c9892af58b7b161f2a532935dcccff8c8048c6
Author: John <78893154+cmp-nct@users.noreply.github.com>
Date:   Thu Nov 30 23:11:14 2023 +0100

    llava : ShareGPT4V compatibility (vision encoder only loading) (#4172)
    
    * ShareGPT4 compatibility (vision encoder only loading)
    
    Load only a CLIP vision encoder (as supplied by ShareGPT finetunes)
    Corrects the argument parsing for --img_mean and --img_std (which were previously not parsed but attempted to access)
    Defines defaults for img_mean and img_std which are equal to the llava 1.5 CLIP encoder, so you do not have to provide them
    
    * Update convert-image-encoder-to-gguf.py

commit 8efa0f6ebed53c9453e6721da86fb294e5015909
Author: Andrew Godfrey <AndrewGodfrey@users.noreply.github.com>
Date:   Thu Nov 30 13:56:19 2023 -0800

    main : pass LOG_TEE callback to llama.cpp log (#4033)
    
    * main : Call llama_log_set to use LOG_TEE
    
    * tabs to spaces

commit 524907aa768a26cbf83d8e2eb30547e2ee1d1b1a
Author: vodkaslime <646329483@qq.com>
Date:   Fri Dec 1 05:49:21 2023 +0800

    readme : fix (#4135)
    
    * fix: readme
    
    * chore: resolve comments
    
    * chore: resolve comments

commit 3bd2c7ce1b752973cf937482a0333e85d1681e2b
Author: Juraj Bednar <juraj@bednar.io>
Date:   Thu Nov 30 22:46:01 2023 +0100

    docker : add finetune option (#4211)

commit bde629bb53b85886ee0fe83524c1efe2689bc618
Author: Miwa / Ensan <63481257+ensan-hcl@users.noreply.github.com>
Date:   Fri Dec 1 06:45:17 2023 +0900

    batched.swift : update README.md (#4214)
    
    docs: update how to run

commit f7f9e06212d44530b3200033286049dbdf84b3d3
Author: Li Tan <tanliboy@gmail.com>
Date:   Thu Nov 30 13:44:11 2023 -0800

    cmake : fix the metal file foder path (#4217)

commit 74daabae6927b99e7333d6126dee35193c418457
Author: Dawid Wysocki <62249621+TortillaZHawaii@users.noreply.github.com>
Date:   Thu Nov 30 22:43:32 2023 +0100

    readme : fix typo (#4253)
    
    llama.cpp uses GitHub Actions, not Gitlab Actions.

commit b18c66ca6eee4fe0465cff5042daf05005dc9ab2
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Nov 30 22:43:08 2023 +0100

    llama : fix alignment of general.name in print meta (#4254)
    
    * llama: fix alignment of general.name in print meta
    
    This commit fixes the alignment of the general.name field in the
    llm_load_print_meta function.
    
    Currently the output looks like this:
    ```console
    llm_load_print_meta: model ftype      = mostly Q4_0
    llm_load_print_meta: model params     = 13.02 B
    llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW)
    llm_load_print_meta: general.name   = LLaMA v2
    ```
    And with this commit it looks like this:
    ```console
    llm_load_print_meta: model ftype      = mostly Q4_0
    llm_load_print_meta: model params     = 13.02 B
    llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW)
    llm_load_print_meta: general.name     = LLaMA v2
    ```
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * llama: fix alignment of special tokens
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit f4d973cecb7368c985720ba9100ae6abba14806d
Author: slaren <slarengh@gmail.com>
Date:   Thu Nov 30 22:42:23 2023 +0100

    convert.py : fix llama/llama2 conversion due to vocab_size=-1 (#4258)

commit 954e22858c5cea1dc03e9172d3879402af2b5990
Author: tarcey <cey.tarik@gmail.com>
Date:   Thu Nov 30 22:40:23 2023 +0100

    llama : fix typical sampling (#4261)
    
    Typical sampling was broken because after copying new_candidates into canditates, the "sorted" bool is left at "true", but the new data is no longer sorted according to probability. Patch to set "sorted" to false.
    
    Test: Generating with temp=0.0001 (approx. argmax)  should generate the same sequence at typical>=1.0 and typical=0.9999 (approx. disabled, but enters the typical sampling codepath).

commit e2bd725f4b39bc5c6234858d158e01248f5ab5bd
Author: rhjdvsgsgks <26178113+rhjdvsgsgks@users.noreply.github.com>
Date:   Thu Nov 30 20:50:40 2023 +0000

    py : fix oai proxy (#3972)
    
    * fix oai proxy
    
    fix generation not stoped while bot stop talking in chat mode
    
    fix possible `slot_id` not exist
    
    response for cors (and pre flight)
    
    * oai proxy: workaround for some client (such as Chatbox)
    
    * use stop as separator to replace hardcoded `\n`

commit 1f5cd83275fabb43f2ae92c30033b384a3eb37b4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 29 11:00:17 2023 +0200

    examples : add readme files

commit 4fea3420ee3918d125d74c94d962a6ea82875351
Author: Peter Sugihara <peter@campsh.com>
Date:   Tue Nov 28 23:16:34 2023 -0800

    readme : add FreeChat (#4248)

commit 64e64aa2557d97490b2fe1262b313e2f4a1607e3
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Tue Nov 28 04:51:11 2023 -0500

    ggml : restore abort() in GGML_ASSERT (#4242)

commit 8406b0924bf323f37d536dee8b8165c1f3d9d11d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Nov 28 10:32:03 2023 +0200

    ggml : re-enable BLAS for CPU when src0 != F32 + remove redundant full offload checks in llama.cpp (#4240)
    
    * ggml : use blas even if src0 is not F32
    
    * llama : use n_threads_batch only when n_tokens >= 32
    
    ggml-ci
    
    * llama : revert n_threads_batch logic
    
    ggml-ci

commit b38a16dfcff88d547f78f52d1bea31b84a05aff7
Author: bandoti <141645996+bandoti@users.noreply.github.com>
Date:   Mon Nov 27 15:25:42 2023 -0400

    cmake : fix issue with version info not getting baked into LlamaConfig.cmake (#3970)
    
    * Split CPP generation from build-info query
    
    * Remove blank lines
    
    * Add BUILD_SHARED_LIBS option

commit 0dab8cd7cca7e1bc3550dcb4797b9062cdbb1ebd
Author: Kasumi <90275229+kasumi-1@users.noreply.github.com>
Date:   Tue Nov 28 01:39:42 2023 +0800

    readme : add Amica to UI list (#4230)

commit bb03290c17540768a16000a2b01ee4f22440aba1
Author: Bailey Chittle <39804642+bachittle@users.noreply.github.com>
Date:   Mon Nov 27 09:56:52 2023 -0500

    examples : iOS example with swift ui (#4159)
    
    * copy to llama.cpp as subdir
    
    * attempt enabling metal, fails
    
    * ggml metal compiles!
    
    * Update README.md
    
    * initial conversion to new format, utf8 errors?
    
    * bug fixes, but now has an invalid memory access :(
    
    * added O3, now has insufficient memory access
    
    * begin sync with master
    
    * update to match latest code, new errors
    
    * fixed it!
    
    * fix for loop conditionals, increase result size
    
    * fix current workflow errors
    
    * attempt a llama.swiftui workflow
    
    * Update .github/workflows/build.yml
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit f3b269813f6147c5b5cda082e6b45cf04a932e0d
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Sun Nov 26 22:58:43 2023 -0500

    ggml : fix -Warray-bounds warning with gcc (#4231)

commit 3e73d31d9cc0232882ce61c64742aff3ecfec416
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Nov 26 21:51:46 2023 +0200

    lookahead : support `-n -1` infinite generation

commit 9656026b53236ed7328458269c4c798dd50ac8d1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Nov 26 20:42:51 2023 +0200

    readme : update hot topics

commit 922754a8d60080e956891f6cee1fb03aa48d57c6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Nov 26 20:33:07 2023 +0200

    lookahead : add example for lookahead decoding (#4207)
    
    * lookahead : init
    
    * lookahead : generate and store n-grams
    
    * lookahead : use loop instead recursion to generate n-grams
    
    * lookahead : initial working implementation
    
    * lookahead : filter repeating n-grams
    
    * lookahead : use deterministic init
    
    * lookahead : add to Makefile
    
    * lookahead : fix a bug in the seq_id of the lookahead tokens
    
    * lookahead : add comments
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 22da05536ff4ad963080773bef1fb839fdab95d3
Author: Xiao-Yong Jin <jinxiaoyong@gmail.com>
Date:   Sun Nov 26 02:30:02 2023 -0600

    metal : fix yarn (#4220)
    
    get the correct n_orig_ctx in metal

commit 1ddb52ec38f9931925a587f45a23b1c37152c028
Author: Galunid <karolek1231456@gmail.com>
Date:   Sat Nov 25 22:45:02 2023 +0100

    scripts : Use mmap in torch load (#4202)
    
    * Use mmap in torch load, prefer .bin files when loading
    
    * Revert .bin > .safetensors preference

commit f837c3a992b2b6146936cb120871a8cf9d0e3857
Author: Marcus Dunn <51931484+MarcusDunn@users.noreply.github.com>
Date:   Sat Nov 25 08:58:23 2023 -0800

    llama : grammar `reserve` space in `decode_utf8` (#4210)
    
    * reserve space for codepoints
    
    * improvement for the appended 0

commit 3014b5415d08e3dff961da6eea835b9760a701b8
Author: crasm <crasm@git.vczf.us>
Date:   Sat Nov 25 10:47:07 2023 -0500

    Update docs for yarn_ext_factor <0.0 as unspecified instead of NaN (#4189)

commit 04814e718edb13bdf8cca861dc2e5ab4e1995c30
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 25 12:02:13 2023 +0200

    readme : update hot topics

commit af19d3573481d409b3c4e55494810eb1f65a9aae
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 25 11:29:06 2023 +0200

    server : OAI API compatibility (#4198)
    
    * Add openai-compatible POST /v1/chat/completions API endpoint to server example
    
    * fix code style
    
    * Update server README.md
    
    * Improve server README.md
    
    * Fix server.cpp code style according to review
    
    * server : some style changes
    
    * server : indentation
    
    * server : enable special tokens during tokenization by default
    
    * server : minor code style
    
    * server : change random string generator
    
    * straightforward /v1/models endpoint
    
    ---------
    
    Co-authored-by: kir-gadjello <111190790+kir-gadjello@users.noreply.github.com>
    Co-authored-by: Tobi Lütke <tobi@Tobis-MacBook-Pro.local>

commit e9c13ff78114af6fc6a4f27cc8dcdda0f3d389fb
Author: slaren <slarengh@gmail.com>
Date:   Fri Nov 24 18:10:01 2023 +0100

    llama : set metal log callback correctly (#4204)

commit 8a052c131ed3525313cdb84e5ae4e2b6cf8d2e24
Author: slaren <slarengh@gmail.com>
Date:   Fri Nov 24 18:04:31 2023 +0100

    ggml-cuda : support stablelm rope (#4156)
    
    * ggml-cuda : support stablelm rope
    
    * remove unused freq_base kernel parameter
    
    * add n_dims parameter to llm_build_k_shift, default to n_rot via overload
    
    * llama : fix llm_build_k_shift args
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 189d68446e7ef21e8f3af3c0a3d91c35a39aec89
Author: Galunid <karolek1231456@gmail.com>
Date:   Fri Nov 24 15:02:49 2023 +0100

    convert : fix tensors using grad in some models (#4173)

commit 2568a4bf548d7392e9c78c008b33b4c11d53fe95
Author: eastriver <lee@eastriver.dev>
Date:   Fri Nov 24 18:25:10 2023 +0900

    main.swift : fix eos checking (#4197)
    
    llama_token_eos(const struct llama_model *) is currently getting struct llama_context type variable context as a parameter.

commit b35f3d0def3efde92ed465d92a267430d957e87d
Author: Aaryaman Vasishta <aaryaman.vasishta@amd.com>
Date:   Fri Nov 24 16:52:39 2023 +0900

    readme : use PATH for Windows ROCm (#4195)
    
    * Update README.md to use PATH for Windows ROCm
    
    * Update README.md
    
    * Update README.md

commit 55978ce09b69d3987d17d08d92d8cc27193e0773
Author: Haohui Mai <ricetons@gmail.com>
Date:   Thu Nov 23 13:56:53 2023 -0800

    Fix incorrect format strings and uninitialized variables. (#4133)
    
    * Fix incorrect format strings and uninitialized variables.
    
    * Address comments
    
    * Add the missing include statement

commit 6b0a7420d03b9d13cb0e9439a01ce8476d8bf093
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 23 19:07:56 2023 +0200

    llama : KV cache view API + better KV cache management (#4170)
    
    * llama : keep track of used KV cells + better KV cache management
    
    * llama : zero KV cache used upon clear
    
    ggml-ci
    
    * llama : allow exporting a view of the KV cache (#4180)
    
    * Allow exporting a view of the KV cache
    
    * Allow dumping the sequences per cell in common
    
    * Track max contiguous cells value and position as well
    
    * Fix max contiguous empty cells index calculation
    
    Make dump functions deal with lengths or sequences counts > 10 better
    
    * Fix off by one error in dump_kv_cache_view
    
    * Add doc comments for KV cache view functions
    
    Eliminate cell sequence struct; use llama_seq_id directly
    
    Minor cleanups
    
    * common : add -dkvc arg for enabling kv cache dumps
    
    ---------
    
    Co-authored-by: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>

commit d103d935c0e75769a6a597f7a64cab72c6cc3e79
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 23 13:51:22 2023 +0200

    readme : update hot topics

commit 9d5949f04b1f8b76184818abbd99938c2020803f
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Nov 23 12:34:20 2023 +0100

    examples : fix typo in parallel example doc comment (#4181)
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit ff8238f71d56245f4a6dbea693f4ebd81263464d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 23 11:35:04 2023 +0200

    docs : add llama-star arch idea

commit 8e672efe632bb6a7333964a255c4b96f018b9a65
Author: Galunid <karolek1231456@gmail.com>
Date:   Tue Nov 21 16:22:30 2023 +0100

    stablelm : simplify + speedup generation (#4153)

commit 0b871f1a04ef60e114bbe43004fd9c21114e802d
Author: Galunid <karolek1231456@gmail.com>
Date:   Mon Nov 20 19:30:00 2023 +0100

    finetune - update readme to mention llama support only (#4148)

commit dfc7cd48b1cc31d759c093e917a18c0efe03d0e8
Author: Aaryaman Vasishta <aaryaman.vasishta@amd.com>
Date:   Tue Nov 21 00:02:46 2023 +0900

    readme : update ROCm Windows instructions (#4122)
    
    * Update README.md
    
    * Update README.md
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

commit 881800d1f083c39431cef288347082be516d1c80
Author: Seb C <47074056+Sebby37@users.noreply.github.com>
Date:   Tue Nov 21 00:26:59 2023 +1030

    main : Add ChatML functionality to main example (#4046)
    
    Co-authored-by: Sebastian Cramond <sebby37@users.noreply.github.com>

commit f23c0359a32871947169a044eb1dc4dbffd0f405
Author: Galunid <karolek1231456@gmail.com>
Date:   Mon Nov 20 11:35:47 2023 +0100

    ci : add flake8 to github actions (python linting) (#4129)
    
    Disabled rules:
    
    * E203 Whitespace before ':' - disabled because we often use 'C' Style where values are aligned
    
    * E211 Whitespace before '(' (E211) - disabled because we often use 'C' Style where values are aligned
    
    * E221 Multiple spaces before operator - disabled because we often use 'C' Style where values are aligned
    
    * E225 Missing whitespace around operator - disabled because it's broken so often it seems like a standard
    
    * E231 Missing whitespace after ',', ';', or ':' - disabled because we often use 'C' Style where values are aligned
    
    * E241 Multiple spaces after ',' - disabled because we often use 'C' Style where values are aligned
    
    * E251 Unexpected spaces around keyword / parameter equals - disabled because it's broken so often it seems like a standard
    
    * E261 At least two spaces before inline comment - disabled because it's broken so often it seems like a standard
    
    * E266 Too many leading '#' for block comment - sometimes used as "section" separator
    
    * E501 Line too long - disabled because it's broken so often it seems like a standard
    
    * E701 Multiple statements on one line (colon) - broken only in convert.py when defining abstract methods (we can use# noqa instead)
    
    * E704 Multiple statements on one line - broken only in convert.py when defining abstract methods (we can use# noqa instead)

commit 40a34fe8d034bd484efd79ccbb95059ca6308dcb
Author: Branden Butler <bwtbutler@hotmail.com>
Date:   Mon Nov 20 03:50:04 2023 -0600

    speculative : fix prompt tokenization in speculative example (#4025)
    
    * Support special tokens and not adding BOS to prompt in speculative
    
    * Adapt to new should_add_bos function
    
    * Ensure tgt and dft have same add_bos setting

commit dae06c06e5c6232ae2be4d567dd5101e1e96c814
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Nov 19 19:16:07 2023 +0200

    Revert "finetune : add --n-gpu-layers flag info to --help (#4128)"
    
    This reverts commit 05e8301e4593e2a67b4bae24f093dd12ce5cc7c2.

commit 05e8301e4593e2a67b4bae24f093dd12ce5cc7c2
Author: Clark Saben <76020733+csaben@users.noreply.github.com>
Date:   Sun Nov 19 11:56:38 2023 -0500

    finetune : add --n-gpu-layers flag info to --help (#4128)

commit 936c79b2275a8f15f3512e63de615c676904d650
Author: SoftwareRenderer <138734813+SoftwareRenderer@users.noreply.github.com>
Date:   Sun Nov 19 11:54:10 2023 -0500

    server : relay error messages (#4131)

commit 262005ad9ded375e9c544a02c18ef6254fe185a2
Author: kchro3 <62481661+kchro3@users.noreply.github.com>
Date:   Sun Nov 19 08:52:57 2023 -0800

    common : comma should be semicolon (#4137)

commit 35985acffaab1eb4ffcbc22c86063bc630f24a89
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Nov 19 18:50:49 2023 +0200

    gitignore : tokenize

commit e937066420b79a757bf80e9836eb12b88420a218
Author: slaren <slarengh@gmail.com>
Date:   Sun Nov 19 11:10:52 2023 +0100

    gguf-py : export chat templates (#4125)
    
    * gguf-py : export chat templates
    
    * llama.cpp : escape new lines in gguf kv info prints
    
    * gguf-py : bump version
    
    * gguf-py : check chat_template type
    
    * gguf-py : initialize chat_template

commit 28a2e6e7d476717881be6eb9e2d3331342cec57b
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Nov 18 14:48:17 2023 -0700

    tokenize example: Respect normal add BOS token behavior (#4126)
    
    Allow building with Makefile

commit 0b5c3b04572a05f80163a365070fb377a837ac27
Author: Galunid <karolek1231456@gmail.com>
Date:   Sat Nov 18 21:08:33 2023 +0100

    scripts : Remove missed baichuan convert script (#4127)

commit 2923f17f6fec049a71186636c3c4d96408856194
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Nov 18 08:11:18 2023 -0700

    Clean up ggml-cuda.cu warnings when compiling with clang (for ROCM) (#4124)
    
    * ggml-cuda.cu: Clean up warnings when compiling with clang
    
    * ggml-cuda.cu: Move static items into anonymous namespace
    
    * ggml-cuda.cu: Fix use of namespace start macro
    
    * Revert "ggml-cuda.cu: Fix use of namespace start macro"
    
    This reverts commit 26c11490266c096e3e5731e05270a8f73a5b2874.
    
    * Revert "ggml-cuda.cu: Move static items into anonymous namespace"
    
    This reverts commit e29757e0f7535d1ac314300f0324684cc785e06c.

commit bbecf3f415797f812893947998bda4f866fa900e
Author: slaren <slarengh@gmail.com>
Date:   Fri Nov 17 20:39:11 2023 +0100

    llama : increase max nodes (#4115)

commit 8e9361089dd31ae9ae59452a8ee409fd51a16371
Author: Roger Meier <r.meier@siemens.com>
Date:   Fri Nov 17 17:11:23 2023 +0100

    build : support ppc64le build for make and CMake (#3963)
    
    * build: support ppc64le build for make and CMake
    
    * build: keep __POWER9_VECTOR__ ifdef and extend with __powerpc64__
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 5ad387e994dde77a47ec547a4a65f7611dc325f4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Nov 17 18:01:38 2023 +0200

    tokenize : fix trailing whitespace

commit 2fa02b4b3d86182381311c98b75065ee1b7c2930
Author: zakkor <edward.partenie@gmail.com>
Date:   Fri Nov 17 17:36:44 2023 +0200

    examples : add tokenize (#4039)

commit 2ab0707acbf3a3ca9e5bc5959c7920c22eba2257
Author: Don Mahurin <dmahurin@users.noreply.github.com>
Date:   Fri Nov 17 07:32:34 2023 -0800

    convert : use 'model' value if it exists. This allows karpathy/tinyllamas to load (#4089)
    
    Co-authored-by: Don Mahurin <@>

commit 11173c92d6eaa2bd1308c2389f44f838480836ac
Author: John <78893154+cmp-nct@users.noreply.github.com>
Date:   Fri Nov 17 16:24:30 2023 +0100

    py : Falcon HF compatibility (#4104)
    
    Falcon HF compatibility

commit 9e87ef60e18d69338c5efea314aa7e718bf2040a
Author: Jannis Schönleber <joennlae@gmail.com>
Date:   Fri Nov 17 16:24:07 2023 +0100

    common : improve yaml log escaping (#4080)
    
    * logging: improve escaping in yaml output
    
    * logging: include review feedback

commit c7cce1246e248124117ae5bc058923e3ade95f11
Author: Huawei Lin <huaweilin.cs@gmail.com>
Date:   Fri Nov 17 10:22:56 2023 -0500

    llava : fix compilation warning that fread return value is not used (#4069)

commit f7d5e975424ff0eea55ca5a9181ac8e15553c1fc
Author: Jiří Podivín <66251151+jpodivin@users.noreply.github.com>
Date:   Fri Nov 17 16:20:53 2023 +0100

    py : remove superfluous import statements (#4076)
    
    Signed-off-by: Jiri Podivin <jpodivin@gmail.com>
    Co-authored-by: Jiri Podivin <jpodivin@redhat.com>

commit ba4cf5c0bf37a729d29e899dadf14541cddd23d4
Author: Jiří Podivín <66251151+jpodivin@users.noreply.github.com>
Date:   Fri Nov 17 16:19:16 2023 +0100

    train : move number of gpu layers argument parsing to common/train.cpp (#4074)
    
    - introduces help entry for the argument
     - cuts '--gpu-layers' form in order to simplify usage and documentation.
    
    Signed-off-by: Jiri Podivin <jpodivin@gmail.com>
    Co-authored-by: Jiri Podivin <jpodivin@redhat.com>

commit e85bb1a8e736228a1f0d965777de5f77f22834b8
Author: slaren <slarengh@gmail.com>
Date:   Fri Nov 17 16:17:37 2023 +0100

    llama : add functions to get the model's metadata (#4013)
    
    * llama : add functions to get the model's metadata
    
    * format -> std::to_string
    
    * better documentation

commit 3e916a07ac093045d88ef0c4fa78647ae0efc010
Author: gwjr <502526+gwjr@users.noreply.github.com>
Date:   Fri Nov 17 14:48:19 2023 +0000

    finetune : speed-up ggml_compute_forward_out_prod_f32 via BLAS (#4079)
    
    * Remove logically superfluous assertions and order by dimension
    
    * Use cblas_sgemm() to implement ggml_compute_forward_out_prod()
    
    * Remove ggml_compute_forward_out_prod_use_blas(), fix compiling errors on cmake/zig, remove trailing whitespace
    
    * Add openBLAS support for sgemm() in compute_forward_out_prod()

commit 947f64f1630bb8b0b363a3bb5e29e11425312d57
Author: Andrew Godfrey <AndrewGodfrey@users.noreply.github.com>
Date:   Fri Nov 17 02:23:11 2023 -0800

    finetune : zero the loraB initial vectors (#4082)
    
    * finetune : zero the loraB initial vectors
    
    Without this, the first iteration is starting out far from the base model, instead of exactly on it.
    Zeroing loraB is what the paper recommends. loralib also zeroes at least one of the init vector pairs
    (though it departs from the paper in using a different distribution for the other vector, in some cases).
    
    * tabs to spaces
    
    * Use ggml_set_zero instead of adding a new function

commit b83e149ec6264d078e6a47412e7347bf5c2bfcc9
Author: Andrew Godfrey <AndrewGodfrey@users.noreply.github.com>
Date:   Fri Nov 17 00:01:15 2023 -0800

    cuda : get_row_rounding F32 (#4095)
    
    * Fix #4017
    
    * Update ggml-cuda.cu
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Update ggml-cuda.cu
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

commit 4f447a48339977073a1af4f33ae873465ff64994
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Nov 17 10:00:15 2023 +0200

    llama : fix data units (#4101)
    
    * llama : fix data units
    
    ggml-ci
    
    * Revert "llama : fix data units"
    
    This reverts commit f5feac831fe225ed7f3db938d115732a49dccfc4.
    
    * llama : disambiguate data units
    
    ggml-ci

commit 91f6499393d2d999331fbfdba47a7f8b9f913f0d
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Thu Nov 16 19:14:37 2023 -0700

    Respect tokenizer.ggml.add_bos_token value when tokenizing (#4040)
    
    * gguf-py: gguf-dump: Respect --no-tensor flag in JSON mode.
    
    * Respect add_bos_token GGUF metadata value
    
    * gguf-py: Try to fix SpecialVocab giving up too easily for the Nth time

commit 8da46278e1a57107591653275f8e03a281de94f0
Author: texmex76 <40733439+texmex76@users.noreply.github.com>
Date:   Thu Nov 16 16:01:48 2023 +0100

    gguf : fix potential infinite loops while parsing (#4100)
    
    Co-authored-by: Bernhard Gstrein <gstrein@cs.uni-freiburg.de>

commit a6fc554e268634494f33b0de76f9dde650dd292f
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Wed Nov 15 11:34:47 2023 -0500

    llama : restore prefix space in llama tokenizer (#4081)

commit 1cf2850d52bb027aa215e039ed9a0c61beeef8d3
Author: slaren <slarengh@gmail.com>
Date:   Wed Nov 15 13:58:13 2023 +0100

    ggml-cuda : increase max graph size (#4084)

commit 6bb4908a17150b49373b5f977685b2e180a04f6f
Author: Michael Potter <NanoTekGuy@Gmail.com>
Date:   Tue Nov 14 09:34:41 2023 -0800

    Fix MacOS Sonoma model quantization (#4052)
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 36eed0c42c5b0bf74af81fb9243d262014f9382f
Author: Galunid <karolek1231456@gmail.com>
Date:   Tue Nov 14 11:17:12 2023 +0100

    stablelm : StableLM support (#3586)
    
    * Add support for stablelm-3b-4e1t
    * Supports GPU offloading of (n-1) layers

commit b46d12f86d56bef3dc8b596dfb3d22f3b08102be
Author: afrideva <95653597+afrideva@users.noreply.github.com>
Date:   Mon Nov 13 17:03:40 2023 -0800

    convert.py: also look for plain model.safetensors  (#4043)
    
    * add safetensors to convert.py help message
    
    * Check for single-file safetensors model
    
    * Update convert.py "model" option help message
    
    * revert convert.py help message change

commit bd90eca237b498dd106d315dcb9ad3e6fae3906f
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Mon Nov 13 18:20:52 2023 +0300

    llava : fix regression for square images in #3613 (#4056)

commit 3d68f364f15778dc326f5024f2e5af1ad6dfddef
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Nov 13 16:55:52 2023 +0200

    ggml : sync (im2col, GPU conv, 32-bit arm compat) (#4060)
    
    ggml-ci

commit c049b37d7baf558944501705b91ac89b26ee3e41
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Nov 13 14:18:08 2023 +0200

    readme : update hot topics

commit 4760e7cc0b68570d58f55e8dda469805d1759d0d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Nov 13 14:16:23 2023 +0200

    sync : ggml (backend v2) (#3912)
    
    * sync : ggml (backend v2) (wip)
    
    * sync : migrate examples and llama.cpp to dynamic graphs (wip)
    
    * sync : update tests + fix max op params to 64
    
    ggml-ci
    
    * sync : ggml-cuda
    
    ggml-ci
    
    * llama : fix save/load state context size
    
    ggml-ci
    
    * sync : try to fix build on tvOS
    
    * sync : pass custom graph sizes in training examples
    
    * sync : update graph copies to new ggml API
    
    * sync : update sync-ggml.sh with new files
    
    * scripts : fix header in sync script
    
    * train : fix context size calculations
    
    * llama : increase inference graph size up to 4096 nodes
    
    * train : allocate grads for backward graphs
    
    * train : allocate grads for gb_tmp

commit bb50a792ec2a49944470c82694fa364345e95170
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Mon Nov 13 01:58:15 2023 -0700

    Add ReLU and SQR CUDA ops to (partially) fix Persimmon offloading (#4041)
    
    * Add ReLU and SQR CUDA ops to fix Persimmon offloading
    
    * Persimmon loader: More helpful error on CUDA/ROCM when offloading too many layers

commit 21fd874c8d2a14dea2d56724e4357c0824aee6a8
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sun Nov 12 16:39:37 2023 -0700

    gguf-py: gguf_writer: Use bytearray to build metadata (#4051)
    
    * gguf-py: gguf_writer: Use BytesIO to build metadata
    
    * Use bytearray instead
    
    Bump gguf-py package version

commit 532dd74e38c29e16ea1cfc4e7eedb4f2fab3f3cd
Author: Richard Kiss <him@richardkiss.com>
Date:   Sat Nov 11 22:04:58 2023 -0800

    Fix some documentation typos/grammar mistakes (#4032)
    
    * typos
    
    * Update examples/parallel/README.md
    
    Co-authored-by: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
    
    ---------
    
    Co-authored-by: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>

commit e86fc56f7521ca4b18d1d9939e82abd40c2f1c01
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Sat Nov 11 18:35:31 2023 +0300

    Fix gguf-convert-endian script (#4037)
    
    * Fix gguf-convert-endian script
    
    * Bump version and update description

commit d96ca7ded77df764db797b68b4a29e34c5b56285
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Sat Nov 11 05:48:21 2023 +0000

    server : fix crash when prompt exceeds context size (#3996)

commit 34b0a082074b073eb14c2bd93c0c070e20ddcd16
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Fri Nov 10 22:04:50 2023 -0700

    gguf-py: Refactor and allow reading/modifying existing GGUF files (#3981)
    
    * gguf-py: Refactor and add file reading support
    
    * Replay changes from #3871
    
    Credit to @cebtenzzre for that pull
    
    * Various type annotation fixes.
    
    * sort imports with isort (again)
    
    * Fix missing return statement in add_tensor
    
    * style cleanup with flake8
    
    * fix NamedTuple and Enum usage
    
    * Fix an issue with state init in GGUFReader
    
    Move examples to an examples/ directory
    
    Clean up examples
    
    Add an example of modifying keys in a GGUF file
    
    Update documentation with info on examples
    
    Try to support people importing gguf/gguf.py directly
    
    * Damagage is not a word.
    
    * Clean up gguf-py/examples/modify_gguf.py whitespace
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Update gguf-py/examples/modify_gguf.py formatting
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Update gguf-py/gguf/gguf_reader.py type hint
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Make examples executable, formatting changes
    
    * Add more information to GGUFReader and examples comments
    
    * Include a gguf Python package version bump
    
    * Add convert-gguf-endian.py script
    
    * cleanup
    
    * gguf-py : bump minor version
    
    * Reorganize scripts
    
    * Make GGUFReader endian detection less arbitrary
    
    * Add JSON dumping support to gguf-dump.py
    
    Which I kind of regret now
    
    * A few for gguf-dump.py cleanups
    
    * Murder accidental tuple in gguf-py/scripts/gguf-dump.py
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * cleanup
    
    * constants : remove unneeded type annotations
    
    * fix python 3.8 compat
    
    * Set up gguf- scripts in pyproject.toml
    
    * And include scripts/__init__.py, derp
    
    * convert.py: We can't currently support Q8_0 on big endian.
    
    * gguf-py: SpecialVocab: Always try available sources for special token ids
    
    gguf-py: SpecialVocab: Try to load merges from merges.txt if not in tokenizer.json
    
    gguf-py: SpecialVocab: Add 'add_bos_token' type bools to GGUF metadata
    u
    
    * cleanup
    
    * Promote add_X_token to GGUF metadata for BOS and EOS
    
    ---------
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

commit 4a4fd3eefad5bd17ab6bcd8e2181b4f62eae76cf
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Sat Nov 11 06:49:33 2023 +0800

    server : allow continue edit on completion mode (#3950)
    
    * server : allow continue edit on completion mode
    
    * server : handle abort case in runCompletion
    
    * server : style improvement

commit df9d1293defe783f42bc83af732d3c670552c541
Author: Galunid <karolek1231456@gmail.com>
Date:   Fri Nov 10 14:24:54 2023 +0100

    Unbreak persimmon after #3837 (#4010)

commit a75fa576abba9d37f463580c379e4bbf1e1ad03c
Author: Galunid <karolek1231456@gmail.com>
Date:   Thu Nov 9 11:09:29 2023 +0100

    scripts: Generalize convert scripts (#3838)
    
    * Replace convert-*-hf-to-gguf.py files with convert-hf-to-gguf.py

commit 57ad015dc3011b046ed5a23186c86ea55f987c54
Author: Mihai <mihai.chirculescu@yahoo.com>
Date:   Thu Nov 9 04:00:34 2023 +0200

    server : add min_p param (#3877)
    
    * Update server.cpp with min_p after it was introduced in https://github.com/ggerganov/llama.cpp/pull/3841
    
    * Use spaces instead of tabs
    
    * Update index.html.hpp after running deps.sh
    
    * Fix test - fix line ending

commit 875fb42871a0f5a88fbe31a0b5edd697b84038e4
Author: slaren <slarengh@gmail.com>
Date:   Wed Nov 8 13:15:14 2023 +0100

    ggml-alloc : fix backend assignments of views (#3982)

commit 0a7c980b6f94a049cb804573df2d8092a34df8e4
Author: Jared Van Bortel <cebtenzzre@gmail.com>
Date:   Tue Nov 7 12:43:04 2023 -0500

    gguf : track writer state, free unneeded tensors, cleanup (#3871)

commit 413503d4b92500d82b002d03c580a71a54747138
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Nov 7 19:25:32 2023 +0200

    make : do not add linker flags when compiling static llava lib (#3977)

commit e9c1cecb9d7d743d30b4a29ecd56a411437def0a
Author: xaedes <xaedes@gmail.com>
Date:   Tue Nov 7 09:04:51 2023 +0100

    ggml : fix backward rope after YaRN (#3974)
    
    * fix backward process of rope
    
    rope backward process was broken after YaRN RoPE (#2268) implementation, due to missing changes in backward functions.
    
    the code for the backward process is nearly identically to the forward process:
    the only difference is the sign of the sin-values.
    
    to avoid future regressions remove the near-duplicate backward functions and reuse the forward code:
    
    for this a new function argument `bool forward` was added to `ggml_compute_forward_rope_f32` and `ggml_compute_forward_rope_f16`.
    the sin-values will be negated when forward is false.
    
    * fix finetune rope call to use correct default attn_factor of 1.0f
    
    * remove unused `ggml_rope_xpos_back`
    
    it is better to have only one `ggml_rope_back` function that accepts all rope parameters, so that `ggml_compute_backward` can propagate all parameters without having to switch between different rope_back variants.
    
    * fix comments explaining the sinus sign in ggml_forward_rope
    
    * add missing function arguments in declaration
    
    * fix function argument type in declaration

commit 54b4df8886103b436a4bb3b60f4d84824f9e8868
Author: Matthew Tejo <matthew.tejo@gmail.com>
Date:   Mon Nov 6 23:43:59 2023 -0800

    Use params when loading models in llava-cli (#3976)
    
    llava-cli was loading models with default params and ignoring settings
    from the cli. This switches to a generic function to load the params
    from the cli options.

commit 46876d2a2c92e60579dc732cdb8cbd243b06f317
Author: Meng Zhang <meng@tabbyml.com>
Date:   Mon Nov 6 22:49:08 2023 -0800

    cuda : supports running on CPU for GGML_USE_CUBLAS=ON build (#3946)
    
    * protyping the idea that supports running on CPU for a GGML_USE_CUBLAS=on build
    
    * doc: add comments to ggml_cublas_loaded()
    
    * fix defined(...)

commit 381efbf480959bb6d1e247a8b0c2328f22e350f8
Author: Damian Stewart <d@damianstewart.com>
Date:   Mon Nov 6 22:36:23 2023 +0100

    llava : expose as a shared library for downstream projects (#3613)
    
    * wip llava python bindings compatibility
    
    * add external llava API
    
    * add base64 in-prompt image support
    
    * wip refactor image loading
    
    * refactor image load out of llava init
    
    * cleanup
    
    * further cleanup; move llava-cli into its own file and rename
    
    * move base64.hpp into common/
    
    * collapse clip and llava libraries
    
    * move llava into its own subdir
    
    * wip
    
    * fix bug where base64 string was not removed from the prompt
    
    * get libllava to output in the right place
    
    * expose llava methods in libllama.dylib
    
    * cleanup memory usage around clip_image_*
    
    * cleanup and refactor *again*
    
    * update headerdoc
    
    * build with cmake, not tested (WIP)
    
    * Editorconfig
    
    * Editorconfig
    
    * Build with make
    
    * Build with make
    
    * Fix cyclical depts on Windows
    
    * attempt to fix build on Windows
    
    * attempt to fix build on Windows
    
    * Upd TODOs
    
    * attempt to fix build on Windows+CUDA
    
    * Revert changes in cmake
    
    * Fix according to review comments
    
    * Support building as a shared library
    
    * address review comments
    
    ---------
    
    Co-authored-by: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>

commit 2833a6f63c1b87c7f4ac574bcf7a15a2f3bf3ede
Author: slaren <slarengh@gmail.com>
Date:   Sun Nov 5 18:45:16 2023 +0100

    ggml-cuda : fix f16 mul mat (#3961)
    
    * ggml-cuda : fix f16 mul mat
    
    ggml-ci
    
    * silence common.cpp warning (bonus)

commit d9ccce2e339ca0396560d18b8637f2c848d72a08
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sun Nov 5 10:06:06 2023 -0700

    Allow common process_escapes to handle \x sequences (#3928)
    
    * Allow common process_escapes to handle \x sequences
    
    * Fix edge case when second hex digit is NUL

commit bb60fd0bf6bb270744d86dd45b3a95af01b7de45
Author: Thái Hoàng Tâm <75922889+RoyalHeart@users.noreply.github.com>
Date:   Sun Nov 5 23:15:27 2023 +0700

    server : fix typo for --alias shortcut from -m to -a (#3958)

commit 132d25b8a62ea084447e0014a0112c1b371fb3f8
Author: Jared Van Bortel <cebtenzzre@gmail.com>
Date:   Sun Nov 5 10:08:57 2023 -0500

    cuda : fix disabling device with --tensor-split 1,0 (#3951)
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 3d48f42efcd05381221654376e9f6f69d76af739
Author: Meng Zhang <meng@tabbyml.com>
Date:   Sun Nov 5 04:40:08 2023 -0800

    llama : mark LLM_ARCH_STARCODER as full offload supported (#3945)
    
    as done in https://github.com/ggerganov/llama.cpp/pull/3827

commit c41ea36eaa3548776de4cb3d5d49b925cd3fc0f2
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Sun Nov 5 08:03:09 2023 +0000

    cmake : MSVC instruction detection (fixed up #809) (#3923)
    
    * Add detection code for avx
    
    * Only check hardware when option is ON
    
    * Modify per code review sugguestions
    
    * Build locally will detect CPU
    
    * Fixes CMake style to use lowercase like everywhere else
    
    * cleanup
    
    * fix merge
    
    * linux/gcc version for testing
    
    * msvc combines avx2 and fma into /arch:AVX2 so check for both
    
    * cleanup
    
    * msvc only version
    
    * style
    
    * Update FindSIMD.cmake
    
    ---------
    
    Co-authored-by: Howard Su <howard0su@gmail.com>
    Co-authored-by: Jeremy Dunn <jeremydunn123@gmail.com>

commit a7fac013cf1cc7bbc0160a226aa2412e9f22e78a
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Sun Nov 5 07:46:44 2023 +0000

    ci : use intel sde when ci cpu doesn't support avx512 (#3949)

commit 48ade94538fa509465d71023e49d07aab0ec8cd5
Author: slaren <slarengh@gmail.com>
Date:   Sun Nov 5 08:12:13 2023 +0100

    cuda : revert CUDA pool stuff (#3944)
    
    * Revert "cuda : add ROCM aliases for CUDA pool stuff (#3918)"
    
    This reverts commit 629f917cd6b96ba1274c49a8aab163b1b189229d.
    
    * Revert "cuda : use CUDA memory pool with async memory allocation/deallocation when available (#3903)"
    
    This reverts commit d6069051de7165a4e06662c89257f5d2905bb156.
    
    ggml-ci

commit f28af0d81aa1010afa5de74cf627dcb04bea3157
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Nov 4 16:20:34 2023 -0600

    gguf-py: Support 01.AI Yi models (#3943)

commit d9b33fe95bd257b36c84ee5769cc048230067d6f
Author: Peter Sugihara <peter@campsh.com>
Date:   Fri Nov 3 12:18:18 2023 -0700

    metal : round up to 16 to fix MTLDebugComputeCommandEncoder assertion (#3938)

commit 5ba37461711095c0284233dbd14f0d9010cdbf56
Author: Xiao-Yong Jin <jinxiaoyong@gmail.com>
Date:   Fri Nov 3 13:00:31 2023 -0500

    ggml-metal: fix yarn rope (#3937)

commit abb77e7319aabc0b5cfb7c22da690a692489b6b7
Author: slaren <slarengh@gmail.com>
Date:   Fri Nov 3 12:13:09 2023 +0100

    ggml-cuda : move row numbers to x grid dim in mmv kernels (#3921)

commit 8f961abdc4e134c83bf8c2ad618ab256b4cae0f9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Nov 3 09:41:17 2023 +0200

    speculative : change default p_accept to 0.5 + CLI args (#3919)
    
    ggml-ci

commit 05816027d649f977468fc804cdb54e99eac246d1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Nov 3 09:24:00 2023 +0200

    common : YAYF (yet another YARN fix) (#3925)
    
    ggml-ci

commit 3fdbe6b66b7b5c6ad3b2f245cbad1517c27ff776
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Nov 3 02:31:58 2023 -0400

    llama : change yarn_ext_factor placeholder to -1 (#3922)

commit 629f917cd6b96ba1274c49a8aab163b1b189229d
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Thu Nov 2 13:58:22 2023 -0600

    cuda : add ROCM aliases for CUDA pool stuff (#3918)

commit 51b2fc11f7f605fff49725a4540e9a6ef7b51b70
Author: Andrei <abetlen@gmail.com>
Date:   Thu Nov 2 15:40:31 2023 -0400

    cmake : fix relative path to git submodule index (#3915)

commit 224e7d5b14cbabab7ae45c64db2cfde979c8455d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 2 20:44:12 2023 +0200

    readme : add notice about #3912

commit c7743fe1c1cbda5a886362aa371480360580fdf0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 2 20:32:11 2023 +0200

    cuda : fix const ptrs warning causing ROCm build issues (#3913)

commit d6069051de7165a4e06662c89257f5d2905bb156
Author: Oleksii Maryshchenko <oleksii.maryshchenko@gmail.com>
Date:   Thu Nov 2 18:10:39 2023 +0100

    cuda : use CUDA memory pool with async memory allocation/deallocation when available (#3903)
    
    * Using cuda memory pools for async alloc/dealloc.
    
    * If cuda device doesnt support memory pool than use old implementation.
    
    * Removed redundant cublasSetStream
    
    ---------
    
    Co-authored-by: Oleksii Maryshchenko <omaryshchenko@dtis.com>

commit 4ff1046d75e64f0e556d8dcd930ea25c23eb8b18
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 2 16:22:30 2023 +0200

    gguf : print error for GGUFv1 files (#3908)

commit 21958bb393a654591ed26f339791b752d58f5c8b
Author: slaren <slarengh@gmail.com>
Date:   Thu Nov 2 13:10:33 2023 +0100

    cmake : disable LLAMA_NATIVE by default (#3906)

commit 2756c4fbffab097736d5116007872d86456a544a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 2 11:20:21 2023 +0200

    gguf : remove special-case code for GGUFv1 (#3901)
    
    ggml-ci

commit 1efae9b7dca2a5cc5aa21c1997b538022964ea19
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 2 09:54:18 2023 +0200

    llm : prevent from 1-D tensors being GPU split (#3697)

commit b12fa0d1c13596869c512f49a526b979c94787cc
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Nov 2 02:50:16 2023 -0400

    build : link against build info instead of compiling against it (#3879)
    
    * cmake : fix build when .git does not exist
    
    * cmake : simplify BUILD_INFO target
    
    * cmake : add missing dependencies on BUILD_INFO
    
    * build : link against build info instead of compiling against it
    
    * zig : make build info a .cpp source instead of a header
    
    Co-authored-by: Matheus C. França <matheus-catarino@hotmail.com>
    
    * cmake : revert change to CMP0115
    
    ---------
    
    Co-authored-by: Matheus C. França <matheus-catarino@hotmail.com>

commit 4d719a6d4e74b9a98e75f826f865f3153717d54b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 2 08:35:10 2023 +0200

    cuda : check if this fixes Pascal card regression (#3882)

commit 183b3fac6c28e65d23ac0230c1dd6fb84bf0154d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 2 08:33:37 2023 +0200

    metal : fix build errors and kernel sig after #2268 (#3898)

commit 2fffa0d61fa10e4b466e78cabcc6a4e16717b580
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Nov 2 01:49:44 2023 -0400

    cuda : fix RoPE after #2268 (#3897)

commit 0eb332a10f3f14a3746c391bf80ff5e7bdf29d5d
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Wed Nov 1 19:29:14 2023 -0400

    llama : fix llama_context_default_params after #2268 (#3893)

commit d02e98cde035d91ed8032ab943d1d504fe9da394
Author: slaren <slarengh@gmail.com>
Date:   Wed Nov 1 23:10:09 2023 +0100

    ggml-cuda : compute ptrs for cublasGemmBatchedEx in a kernel (#3891)
    
    * ggml-cuda : compute ptrs for cublasGemmBatchedEx in a kernel
    
    * fix warnings

commit 898aeca90a9bb992f506234cf3b8b7f7fa28a1df
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Wed Nov 1 18:04:33 2023 -0400

    llama : implement YaRN RoPE scaling (#2268)
    
    Co-authored-by: cebtenzzre <cebtenzzre@gmail.com>
    Co-authored-by: Jeffrey Quesnelle <jquesnelle@gmail.com>

commit c43c2da8afacaddfe51c09b21dbd9922cd0ea46b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 1 23:08:30 2023 +0200

    llm : fix llm_build_kqv taking unused tensor (benign, #3837)

commit 523e49b11174368cd73460fa5eae7b39d856f300
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 1 23:00:50 2023 +0200

    llm : fix falcon norm after refactoring (#3837)

commit e16b9fa4baa8a09c6619b116159830e898050942
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 1 21:25:00 2023 +0200

    metal : multi-simd softmax (#3710)
    
    ggml-ci

commit ff8f9a88da0018972dfdf6fe64b5c8992caabd9c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 1 21:15:55 2023 +0200

    common : minor (#3715)

commit 50337961a678fce4081554b24e56e86b67660163
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 1 20:11:02 2023 +0200

    llm : add llm_build_context (#3881)
    
    * llm : add llm_build_context
    
    * llm : deduce norm eps based on type + explict max_alibi_bias, clamp_kqv
    
    * llm : restore the non-graph llm_build_ functional API
    
    ggml-ci
    
    * llm : cleanup + comments

commit 0e40806c1cb3bdf9955ed807ffbe212be85b4c67
Author: bandoti <141645996+bandoti@users.noreply.github.com>
Date:   Wed Nov 1 14:42:01 2023 -0300

    common : allow caller to handle help/argument exceptions (#3715)
    
    * Allow caller to handle help/argument exceptions
    
    * Prepend newline to usage output
    
    * Add new gpt_params_parse_ex function to hide arg-parse impl
    
    * Fix issue blocking success case
    
    * exit instead of returning false
    
    * Update common/common.h
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update common/common.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit a2758d08e44ce3624d233af4d23c6843e2e735b5
Author: staviq <staviq@gmail.com>
Date:   Wed Nov 1 15:18:27 2023 +0100

    log : make generating separate log files optional (#3787)
    
    * impl --log-new, --log-append
    
    * Update common/log.h
    
    Co-authored-by: cebtenzzre <cebtenzzre@gmail.com>
    
    * Update common/log.h
    
    Co-authored-by: cebtenzzre <cebtenzzre@gmail.com>
    
    * Apply suggestions from code review
    
    Co-authored-by: cebtenzzre <cebtenzzre@gmail.com>
    
    ---------
    
    Co-authored-by: cebtenzzre <cebtenzzre@gmail.com>

commit e75dfdd31b6a3dfa0627ba4ac3bb4b36e9db588e
Author: l3utterfly <gc.pthzfoldr@gmail.com>
Date:   Wed Nov 1 21:40:43 2023 +0800

    sampling : null grammar field after reset (#3885)

commit 9a3b4f6c86503c9cfc049d4d0fdeafef12806f5e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 1 13:50:45 2023 +0200

    ggml : fix UNUSED macro (#3762)

commit 73bdcb395ef9a997d9c02950c7cd4249546162cd
Author: Andrew Godfrey <AndrewGodfrey@users.noreply.github.com>
Date:   Wed Nov 1 04:49:04 2023 -0700

    finetune : add -ngl parameter (#3762)
    
    * Add '-ngl' support to finetune.cpp
    
    * Add fprintf in ggml_cuda_op_add
    
    When I tried CUDA offloading during finetuning following the readme, I got an assert here.
    This probably isn't an important case because inference later gives a warning saying you should use f16 or f32 instead when using lora
    
    * Add 'finetune.sh', which currently fails when using GPU
    
    "error: operator (): Finetuning on tensors with type 'f16' is not yet supported"
    
    * tweak finetune.sh
    
    * Suppress some warnings in ggml.c
    
    * Add f16 implementation to ggml_compute_forward_add_f16_f32
    
    * Add an f16 case to ggml_add_cast_impl and llama_build_lora_finetune_graphs
    
    * finetune.sh: Edit comments
    
    * Add "add_f16_f32_f32_cuda"
    
    * Tweak an error message
    
    * finetune.sh: Add an optional LLAMA_MODEL_DIR variable
    
    * finetune.sh: Add an optional LLAMA_TRAINING_DIR variable
    
    * train : minor
    
    * tabs to spaces
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: cebtenzzre <cebtenzzre@gmail.com>

commit f0e209324a7f663225791897877bf610f1af152d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 1 11:29:07 2023 +0200

    scripts : add server-llm.sh (#3868)
    
    * scripts : add deploy-server.sh
    
    * scripts : rename to server-llm.sh
    
    * scripts : working curl pipe

commit ca190bca8e844d171020d6147687e71472d71734
Author: Adrian Hesketh <a-h@users.noreply.github.com>
Date:   Wed Nov 1 09:28:28 2023 +0000

    server : re-enable completion and embedded at the same time (#3876)

commit 71e3718abdb2771b50c9606d3a7569623a0b0afe
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 1 08:04:02 2023 +0200

    llama : refactor graph build code (#3837)
    
    * llama : factor out ggml-alloc from graph graph build functions
    
    ggml-ci
    
    * metal : disable kernel load log
    
    * llama : factor out tensor offloading outside the build call (wip)
    
    ggml-ci
    
    * llama : offload rest of the models
    
    ggml-ci
    
    * llama : update offload log messages to print node index
    
    * llama : comments
    
    * llama : support offloading result_norm + comments
    
    * llama : factor graph input into a function
    
    * llama : do tensor offload only with CUDA
    
    * llama : fix res_norm offloading
    
    * llama : try to optimize offloading code
    
    * llama : fix non-CUDA build
    
    * llama : try to fix build
    
    * llama : move refact in correct place + optimize graph input
    
    * llama : refactor tensor offloading as callback
    
    * llama : add layer index to all tensor names
    
    * llama : add functional header
    
    * llama : comment
    
    ggml-ci
    
    * llama : remove obsolete map for layer counting
    
    * llama : add llm_build helper functions (#3848)
    
    * llama : add llm_build_norm helper function
    
    ggml-ci
    
    * llama : add llm_build_ffn helper function (#3849)
    
    ggml-ci
    
    * llama : add llm_build_k_shift helper
    
    ggml-ci
    
    * llama : fix offloading after recent changes
    
    * llama : add llm_build_kv_store helper
    
    ggml-ci
    
    * llama : remove obsolete offload names
    
    * llama : fix llm_build_k_shift to use n_head_kv instead of n_head
    
    * llama : simplify falcon Q, K, V computation
    
    * llama : remove obsolete comments in build graphs
    
    * llama : add llm_build_kqv helper
    
    ggml-ci
    
    * llama : minor
    
    * llama : add LLAMA_OFFLOAD_DEBUG + fix starcoder offloading
    
    * llama : fix input allocation logic
    
    * llama : update offload functions for KQ tensors
    
    * llama : normalize tensor names
    
    ggml-ci
    
    * llama : enable warning about not offloaded tensors
    
    * llama : remove extra ; + deduplicate gate_b logic
    
    * llama : add llm_build_inp_embd helper

commit 238657db2364cfb728c694470a4a81702afea760
Author: kalomaze <66376113+kalomaze@users.noreply.github.com>
Date:   Tue Oct 31 14:44:49 2023 -0500

    samplers : Min-P sampler implementation [alternative to Top P/Top K] (#3841)
    
    * Introduce the new Min-P sampler by @kalomaze
       The Min-P sampling method was designed as an alternative to Top-P, and aims to ensure a balance of quality and variety. The parameter *p* represents the minimum probability for a token to be considered, relative to the probability of the most likely token.
    
    * Min-P enabled and set to 0.05 default
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: cebtenzzre <cebtenzzre@gmail.com>

commit 07178c98e1b61a5e2af39d347add12e7eb9e08e1
Author: Tungsten842 <886724vf@anonaddy.me>
Date:   Tue Oct 31 18:24:03 2023 +0100

    flake.nix: fix for rocm 5.7 (#3853)

commit 207b51900e15cc7f89763a3bb1c565fe11cbb45d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Oct 30 19:19:15 2023 +0200

    ggml : move FP16 <-> FP32 code to ggml-impl.h (#3861)
    
    * ggml : move FP16 <-> FP32 stuff to ggml-impl.h
    
    ggml-ci
    
    * tests : fix ARM build
    
    * ggml : explicitly initialize deprecated type traits
    
    * ggml : add math.h to ggml-impl.h
    
    * ggml : remove duplicate static assert macros
    
    * ggml : prefix lookup tables with ggml_
    
    ggml-ci
    
    * ggml-impl : move extern "C" to start of file

commit 6e08281e588bbba1a5d180290a94a43f167f3a1a
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sun Oct 29 11:31:40 2023 -0600

    Extend llama_kv_cache_seq_rm to allow matching any sequence (#3843)
    
    * Extend llama_kv_cache_seq_rm to allow matichng any sequence
    
    * Replace llama_kv_cache_tokens_rm with llama_kv_cache_clear
    
    Use llama_kv_cache_clear for cache clearing
    
    Change calls to llama_kv_cache_tokens_rm that want to delete by position to use llama_kv_cache_seq_rm functionality

commit 2046eb4345e62c4575b3cdc0115a51db89f3fb70
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Sun Oct 29 12:33:47 2023 -0400

    make : remove unnecessary dependency on build-info.h (#3842)

commit 71a09da301705b9c5ad4ca3cf3fbd966dd3f1ec5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 29 18:32:51 2023 +0200

    llama : fix kv shift bug (#3835)
    
    ggml-ci

commit d69d777c02b9ac405a95f3cbfba219a990caefff
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 29 18:32:28 2023 +0200

    ggml : quantization refactoring (#3833)
    
    * ggml : factor all quantization code in ggml-quants
    
    ggml-ci
    
    * ggml-quants : fix Zig and Swift builds + quantize tool
    
    ggml-ci
    
    * quantize : --pure option for disabling k-quant mixtures
    
    ---------
    
    Co-authored-by: cebtenzzre <cebtenzzre@gmail.com>

commit ff3bad83e29e3009010cbc923bebd769055eaa7f
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Sat Oct 28 16:41:07 2023 +0200

    flake : update flake.lock for newer transformers version + provide extra dev shell (#3797)
    
    * flake : update flake.lock for newer transformers version + provide extra dev shell with torch and transformers (for most convert-xxx.py scripts)

commit 82a6646e0221216c41edcdf99f5a44bb051391f5
Author: Aarni Koskela <akx@iki.fi>
Date:   Sat Oct 28 15:43:01 2023 +0300

    metal : try cwd for ggml-metal.metal if bundle lookup fails (#3793)
    
    * Try cwd for ggml-metal if bundle lookup fails
    
    When building with `-DBUILD_SHARED_LIBS=ON -DLLAMA_METAL=ON -DLLAMA_BUILD_SERVER=ON`,
    `server` would fail to load `ggml-metal.metal` because `[bundle pathForResource:...]`
    returns `nil`.  In that case, fall back to `ggml-metal.metal` in the cwd instead of
    passing `null` as a path.
    
    Follows up on #1782
    
    * Update ggml-metal.m
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit ba231e8a6dd8ad82acfe0e4d492ff7cef6b3f0a1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Oct 28 15:25:33 2023 +0300

    issues : change label from bug to bug-unconfirmed (#3748)

commit 8a2f2fea2914aaa3f4b2f82800c7de15f15bdb09
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Oct 28 15:25:15 2023 +0300

    convert : ignore tokens if their IDs are within [0, vocab_size) (#3831)

commit bd6d9e205982b34e0ba2c3b22bbf31a1ef1a1bb5
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Oct 28 05:54:24 2023 -0600

    llama : allow quantizing k-quants to fall back when tensor size incompatible (#3747)
    
    * Allow quantizing k-quants to fall back when tensor size incompatible
    
    * quantizing: Add warning when tensors were incompatible with k-quants
    
    Clean up k-quants state passing a bit

commit ee1a0ec9cb367ba41d138134795cbbbe93d2bf1c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Oct 28 14:23:11 2023 +0300

    llama : add option for greedy sampling with probs (#3813)
    
    * llama : add option for greedy sampling with probs
    
    * llama : add comment about llama_sample_token_greedy() missing probs
    
    * sampling : temp == 0.0 -> no probs, temp < 0.0 -> probs

commit 177461104b454163473dced2a5038f4e016cdb7e
Author: Henk Poley <HenkPoley@gmail.com>
Date:   Sat Oct 28 12:16:33 2023 +0200

    common : print that one line of the syntax help *also* to standard output (#3823)

commit fdee152e4eebb78c191df0b074857111d7f2aba7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Oct 28 12:06:08 2023 +0300

    starcoder : add GPU offloading (#3827)
    
    * starcoder : do not GPU split 1D bias tensors
    
    * starcoder : offload layers to GPU
    
    ggml-ci

commit 41aee4df821854f37d90a45281f03b6db8d27de2
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Fri Oct 27 15:40:07 2023 -0600

    speculative : ensure draft and target model vocab matches (#3812)
    
    * speculative: Ensure draft and target model vocab matches
    
    * Tolerate small differences when checking dft vs tgt vocab

commit 6d459cbfbe5a011dfca94f9550527a504b6f9aa1
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Oct 27 17:33:53 2023 -0400

    llama : correctly report GGUFv3 format (#3818)

commit c8d6a1f34ab6f1b6bd468d256e535a61f98f114c
Author: Thibault Terrasson <thibault.terrasson@gmail.com>
Date:   Fri Oct 27 16:37:41 2023 +0200

    simple : fix batch handling (#3803)

commit 2f9ec7e271220a78fe27c9e6ccbcc0dda31cda0f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Oct 27 17:01:23 2023 +0300

    cuda : improve text-generation and batched decoding performance (#3776)
    
    * cuda : prints wip
    
    * cuda : new cublas gemm branch for multi-batch quantized src0
    
    * cuda : add F32 sgemm branch
    
    * cuda : fine-tune >= VOLTA params + use MMQ only for small batches
    
    * cuda : remove duplicated cuBLAS GEMM code
    
    * cuda : add CUDA_USE_TENSOR_CORES and GGML_CUDA_FORCE_MMQ macros
    
    * build : add compile option to force use of MMQ kernels

commit 34b2a5e1ee4fe6295fb4420eb91131d743694c65
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Oct 26 22:53:37 2023 +0300

    server : do not release slot on image input (#3798)

commit 6961c4bd0b5176e10ab03b35394f1e9eab761792
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 25 10:26:27 2023 +0300

    batched-bench : print params at start

commit cc448774866e6479c750bd7c135cd8f92cedee67
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 25 10:09:16 2023 +0300

    log : disable pid in log filenames

commit ad939626577cd25b462e8026cc543efb71528472
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Tue Oct 24 16:10:43 2023 -0400

    server : add parameter -tb N, --threads-batch N (#3584) (#3768)
    
    Co-authored-by: Michael Coppola <m18coppola@gmail.com>
    Co-authored-by: Michael Coppola <info@michaeljcoppola.com>

commit 1717521cdb976a2219888b0e5cba36e210eee9df
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 24 23:08:20 2023 +0300

    server : do not block system prompt update (#3767)
    
    * server : do not block system prompt update
    
    * server : update state machine logic to process system prompts
    
    * server : minor

commit b2f7e04bd312eaf97eee0523aa09d950d585626b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 24 21:51:20 2023 +0300

    sync : ggml (conv ops + cuda MSVC fixes) (#3765)
    
    ggml-ci

commit abd21fc99f1d35e2081e4c01dc09c71a86bf3c5a
Author: John Smith <67539080+kingsidelee@users.noreply.github.com>
Date:   Wed Oct 25 01:48:45 2023 +0800

    cmake : add missed dependencies (#3763)

commit 2b4ea35e56792064598e922e46d081e02bc96b94
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 24 16:48:37 2023 +0300

    cuda : add batched cuBLAS GEMM for faster attention (#3749)
    
    * cmake : add helper for faster CUDA builds
    
    * batched : add NGL arg
    
    * ggml : skip nops in compute_forward
    
    * cuda : minor indentation
    
    * cuda : batched cuBLAS GEMMs for src0 F16 and src1 F32 (attention ops)
    
    * Apply suggestions from code review
    
    These changes plus:
    
    ```c++
    #define cublasGemmBatchedEx hipblasGemmBatchedEx
    ```
    
    are needed to compile with ROCM. I haven't done performance testing, but it seems to work.
    
    I couldn't figure out how to propose a change for lines outside what the pull changed, also this is the first time trying to create a multi-part review so please forgive me if I mess something up.
    
    * cuda : add ROCm / hipBLAS cublasGemmBatchedEx define
    
    * cuda : add cublasGemmStridedBatchedEx for non-broadcasted cases
    
    * cuda : reduce mallocs in cublasGemmBatchedEx branch
    
    * cuda : add TODO for calling cublas from kernel + using mem pool
    
    ---------
    
    Co-authored-by: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>

commit daab3d7f45832e10773c99f3484b0d5b14d86c0c
Author: Galunid <karolek1231456@gmail.com>
Date:   Tue Oct 24 09:17:17 2023 +0200

    Add more tokenizer tests (#3742)
    
    * Add more tokenizer tests
    
    * Add starcoder
    
    * Update test vocab files
    
    * Restrict bpe tokenizer tests to unicode planes
    
    * Update comment
    
    * Comment cosmetics
    
    * Remove bloom vocab/test

commit 469c9addef75893e6be12edda852d12e840bf064
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 24 09:46:50 2023 +0300

    metal : handle ggml_scale for n%4 != 0 (close #3754)
    
    ggml-ci

commit e3932593d46c30145301a13097895f9376cba509
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Oct 23 23:46:05 2023 +0300

    Revert "make : add optional CUDA_NATIVE_ARCH (#2482)"
    
    This reverts commit 96981f37b1e3f450d9e63e571514217bf60f0a7f.
    
    See:
    
    https://github.com/ggerganov/llama.cpp/pull/2482#issuecomment-1775975866

commit 9d02956443e5c1ded29b7b5ed8a21bc01ba6f563
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Mon Oct 23 22:57:16 2023 +0300

    issues : separate bug and enhancement template + no default title (#3748)

commit 69a6735087c3634963c642fd69f0851ac479cd78
Author: Galunid <karolek1231456@gmail.com>
Date:   Mon Oct 23 21:46:00 2023 +0200

    Update special token handling in conversion scripts for gpt2 derived tokenizers (#3746)
    
    We still have the heads up in `README.md` regarding `bpe` tokenizers and this patch is needed for
    
    - a couple of tokenizer tests
    - some more `special` and `non-special` added tokens handling (as far as I understand it)
    
    * Update special token handling
    
    * Add mpt

commit 5be6c803fa5378f62a1590f3ad8c6b64c7c0c2ce
Author: Marcus Dunn <51931484+MarcusDunn@users.noreply.github.com>
Date:   Mon Oct 23 12:40:03 2023 -0700

    llama : remove token functions with `context` args in favor of `model` (#3720)
    
    * added `llama_model_token_*` variants to all the `llama_token_*` functions.
    
    * added `LLAMA_API`
    
    * formatting
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * removed old `llama_token` functions
    
    * changed 3 more functions to take in model
    
    - `llama_token_get_text`
    - `llama_token_get_score`
    - `llama_token_get_type`
    
    * added back docs
    
    * fixed main.cpp
    
    * changed token functions to use new model variants
    
    * changed token functions to use new model variants
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 6336701c9378c23c85d1c0e464b663ca2bbb8e60
Author: Galunid <karolek1231456@gmail.com>
Date:   Mon Oct 23 17:47:03 2023 +0200

    Fix baichuan convert script not detecing model (#3739)
    
    It seems nobody objects.

commit 96981f37b1e3f450d9e63e571514217bf60f0a7f
Author: Alex <awhill19@icloud.com>
Date:   Sun Oct 22 15:56:53 2023 -0400

    make : add optional CUDA_NATIVE_ARCH (#2482)
    
    Use the environment variable `CUDA_NATIVE_ARCH` if present to set NVCC arch. Otherwise, use `native`.

commit 438c2ca83045a00ef244093d27e9ed41a8cb4ea9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 22 22:53:08 2023 +0300

    server : parallel decoding and multimodal (#3677)
    
    * implementing parallel decoding in server example
    
    * crash fixed
    
    * save dev progress
    
    * refactored sampling function
    
    * completion endpoint working
    
    * multiple client support
    
    * grammar + no stream completion
    
    * cached prompt support
    
    * chat.mjs support cached prompt + some fixes
    
    * server ui now support multiple clients
    
    * unused change reverted
    
    * fixed timings per slot
    
    * add context swap
    
    * add changes to README.md
    
    * llava multimodal integration
    
    * fixed tokens probs
    
    * add multimodal input - alfa
    
    * refactor code + remove unused comments + improved README.md
    
    * fix compilation errors with llvm
    
    * notify the user from server ui that multimodality is unavialable
    
    * some ci fixes
    
    * fix ci make build undefined ref errors
    
    * fix long prompt than ctx proposed in #3639
    
    * fixed premature end due stop word
    
    * context shift fixed
    
    * fix llava implementation
    
    * sync README.md changes
    
    * readme change
    
    * update api like OpenAI
    
    * multimodal support enabled by default
    
    * fix make bui;d errors
    
    * fix multiple clients
    
    * fix zig build
    
    * new sampling API
    
    * latest changes of sampling API
    
    * server : coding-style normalization
    
    * server : coding-style normalization (part 2)
    
    * server : remove beam-search functionality
    
    * server : bug fix in ingest_images
    
    n_tokens is incremented internally by llama_batch_add
    
    * server : use refs + use llama_batch_clear()
    
    * server : snake case
    
    * server : minor sync
    
    * added thread safe pipeline
    
    * server : bach has to be allocated for n_parallel sequences
    
    * server : no need for atomic int - already using mutex
    
    * server : logs + minor code style
    
    * server : fix multibyte handle in partial response (#3706)
    
    * fix image load + view image in chat
    
    * make : silence stb warnings
    
    * clip : link to ggml, not to llama
    
    * server : fix switch fallthrough
    
    * server : fix crash in Debug on macOS (I have no idea why this fixes it!?)
    
    * server : refactor ctx_sampling init + n_ctx + names
    
    * server : bug fix for prompt caching
    
    * Do not save/load image_data to localStorage
    
    * editorconfig : new line in index.html
    
    * server : completion requests remember slot_id
    
    * Update readme to document multimodal in server
    
    * server : minor style
    
    * Update readme to document multimodal in server
    
    * server : hide ctx_sampling->prev behind API (#3696)
    
    * server : apply fix from #3722
    
    * server : fix slot reuse
    
    * server : add comment about changing slot_state to bool
    
    ---------
    
    Co-authored-by: FSSRepo <go778sgt@gmail.com>
    Co-authored-by: Damian Stewart <d@damianstewart.com>
    Co-authored-by: Steward Garcia <57494570+FSSRepo@users.noreply.github.com>
    Co-authored-by: Jhen-Jie Hong <iainst0409@gmail.com>
    Co-authored-by: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>

commit 9e70cc03229df19ca2d28ce23cc817198f897278
Author: goerch <jhr.walter@t-online.de>
Date:   Sun Oct 22 21:21:42 2023 +0200

    Add test for MPT tokenization (#3728)
    
    * Add test for MPT tokenization
    
    * Revert code motion
    
    * Remove unnecessary restriction in test case
    
    * Clarify logic in conversion

commit 5a42a5f8e8a86da9ac88008d748cf232a83aa0e1
Author: Ian Scrivener <github@zilogy.asia>
Date:   Mon Oct 23 05:16:43 2023 +1100

    readme : remove unsupported node.js library (#3703)
    
    - https://github.com/Atome-FE/llama-node is quite out of date
    - doesn't support recent/current llama.cpp functionality

commit a5e7dbd6141128bfa3c40a19c2945a181df625d3
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sun Oct 22 12:14:56 2023 -0600

    llama : validate special token ids are in range when loading GGUF model (#3635)
    
    * Add validation for special token ids to llama.cpp
    
    Small optimization for llama_byte_to_token SPM mode
    
    * Fix BPE newline check, only I could break something so simple
    
    * Killll meeeeee
    
    * Account for GGUF_KEY_KEY only setting when the key exists
    
    * Minor code cleanups.
    
    * Fix convert.py error msg when added tokens are out of range
    
    * Make gguf SpecialVocab vocab size-aware
    
    Update conversion scripts accordingly
    
    * Avoid a string copy
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit d3956aea53369455008159cc405ed4c496976692
Author: vvhg1 <94630311+vvhg1@users.noreply.github.com>
Date:   Sun Oct 22 20:09:51 2023 +0200

    main : escape prompt for cfg_negative_prompt and consecutive inputs in main with interactive (#3623)
    
    * infill tokens correction
    
    * serverinfill tokens correction
    
    * removing any leading whitespace from infill suffix and removing leeading space token from suffix when params.escape
    
    * removing any leading whitespace from infill suffix and removing leeading space token from suffix when params.escape
    
    * only rm when params.escape, rm space if possible which is added back or rm added space token
    
    * only rm when params.escape, rm space if possible which is added back or rm added space token
    
    * Revert "only rm when params.escape, rm space if possible which is added back or rm added space token"
    
    This reverts commit 63ba0b621f21077c0e3bc6ba6a327534123cb738.
    
    * fix interactive prompt escaping and fix server infill leading space handling
    
    * rm unnecessary bool check
    
    * process escapes for neg prompt and interactive consec prompts
    
    * removed unneccessary static string escape

commit 22c69a27945e7acf9690dd3210d316f22182751c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 22 08:37:20 2023 +0300

    batched : add len CLI argument

commit 465219b9143ac01db0990bbcb0a081ef72ec2008
Author: shibe2 <shibe@tuta.io>
Date:   Thu Oct 12 16:01:23 2023 +0400

    CLBlast: Add outer loops over src0 for broadcasting in mulmat
    
    Reduce repeated dequantization of the same data.

commit d1031cf49c3b958b915fd558e23453471c29ac33
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Oct 20 21:07:23 2023 +0300

    sampling : refactor init to use llama_sampling_params (#3696)
    
    * sampling : refactor init to use llama_sampling_params
    
    * llama : combine repetition, frequency and presence penalties in 1 call
    
    * examples : remove embd-input and gptneox-wip
    
    * sampling : rename penalty params + reduce size of "prev" vector
    
    * sampling : add llama_sampling_print helper
    
    * sampling : hide prev behind API and apply #3661
    
    ggml-ci

commit 8cf19d60dc93809db8e51fedc811595eed9134c5
Author: Qin Yue Chen <71813199+chenqiny@users.noreply.github.com>
Date:   Fri Oct 20 06:19:40 2023 -0500

    gguf : support big endian platform (#3552)
    
    * check whether platform is 390x if yes->do not import immintrin.h
    
    * support s390x big endian
    
    * support --bigendian option for s390x
    1. verified with baichuan7b-chat with float 16 on s390x
    2. verified with baichuan7b-chat
    3. verified with chinese-alpaca-2-13b-f16
    
    * update format based on editor-config checker result
    
    * Update convert-baichuan-hf-to-gguf.py
    
    * 1. check in ggml.c if endianess is not match
    2. update GGUF version
    3. change get_pack_prefix to property
    4. update information log
    
    * always use "GGUF" as beginng of GGUF file
    
    * Compare "GGUF" with file header char by char
    1.  Set GGUF_MAGIC to "GGUF" string instead of int value
    2. Compare "GGUF" char by char to ensure its byte order
    3. Move bytes swap code from convert.py to gguf.py write_tensor_data
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit a0edf73bda31c7c4e649e6f07c6fd30a729929cd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Oct 20 13:06:10 2023 +0300

    server : fix uninitialized sampling context (close #3685)

commit f439e506e8ae8b01df2ae2156380f8156d7553e3
Author: Herman Semenov <GermanAizek@yandex.ru>
Date:   Fri Oct 20 10:02:12 2023 +0000

    ggml : fix rope + llama minor optimizations (#3560)
    
    * Minor fixes and fixed memleak
    
    * Using const auto references in range-based loop C++17

commit e78f3ef24af4ca74e77e725644b41ae8ca3b10a5
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Oct 20 01:32:08 2023 -0400

    convert : restore compat with old Falcon models (#3680)

commit f3b25e40438b3c8383caabf4e7b89863145a9f0e
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Thu Oct 19 19:40:41 2023 +0300

    multimodal : add BakLLaVA conversion support (#3682)

commit 60abea9798f47b918a9f38c66edfd88c526d20f6
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Thu Oct 19 16:59:11 2023 +0300

    llava : avoid segfault in case of non-existent mmproj file (#3674)

commit 004797f6ac135383f8c1d1f5bd415ddee2f79318
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 18 21:44:43 2023 +0300

    readme : update hot topics

commit 4e82b2ea3fa6482915d147bc9f46e70b9ada7700
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 18 18:49:40 2023 +0300

    speculative : bug fixes

commit 0e89203b517c95ec6675eda75d200a60d1e8921d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 18 16:21:57 2023 +0300

    speculative : add tree-based sampling example (#3624)
    
    * sampling : one sequence per sampling context
    
    ggml-ci
    
    * speculative : add tree-based sampling support
    
    ggml-ci
    
    * speculative : reuse the n_parallel CLI param
    
    * speculative : refactor sampling
    
    * examples : fix build after sampling refactoring
    
    ggml-ci
    
    * batched : fix n_seq_id
    
    * sampling : fix malloc
    
    ggml-ci
    
    * swift : fix build
    
    ggml-ci
    
    * swift : try to fix build
    
    ggml-ci
    
    * prompts : add assistant.txt
    
    * common : add llama_batch_add() and llama_batch_clear() helpers
    
    * speculative : minor refactor
    
    ggml-ci
    
    * minor : comments + rename
    
    ggml-ci
    
    * speculative : fix off-by-one for n_drafted
    
    * speculative : fix the n_drafted fix + p constants

commit c67fe68e417f766970fb1feaf2e66458aa24116a
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Wed Oct 18 07:21:48 2023 -0500

    metal : implement q5_0 and q5_1 kernels (#3648)
    
    * metal : implement dequantize_q5_0
    
    * metal : block_q_n_dot_y for block_q5_0 (broken)
    
    * metal : revert unnecessary change
    
    * metal : implement dequantize_q5_1
    
    * metal : block_q_n_dot_y for q5_1 (broken)
    
    * metal : fix block_q_n_dot_y
    
    * minor : spaces / formatting
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 1117d06607d2d885640ac501f05f0aae5494e2c5
Author: shibe2 <shibe@tuta.io>
Date:   Wed Oct 18 16:09:22 2023 +0400

    opencl : fix element-wise multiplication (#3656)

commit cb33f43a2a9f5a5a5f8d290dd97c625d9ba97a2f
Author: slaren <slarengh@gmail.com>
Date:   Tue Oct 17 22:24:50 2023 +0200

    fix embeddings when using CUDA (#3657)

commit e1675d133c31e1c8de2f06be7164e12c0ba6cf2c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 17 22:34:26 2023 +0300

    llama : avoid fprintf in favor of LLAMA_LOG (#3538)

commit 8402566a7c436bfbde8e7b0461faee50298106a0
Author: BarfingLemurs <128182951+BarfingLemurs@users.noreply.github.com>
Date:   Tue Oct 17 14:13:21 2023 -0400

    readme : update hot-topics & models, detail windows release in usage (#3615)
    
    * Update README.md
    
    * Update README.md
    
    * Update README.md
    
    * move "Running on Windows" section below "Prepare data and run"
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 40e5ce054f4c4fa555e4510ea5f760bb29185332
Author: shibe2 <shibe@tuta.io>
Date:   Wed Oct 11 21:30:06 2023 +0400

    CLBlast: Fix temporary buffer size for f16 conversion (wsize)
    
    Fix buffer overflow.
    Reduce the size to fit just one 2D slice.
    Assert sufficient size.

commit a5e8c1d8c71f01d98ae2ec63a57c118664f9764d
Author: slaren <slarengh@gmail.com>
Date:   Tue Oct 17 19:00:58 2023 +0200

    train-text-from-scratch : fix assert failure in ggml-alloc (#3618)

commit e74c705e15cd228ad696c4a3cdea6d6fb4ff434c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 17 19:52:53 2023 +0300

    editorconfig : remove trailing spaces

commit 3ad1e3f1a10c1f66b4f1cd7510e0977fadbc0dfd
Author: coezbek <c.oezbek@gmail.com>
Date:   Tue Oct 17 18:51:02 2023 +0200

    server : documentation of JSON return value of /completion endpoint (#3632)
    
    * Added documentation of JSON return value of /completion endpoint
    
    * Update examples/server/README.md
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 1142013da40e98946a109b141dd858f0ed996051
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 17 19:12:46 2023 +0300

    save-load-state : fix example + add ci test (#3655)
    
    * save-load-state : fix example (close #3606)
    
    * ci : add test for save-load-state example
    
    ggml-ci

commit 5fe268a4d9ce09f3a6c77239af583d3a8e49d54c
Author: ldwang <ftgreat@163.com>
Date:   Tue Oct 17 23:52:33 2023 +0800

    readme : add Aquila2 links (#3610)
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    Co-authored-by: ldwang <ftgreat@gmail.com>

commit 1a159553f921a9209fed8c714494e57b3649f232
Author: staviq <staviq@gmail.com>
Date:   Tue Oct 17 17:11:01 2023 +0200

    tokenizer : special token handling (#3538)
    
    * Rewrite special token handling from #1931
    
    * shorten param name, add st verification by type
    
    * use offsets instead of copy by substr
    
    * formatting, remove copying iterator on delete
    
    * llama : normalize code-style
    
    * swift fix
    
    * print pfx/sfx if verb, main: split pfx input sfx
    
    * dont add space when using special tokens
    
    * minor : comment + spacing
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 281ef73c258cc1eebec8a64264240432d5878c4b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 17 09:19:28 2023 +0300

    k-quants : fix quantization ranges (#3646)

commit 940efa95fec0b8a98c226a889d2ad839dfeeae0d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Oct 16 23:58:00 2023 +0300

    llava : fix tokenization to not add bos between image embeddings and user prompt (#3645)
    
    * llava : fix tokenization to not add bos after system prompt
    
    * set seed
    
    ---------
    
    Co-authored-by: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>

commit 11bff290458f12f020b588792707f76ec658a27a
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Sun Oct 15 02:32:06 2023 -0400

    MPT : support GQA for replit-code-v1.5 (#3627)

commit 11dc1091f64b24ca6d643acc6d0051117ba60161
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Sat Oct 14 13:52:44 2023 +0300

    Honor -ngl option for Cuda offloading in llava (#3621)

commit 2a4bcbacead886996f175f33479d1d874a3e577f
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Oct 13 12:33:16 2023 +0200

    llama : remove n_threads from llama_decode_internal (#3614)
    
    This commit removes `n_threads` from the `llama_decode_internal`
    functions doc comment as it does not exist anymore.
    
    It looks like this parameter was removed in
    Commit 16bc66d9479edd5ee12ec734973554d4493c5dfa ("llama.cpp : split
    llama_context_params into model and context params").
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

commit 424b6381c4daeed62e6600e0402e72f39845b58d
Author: slaren <slarengh@gmail.com>
Date:   Fri Oct 13 12:23:10 2023 +0200

    ggml : add context enumeration functions (#3605)
    
    finetune : fix assert failure in ggml-alloc

commit 1e0e873c373c33989beb6bc64d83cd572ab7fe2b
Author: shibe2 <shibe@tuta.io>
Date:   Thu Oct 12 23:59:47 2023 +0400

    CLBlast: Fix matrix-vector multiplication (#3544)

commit 370359e5baf619f3a8d461023143d1494b1e8fde
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Thu Oct 12 18:23:18 2023 +0300

    examples: support LLaVA v1.5 (multimodal model) (#3436)
    
    * WIP: start implementing LLaVA
    
    * rm scratch buf for now, will revert after cleanup
    
    * LLaVA image encoder is working. will combine with llama
    
    * Add llava inference code, but it's buggy. debugging
    
    * LLaVA is working e2e, needs to optimize memory allocation + cleanup
    
    * Use ggml_allocr + rm unnecessary code
    
    * fix: crlf -> lf
    
    * fix: new line at EoF
    
    * fix: trailing whitespace
    
    * Add readme
    
    * Update readme
    
    * Some cleanup
    
    * Are you happy editorconfig?
    
    * rm unused batch image preprocessing
    
    * rm unused import
    
    * fix: rm designated initializers
    
    * introduce pad-to-square mode for non-square images
    
    * are you happy editorconfig?
    
    * gitignore /llava
    
    * Handle cases where image file does not exist
    
    * add llava target to Makefile
    
    * add support for 13b model variant
    
    * Maybe seed is unlucky?
    
    * Check if apples are compared to apples
    
    * are you happy editorconfig?
    
    * Use temperature = 0.1 by default
    
    * command line: use gpt_params_parse()
    
    * minor
    
    * handle default n_predict
    
    * fix typo
    
    * llava : code formatting, rename files, fix compile warnings
    
    * do not use Wno-cast-qual for MSVC
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 9e24cc6e2e589d405bd1720c400f5b0b9d0ca3ee
Author: uint256_t <maekawatoshiki1017@gmail.com>
Date:   Thu Oct 12 22:36:16 2023 +0900

    docs : fix typo GOMP_CPU_AFFINITY (#3597)

commit d28e572c0270eb72649dbcc3a347e36c05c2934a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Oct 12 14:31:05 2023 +0300

    cmake : fix add_compile_options on macOS

commit f3040beaab5228b1a9dfe5675a200379478f7204
Author: Ian Scrivener <github@zilogy.asia>
Date:   Thu Oct 12 22:10:50 2023 +1100

    typo : it is `--n-gpu-layers` not `--gpu-layers` (#3592)
    
    fixed a typo in the MacOS Metal run doco

commit 1a8c8795d64b04df96c28f29faac2d6e256f53bc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Oct 12 13:44:56 2023 +0300

    ci : check if there is enough VRAM (#3596)
    
    ggml-ci

commit b016596d903641f8825cd94bb6742e1de0c21017
Author: Aarni Koskela <akx@iki.fi>
Date:   Thu Oct 12 15:51:53 2023 +0900

    server : add completion mode (no chat) (#3582)

commit 6b3ae4da92485f979a0f45774fcf68597634db0b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Oct 12 09:35:19 2023 +0300

    prompts : add mnemonics.txt

commit 57dd55e2c742bfc50e0f5c6fb95c14118cff44f6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Oct 12 09:29:04 2023 +0300

    server : fix kv cache management (#3588)

commit b8fe4b5cc9cb237ca98e5bc51b5d189e3c446d13
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 11 23:55:08 2023 +0300

    main : fix session loading bug (#3400)

commit a8bdd65525ae86dea905e9866ad369b53e30ac14
Author: Michael Coppola <m18coppola@gmail.com>
Date:   Wed Oct 11 15:42:22 2023 -0400

    server : add parameter -tb N, --threads-batch N (#3584)
    
    Co-authored-by: Michael Coppola <info@michaeljcoppola.com>

commit 70c29da118cdb02bfcbd0376c32b5b2236e48e48
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Wed Oct 11 13:35:46 2023 -0600

    common : fix mirostat state when using multiple sequences (#3543)
    
    * Fix mirostat state when using multiple sequences
    
    * Fix mirostat by completely refactoring sampling!
    
    * Try to fix zig build.
    
    * Export function to fetch/create default sampler states
    
    Code formatting cleanups and add some comments
    
    Silence a warning about id not being used when logging is disabled
    
    * Apply some renaming suggestions.
    
    Fix comments that were out of sync with the pull.
    
    * Use more consistant naming convention for sampling contexts

commit 8c70a5ff25964f0a81e20d142a2f5ac5baff22fc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 11 21:25:33 2023 +0300

    batched : add bench tool (#3545)
    
    * batched : add bench tool
    
    * batched : minor fix table
    
    * batched-bench : add readme + n_kv_max is now configurable
    
    * batched-bench : init warm-up batch
    
    * batched-bench : pass custom set of PP, TG and PL
    
    * batched-bench : add mmq CLI arg

commit 24ba3d829e31a6eda3fa1723f692608c2fa3adda
Author: Zane Shannon <z@zcs.me>
Date:   Wed Oct 11 04:14:05 2023 -0700

    examples : add batched.swift + improve CI for swift (#3562)

commit 9f6ede19f3cfa50d4a51a5babb056c3f8a450b80
Author: Galunid <karolek1231456@gmail.com>
Date:   Wed Oct 11 01:02:49 2023 +0200

    Add MPT model to supported models in README.md (#3574)

commit 233fc1c69f6f415f35363e18a755f9610e89161b
Author: goerch <jhr.walter@t-online.de>
Date:   Tue Oct 10 18:59:52 2023 +0200

    Minor improvements in GPT2 tokenizer (#3567)
    
    * Fixing minor bugs in bpe_gpt2_preprocess
    
    * Don't add bos token in test

commit c5b49360d0d9e49f32e05a9116e90bd0b39a282d
Author: Xingchen Song(宋星辰) <xingchensong1996@163.com>
Date:   Wed Oct 11 00:28:50 2023 +0800

    readme : add bloom (#3570)

commit 02d2875deff28599c6c2c6e1886fab002ffe43b1
Author: Xingchen Song(宋星辰) <xingchensong1996@163.com>
Date:   Tue Oct 10 22:48:21 2023 +0800

    llm : add bloom models (#3553)
    
    * feat: Support bloom models
    
    * fix(bloom): fix model size
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 0aa6595ae02f97f2e5ffd74bf57a8b21ac83b272
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Tue Oct 10 06:31:13 2023 -0500

    swift : improvements and fixes (#3564)
    
    * swift : use macOS 12 as minimum requirement
    
    * swift : add missing ggml-backend.c source
    
    * swift : add -O3 -DNDEBUG unsafe flags

commit f5f9121de140eff558f13b5c5e78a3a3b6b94377
Author: Jan Ploski <jpl@plosquare.com>
Date:   Tue Oct 10 09:50:23 2023 +0200

    llm : add MPT support (#3417)
    
    * CUDA: added support for ggml_clamp (see also: https://github.com/ggerganov/ggml/issues/545)
    
    * mpt : added an implementation based (mostly) on falcon integration, modified with deltas from ggml/examples/mpt
    
    * mpt : protect against "clip_qkv": null in mpt-7b
    
    * mpt : quick fix to avoid "Strange model" warning when quantizing MPT models
    
    * mpt : addendum to changeset:84e30e8 - leave parameter clamp_kqv out from metadata rather than use 0.0 to indicate "no clamping" (more compliant with the current GGUF spec?)
    
    * mpt : standardized all tensor names to follow GGUF spec
    
    * mpt : addendum to changeset:1be89c40 - use "req" parameter of GGUF_GET_KEY macro instead of duplicate code
    
    * mpt : fixed comment s/gptneox/mpt/
    
    * mpt : remove tabs, trailing whitespace
    
    * mpt : removed ne01 + n_past == ne00 assertion from alibi (cuda/f32) and rope_shift from build_mpt
    
    * mpt : updated convert-mpt-hf-to-gguf.py to reflect changes made to convert-gptneox-hf-to-gguf.py in pr:3252
    
    * comment out n_past instead of marking it unused
    
    * mpt : removed hardcoded +178 from convert script in favor of utilizing hparams["vocab_size"]
    
    * mpt : remove unused tokenizer_json in convert script
    
    * ggml : remove obsolete n_past assert in ggml_alibi
    
    * llama : print clam_kqv and max_alibi_bias hparams
    
    ---------
    
    Co-authored-by: Cebtenzzre <cebtenzzre@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 11ea5c7d96f2c28e1c99659e08ec0a44574056e2
Author: vvhg1 <94630311+vvhg1@users.noreply.github.com>
Date:   Tue Oct 10 09:31:21 2023 +0200

    infill. : fix tokenization (#3508)
    
    * infill tokens correction
    
    * serverinfill tokens correction
    
    * removing any leading whitespace from infill suffix and removing leeading space token from suffix when params.escape
    
    * removing any leading whitespace from infill suffix and removing leeading space token from suffix when params.escape
    
    * only rm when params.escape, rm space if possible which is added back or rm added space token
    
    * only rm when params.escape, rm space if possible which is added back or rm added space token
    
    * Revert "only rm when params.escape, rm space if possible which is added back or rm added space token"
    
    This reverts commit 63ba0b621f21077c0e3bc6ba6a327534123cb738.
    
    * fix interactive prompt escaping and fix server infill leading space handling
    
    * rm unnecessary bool check

commit 95bd60a0a69f57e9a2ff1269667ea484a1a9bb40
Author: slaren <slarengh@gmail.com>
Date:   Mon Oct 9 14:44:58 2023 +0200

    ggml-alloc : fix assert in debug builds (#3555)

commit fcca0a700487999d52a525c96d6661e9f6a8703a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Oct 9 14:32:17 2023 +0300

    refact : fix convert script + zero out KV cache to avoid nans (#3523)
    
    * refact : fix convert script + zero out KV cache to avoid nans
    
    * ggml : silu(-inf) should never happen
    
    * metal : assert various kernel requirements

commit dcc09d25961c5d0626bc148e558ee841141748f7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Oct 9 14:28:27 2023 +0300

    metal : do not use mul_mm kernels when ne00 < 64 (#3542)

commit db3abcc114d5d1790ba814aa1a80ac673d4ccc3e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 8 20:19:14 2023 +0300

    sync : ggml (ggml-backend) (#3548)
    
    * sync : ggml (ggml-backend)
    
    ggml-ci
    
    * zig : add ggml-backend to the build

commit eee42c670e6fa6df9cf17e7ffc319f74cbd81354
Author: Matheus C. França <matheus-catarino@hotmail.com>
Date:   Sun Oct 8 10:59:20 2023 -0300

    ci : add Zig CI/CD and fix build (#2996)
    
    * zig CI/CD and fix build
    
    Signed-off-by: Matheus Catarino França <matheus-catarino@hotmail.com>
    
    * fix build_compiler
    
    * ci : remove trailing whitespace
    
    ---------
    
    Signed-off-by: Matheus Catarino França <matheus-catarino@hotmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 8e6716a102e390e930594d51302730184dac83cc
Author: Ryder Wishart <ryderwishart@gmail.com>
Date:   Sun Oct 8 03:55:58 2023 -0700

    api_like_OAI.py : compat with Microsoft Guidance (#2746)
    
    Check for None in addition to empty string check in all request params
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 9c38d181d40b9d27f8f42152c18e7c70bfffcf37
Author: arcrank <arcrank@gmail.com>
Date:   Sun Oct 8 06:52:57 2023 -0400

    api_like_OAI.py : simplify function (#2796)
    
    Simplify function

commit a1202a31ed8c35705bd09fe91c3e7410c619bd70
Author: Johannes Rudolph <johannes.rudolph@gmail.com>
Date:   Sun Oct 8 12:21:19 2023 +0200

    k-quants : fix comments about block sizing (#3499)

commit 94e502dfb79430870b42b8e8ee132b4aaa93e4a8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 8 11:24:50 2023 +0300

    ci : enable on obj-c changes + fix metal build (#3540)

commit 7d8b24932fe788a4cda76459a0c5df3e0073cb98
Author: Luo Tian <lt@basecity.com>
Date:   Sun Oct 8 16:24:01 2023 +0800

    zig : fix build by introducing train.cpp (#3539)

commit b0ec5218c3d24755786b80ecce9cf4ffc07583f8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 8 10:01:53 2023 +0300

    metal : support MTLGPUFamily < Apple7, formatting, style (#3524)
    
    * metal : improve decoding speed for batches of 2-16
    
    * metal : rename kernels mul_mat_ to mul_mv_
    
    * metal : indentations
    
    * minor
    
    * metal : print more GPU info + disable mul_mm for MTLGPUFamiliy < Apple7

commit 63d3b06a4318329f92b078e8aa0be7ab6e9f871f
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Oct 7 23:22:17 2023 -0600

    llama : fix missing break in Persimmon arch case statements (#3535)

commit a16e89cec83b4bd5f6af8f1ce1400f94c12356f9
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Oct 7 15:31:41 2023 -0600

    Fix trying to strip newline from empty prompt and cfg prompt file content (#3534)

commit 4d0383321184aadf91968d9e3c6a45286ed2473b
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Sat Oct 7 22:14:10 2023 +0300

    gguf.py : fix CI for publishing GGUF package (#3532)
    
    * Fix CI for publishing GGUF package
    
    * Bump version
    
    * fix
    
    * bump version
    
    * bump version
    
    * bump version

commit c47066d833c6c112e0d23342aa62c3250dd33c81
Author: Tom C <tom.corelis@gmail.com>
Date:   Sat Oct 7 02:56:15 2023 -0700

    py : change version of numpy requirement to 1.24.4 (#3515)
    
    Co-authored-by: Lyjia <me@lyjia.us>

commit f1782c68de13b64bb5283fc2038f584e47be9fd2
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Sat Oct 7 04:41:52 2023 -0400

    quantize : fail fast on write errors (#3521)

commit c26765a0a148b47e5b541df32438c3ad2a0a8314
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Sat Oct 7 03:40:27 2023 -0500

    metal : support default.metallib load & reuse code for swift package (#3522)
    
    * metal : support load default.metallib & reuse code for swift package
    
    * metal : use SWIFT_PACKAGE def instead of define GGML_SWIFT

commit 0e797c2fc571b866090f7d60ac7d39d8533593f2
Author: Phillip Kravtsov <phillip@kravtsov.net>
Date:   Sat Oct 7 00:12:43 2023 -0700

    llm : support Adept Persimmon 8B (#3410)
    
    * Produces garbage output
    
    * wip: correct tensors up to RoPE
    
    * correct tensors thru RoPE
    
    * Correct outputs through masked & softmax'd KQ
    
    * fp32 works
    
    * Rename adept->persimmon
    
    * Produces correct outputs
    
    * clean up convert scripts
    
    * remove printing logic from ggml.c
    
    * remove prints from llama.cpp & fix merge
    
    * trivial cleanups
    
    * Add offload funcs
    
    * update conversion script to directly take adept artifacts rather than .saftensors file
    
    * Fix norm eps bug
    
    * Support sqr and concat on metal, persimmon-8b-q4 runs correctly
    
    * Small changes from review
    
    * Formatting changes
    
    * Minor changes to conversion script
    
    * Remove old script
    
    * Fix editorconfig formatting
    
    * Fix build
    
    * add overlooked offload code ggml-ci

commit 3a716b4dae545c3db307594fbc509a95d3e21b6e
Author: goerch <jhr.walter@t-online.de>
Date:   Sat Oct 7 06:57:01 2023 +0200

    Fix for #3454 (#3455)
    
    Fix: `sentencepiece` tokenizers with added tokens failed with an incorrect assertion

commit 1faaae8c2bdc4a21302e367e0754c3fe74a8113e
Author: BarfingLemurs <128182951+BarfingLemurs@users.noreply.github.com>
Date:   Fri Oct 6 15:13:36 2023 -0400

    readme : update models, cuda + ppl instructions (#3510)

commit cb13d73a720c42d1958bff79b6869d77b26b8cea
Author: Mihai <mihai.chirculescu@yahoo.com>
Date:   Fri Oct 6 21:39:33 2023 +0300

    server : docs fix default values and add n_probs (#3506)

commit 9ca79d5cbbc8d43f2bff951404b6a40ff1ee3788
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Fri Oct 6 10:10:13 2023 -0600

    kv cache slot search improvements (#3493)
    
    * kv cache slot search improvements
    
    * Use n_ctx in kv find slot for consistency
    
    * Ensure kv cache head points to a valid slot in llama_decode internal
    
    * Add some comments to prevent dumb people (like me) from getting confused.

commit 0c731ca4039ccff86ffab90eaae4ca98037c4496
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Oct 6 16:35:55 2023 +0300

    prompts : fix editorconfig checks after #3416

commit a8777ad84e00cda0399e827cdf971e2c3fab1da2
Author: pudepiedj <pudepiedj@gmail.com>
Date:   Fri Oct 6 14:16:38 2023 +0100

    parallel : add option to load external prompt file (#3416)
    
    * Enable external file and add datestamp
    
    * Add name of external file at end
    
    * Upload ToK2024
    
    * Delete ToK2024.txt
    
    * Experiments with jeopardy
    
    * Move ParallelQuestions to /proimpts and rename
    
    * Interim commit
    
    * Interim commit
    
    * Final revision
    
    * Remove trailing whitespace
    
    * remove cmake_all.sh
    
    * Remove cmake_all.sh
    
    * Changed .gitignore
    
    * Improved reporting and new question files.
    
    * Corrected typo
    
    * More LLM questions
    
    * Update LLM-questions.txt
    
    * Yet more LLM-questions
    
    * Remove jeopardy results file
    
    * Reinstate original jeopardy.sh
    
    * Update examples/parallel/parallel.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 97af49fa395df77e4c18af0e1655b2fee67c9686
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Fri Oct 6 07:44:24 2023 -0500

    server : reuse llama_sample_token common util (#3494)
    
    * server : reuse llama_sample_token common function
    
    * common : use n_probs for temperature sampling

commit 16820a5a0d885113f21021ce934f0b0027b9d69a
Author: l3utterfly <gc.pthzfoldr@gmail.com>
Date:   Fri Oct 6 18:47:59 2023 +0800

    llama : correct hparams comparison (#3446)
    
    * fixed floating point comparison issues
    
    * updated implementation for hparam comparison to handle inf and NaN
    
    * fixed code review comments
    
    * minor simplification
    
    * rename is_float_eq -> is_float_close
    
    ---------
    
    Co-authored-by: Cebtenzzre <cebtenzzre@gmail.com>

commit 04b2f4386eda0264287156104cbf9d1b87895422
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Fri Oct 6 05:36:43 2023 -0500

    ci : fix xcodebuild destinations (#3491)
    
    * ci : fix xcodebuild destinations
    
    * ci : add .swift to paths

commit 48edda30ee545fdac2e7a33d505382888f748bbf
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Oct 5 15:00:34 2023 -0400

    convert : update Falcon script for new HF config (#3448)
    
    Also adds Falcon-180B support.
    Closes #3049
    
    Co-authored-by: jb <jonathan.t.barnard@gmail.com>

commit 45eba9369fbcbd7f677eba9a2d3e4ffcfdc81824
Author: Kenvix ⭐ <kenvixzure@live.com>
Date:   Fri Oct 6 01:16:39 2023 +0800

    build : use std::make_tuple() for compatibility with older GCC versions (#3488)

commit acec9eaaa93315711c11d15afa8d245d164b7cff
Author: staviq <staviq@gmail.com>
Date:   Thu Oct 5 18:17:29 2023 +0200

    common : process escape sequences in reverse prompts (#3461)

commit e2583cbc29cd7d6d1403f338842c07dfc0467e6c
Author: shibe2 <shibe@tuta.io>
Date:   Thu Oct 5 15:57:03 2023 +0400

    CLBlast: Fix handling of on-device tensor data
    
    Fix uploading tensor data to device, including 3D, 4D, and non-contiguous tensors.
    Use correct offsets into data that is already in VRAM.
    Correct handling of OpenCL events when multiple commands are queued.

commit e8b8d32e8663ffc55a02c9721af3a5190382cbb0
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Thu Oct 5 09:02:55 2023 -0500

    server : fix incorrect num_tokens_predicted (#3480)

commit 8f3a642ec1d878b2d0a0d15e3a4277f522790d4c
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Thu Oct 5 09:00:07 2023 -0500

    swift : disable ACCELERATE_NEW_LAPACK (#3481)

commit 0745384449fe8d89d6d99c93153569079e853247
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Thu Oct 5 08:56:21 2023 -0500

    ci : add swift build via xcodebuild (#3482)

commit 019ba1dcd0c7775a5ac0f7442634a330eb0173cc
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Wed Oct 4 08:20:28 2023 -0600

    convert : fix Baichuan2 models by using vocab size in config.json (#3299)
    
    Use local GGUF package when possible in Baichuan converter

commit beabc8cfb0145b48aad68fefc573d316fe9c3a8a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 4 16:50:44 2023 +0300

    readme : add project status link

commit 0d152b37fecd5a4838330d47bb034cebf1681779
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 4 16:25:41 2023 +0300

    ggml : fix build after #3329

commit f8c90cdbaa729e64493164c1aba7ea80da7b716f
Author: ds5t5 <145942675+ds5t5@users.noreply.github.com>
Date:   Wed Oct 4 06:23:39 2023 -0700

    llm : add Refact model (#3329)
    
    * add refact model
    
    * resolve comments
    
    * rebase to the latest
    
    * solve alibi cpu error
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit f93af02488179b9c52d0d391b08ae4c4d891b8d3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 4 15:29:58 2023 +0300

    sync : ggml (conv 1d + 2d updates, UB fixes) (#3468)
    
    * sync : ggml (conv 1d + 2d updates)
    
    ggml-ci
    
    * ggml : fix UB in q5_0 and q5_1 quantize code
    
    ggml.c:1033:39: runtime error: left shift of 1 by 31 places cannot be represented in type 'int'
    SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior
    
    ggml.c:1081:39: runtime error: left shift of 1 by 31 places cannot be represented in type 'int'
    SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior
    
    ggml-ci
    
    * tests : fix UB in test-quantize-perf

commit f72f8f22c9cb60465b2e79df2767e4ba9604e576
Author: Merrick Christensen <merrick.christensen@gmail.com>
Date:   Wed Oct 4 00:33:13 2023 -0600

    finetune : readme fix typo (#3465)
    
    Fix small typo

commit 79f34abddb72ac5ddbf118f3d87520b611a10a7d
Author: Tameem <113388789+AhmadTameem@users.noreply.github.com>
Date:   Tue Oct 3 23:38:19 2023 +0500

    ggml : add RISC-V Vector Support for K-Quants and improved the existing intrinsics (#3453)
    
    * Added RVV intrinsics support for Q8 quantize row and also improved the existing dot product function for risc-v.
    
    The RVV intrinsics is added for the following quantize row functions
       quantize_row_q8_0
       quantize_row_q8_1
    
    The following dot product functions have also been optimized by using LMUL = 1/2 instead of LMUL = 1
       ggml_vec_dot_q4_0_q8_0
       ggml_vec_dot_q4_1_q8_1
       ggml_vec_dot_q5_0_q8_0
       ggml_vec_dot_q5_1_q8_1
    
    And vector initialization in Q5 by temporary array is also replaced by the vid intrinsics
    
    Signed-off-by: Ahmad Tameem <ahmad.tameem@10xengineers.ai>
    
    * Added RVV intrinsics support for k_quants
    
    This adds RISC-V Vector intrinsics support for the following K_quants functions for both QKK = 256 and QKK = 64
       ggml_vec_dot_q2_K_q8_K
       ggml_vec_dot_q3_K_q8_K
       ggml_vec_dot_q4_K_q8_K
       ggml_vec_dot_q5_K_q8_K
       ggml_vec_dot_q6_K_q8_K
    
    Signed-off-by: Ahmad Tameem <ahmad.tameem@10xengineers.ai>
    
    ---------
    
    Signed-off-by: Ahmad Tameem <ahmad.tameem@10xengineers.ai>

commit 8186242b6d67cf87ae179fb1a62f52fdf0e5c5eb
Author: h-h-h-h <13482553+h-h-h-h@users.noreply.github.com>
Date:   Tue Oct 3 20:16:15 2023 +0200

    main : consistent prefix/suffix coloring (#3425)
    
    * Typo
    
    * No `--in-prefix` coloring
    
    The `--in-prefix` text was inconsistently colored. Now, it's never colored, just like the `--in-suffix` text.

commit ac2219fef34eb5b713c286c34c6e4162c39c8f3b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 3 21:04:01 2023 +0300

    llama : fix session saving/loading (#3400)
    
    * llama : fix session saving/loading
    
    * llama : temp fix for clearing "future" tokens from the KV cache
    
    * llama : fix handling of "future" tokens when loading sessions
    
    * llama : fix comments for llama_kv_cache API

commit 48be797ffbd80b062f55778e09e97180eb25d2ab
Author: Alex Klinkhamer <git@grencez.dev>
Date:   Tue Oct 3 10:09:28 2023 -0700

    llama : expose model's rope_freq_scale in the API (#3418)
    
    so it can be scaled further before creating a context.

commit f56e1baec361b5381e32ee6b6e56e4f00e002dfe
Author: Jiahao Li <liplus17@163.com>
Date:   Wed Oct 4 00:55:21 2023 +0800

    metal : alibi for arbitrary number of heads (#3426)

commit 017efe899d8fa76118aef88e963210d48da01172
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Tue Oct 3 16:53:15 2023 +0000

    cmake : make LLAMA_NATIVE flag actually use the instructions supported by the processor (#3273)
    
    * fix LLAMA_NATIVE
    
    * syntax
    
    * alternate implementation
    
    * my eyes must be getting bad...
    
    * set cmake LLAMA_NATIVE=ON by default
    
    * march=native doesn't work for ios/tvos, so disable for those targets. also see what happens if we use it on msvc
    
    * revert 8283237 and only allow LLAMA_NATIVE on x86 like the Makefile
    
    * remove -DLLAMA_MPI=ON
    
    ---------
    
    Co-authored-by: netrunnereve <netrunnereve@users.noreply.github.com>

commit ff5a3f0c09dfa0a8e0bf76d1748df5c6dee0e8ff
Author: goerch <jhr.walter@t-online.de>
Date:   Tue Oct 3 09:16:26 2023 +0200

    Work on the BPE tokenizer (#3252)
    
    * Work on the BPE tokenizer
    
    Tokenizer tests work for Falcon-7B
    
    * Try to fix build problem
    
    * Fix debug assertion failure
    
    * Fix MSVC Unicode BOM problem
    
    * Cleanup and an improvement
    
    * Fix compiler warning
    
    * Cleanup
    
    * Test doesn't work over the full range of Unicodes
    
    * Update .gitignore and Makefile
    
    * Another Makefile rule
    
    * Testing Aquila
    
    * Moving byte decoding back to `token_to_piece` ...
    
    ... because everyone is using it.
    
    * Guarding some unusable code pathes
    
    * Streamlining code and adding some more assertions
    
    Important change: I'm classifying added tokens as control tokens now for BPE.
    
    * Adding a comment
    
    * Adding another assertion
    
    * Fixed vocabulary guarding assertions
    
    * Fix PR for recent change
    
    * Fix PR for recent change
    
    * Fix for compiler warning
    
    * Fix PR for recent change
    
    * Fix PR for recent change
    
    * Fix PR for recent change
    
    * Fix for compiler warning
    
    * Fixes for more compiler warnings
    
    * Remove unused code
    
    * Fix initialization of static maps
    
    * Add scores and token types back, adapt gptneox
    
    * Update llama.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update unicode.h
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update unicode.h
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Ported Starcoder and added some assertions
    
    * Fix coding style
    
    * Apply @jploski 's fix for missing tokens
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 1c84003c08027f5d3a4cb876f51d6b6224a34d0e
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Mon Oct 2 18:07:24 2023 -0400

    convert : fix vocab size when not defined in hparams (#3421)

commit e78f0b0d0572168f328dd0e2ed3175a53fe52acc
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Mon Oct 2 15:38:43 2023 -0400

    cmake : increase minimum version for add_link_options (#3444)

commit 665018c749101e81c816675198e731e47d6b1dbe
Author: shibe2 <shibe@tuta.io>
Date:   Mon Oct 2 23:26:15 2023 +0400

    CLBlast: Add broadcast support for matrix multiplication (#3402)
    
    Broadcast src0 into src1 across dimensions 2 and 3 when needed.
    This is required for models that use GQA.

commit 29a404a951fb0b3f9c3b6ab8c4c9c76ac50d2bb3
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Mon Oct 2 15:20:28 2023 -0400

    gguf : add BERT, MPT, and GPT-J arch info (#3408)

commit 0fe321031a5c670ab5fb5f49d69c4c91d783c93f
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Mon Oct 2 14:58:46 2023 -0400

    gguf : general usability improvements (#3409)

commit 9476b012260a2fb6c67976582d64484ce7406ed9
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Mon Oct 2 09:16:50 2023 -0400

    cmake : make CUDA flags more similar to the Makefile (#3420)
    
    * cmake : fix misuse of cxx_flags
    
    * cmake : make CUDA flags more similar to the Makefile
    
    * cmake : fix MSVC build

commit a03ce38455544121c5c00cf845def1443acd6ac8
Author: xaedes <xaedes@gmail.com>
Date:   Mon Oct 2 15:15:45 2023 +0200

    finetune : fix #3404 (#3437)
    
    the shapes for init model of gqa models was wrong

commit a84767698495d72e44044f1f6db1c1cc721bfd15
Author: Adrian <smith.adriane@gmail.com>
Date:   Mon Oct 2 03:49:59 2023 -0700

    metal : set log callback before initializing (#3427)

commit 095231dfd32679e32300f8ffaf1770b693ea64b0
Author: bandoti <141645996+bandoti@users.noreply.github.com>
Date:   Mon Oct 2 06:51:49 2023 -0300

    cmake : fix transient definitions in find pkg (#3411)

commit ea55295a745c084f588be20710f5a1a12abb1109
Author: Kevin Ji <1146876+kevinji@users.noreply.github.com>
Date:   Mon Oct 2 04:53:53 2023 -0400

    docker : ignore Git files (#3314)

commit c97f01c362ac102c6994edb80008f8608539553a
Author: vvhg1 <94630311+vvhg1@users.noreply.github.com>
Date:   Mon Oct 2 09:42:02 2023 +0200

    infill : add new example + extend server API (#3296)
    
    * vvhg-code-infill (#1)
    
    * infill in separate example (#2)
    
    * reverted changes to main and added infill example
    
    * cleanup
    
    * naming improvement
    
    * make : add missing blank line
    
    * fix missing semicolon
    
    * brought infill up to current main code
    
    * cleanup
    
    ---------
    
    Co-authored-by: Cebtenzzre <cebtenzzre@gmail.com>

commit f5ef5cfb18148131fcf45bdd2331f0db5ab7c3d0
Author: slaren <slarengh@gmail.com>
Date:   Sat Sep 30 18:12:57 2023 +0200

    ggml-cuda : perform cublas mat mul of quantized types as f16 (#3412)
    
    * ggml-cuda : perform cublas matrix multiplication of quantized types as fp16
    
    * rename CC_TURING to CC_VOLTA
    
    * disable fp16 mat mul completely with multi GPU

commit 40e07a60f9ce06e79f3ccd4c903eba300fb31b5e
Author: slaren <slarengh@gmail.com>
Date:   Fri Sep 29 18:42:32 2023 +0200

    llama.cpp : add documentation about rope_freq_base and scale values (#3401)
    
    * llama.cpp : add documentation about rope_freq_base and scale values
    
    * add notice to hot topics

commit bc34dd4f5b5a7c10ae3ed85a265ce6f2ed2fab79
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 29 19:05:18 2023 +0300

    train : fix KQ_pos allocation (#3392)
    
    * train : fix KQ_pos allocation
    
    * make sure KQ_pos is not reallocated in finetune
    
    ---------
    
    Co-authored-by: xaedes <xaedes@gmail.com>

commit 2777a84be429401a2b7d33c2b6a4ada1f0776f1b
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Sep 29 09:48:45 2023 -0400

    llama : quantize up to 31% faster on Linux and Windows with mmap (#3206)
    
    * llama : enable mmap in quantize on Linux -> 31% faster
    
    * also enable mmap on Windows
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 0a4a4a098261ddd26480371eaccfe90d1bf6488a
Author: BarfingLemurs <128182951+BarfingLemurs@users.noreply.github.com>
Date:   Fri Sep 29 08:50:35 2023 -0400

    readme : update hot topics + model links (#3399)

commit 569550df20c1ede59ff195a6b6e900957ad84d16
Author: Andrew Duffy <a10y@users.noreply.github.com>
Date:   Fri Sep 29 07:15:57 2023 -0400

    readme : add link to grammars app (#3388)
    
    * Add link to grammars app per @ggernagov suggestion
    
    Adding a sentence in the Grammars section of README to point to grammar app, per https://github.com/ggerganov/llama.cpp/discussions/2494#discussioncomment-7138211
    
    * Update README.md

commit c71bf2c45c5140203184f50b259828107658e900
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Fri Sep 29 13:25:13 2023 +0800

    swift : fix build on xcode 15 (#3387)

commit bc39553c901a91cfcb757863586250838c83eeab
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Sep 28 17:41:44 2023 -0400

    build : enable more non-default compiler warnings (#3200)

commit 0ccfc62a96a6b59a8faa14d1b350493f4cd51ae2
Author: Hua Jiang <allenhjiang@outlook.com>
Date:   Thu Sep 28 13:06:18 2023 -0700

    ggml_tensor: update the structure comments. (#3283)
    
    * ggml_tensor: update the structure comments.
    
    * remove semicolon
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Update ggml.h
    
    ---------
    
    Co-authored-by: Cebtenzzre <cebtenzzre@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>

commit 7f1a0fe709ea1a861da2f3759f58a28bf8953c12
Author: Qu Zongfu <43257352+yancaoweidaode@users.noreply.github.com>
Date:   Fri Sep 29 03:51:52 2023 +0800

    ggml : release the requested thread pool resource (#3292)
    
    * Release the requested thread pool resource
    
    * Release the requested thread pool resource 2
    
    ---------
    
    Co-authored-by: Zongfu ZF3 Qu <quzf3@Lenovo.com>

commit 16bc66d9479edd5ee12ec734973554d4493c5dfa
Author: slaren <slarengh@gmail.com>
Date:   Thu Sep 28 21:42:38 2023 +0200

    llama.cpp : split llama_context_params into model and context params (#3301)
    
    * llama.cpp : split llama_context_params into model and context params
    
    ggml-ci
    
    * fix metal build
    
    * fix freq_base/scale default to model value
    
    * llama-bench : keep the same model between tests when possible
    
    * move n_threads to llama_context_params, add n_threads_batch
    
    * fix mpi build
    
    * remove kv_size(), cuda scratch fixes
    
    * remove low-vram option
    
    * add n_threads_batch to system info, refactor to get_system_info()
    
    * add documentation about --threads-batch to the READMEs
    
    * llama-bench fix
    
    * main : fix rope freq/scale warning
    
    * llama.cpp : add llama_get_model
    common : add llama_tokenize from model
    
    * remove duplicated ctx/model functions
    
    ggml-ci
    
    * cuda : print total VRAM used

commit 0512d66670de3f650c579519833c085014b0f200
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Thu Sep 28 19:31:04 2023 +0000

    ci : multithreaded builds (#3311)
    
    * mac and linux threads
    
    * windows
    
    * Update build.yml
    
    * Update build.yml
    
    * Update build.yml
    
    * automatically get thread count
    
    * windows syntax
    
    * try to fix freebsd
    
    * Update build.yml
    
    * Update build.yml
    
    * Update build.yml

commit 0e76a8992c8200237bbc6471a53fb8796b3872f7
Author: xaedes <xaedes@gmail.com>
Date:   Thu Sep 28 20:40:11 2023 +0200

    train : finetune LORA (#2632)
    
    * fix track_max_mem in forward_batch_wo_cache_flash_attn_train
    
    * remove unnecessary Adam(W) optimizer tensors.
    
    reduces optimizer memory overhead from 7*modelsize to 2*modelsize.
    
    additionally allows to optimize models with more than 2^31 parameters by replacing int with int64_t.
    
    bumps training checkpoint file version, but old checkpoints can still be read.
    new version with less tensors is saved.
    
    * add gradient clipping to AdamW
    
    * Fix reset of unused g->nodes and g->grads to NULL
    
    * implement gradient checkpointing for training
    
    reduces memory overhead from O(n_layer) to O(sqrt(n_layer))
    
    as explained in readme of https://github.com/cybertronai/gradient-checkpointing
    
    * remove unused compute buffer 3
    
    * add and use function ggml_build_backward_expand to avoid stack overflows with large maximum number of nodes
    
    GGML_API void ggml_build_backward_expand(struct ggml_context * ctx, struct ggml_cgraph * gf, struct ggml_cgraph * gb, bool keep);
    
    * change AdamW decay parameter to work like the torch AdamW decay parameter
    
    It is now relative to Adam learning rate `alpha*sched`.
    Before that it was relative to `sched` only.
    
    `alpha` being the maximum learning rate and `sched` being a scaling parameter in [0..1]
    
    * change default AdamW weight decay parameter used in training to 0.1 as used in nanoGPT
    
    * change default AdamW weight decay parameter defined in ggml to 0.0, making Adam default instead of AdamW
    
    btw: the default weight decay parameter for torch.optim.AdamW is 0.01
    
    * bug fixes for cross entropy loss
    
    ggml_cross_entropy_loss: sums where not correctly added in workload of each thread
    ggml_cross_entropy_loss_back: simplify backward process, reducing numerical issues
    
    guard usage of exp f16 lookup in cross entropy by #define GGML_CROSS_ENTROPY_EXP_FP16
    
    cross entropy loss is only used once during training, but it is quite sensitive to numerical errors introduced by exp-f16-lookup.
    so exp-f16-lookup for cross entropy loss is disabled by default, trading better gradients for very slightly worse runtime performance.
    
    * fix test-grad0 for cross_entropy_loss
    
    the second argument to cross_entropy_loss must sum up to 1 for each row
    
    * fix test-grad0 for soft_max
    
    dont use only sum as aggregation, because sum of softmax is always 1 -> finite differences should not work
    instead use sum(log(soft_max()*(1-eps)+eps)); use eps to avoid log(0)
    
    * improve finite differences of test-grad0 by using double instead of float
    
    * change cross_entropy_loss to output average over all rows
    
    this helps keeping the loss and gradients in a sane range
    
    * improve gradient checkpointing
    
    sqrt(n_layers) is only the best checkpoint step when mem size of checkpoints and mem size of layers are equal.
    since layers require more memory than the single-tensor-checkpoint we use, the optimal values are compute different:
    
    ```
      given: n, u, v
      objective: minimize(a*u+b*v) where a*b=n, a>0, b>0
      b=n/a
      minimize(a*u+v*n/a)
      diff(a*u+v*n/a, a) = u - (v*n/a)/a
      diff(a*u+v*n/a, a) == 0
      u - (v*n/a)/a == 0
      u == v*n/(a*a)
      u*a*a = v*n
      a*a = v*n/u
      a = sqrt(n*v/u)
    ```
    
    this change results in more checkpoints, requiring less layers to store between checkpoints, overall improving memory usage.
    
    * disable gradient checkpointing debug output
    
    * llama : fix rope usage in train-text-from-scratch after ChatGLM change
    
    * add more training parameters:
    
    --enable-restart N         Only for Adam optimizer. Enable restarts of cos-decay
    --disable-restart N        Only for Adam optimizer. Disable restarts of cos-decay
    --opt-past N               Number of optimization iterations to track for delta convergence test. Disabled when zero.
    --opt-delta N              Maximum delta for delta convergence test. Disabled when <= zero.
    --opt-max-no-improvement N Maximum number of optimization iterations with no improvement. Disabled when <= zero.
    --adam-epsf N              AdamW epsilon for convergence test. Disabled when <= zero.
    --adam-min-alpha N         Adam minimum learning rate alpha, usually 0.1 * alpha
    
    * replace memcpy with reshape operation so that the graph is not cut at the input
    
    this makes it possible to store other values into the input tensor and then simply recompute the graph without rebuilding it
    
    * remove unused function argument from get_example_targets_batch
    
    * measure and print total training time
    
    * add optimization callback to ggml_opt_resume_g
    
    this callback is called before each iteration with custom data and pointer to learning schedule parameter (only used in Adam(W)).
    
    can be used for dynamic learning schedule and setting input data for batches before each iteration
    
    * use optimization callback in training
    
    allows dynamic learning schedule and different batch data for each iteration without relying on low n_iter and high n_examples parameters
    
    reduces runtime by avoiding restart of optimization function and improves training convergence by providing a different batch for each iteration
    
    * add minimum number of tensor dimensions to apply weight decay (default 2)
    
    this allows to not apply weight decay to bias parameters
    
    * rename training parameter cos-decay-alpha to cos-decay-min and clarify that adam-min-alpha also applies to warmup
    
    * fix increase of model.train_samples and model.train_tokens
    
    now that each optimizer iteration gets its own batch we need to multiply by number of opt iterations
    
    * change sampling parameters for prediction after training to defaults of common.h
    
    and clarify what is context for prediction and what are generated tokens
    
    * tighten abs error bounds for cross_entropy_loss in test-grad0
    
    * add conditional compilation of using F16 exp in flash attention
    
    uncomment `// #define GGML_FLASH_ATTN_EXP_FP16` to enable usage of f16 exp in flash attention
    
    * tighten abs error bounds for flash_attn in test-grad0
    
    * tighten abs error bounds for sqrt in test-grad0
    
    * remove out-commented vectorized code of opt_adam
    
    the vectorized code might be bit faster for low number of parameters, but it had a big memory usage overhead
    
    * ggml : update ggml_rms_norm_back with configurable eps
    
    * llama training : fix ggml_rms_norm_back calls to pass configurable eps
    
    * remove trailing whitespace
    
    * add train function using automatic gradient checkpointing backward pass and allocator
    
    * in train function replace add_inplace by regular add
    
    because using add_inplace seems to result in different gradients
    
    * don't use allocate hash_map on context
    
    because the context has no_alloc=True when using memory allocator resulting in NULL data pointers
    
    * correctly clone reshape and permute operations by also cloning tensor->nb values
    
    * fix variable name and add missing type cast
    
    * terminate recursive tensor cloning when reaching tensor without src tensors
    
    * correctly clone view tensors by setting data pointers
    
    without this the checkpointing would only work when being used together with memory allocator
    
    * fix variable names
    
    * swap arguments to commutative ops to be the same as in `forward_batch_wo_cache_flash_attn`
    
    * add input tensors as checkpoints
    
    so that recursive tensor cloning of gradient checkpointing terminates on input tensors
    
    * fix variable name and add missing boolean negation
    
    * make sure some tensors are not reallocated by inserting new temporary nodes depending on them:
    
    output and parameter gradient tensors need to be available at the end of the graph execution
    
    parameter gradient tensors also need to be available before the graph execution because they are set to zero before each optimizer iteration
    
    checkpoint tensors are allocated all together to reduce memory allocator fragmentation
    
    afterwards, in addition to the temporary nodes, we also need to reset the temporary leafs
    
    * fix ASSERT to work with zero layers
    
    * add training options whether to use allocator and/or unified training function
    
    * integrate unified training function which may use memory allocator
    
    the unified training function also supports arguments whether to use flash attention and/or gradient checkpointing
    
    * format name of cloned tensors with " (clone)" suffix
    
    * set names for tensors in unified train function for easier debugging
    
    * allocate graph on context using ggml_new_graph
    
    * remove handwritten training functions
    
    * remove unused training parameters "use_scratch" and "use_unified"
    
    * remove trailing whitespace
    
    * remove unused train params: mem_compute1_gb & mem_compute2_gb
    
    mem_compute_gb is used for compute when automatic memory allocator is not enabled, otherwise it can be very small to only hold the tensor definitions
    mem_compute0_gb is used for automatic memory allocator (as long as measurement of max required size is not implemented)
    
    * remove unused forward_batch function
    
    * add debug asserts in ggml_allocr_alloc to some common pitfalls when using this function directly
    
    * only use ggml_allocr_alloc when tensor has NULL data and is no view
    
    * fix test when to create temporary backward graph
    
    temporary backward graph is only necessary when using checkpointing
    
    * fix memory "leak" in optimizers
    
    each iteration a new cplan with new memory for work data was allocated.
    now cplan creation only happens at the start of optimization, with each iteration reusing the cplan and its work data.
    
    * reverse order of for loop in ggml_build_backward_expand to save memory when using gradient checkpointing and allocator
    
    with this loop order gradient checkpointing with allocator on 16 layer model saves 13% memory; 2 layer memory it saves 2% memory.
    
    the computation results are the same
    
    * add API functions to access llama model tensors
    
    * add stub example for finetuning, based on train-text-from-scratch
    
    * move and remove code
    
    * add API functions to access remaining model parameters:
    
    mult, head and rot
    
    * first draft for LORA finetune training
    
    * remove const model and layer arguments in API functions for accessing model tensors
    
    * bug fixes to make finetune compile
    
    automatic allocator does not work yet
    
    * add debug prints for training memory improvements
    
    * fix names of lora tensors
    
    * avoid stack overflow resulting from big ggml_cgraph
    
    replace stack allocation and ggml_build_forward by ggml_new_graph in combination with ggml_build_forward_expand
    
    * replace llama API functions to get model tensors by one function to get model tensor by name
    
    LLAMA_API struct ggml_tensor * llama_get_model_tensor(struct llama_model * model, const char * name);
    
    * remove unused call to not existing llama_get_layer_from_model
    
    * implement ggml_compute_forward_out_prod_q_f32
    
    * remove trailing whitespace
    
    * add lora finetune support on quantized base model tensors
    
    * add ggml_add_cast API function
    
    this function works like ggml_add, but accepts a data type for the resulting tensor.
    only supported for quantized src0 input.
    
    * use ggml_add_cast in finetuning
    
    lora-applied weights will now have data type F32, which improves gradients when finetuning quantized base models
    
    * bug fix: actually use result type passed to ggml_add_cast
    
    * make sure base model tensors data cannot be used in viewable operations
    
    memory allocator would try to make lora application inplace on base model tensors.
    since those are memory mapped this will result in memory access violations
    
    * fix bug in ggml_out_prod which resulted in wrong n_dims of result tensors
    
    * avoid keeping in memory ALL of the gradients
    
    The problem here stems from ggml_graph_reset. This function is called in the optimization function, before each graph computation, to reset the gradients to zero. This required a unique memory slot for each gradient: allocating memory from a previosly freed memory location might lead to non-zero input gradients.
    
    During ggml_compute_backward the gradients are build stepwise by adding or substracting new values, starting from a OP_NONE tensor which needs to contain zero-values. This requires the graph reset.
    
    To avoid this I now remember in ggml_build_backward_expand the original OP_NONE gradient tensors in a hash table, which is passed to ggml_compute_backward. There instead of using add (or sub or similar) I test whether the existing gradient to be changed is a zero-valued-tensor by looking up its existence in the hash table. When it is such a zero-tensor it will not be modified, but replaced by the value to be added, otherwise the regular add (not inplace, allocator will take care of this) will be used. This way none of those zero-tensor values will be necessary in the final backward graph and more importantly they won't need a unique memory slot, just to make them zero.
    
    * remove trailing whitespace
    
    * remove debug prints and function to compute tensor data hash
    
    * improve optimization iteration prints
    
    * adjust maximal values to support finetuning 3B models
    
    * change default finetune params lora_r and lora_alpha to match the n_rank parameters of 4
    
    * bug fix: make sure finetune input gradient is allocated at begin and kept until end
    
    * remove unnecessary src tensor from ggml_get_rows_back
    
    we don't need data of src[2] for computation, only to setup the correct output shape.
    remove dependency on src[2], so that allocator can work more freely.
    
    the computational graph is still completely determined, because the output shape is naturally included.
    this is similar to how ggml_reshape does it.
    
    * remove unnecessary src tensor from ggml_repeat & ggml_repeat_back
    
    we don't need data of src[1] for computation, only to setup the correct output shape.
    remove dependency on src[1], so that allocator can work more freely.
    
    the computational graph is still completely determined, because the output shape is naturally included
    
    * resolve todo
    
    allocator will only make it inplace when they are of the same type
    
    * mixing multiple LORA adapters is now possible
    
    pass more than one '--lora FNAME' argument to apply more than one LORA.
    use '--lora-scaled FNAME S' when you want to specify a user-defined scale for an adapter.
    
    * add option to save finetune output every N iterations
    
    * also save latest finetune output with ITERATION="LATEST" and print where files are saved
    
    saving with LATEST makes it easier to resume training from the latest checkpoint
    the string "LATEST" can be configured with command line option "--fn-latest STR"
    
    * update checkpoint train stats before saving via "--save-every"
    
    * add command line option `--rank-wo N` for rank of wo tensor
    
    * update finetune README
    
    * fix dump_non_result_info_yaml to output multiple lora adapters
    
    * bug fix: replace GGML_TYPE_SIZE[t] by ggml_type_size(t)
    
    * replace llama_n_mult by llama_n_ff
    
    * finetune bug fixes to compile with merged in code from master
    
    * remove prediction related code to reduce duplicated code with main
    
    use main instead
    
    * reduce large memory overhead in train-text-from-scratch
    
    all gradients had to be pinned so that graph_reset works correctly.
    this is no longer necessary with the changes to ggml_compute_backward introduced in this PR.
    
    * add comment explaining why finetune checkpoints are allocated in one block
    
    * make default value of float member a float literal
    
    * handle rms_norm and rope parameters the same as in train-text-from-scratch
    
    * remove unused code
    
    * remove vocab related code as it is unnecessary
    
    * add LLM_KV_TRAINING_TYPE to train-text-from-scratch checkpoints
    
    so that they can be differentiated from lora finetune checkpoints
    
    * add gguf constants and load/save functions from train-text-from-scratch
    
    * add load & save lora finetune checkpoints via gguf
    
    * add python script to convert old finetune checkpoint files to gguf
    
    * remove old checkpoint save & load code
    
    * remove code to print data checksums which was used to verify correctness of new gguf code
    
    * omit tokenization when training is disabled, only save llama lora adapter
    
    training can be disabled by passing '-n 0' to finetune
    
    * remove trailing whitespace
    
    * update README.md
    
    * implement ggml_compute_forward_repeat_f16
    
    * avoid stack overflow of large cgraphs in test-grad0
    
    * add ggml API functions ggml_unravel_index, ggml_get_i32_nd and its analogs for set and for f32
    
    ggml_get_i32_1d, ggml_set_i32_1d, ggml_get_f32_1d, ggml_set_f32_1d now support non-contiguous tensors.
    in case of non-contiguous tensor, the 1d index is unraveled into a multi index using ggml_unravel_index to be passed to '_nd' function equivalent.
    
    this fixes a bug in test-grad0 which happens due to ggml_build_backward not building purely contiguous tensors anymore
    
    * increase test-grad0 context mem size to accommodate for bigger cgraph
    
    * add sanity check to ggml_compute_backward, asserting the correct shape of gradients
    
    * fix ggml_acc_or_set to return tensor of correct shape
    
    * remove unused 'inplace' argument from ggml_compute_backward function
    
    inplace operations to add gradients are no longer created by ggml_compute_backward
    use allocator to automatically make inplace operations
    
    * add missing argument 'int i0' to ggml_get_i32_nd & ggml_set_i32_nd header declarations
    
    * fix error message in ggml_allocr_alloc to display actual max_avail
    
    * fix check_gradient
    
    ggml_build_backward_expand was previously replaced by ggml_build_backward, but the assignment of forward graph to backward graph missing
    
    * use tensor->view_src instead of ggml_is_view and get_view_source
    
    * move gradient checkpointing code into ggml, new API function:
    
    // build gradient checkpointing backward graph gb for gf using provided checkpoints
    // gb_tmp will contain original backward graph with rewritten backward process nodes,
    // but without the second forward pass nodes.
    GGML_API void ggml_build_backward_gradient_checkpointing(
            struct ggml_context   * ctx,
            struct ggml_cgraph    * gf,
            struct ggml_cgraph    * gb,
            struct ggml_cgraph    * gb_tmp,
            struct ggml_tensor  * * checkpoints,
            int                     n_checkpoints);
    
    * replace custom data getters and setters by ggml functions
    
    * train-text-from-scratch can train (full finetune) gguf models
    
    just pass the gguf model via `--checkpoint-in FN`.
    after this, to continue training, pass the generated checkpoint instead of the original gguf model.
    
    tested with smaller models, bigger models may exceed available memory.
    use (LORA) finetune for those.
    
    * remove trailing whitespace
    
    * add option to save train-text-from-scratch output every N iterations
    
    * update README.md
    
    * fix warnings
    
    * fix warnings
    
    * remove finetune option to disable allocator
    
    the allocator should always be used.
    by making sure that it is always used it gets easier to implement automatic memory requirements computation
    
    * add tensor checkpoints only when gradient checkpointing is enabled
    
    * initialize opt ggml context if none was provided
    
    * add ggml-alloc API function 'ggml_allocr_max_size' to get max size of alloc
    
    GGML_API size_t ggml_allocr_max_size(struct ggml_allocr * alloc);
    
    * finetune: automatically allocate all memory and changes to command line options
    
    remove '--n_examples N' parameter, as it no longer makes sense to call optimization process multiple times in a loop.
    add '--only_write_lora' command line option: will skip tokenization and training, to only write a llama.cpp comptabile LORA adapter.
    remove memory buffer related command line options.
    improve iteration console output.
    
    * add finetune to Makefile
    
    * update README.md
    
    * print time per iteration and estimate remaining time
    
    * increase measured alloc size by tensor_alignment
    
    ggml_allocr_reset will reduce the given size by up to tensor_alignment-1
    
    * fix README.md
    
    * add some more allocator debug prints
    
    * bug fix, probably solves the 'ggml_allocr_alloc: not enough space in the buffer' issue
    
    * revert last commit
    
    "bug fix, probably solves the 'ggml_allocr_alloc: not enough space in the buffer' issue"
    
    "alloc was freeing an externally allocated tensor, because it calculated the end of allocator memory as alloc->data + alloc->max_size instead of alloc->data + alloc->size."
    
    This is intentional to reduce the risk of freeing external tensors when measuring. Unless max_size is not properly calculated, I don't see why this is an issue.
    
    * remove unnecessary "0x" before "%p" output
    
    * move measurement memory segment to upper region of the address space
    
    * update README.md
    
    * fix printf format warnings
    
    * add missing gguf_free in load_checkpoint_lora_file
    
    * load default rms_norm and rope parameters from base model
    
    * add gradient accumulation
    
    specify number accumulation steps with '--grad-acc N'.
    this will simulate a bigger batch size of grad_acc*batch.
    
    * fix tracking of train_samples and train_tokens
    
    * build : fix compile warnings
    
    * ggml : fix L-BFGS linesearch loop
    
    * improve finetune time measurement
    
    fix printf warnings on system where int64_t is (long int).
    change time datatypes to double because values get big with long training times.
    exclude file saving from time measurement.
    converge faster to actual time per iteration by removing very small first duration before first iteration was performed.
    fix bug in output of total training time, the reported value was 1000 times to small.
    
    * specify default lora rank with '--lora-r N'
    
    '--lora-r N' will specify default rank for all tensors
    '--rank-wq N', etc. will override this default rank for specific tensor types.
    
    * fix gradient accumulation bug where the same batch was used for each microstep
    
    * fix gradient accumulation bug where the same batch was used for each microstep
    
    * support grouped-query-attention in ggml_flash_attn and ggml_flash_attn_back
    
    k and v can now be repeated in q along ne[2]
    
    in forward pass just use modulo to compute k and v indices, like ik2 = iq2 % nek2.
    
    in backard pass this won't work as easy, because multiple threads will compete to accumulate to the same k->grad[:,ik1,ik2,ik3] and v->grad[:,iv1,iv2,iv3].
    so we change the parallelization over q rows to be over k rows. this ensures non-overlapping (ik2,ik3) across threads.
    in each thread we then iterate over the number of repetitions of k/v in q to compute iq2 as iq2 = ik2 + irep*nek2.
    
    since ne2 is not the same for q,k and v we also change how the gradients are concatenated into the result tensor.
    additionally the offsets of gradq, gradk and gradv in the result tensor are now memory aligned.
    
    we also simplify the compute_backward part of flash_attn to use ggml_reshape instead of switching over the number of dimensions.
    this needs a small change to ggml_reshape, removing the assertion of second argument to be contiguous.
    since only the shape (ne) of the second reshape argument is of relevance, its memory layout (nb) is irrelevant -> it can very well be non-contiguous.
    
    change test-grad0 to also test for repeated k/v in q.
    
    this changes the rng and now results in small gradient differences in softmax. these solely come from using f16 exp table lookup in forward softmax: when temporarily changing softmax to use actual exp function, the reported gradient differences go away. gradient differences coming solely from f16 table lookup are acceptable.
    added a note to explain this.
    
    * add llama API functions to get grouped-query-attention n_head parameter 'n_head_kv'.
    
    * fix finetune to support grouped-query-attention (using flash-attention)
    
    note: ggml changes to ggml_out_prod are necessary to support grouped-query-attention without flash-attention.
    
    * support broadcastable a in out_prod(a, b) and backward pass of broadcasting mul_mat(a, b)
    
    * test broadcasting mul_mat backward pass
    
    * decouple random number generator of each operation test
    
    when changing one test the rng of others tests is not influenced anymore
    
    * add comment briefly describing what ggml_repeat_back does
    
    * simplify broadcasting mul_mat backward using ggml_repeat_back
    
    * add cgraph evaluation order member and corresponding enum type
    
    this controls in which order ggml_build_forward visits source nodes.
    by default the nodes are visited left to right, i.e. src[0] first.
    in some cases it is beneficial for ggml-alloc to visit in a different order.
    two possible orders are supported: left-to-right (src[0] first) and right-to-left (src[0] last).
    
    * measure max compute size for each cgraph eval order and use best order
    
    this can bring huge memory savings:
    e.g. codellama-34b with n_ctx=64, n_batch=1 goes from 92927.8mb down to 4627.6 MB
    
    * remove unused command line options
    
    * add sample start patterns and options to force new or by default resume last shuffling
    
    * update shuffle rng state on reshuffle
    
    * exclude known zero values from computations in flash_attn_f32 & flash_attn_back_f32
    
    * remove probably unnecessary exception type flags from stringstream
    
    * pass correct max number of tokens to llama_tokenize
    
    * account for possible leading whitespace that will be added by tokenizer
    e.g. '\t' will be tokenized by llama spm tokenizer to [29871, 12]
    
    * use unrolled vec_mad in out_prod
    
    y is vec_mad result vec.
    x is vec_mad input vec.
    v is vec_mad input scalar.
    
    ggml_vec_mad_f32_unroll will internally loop over x and v with same y.
    
    GGML_VEC_MAD_UNROLL is by default defined to 32.
    
    This value is empirical optimized using performance test runs of out-prod in openllama-3b finetune with 256 context length and batch size 1. It gives 23% performance boost for out_prod.
    
    Full measurements of out-prod runtime in ms:
            unroll_xv       unroll_yv
    1       67014.643       87826.469
    2       77117.552       89077.656
    4       72091.311       109121.657
    8       61077.543       88678.334
    16      56914.67        79514.947
    24      59024.595       84350.254
    28      55952.446       83368.73
    32      51476.658       85177.745
    36      55973.792       84659.92
    40      55139.616       93844.738
    48      60736.392       93330.267
    64      99856.878       116994.99
    
    Second column is when unrollying yv instead of xv
    
    * set lora_alpha to value of lora_r if it is not set via command line
    
    otherwise only changing lora_r will change scaling of lora adapter used in prediction
    
    * reshuffle original sample order instead of the previous shuffled order
    
    otherwise resumed reshuffle will not result in same sample order
    
    * block tiling for out-prod inspired by mul-mat
    
    block sizes are empirically optimized
    
    roughly doubles the flops of out-prod
    
    * exclude some more known zero values from computations in flash_attn_f32 & flash_attn_back_f32
    
    * add static keywords
    
    * remove outcommented old code
    
    * update train-text-from-scratch with tokenization, sample selection and shuffling from finetune
    
    * remove lbfgs related train parameters
    
    * move common train functions into common/train.[h|cpp]
    
    * move train state into struct train_state
    
    * move train data saving code into callback to unify code of opt_callback
    
    train_params are still different in finetune and train-text-from-scratch, so it can't yet be moved to train.h|cpp
    
    * move common train params into common/train
    
    * move common opt_callback into common/train
    
    * fix consume_common_train_arg
    
    * save and load head_count_kv in lora checkpoints
    
    * increase train_samples by used_samples instead of number of batches
    
    on batch can contain more than one sample when option "fill_with_next_samples" is used
    
    * fix usage of llama_tokenize
    
    * remove static from process_escape since we need it exposed in header
    
    * fix code formating of long function declarations
    
    * fix condition in load_train_state_gguf
    
    * use die("msg") instead of replace GGML_ASSERT(!"msg") or throw std::runtime_error("msg")
    
    * fix saving and loading of training type
    
    * remove terminating '\0' from tokenization
    
    (llama_tokenize is now passed the string length instead of relying on terminating '\0')
    
    * fix compile warnings
    
    * fix compile warnings
    
    * use new/delete for train_state instead of malloc/free
    
    using malloc may result in seg faults when trying to assign string fields
    
    * assert that sample_count > 0, avoiding division by zero
    
    * fix frand to return value in interval [0,1)
    
    * add train option "--sample-random-offsets"
    
    Use samples beginning at random offsets.
    The offset is only applied to the first sample in each batch context window.
    Together with "--fill-with-next-samples" this may help for training endless text generation.
    
    For example given a dataset containing samples "abcd", "ABCD", "0123".
    With context size of 8 and options "--fill-with-next-samples", "--no-separate-with-eos", "--no-separate-with-bos",
    the context windows of batches could only be filled with "abcdABCD", "ABCDabcd", "0123abcd", etc.
    
    With "--sample-random-offsets" it can also be filled with "23abcdAB", "bcd0123A", etc.
    
    * deduplicate code into function
    
    * remove n_rot hparam, as it must always be hparam.n_embd_head()
    
    * align code
    
    * assert correct base model tensor shapes
    
    * move some params from lora hparams into model hparams and load model params from gguf
    
    this equalizes the model definition in finetune and text-from-scratch and removes the need for additional llama api functions to get model parameters
    
    * remove now unnecessary llama API functions to get model params that where added by this PR
    
    * train-text-from-scratch: automatically allocate model tensors, remove option '--mem-model N'
    
    * train-text-from-scratch: automatically allocate opt context
    
    * train-text-from-scratch: automatically allocate input tensors
    
    * train-text-from-scratch: automatically allocate compute memory
    
    * remove unused options and equalize train-text-from-scratch with finetune
    
    * initialize opt->loss_after with zero
    
    * add export-lora program
    
    * remove trailing whitespace
    
    * add export-lora build in Makefile
    
    * remove unused struct tensor_info from export-lora
    
    * add export-lora build dependency to llama
    
    because it depends on common, which depends on llama
    
    * update finetune README.md
    
    * cancel optimization when specified number of epochs is completed
    
    * improve handling of export-lora arguments
    
    print errors and warnings when files could not be read or created
    
    * Fix export-lora.cpp "not enough space in the context's memory pool" (#1)
    
    * Fix export-lora.cpp "not enough space in the context's memory pool"
    
    Without this patch, export-lora would sometimes error with "not enough space in the context's memory pool (needed 656784, available 656800)".
    
    * increase required context size by 5*GGML_MEM_ALIGN instead of plain 16
    
    ---------
    
    Co-authored-by: xaedes <xaedes@gmail.com>
    
    * improve handling of not yet supported tensor types
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: meatbag-18a <145869052+meatbag-18a@users.noreply.github.com>

commit 2db94d98eda56982d80238840b0652b4137a2a84
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Sep 28 14:30:31 2023 -0400

    gguf : basic type checking in gguf_get_* (#3346)

commit ecf90b1a5114034bc0939b3968f549fe4d63cf6d
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Sep 28 14:30:15 2023 -0400

    gguf : make token scores and types optional (#3347)

commit 2619109ad57d7a75388a9cce51e5da645410d92e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Sep 28 19:36:36 2023 +0300

    ci : disable freeBSD builds due to lack of VMs (#3381)

commit ec893798b7a2a803466cc8f063051499ec3d96f7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Sep 28 19:04:36 2023 +0300

    llama : custom attention mask + parallel decoding + no context swaps (#3228)
    
    * tests : verify that RoPE is "additive"
    
    * llama : replace ggml_diag_mask_inf with ggml_add (custom -inf mask)
    
    * ggml : ggml_rope now takes a vector with positions instead of n_past
    
    * metal : add rope_f16 kernel + optimize cpy kernels
    
    * llama : unified KV cache + batch inference API
    
    * llama : add new llama_decode() API that works with llama_batch
    
    * llama : add cell_max heuristic for more efficient kv_cache
    
    * llama : extend llama_kv_cache API
    
    * llama : more robust cell_max heuristic + wip shift
    
    * metal : disable concurrency optimization
    
    * llama : add llama_kv_cache_shift_seq + no more context swaps
    
    * llama : apply K-cache roping for Falcon and Baichuan
    
    * speculative : fix KV cache management
    
    * parallel : example for serving multiple users in parallel
    
    * parallel : disable hot-plug to avoid cache fragmentation
    
    * fixes : speculative KV cache + llama worst-case graph
    
    * llama : extend batch API to select which logits to output
    
    * llama : fix worst case graph build
    
    * ggml-cuda : update rope implementation for parallel decoding (#3254)
    
    * ggml-cuda : update rope implementation for parallel decoding
    
    * better solution for p0 computation
    
    * fix rope
    
    * simpler rope implementation
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * make : add parallel to build + fix static functions in llama.cpp
    
    * simple : fix token counting
    
    * parallel : various improvements
    
    * llama : fix cell_max logic + rename functions
    
    * parallel : try smaller batches when the KV cache is fragmented
    
    * parallel : fix sequence termination criteria
    
    * llama : silence errors KV cache errors
    
    * parallel : remove new line from prompt
    
    * parallel : process system prompt once + configurable paramters + llama API
    
    * parallel : remove question with short answers
    
    * parallel : count cache misses
    
    * parallel : print misses on each request
    
    * parallel : minor
    
    * llama : fix n_kv to never become 0
    
    * parallel : rename hot-plug to continuous-batching
    
    * llama : improve llama_batch API + simplify parallel example
    
    * simple : add parallel decoding support
    
    * simple : improve comments + free batch
    
    * ggml-cuda : add rope f16, restore performance with parallel decoding (#3272)
    
    * ggml-cuda : add rope f16, restore performance
    
    * offload KQ_mask with all models
    
    * fix rope shift
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * llama : disable MPI for now
    
    ggml-ci
    
    * train : make KQ_pos memory buffer permanent via dummy scale op
    
    * ggml : revert change to ggml_cpy, add ggml_cont_Nd instead (#3275)
    
    ggml-ci
    
    * parallel : fix bug (extra BOS) + smaller token_prev array
    
    * parallel : fix cases where the input prompts can overflow the batch
    
    * parallel : add disabled experimental batch chunking in powers of two
    
    * llama : llama.h formatting + comments
    
    * simple : add README.md
    
    * llama : fix kv cache heuristic when context is less than 32
    
    * parallel : fix crash when `-n -1`
    
    * llama : simplify returns if/else branches
    
    * metal : use mm kernels for batch size > 2
    
    * examples : utilize new llama_get_logits_ith()
    
    * examples : add example for batched decoding
    
    * examples : do not eval prompt 2 times (close #3348)
    
    * server : clear the KV cache beyond n_past before llama_decode
    
    * server : avoid context swaps by shifting the KV cache
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 45855b3f1c7bdd0320aa632334d0b3e8965c26c4
Author: Kevin Ji <1146876+kevinji@users.noreply.github.com>
Date:   Thu Sep 28 09:11:32 2023 -0400

    docs : mark code as Bash (#3375)

commit 4aea3b846ec151cc6d08f93a8889eae13b286b06
Author: Pierre Alexandre SCHEMBRI <pa.schembri@gmail.com>
Date:   Thu Sep 28 14:13:37 2023 +0200

    readme : add Mistral AI release 0.1 (#3362)

commit da0400344be12074e67dcabc565140289cf7efaa
Author: slaren <slarengh@gmail.com>
Date:   Thu Sep 28 12:08:28 2023 +0200

    ggml-cuda : perform cublas fp16 matrix multiplication as fp16 (#3370)
    
    * ggml-cuda : perform cublas fp16 matrix multiplication as fp16
    
    * try to fix rocm build
    
    * restrict fp16 mat mul to volta and up

commit e519621010cac02c6fec0f8f3b16cda0591042c0
Author: Zhang Peiyuan <a1286225768@gmail.com>
Date:   Thu Sep 28 02:45:20 2023 +0800

    convert : remove bug in convert.py permute function (#3364)

commit ac43576124a75c2de6e333ac31a3444ff9eb9458
Author: Richard Roberson <richardr1126@gmail.com>
Date:   Wed Sep 27 10:25:12 2023 -0600

    make-ggml.py : compatibility with more models and GGUF (#3290)
    
    * Resync my fork with new llama.cpp commits
    
    * examples : rename to use dash instead of underscore
    
    * New model conversions
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 20c7e1e804690f3db58bd33eb56f8c6aa4735c63
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Wed Sep 27 12:18:07 2023 -0400

    gguf : fix a few general keys (#3341)

commit dc6897404e141c74cbbf8030ecfebd74e1815411
Author: Rickard Hallerbäck <rickard.hallerback@gmail.com>
Date:   Wed Sep 27 17:48:33 2023 +0200

    metal : reusing llama.cpp logging (#3152)
    
    * metal : reusing llama.cpp logging
    
    * cmake : build fix
    
    * metal : logging callback
    
    * metal : logging va_args memory fix
    
    * metal : minor cleanup
    
    * metal : setting function like logging macro to capital letters
    
    * llama.cpp : trailing whitespace fix
    
    * ggml : log level enum used by llama
    
    * Makefile : cleanup ggml-metal recipe
    
    * ggml : ggml_log_callback typedef
    
    * ggml : minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 527e57cfd8a9a26bf622c0510c21c2508a24be26
Author: Jag Chadha <jagtesh@gmail.com>
Date:   Wed Sep 27 11:34:32 2023 -0400

    build : add ACCELERATE_NEW_LAPACK to fix warning on macOS Sonoma (#3342)

commit ffe88a36a913e5792aa383f0726bdbcf632e7191
Author: BarfingLemurs <128182951+BarfingLemurs@users.noreply.github.com>
Date:   Wed Sep 27 11:30:36 2023 -0400

    readme : add some recent perplexity and bpw measurements to READMES, link for k-quants (#3340)
    
    * Update README.md
    
    * Update README.md
    
    * Update README.md with k-quants bpw measurements

commit 99115f3fa654b593099c6719ad30e3f54ce231e1
Author: DAN™ <dranger003@gmail.com>
Date:   Mon Sep 25 18:45:33 2023 -0400

    cmake : fix build-info.h on MSVC (#3309)

commit 1726f9626f21f102d8e01df06c23a7f94add7990
Author: 2f38b454 <dxf@protonmail.com>
Date:   Tue Sep 26 02:24:52 2023 +0800

    docs: Fix typo CLBlast_DIR var. (#3330)

commit a98b1633d5a94d0aa84c7c16e1f8df5ac21fc850
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Mon Sep 25 13:48:30 2023 +0200

    nix : add cuda, use a symlinked toolkit for cmake (#3202)

commit c091cdfb24621710c617ea85c92fcd347d0bf340
Author: slaren <slarengh@gmail.com>
Date:   Sat Sep 23 21:48:24 2023 +0200

    llama-bench : add README (#3317)
    
    * llama-bench : add README
    
    * minor edit

commit 51a7cf5c6e490b2f51c82daa76c4ca4f8d845826
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Sat Sep 23 05:28:50 2023 -0400

    examples : fix RoPE defaults to match PR #3240 (#3315)

commit bedb92b603886768ad51e629f81eda15ff6b86f5
Author: Kevin Ji <1146876+kevinji@users.noreply.github.com>
Date:   Fri Sep 22 23:52:23 2023 -0400

    scripts : use `/usr/bin/env` in shebang (#3313)

commit bc9d3e3971e5607a10ff4c24e39568ce1ac87271
Author: Lee Drake <b.lee.drake@gmail.com>
Date:   Thu Sep 21 13:00:24 2023 -0600

    Update README.md (#3289)
    
    * Update README.md
    
    * Update README.md
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit 36b904e20003017f50108ae68359ef87a192dae2
Author: shibe2 <shibe@tuta.io>
Date:   Thu Sep 21 22:10:26 2023 +0400

    ggml-opencl.cpp: Make private functions static (#3300)

commit 324f3403d54ae4499a1d68623161015f7419fb76
Author: Edward Taylor <edeetee@gmail.com>
Date:   Thu Sep 21 21:08:20 2023 +1200

    zig : fix for updated c lib (#3259)

commit f56c418ab0a635c020bcb5bf44b8f00cb3c9e514
Author: yuiseki <yuiseki@gmail.com>
Date:   Thu Sep 21 17:57:40 2023 +0900

    embedding : update README.md (#3224)

commit 8185710a80531e9ee0c0cb99d3a9c9af1019ab67
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Sep 21 10:43:53 2023 +0200

    CUDA: use only 1 thread if fully offloaded (#2915)

commit 7eb41179edc56083ef4eb2df7967ac9ff38b34fb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Sep 20 20:48:22 2023 +0300

    readme : update hot topics

commit a5661d7e71d15b8dfc81bc0510ba912ebe85dfa3
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Wed Sep 20 12:12:47 2023 -0400

    llama : allow gguf RoPE keys to be overridden with defaults (#3240)

commit 65c2c1c5ab7c5089dbc6d10bc49b9c58f0164317
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Wed Sep 20 12:06:08 2023 -0400

    benchmark-matmult : do not use integer abs() on a float (#3277)

commit 80834daecf4b9021770361a6d5e1b9c7a60e6854
Author: kang <tpdns9032100@gmail.com>
Date:   Wed Sep 20 22:48:22 2023 +0900

    flake : Restore default package's buildInputs (#3262)

commit a40f2b656fab364ce0aff98dbefe9bd9c3721cc9
Author: Alon <alonfaraj@gmail.com>
Date:   Wed Sep 20 15:06:36 2023 +0300

    CI: FreeBSD fix (#3258)
    
    * - freebsd ci: use qemu

commit d119c04c159d015a93567df7e73e0e45a22d0f1d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Sep 20 10:02:39 2023 +0300

    examples : fix benchmark-matmult (#1554)
    
    The precision for Q4_0 has degraded since #1508

commit 8781013ef654270cbead3e0011e33a6d690fb168
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Mon Sep 18 10:03:53 2023 -0400

    make : restore build-info.h dependency for several targets (#3205)

commit 7ddf185537b712ea0ccbc5f222ee92bed654914e
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Mon Sep 18 02:21:47 2023 +0200

    ci : switch cudatoolkit install on windows to networked (#3236)

commit ee66942d7ef7c259528158f9a3bd1c314984d32f
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Sep 17 23:35:20 2023 +0200

    CUDA: fix peer access logic (#3231)

commit 111163e2463171891680feed94371eb9becd9817
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Sep 17 16:37:53 2023 +0200

    CUDA: enable peer access between devices (#2470)

commit 8b428c9bc84be6887d904600d1298b28baffd552
Author: slaren <slarengh@gmail.com>
Date:   Sun Sep 17 14:33:28 2023 +0200

    llama.cpp : show model size and BPW on load (#3223)

commit 578d8c8f5cb72f354bc115ba230ee5b2d803eee7
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Sep 17 14:16:22 2023 +0200

    CUDA: fix scratch malloced on non-main device (#3220)

commit b541b4f0b1d4d9871c831e47cd5ff661039d6101
Author: IsaacDynamo <61521674+IsaacDynamo@users.noreply.github.com>
Date:   Sat Sep 16 19:35:25 2023 +0200

    Enable BUILD_SHARED_LIBS=ON on all Windows builds (#3215)

commit 5dbc2b3213126a31d3be4ade8ca042cb93019682
Author: Vlad <spitfireage@gmail.com>
Date:   Sat Sep 16 17:55:43 2023 +0300

    Enable build with CUDA 11.0 (make) (#3132)
    
    * CUDA 11.0 fixes
    
    * Cleaner CUDA/host flags separation
    
    Also renamed GGML_ASSUME into GGML_CUDA_ASSUME

commit b08e75baea294e366628b898e85c0bd359b58115
Author: goerch <jhr.walter@t-online.de>
Date:   Sat Sep 16 13:41:33 2023 +0200

    Fixing the last deviations from sentencepiece indicated by test-tokenizer-1 (#3170)
    
    * Fix für #2721
    
    * Reenable tokenizer test for LLaMa
    
    * Add `console.cpp` dependency
    
    * Fix dependency to `common`
    
    * Fixing wrong fix.
    
    * Make console usage platform specific
    
    Work on compiler warnings.
    
    * Adapting makefile
    
    * Remove trailing whitespace
    
    * Adapting the other parts of the makefile
    
    * Fix typo.
    
    * Fixing the last deviations from sentencepiece indicated by test-tokenizer-1
    
    * Simplify logic
    
    * Add missing change...
    
    * Fix ugly compiler warning
    
    * llama_tokenize should accept strings containing NUL now
    
    * Adding huichen's test case

commit e6616cf0db2b63189fc34d0076f654af9adecdf8
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Sep 15 16:59:49 2023 -0400

    examples : add compiler version and target to build info (#2998)

commit 3aefaab9e59335ebb07d5205dbc8633efd680e58
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Sep 15 15:38:27 2023 -0400

    check C++ code with -Wmissing-declarations (#3184)

commit 69eb67e28275cd2d57693405f768754a7b2245ad
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Sep 15 15:18:15 2023 -0400

    fix build numbers by setting fetch-depth=0 (#3197)

commit 4fe09dfe665c58a753dc9eb638dd4dca1cd35488
Author: Meng Zhang <meng@tabbyml.com>
Date:   Sat Sep 16 03:02:13 2023 +0800

    llama : add support for StarCoder model architectures (#3187)
    
    * add placeholder of starcoder in gguf / llama.cpp
    
    * support convert starcoder weights to gguf
    
    * convert MQA to MHA
    
    * fix ffn_down name
    
    * add LLM_ARCH_STARCODER to llama.cpp
    
    * set head_count_kv = 1
    
    * load starcoder weight
    
    * add max_position_embeddings
    
    * set n_positions to max_positioin_embeddings
    
    * properly load all starcoder params
    
    * fix head count kv
    
    * fix comments
    
    * fix vram calculation for starcoder
    
    * store mqa directly
    
    * add input embeddings handling
    
    * add TBD
    
    * working in cpu, metal buggy
    
    * cleanup useless code
    
    * metal : fix out-of-bounds access in soft_max kernels
    
    * llama : make starcoder graph build more consistent with others
    
    * refactor: cleanup comments a bit
    
    * add other starcoder models: 3B, 7B, 15B
    
    * support-mqa-directly
    
    * fix: remove max_position_embeddings, use n_train_ctx
    
    * Update llama.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update llama.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Apply suggestions from code review
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * fix: switch to space from tab
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 80291a1d02a07f7f66666fb576c5b1e75aa48b46
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Sep 15 14:02:01 2023 -0400

    common : do not use GNU zero-length __VA_ARGS__ extension (#3195)

commit c6f1491da032238241e01021c8c58d7b540a043f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 15 20:17:24 2023 +0300

    metal : fix bug in soft_max kernels (out-of-bounds access) (#3194)

commit e3d87a6c36eadd84d58763143c6d56a0c771ca40
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Sep 15 12:29:02 2023 -0400

    convert : make ftype optional in simple scripts (#3185)

commit 8c00b7a6ff38e27fa1e471452b8a480913772c2a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 15 19:06:03 2023 +0300

    sync : ggml (Metal F32 support + reduce ggml-alloc size) (#3192)
    
    * sync : ggml (Metal F32 support + reduce ggml-alloc size)
    
    ggml-ci
    
    * llama-bench : fix ggml_cpu_has_metal() duplicate function
    
    ggml-ci

commit 7e50d34be68aae2cc766203703dd188e910e033a
Author: Engininja2 <139037756+Engininja2@users.noreply.github.com>
Date:   Fri Sep 15 06:24:30 2023 -0600

    cmake : fix building shared libs for clang (rocm) on windows (#3176)

commit 235f7c193b02dacfb56319e41a28684b3a2c6db0
Author: Evgeny Kurnevsky <kurnevsky@gmail.com>
Date:   Fri Sep 15 10:10:22 2023 +0200

    flake : use pkg-config instead of pkgconfig (#3188)
    
    pkgconfig is an alias, it got removed from nixpkgs:
    https://github.com/NixOS/nixpkgs/blob/295a5e1e2bacd6e246db8b2bb35d2a9415883224/pkgs/top-level/aliases.nix#L1408

commit a51b68765799e75e17c7622f376bfbeb66f1bd70
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 15 11:09:24 2023 +0300

    metal : relax conditions on fast matrix multiplication kernel (#3168)
    
    * metal : relax conditions on fast matrix multiplication kernel
    
    * metal : revert the concurrnecy change because it was wrong
    
    * llama : remove experimental stuff

commit 76164fe2e65c058e9ee2c3afd0ad6b182ca57e25
Author: Andrei <abetlen@gmail.com>
Date:   Fri Sep 15 04:07:40 2023 -0400

    cmake : fix llama.h location when built outside of root directory (#3179)

commit c2ab6fe661af9834ca6dd75f14e2439938cc22ac
Author: Ali Tariq <ali.tariq@10xengineers.ai>
Date:   Fri Sep 15 13:06:56 2023 +0500

    ci : Cloud-V for RISC-V builds (#3160)
    
    * Added Cloud-V File
    
    * Replaced Makefile with original one
    
    ---------
    
    Co-authored-by: moiz.hussain <moiz.hussain@10xengineers.ai>

commit 2d770505a89a99ce78a5950cf14fc06d3176ffa4
Author: Roland <14355895+rbur0425@users.noreply.github.com>
Date:   Fri Sep 15 03:28:45 2023 -0400

    llama : remove mtest (#3177)
    
    * Remove mtest
    
    * remove from common/common.h and examples/main/main.cpp

commit 98311c427739e3b06527c3ce6b5c021ab6692740
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Sep 14 21:09:53 2023 -0400

    llama : make quantize example up to 2.7x faster (#3115)

commit feea179e9f9921e96e8fb1b8855d4a8f83682455
Author: jneem <joeneeman@gmail.com>
Date:   Thu Sep 14 13:54:47 2023 -0500

    flake : allow $out/include to already exist (#3175)

commit 769266a543f68377a1d904ec2a8c27b38a4025ab
Author: Andrei <abetlen@gmail.com>
Date:   Thu Sep 14 13:38:16 2023 -0400

    cmake : compile ggml-rocm with -fpic when building shared library (#3158)

commit cf8238e7f43cb82a36426af392037e85cd2a3df6
Author: Asbjørn Olling <asbjornolling@gmail.com>
Date:   Thu Sep 14 19:25:00 2023 +0200

    flake : include llama.h in nix output (#3159)

commit 4b8560e72a936b5d536ebd1e7a5dd579984769f3
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Sep 14 13:22:47 2023 -0400

    make : fix clang++ detection, move some definitions to CPPFLAGS (#3155)
    
    * make : fix clang++ detection
    
    * make : fix compiler definitions outside of CPPFLAGS

commit 83a53b753a9499a2a3535c93975b430cb2c828a9
Author: Alon <alonfaraj@gmail.com>
Date:   Thu Sep 14 20:21:25 2023 +0300

    CI: add FreeBSD & simplify CUDA windows (#3053)
    
    * add freebsd to ci
    
    * bump actions/checkout to v3
    * bump cuda 12.1.0 -> 12.2.0
    * bump Jimver/cuda-toolkit version
    
    * unify and simplify "Copy and pack Cuda runtime"
    * install only necessary cuda sub packages

commit 5c872dbca2c7979b1f6dafc97db0774b8bbf9372
Author: akawrykow <142945436+akawrykow@users.noreply.github.com>
Date:   Thu Sep 14 10:19:42 2023 -0700

    falcon : use stated vocab size (#2914)

commit 990a5e226a1a0ac858abe3aa7e5f3b000d4fa665
Author: bandoti <141645996+bandoti@users.noreply.github.com>
Date:   Thu Sep 14 14:04:40 2023 -0300

    cmake : add relocatable Llama package (#2960)
    
    * Keep static libs and headers with install
    
    * Add logic to generate Config package
    
    * Use proper build info
    
    * Add llama as import library
    
    * Prefix target with package name
    
    * Add example project using CMake package
    
    * Update README
    
    * Update README
    
    * Remove trailing whitespace

commit 980ab41afba96106cd29cdf3aa6f948c251cb71f
Author: dylan <canardleteer@users.noreply.github.com>
Date:   Thu Sep 14 09:47:00 2023 -0700

    docker : add gpu image CI builds (#3103)
    
    Enables the GPU enabled container images to be built and pushed
    alongside the CPU containers.
    
    Co-authored-by: canardleteer <eris.has.a.dad+github@gmail.com>

commit e394084166baac09e8ee9a08a4686f907f7e5291
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Thu Sep 14 10:32:26 2023 -0600

    gguf-py : support identity operation in TensorNameMap (#3095)
    
    Make try_suffixes keyword param optional.

commit 4c8643dd6ea1a163bc5979cb69c1e7ab0975bc93
Author: jameswu2014 <545426914@qq.com>
Date:   Fri Sep 15 00:32:10 2023 +0800

    feature : support Baichuan serial models (#3009)

commit 35f73049af6c676a106a5a990a819ae0bc3fcd7d
Author: Leng Yue <lengyue@lengyue.me>
Date:   Thu Sep 14 09:14:44 2023 -0700

    speculative : add heuristic algorithm (#3006)
    
    * Add heuristic algo for speculative
    
    * Constrain minimum n_draft to 2
    
    * speculative : improve heuristic impl
    
    * speculative : be more rewarding upon guessing max drafted tokens
    
    * speculative : fix typos
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 71ca2fad7d6c0ef95ef9944fb3a1a843e481f314
Author: goerch <jhr.walter@t-online.de>
Date:   Wed Sep 13 15:19:44 2023 +0200

    whisper : tokenizer fix + re-enable tokenizer test for LLaMa (#3096)
    
    * Fix für #2721
    
    * Reenable tokenizer test for LLaMa
    
    * Add `console.cpp` dependency
    
    * Fix dependency to `common`
    
    * Fixing wrong fix.
    
    * Make console usage platform specific
    
    Work on compiler warnings.
    
    * Adapting makefile
    
    * Remove trailing whitespace
    
    * Adapting the other parts of the makefile
    
    * Fix typo.

commit 1b6c650d16048d6427dd502a9627e72837265844
Author: Tristan Ross <rosscomputerguy@protonmail.com>
Date:   Wed Sep 13 06:08:52 2023 -0700

    cmake : add a compiler flag check for FP16 format (#3086)

commit 0a5eebb45d5697127b84418576dc479c400c4b3d
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Sep 13 11:20:24 2023 +0200

    CUDA: mul_mat_q RDNA2 tunings (#2910)
    
    * CUDA: mul_mat_q RDNA2 tunings
    
    * Update ggml-cuda.cu
    
    Co-authored-by: Henri Vasserman <henv@hot.ee>
    
    ---------
    
    Co-authored-by: Henri Vasserman <henv@hot.ee>

commit 84e723653ca99d51a74b454984acf2c077468561
Author: FK <sozforex@gmail.com>
Date:   Wed Sep 13 08:50:46 2023 +0200

    speculative: add --n-gpu-layers-draft option (#3063)

commit b52b29ab9d601bb298050bcd2261169bc917ba2c
Author: Eric Sommerlade <es0m@users.noreply.github.com>
Date:   Wed Sep 13 02:54:20 2023 +0100

    arm64 support for windows (#3007)
    
    Co-authored-by: Cebtenzzre <cebtenzzre@gmail.com>

commit 4f7cd6ba9c88d3ca9a207b6e04f8b2b1efd707b8
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Sep 13 00:15:33 2023 +0200

    CUDA: fix LoRAs (#3130)

commit 89e89599fd095172f8d67903b5e227467420f036
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Sep 11 22:58:41 2023 +0200

    CUDA: fix mul_mat_q not used for output tensor (#3127)

commit d54a4027a6ebda98ab0fef7fa0c2247d0bef132a
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Sep 11 19:55:51 2023 +0200

    CUDA: lower GPU latency + fix Windows performance (#3110)

commit 1b0d09259e37898c519edb6c52d58f4d096f10bd
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Mon Sep 11 19:49:06 2023 +0800

    cmake : support build for iOS/tvOS (#3116)
    
    * cmake : support build for iOS/tvOS
    
    * ci : add iOS/tvOS build into macOS-latest-cmake
    
    * ci : split ios/tvos jobs

commit 8a4ca9af569853023ce87f047eb5165df13f2ff1
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Sep 11 13:00:24 2023 +0200

    CUDA: add device number to error messages (#3112)

commit f31b6f4e2d6def3c0bd7c75f75c0c1e8698e0589
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Sep 11 09:30:11 2023 +0200

    metal : PP speedup (#3084)
    
    * Minor speed gains for all quantization types
    
    * metal: faster kernel_scale via float4
    
    * Various other speedups for "small" kernels
    
    * metal: faster soft_max vial float4
    
    * metal: faster diagonal infinity
    
    Although, to me it looks like one should simply
    fuse scale + diagnonal infinity + soft_max on the
    KQtensor.
    
    * Another faster f16 x f32 matrix multiply kernel
    
    * Reverting the diag infinity change
    
    It does work for PP, but somehow it fails for TG.
    Need to look more into it.
    
    * metal: add back faster diagonal infinity
    
    This time more carefully
    
    * metal : minor (readibility)
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 6eeb4d90839bac1e6085e5544654ab5c319ad09a
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Sun Sep 10 17:06:53 2023 +0200

    convert: remove most of the n_mult usage in convert.py (#3098)

commit 21ac3a1503001020122db5dce6adf34b761675f5
Author: kchro3 <62481661+kchro3@users.noreply.github.com>
Date:   Sat Sep 9 02:12:10 2023 -0700

    metal : support for Swift (#3078)
    
    * Metal support for Swift
    
    * update
    
    * add a toggle for arm/arm64
    
    * set minimum versions for all platforms
    
    * update to use newLibraryWithURL
    
    * bump version
    
    Co-authored-by: Jhen-Jie Hong <iainst0409@gmail.com>
    
    ---------
    
    Co-authored-by: Jhen-Jie Hong <iainst0409@gmail.com>

commit 4fd54779550e43e2a29f6840ebcf8f395a2f879e
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Sat Sep 9 16:46:04 2023 +0800

    metal : support build for iOS/tvOS (#3089)

commit ec2a24fedf1de8ebd5f170016953b09ff2806924
Author: takov751 <40316768+takov751@users.noreply.github.com>
Date:   Fri Sep 8 17:06:26 2023 +0100

    flake : add train-text-from-scratch to flake.nix (#3042)

commit 7d99aca759f2f8a1ff39f3bb02a840f69863428b
Author: Ikko Eltociear Ashimine <eltociear@gmail.com>
Date:   Sat Sep 9 01:04:32 2023 +0900

    readme : fix typo (#3043)
    
    * readme : fix typo
    
    acceleation -> acceleration
    
    * Update README.md
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit ba7ffbb2517ff8cf4c689f94a9ad866f3ee71225
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Sep 8 18:01:04 2023 +0200

    metal : Q3_K speedup (#2995)
    
    * Slightly faster Q3_K and Q5_K on metal
    
    * Another Q3_K speedup on metal
    
    Combined with previous commit, we are now +9.6% for TG.
    PP is not affected as this happens via the matrix multiplication
    templates.
    
    * Slowly progressing on Q3_K on metal
    
    We are now 13% faster than master
    
    * nother small improvement for Q3_K on metal
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit e64f5b55783e910d8287363895d652b4bea6527a
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Sep 8 11:43:35 2023 -0400

    examples : make n_ctx warning work again (#3066)
    
    This was broken by commit e36ecdcc ("build : on Mac OS enable Metal by
    default (#2901)").

commit 94f10b91ed69980f299441e49c8dbdb448f0ccc6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 8 18:18:04 2023 +0300

    readme : update hot tpoics

commit b3e9852e471d12cbbe5dad20c81c4766d969739a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 8 17:58:07 2023 +0300

    sync : ggml (CUDA GLM RoPE + POSIX) (#3082)
    
    ggml-ci

commit cb6c44c5e045709b6bb5cc9bb8c9be107c771a78
Author: Przemysław Pawełczyk <przemoc@gmail.com>
Date:   Fri Sep 8 14:09:21 2023 +0200

    build : do not use _GNU_SOURCE gratuitously (#2035)
    
    * Do not use _GNU_SOURCE gratuitously.
    
    What is needed to build llama.cpp and examples is availability of
    stuff defined in The Open Group Base Specifications Issue 6
    (https://pubs.opengroup.org/onlinepubs/009695399/) known also as
    Single Unix Specification v3 (SUSv3) or POSIX.1-2001 + XSI extensions,
    plus some stuff from BSD that is not specified in POSIX.1.
    
    Well, that was true until NUMA support was added recently,
    so enable GNU libc extensions for Linux builds to cover that.
    
    Not having feature test macros in source code gives greater flexibility
    to those wanting to reuse it in 3rd party app, as they can build it with
    FTMs set by Makefile here or other FTMs depending on their needs.
    
    It builds without issues in Alpine (musl libc), Ubuntu (glibc), MSYS2.
    
    * make : enable Darwin extensions for macOS to expose RLIMIT_MEMLOCK
    
    * make : enable BSD extensions for DragonFlyBSD to expose RLIMIT_MEMLOCK
    
    * make : use BSD-specific FTMs to enable alloca on BSDs
    
    * make : fix OpenBSD build by exposing newer POSIX definitions
    
    * cmake : follow recent FTM improvements from Makefile

commit a21baeb12202a9020b48c53beaaf4b355228e8ba
Author: hongbo.mo <352280764@qq.com>
Date:   Fri Sep 8 18:57:55 2023 +0800

    docker : add git to full-cuda.Dockerfile main-cuda.Dockerfile (#3044)

commit 6ff712a6d1a0c85d996e2f681df57a2554cfe5c1
Author: Yui <dev@sleepyyui.com>
Date:   Fri Sep 8 12:32:55 2023 +0200

    Update deprecated GGML TheBloke links to GGUF (#3079)

commit ebc96086af49fe70108cafcea6ab4bebd658a41a
Author: slaren <slarengh@gmail.com>
Date:   Fri Sep 8 04:04:56 2023 +0200

    ggml-alloc : correctly check mmap return value for errors (#3075)

commit 7f412dab9c8801f5d37904f7dce1faf4c2b43b42
Author: Kunshang Ji <kunshang.ji@intel.com>
Date:   Fri Sep 8 09:46:56 2023 +0800

    enable CPU HBM (#2603)
    
    * add cpu hbm support
    
    * add memalign 0 byte check
    
    * Update ggml.c
    
    * Update llama.cpp
    
    * ggml : allow ggml_init with 0 size
    
    * retrigger ci
    
    * fix code style
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 6336d834ec7bff3e93e24182c0f609d2f2bdce26
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Sep 7 14:27:42 2023 -0400

    convert : fix F32 ftype not being saved (#3048)

commit 00d62adb79bf914a95fb9a2e8f42f3029e76d62c
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Sep 7 13:22:29 2023 -0400

    fix some warnings from gcc and clang-tidy (#3038)
    
    Co-authored-by: xaedes <xaedes@gmail.com>

commit 4fa2cc1750b861880de42515cb19c13b2d776ee2
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Sep 7 10:15:01 2023 -0400

    make : improve test target (#3031)

commit 5ffab089a54bc06ae4a9ab533893b558756a1e80
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Sep 7 10:13:50 2023 -0400

    make : fix CPPFLAGS (#3035)

commit 15b67a66c2f2d6032415b28a699b5131962318f1
Author: slaren <slarengh@gmail.com>
Date:   Thu Sep 7 15:52:34 2023 +0200

    llama-bench : use two tokens in the warmup run for prompt evals (#3059)

commit be8c9c245bd129ebabb80e0a7a8dd7daeb4d30af
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Sep 7 15:45:01 2023 +0200

    metal : parallel RoPE on Metal (#3024)
    
    * Parallel RoPE on metal
    
    * PR suggestion
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit be6beeb8d75294552c4918fce06d7b84eebf3d79
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Sep 7 15:42:42 2023 +0200

    metal : correct fix of kernel_norm (#3060)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit c4f496648c1e32efeb714200e7eae7fc7cfbb223
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Sep 7 15:49:09 2023 +0300

    metal : fix kernel_norm (fixes Falcon on Metal) (#3057)
    
    * metal : fix kernel_norm
    
    ggml-ci
    
    * metal : put warning in kernel_norm to not combine the loops
    
    * metal : restore original F16 mat-vec multiplication
    
    It works after the norm fixes
    
    * common : don't do warm-up with more than n_batch tokens (close #3058)
    
    ggml-ci
    
    * metal : minor

commit fec2fb19e4229aac58c98171c46e77144b99f8a3
Author: Przemysław Pawełczyk <przemoc@gmail.com>
Date:   Thu Sep 7 10:15:06 2023 +0200

    ggml : posixify madvise and pagesize (#3037)
    
    * llama : use posix_madvise() instead of madvise() derived from BSD
    
    sed -i 's,\<madvise\>,posix_&,g;s,\<MADV_,POSIX_&,g' llama.cpp
    
    * ggml : use sysconf(_SC_PAGESIZE) instead of getpagesize() derived from BSD
    
    sed -i 's,getpagesize(),sysconf(_SC_PAGESIZE),g' ggml.c
    
    * metal : use sysconf(_SC_PAGESIZE) instead of getpagesize() derived from BSD
    
    sed -i 's,getpagesize(),sysconf(_SC_PAGESIZE),g' ggml-metal.m

commit 178b1850ebd21b349cebbee887950e435c5aa2d3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Sep 6 12:40:57 2023 +0300

    k-quants : fix zero-weight guard in Q6_K (ref #3040)

commit ea2c85d5d2a93d39d0172222917f3195f0e456ff
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Wed Sep 6 02:49:11 2023 -0600

    convert-llama-ggml-to-gguf: Try to handle files older than GGJTv3 (#3023)
    
    * convert-llama-ggmlv3-to-gguf: Try to handle files older than GGJTv3
    
    * Better error messages for files that cannot be converted
    
    * Add file type to GGUF output
    
    * Rename to convert-llama-ggml-to-gguf.py
    
    * Include original file type information in description
    
    * Improve some informational output

commit 9912b9efc8922321fe7202ab42ba913833cbe9cd
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Tue Sep 5 18:21:10 2023 -0400

    build : add LLAMA_METAL_NDEBUG flag (#3033)

commit 9e2023156e5b5acabaf8632e66c6ae68d3703c31
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Tue Sep 5 15:12:00 2023 -0400

    make : use new flag variables for recent changes (#3019)

commit de2fe892af92a5c7b5ef1beb7efbc0524343fbab
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Tue Sep 5 15:10:27 2023 -0400

    examples : replace fprintf to stdout with printf (#3017)

commit c9c3220c485c7bea740a07cda7343677fb3beaae
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Tue Sep 5 19:41:00 2023 +0200

    convert: fix convert.py not working with int filename_stem (#3028)
    
    * fix implicit int to string conversion
    * convert : remove an obsolete pyright comment
    
    ---------
    
    Co-authored-by: Cebtenzzre <cebtenzzre@gmail.com>

commit d59bd97065cd7ded6c4ecab54b1d5e0b1b11e318
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Sep 5 09:55:33 2023 +0200

    Guard against all weights in a super-block being zero (#3010)
    
    * Guard against all weights in a super-block being zero
    
    * Also guard against extremely small weights
    
    Closes #2982
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 35938ee3b0c16f1fbbf240dae21e0228864b938c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Sep 5 10:46:39 2023 +0300

    llama : update logic for number of threads when using BLAS

commit 921772104ba2219bfdc2b2980d05ebc0aa0c92a4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Sep 5 08:46:17 2023 +0300

    speculative : add grammar support (#2991)
    
    * speculative : add grammar support
    
    * grammars : add json_arr.gbnf
    
    * grammar : add comments to new grammar file
    
    * grammar : remove one nested level
    
    * common : warm-up with 2 tokens - seems to work better
    
    * speculative : print draft token pieces
    
    * speculative : reuse grammar parser + better logs and comments
    
    * speculative : avoid grammar_mem
    
    * make : fix speculative build

commit 2ba85c8609309a59d49c45ab43c31800b7ba141c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Sep 4 22:50:50 2023 +0300

    py : minor

commit e36ecdccc8754783f93ad3ac8a09e540101f2ca0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Sep 4 22:26:24 2023 +0300

    build : on Mac OS enable Metal by default (#2901)
    
    * build : on Mac OS enable Metal by default
    
    * make : try to fix build on Linux
    
    * make : move targets back to the top
    
    * make : fix target clean
    
    * llama : enable GPU inference by default with Metal
    
    * llama : fix vocab_only logic when GPU is enabled
    
    * common : better `n_gpu_layers` assignment
    
    * readme : update Metal instructions
    
    * make : fix merge conflict remnants
    
    * gitignore : metal

commit bd33e5ab92e7f214205792fc1cd9ca28e810f897
Author: slaren <slarengh@gmail.com>
Date:   Mon Sep 4 14:59:52 2023 +0200

    ggml-opencl : store GPU buffer in ggml_tensor::extra (#2994)

commit 31035681445181fb414e0def7ec3f84462b3bd97
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Mon Sep 4 06:40:18 2023 -0400

    llama-bench : make cpp file non-executable (#2999)

commit 5b8530d88c489f9d0c0ef3d0886b369f655b792e
Author: Leng Yue <lengyue@lengyue.me>
Date:   Mon Sep 4 03:39:57 2023 -0700

    make : add speculative example (#3003)

commit e4386f417faf894f6706eec005e24d142b577fcb
Author: Aarni Koskela <akx@iki.fi>
Date:   Mon Sep 4 10:28:55 2023 +0200

    server : add a subtle loading animation to the edit box (#2466)
    
    * editorconfig: add override for the server HTML (which already is 2-space indented)
    
    * server: add a subtle loading animation to the edit box

commit 35195689cd835464779c247b1c22ab9247418fd1
Author: Jiahao Li <liplus17@163.com>
Date:   Mon Sep 4 14:53:30 2023 +0800

    2x faster (rms) norm cuda kernels (3.7% e2e improvement) (#2985)
    
    * 2x faster (rms) norm cuda kernels
    
    * Fix code style

commit cf9b08485c4c2d4d945c6e74fe20f273a38b6104
Author: slaren <slarengh@gmail.com>
Date:   Sun Sep 3 20:34:09 2023 +0200

    ggml-alloc : use virtual memory for measurement (#2973)
    
    * ggml-alloc : use virtual memory for measurement
    
    * compatibility fixes for MAP_ANONYMOUS
    
    * fallback to fixed address for systems without virtual memory

commit 47068e517004d90f13c16352bb3b4cafd53a00cd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Sep 3 15:12:08 2023 +0300

    speculative : PoC for speeding-up inference via speculative sampling (#2926)
    
    * speculative : initial example
    
    * speculative : print encoding speed
    
    * speculative : add --draft CLI arg

commit 8f429fa5111901f9646cf998643ac5310846d487
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Sep 3 13:42:56 2023 +0300

    perplexity : fix ETA by warming up the model with an empty run

commit 6519e9c99cffbad19b31bcba86df48c500628c09
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sun Sep 3 04:38:43 2023 -0600

    gguf(python): Fix special vocab handling when id < 0 (#2984)

commit b7f2aa9e512c3be2e863d877cbb1056d7c4a03f8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Sep 3 13:23:33 2023 +0300

    metal : restore 363f0bf and fix reduce in F16_F32 kernels (#2986)

commit 73a12a6344d5da4d8e2eba5d12221b8bc6895931
Author: Alon <alonfaraj@gmail.com>
Date:   Sun Sep 3 13:19:01 2023 +0300

    cov : disable comment in PRs (#2989)

commit 37301347767d555d0a66c043ce4ef6ead8e61c55
Author: opparco <parco.opaai@gmail.com>
Date:   Sun Sep 3 19:18:09 2023 +0900

    llama : fix bpe tokenize from byte (#2889)

commit d9151e6f570eb20bfd54427bd8a337d9b1a08018
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Sep 3 12:40:56 2023 +0300

    metal : revert 6af0bab until we fix it
    
    This restores the generated text to be the same as before #2959

commit afc43d5f82588d2ed71ea104e8262f5e5da13980
Author: Alon <alonfaraj@gmail.com>
Date:   Sun Sep 3 11:48:49 2023 +0300

    cov : add Code Coverage and codecov.io integration (#2928)
    
    * update .gitignore
    
    * makefile: add coverage support (lcov, gcovr)
    
    * add code-coverage workflow
    
    * update code coverage workflow
    
    * wun on ubuntu 20.04
    
    * use gcc-8
    
    * check why the job hang
    
    * add env vars
    
    * add LLAMA_CODE_COVERAGE=1 again
    
    * - add CODECOV_TOKEN
    - add missing make lcov-report
    
    * install lcov
    
    * update make file -pb flag
    
    * remove unused  GGML_NITER from workflows
    
    * wrap coverage output files in COV_TARGETS

commit 6460f758dbd472653296044d36bed8c4554988f5
Author: Wentai Zhang <rchardx@gmail.com>
Date:   Sun Sep 3 16:46:44 2023 +0800

    opencl : fix a bug in ggml_cl_pool_malloc() for ggml_cl_mul_mat_f32() (#2955)
    
    Co-authored-by: Wentai Zhang <wentaizhang@tencent.com>

commit ca82cf7bac0c91d03e3d320b3a865dd006f854ac
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Sep 3 11:06:22 2023 +0300

    metal : more optimizations (#2959)
    
    * Very minor speedup via simd-group synchronization in f16 x f32
    
    * Another very minor speedup on metal
    
    * Quite significant PP speedup on metal
    
    * Another attempt
    
    * Minor
    
    * Massive improvement for TG for fp16
    
    * ~4-5% improvement for Q8_0 TG on metal
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 6a31a3bd9806c85ed08266f6ab65181da0f30d03
Author: kchro3 <62481661+kchro3@users.noreply.github.com>
Date:   Sat Sep 2 23:21:05 2023 -0700

    swift : add support for k-quants (#2983)

commit cff7b0bf07cb46e1ad4fd199f6bdeb538925c8c4
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Sep 2 23:52:13 2023 -0600

    convert.py : BPE fixes (#2938)
    
    * convert.py: BPE fixes?
    
    * Remove unnecessary conditional in addl token error handling

commit 340af42f09a80e32f4998857b4f0543e41124525
Author: Ido S <ido.pluto@gmail.com>
Date:   Sun Sep 3 08:50:51 2023 +0300

    docs : add `catai` to `README.md` (#2967)

commit c42f0ec6b344e14bd81c8612ab1445b3ff77358b
Author: momonga <115213907+mmnga@users.noreply.github.com>
Date:   Sun Sep 3 14:36:28 2023 +0900

    examples : fix gpt-neox (#2943)
    
    Co-authored-by: mmnga <mmnga1mmnga@gmail.com>

commit 2753415afdaf22a18c49608bd9d93cfffc05d435
Author: kchro3 <62481661+kchro3@users.noreply.github.com>
Date:   Sat Sep 2 22:27:25 2023 -0700

    swift : add missing c file to Package.swift (#2978)

commit bc054af97ac68a4b726e972cb283eb9565253ed5
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Sun Sep 3 01:26:59 2023 -0400

    make : support overriding CFLAGS/CXXFLAGS/CPPFLAGS/LDFLAGS (#2886)
    
    * make : remove unused -DGGML_BIG_ENDIAN
    
    * make : put preprocessor stuff in CPPFLAGS
    
    * make : pass Raspberry Pi arch flags to g++ as well
    
    * make : support overriding CFLAGS/CXXFLAGS/CPPFLAGS/LDFLAGS
    
    * make : fix inverted conditional

commit 3358c381f6251bf6e65855e1c93bfaa9ec82ddb3
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Sep 2 11:53:55 2023 -0600

    logging: Fix creating empty file even when disabled (#2966)
    
    * logging: Fix creating empty file even when disabled
    
    * Minor formatting fix
    
    Co-authored-by: staviq <staviq@gmail.com>
    
    ---------
    
    Co-authored-by: staviq <staviq@gmail.com>

commit 52315a421674ff64305dbf082f69e4ec77f0a3f3
Author: bandoti <141645996+bandoti@users.noreply.github.com>
Date:   Sat Sep 2 09:53:18 2023 -0300

    readme : update clblast instructions (#2903)
    
    * Update Windows CLBlast instructions
    
    * Update Windows CLBlast instructions
    
    * Remove trailing whitespace

commit 8b56b4f2c396eae1f4417e5a859557fed989e0ee
Author: Karsten Weiss <knweiss@gmail.com>
Date:   Sat Sep 2 14:29:09 2023 +0200

    metal : show all Metal device instances in the system (#2952)
    
    * ggml_metal_init: Show all Metal device instances in the system
    
    Also show the default Metal device that was picked.
    
    * Update ggml-metal.m
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 21f3d1be867b4d7be07c26f5da6e4bc69bcf4d27
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Sat Sep 2 20:23:45 2023 +0800

    k-quants : fix build on armv7 (android only) (#2920)
    
    * k-quants : fix build on armv7
    
    * ggml : cleanup unused arm32 specific impl
    
    * k-quants : avoid some unused vzero / mzero define
    
    * ggml-alloc : use 4g for MEASURE_MAX_SIZE in 32-bit arm

commit 571083f508266c4eb5cb5457d836df5dd3c173ce
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Sat Sep 2 08:31:46 2023 +0800

    server : avoid aniprompt in probabilities of final response (#2849)

commit f04d0028444bc9b3d4225fba47e19d4c3aeb3741
Author: Engininja2 <139037756+Engininja2@users.noreply.github.com>
Date:   Fri Sep 1 15:33:19 2023 -0600

    cuda : vsubss4 for older versions of ROCm/clang (#2942)

commit 69fdbb9abc8907dd2a9ffdd840cba92d678a660a
Author: ZHAOKAI WANG <sanxianwei@163.com>
Date:   Fri Sep 1 22:06:44 2023 +0800

    readme : quick start command fix (#2908)
    
    * quick start command fix
    
    * quick start win command fix

commit 5d6f19f16b2173afe2d5c6aee2f5c9fc31038eba
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Fri Sep 1 08:02:48 2023 -0600

    Allow quantize to only copy tensors, some other improvements (#2931)
    
    * Allow quantize tool to only copy tensors to allow repackaging models.
    
    * Slightly better logic when requantizing.
    
    * Change help message to go to `stdout`.

commit 0d5893668625456c94bbadfddc53fc69cd51c223
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 1 17:00:40 2023 +0300

    llama2c : rename function

commit 6c9c23429bf4e4fcaaddbebadc4638558430a7f2
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Sep 1 09:53:14 2023 -0400

    make : use unaligned vector moves on MinGW (#2945)
    
    Fixes #2922

commit ee8654bcd0146708988a703e54406d5b553712ea
Author: m3ndax <adrian.goessl@outlook.com>
Date:   Fri Sep 1 15:47:27 2023 +0200

    minor : add const qualifiers (#2853)
    
    * made the methods const
    
    # Conflicts:
    #       examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp
    
    * made method const
    
    * Update convert-llama2c-to-ggml.cpp
    
    removed write_raw and write_u32
    
    * llama2c : remove misleading const
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 49bb9cbe0f598bc43be539b0df8eafb2130cfad3
Author: Konstantin Herud <konstantin.herud@denkbares.com>
Date:   Fri Sep 1 15:36:14 2023 +0200

    docs : add java-llama.cpp to README.md (#2935)

commit ef156499721c67748cde01a5436cb6f0648bb4b4
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Sep 1 09:34:50 2023 -0400

    build : fix most gcc and clang warnings (#2861)
    
    * fix most gcc and clang warnings
    
    * baby-llama : remove commented opt_params_adam
    
    * fix some MinGW warnings
    
    * fix more MinGW warnings

commit d8d6977f48f1fa402ade38ad32c5b5fb1358d059
Author: Ben Siraphob <bensiraphob@gmail.com>
Date:   Fri Sep 1 09:32:14 2023 -0400

    examples : add C grammar (#2357)

commit 5aec2cfaac386eb09aebb75b805860828f00de91
Author: Tameem <113388789+AhmadTameem@users.noreply.github.com>
Date:   Fri Sep 1 18:27:40 2023 +0500

    ggml : add RISC-V vector intrinsics support (#2929)
    
    * added support for RISCV CFLAGS & native compile + cross compile options
    
    * Add RISC-V Vector Intrinsics Support
    
    Added RVV intrinsics for following
       ggml_vec_dot_q4_0_q8_0
       ggml_vec_dot_q4_1_q8_1
       ggml_vec_dot_q5_0_q8_0
       ggml_vec_dot_q5_1_q8_1
       ggml_vec_dot_q8_0_q8_0
    
    Co-authored-by: Sharafat <sharafat.hussain@10xengineers.ai>
    Signed-off-by: Ahmad Tameem <ahmad.tameem@10xengineers.ai>
    
    ---------
    
    Signed-off-by: Ahmad Tameem <ahmad.tameem@10xengineers.ai>
    Co-authored-by: moiz.hussain <moiz.hussain@10xengineers.ai>
    Co-authored-by: Sharafat <sharafat.hussain@10xengineers.ai>

commit 13268c533177a4dc76bce0b465645d74f0d51d55
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 1 13:42:41 2023 +0300

    metal : slight speed-up for add and mul kernels (#2917)

commit 4dcd47d71df8ca4edcc31302744bd93f0c31298e
Author: staviq <staviq@gmail.com>
Date:   Fri Sep 1 11:07:06 2023 +0200

    logs : fix mingw-like builds (fixes #2898) (#2911)
    
    * fix mingw-like builds
    
    * formatting
    
    * make LOG_COMPAT easier to override and extend
    
    * simplify win detection
    
    * fix for #2940

commit 18705a30ef3d6a89e1d7c6cb8cfe8633f760cb53
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Sep 1 05:03:49 2023 -0400

    llama2c : fix segfault and alloc-dealloc-mismatch (#2913)
    
    * llama2c : fix segfault if vocab is not found
    
    * llama2c : fix mismatch between new[] and delete
    
    * llama2c : fix basename on Windows
    
    * llama2c : use a destructor to prevent memory leaks

commit e8d91589258f9204397a7ac5f9b3c857835c98f8
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Sep 1 11:15:57 2023 +0300

    metal: somewhat faster f16 x f32 matrix multiply kernel (#2951)
    
    * Somewhat faster f16 x f32 matrix multiply kernel
    
    * Better use 32 thread groups for f16 x f32
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit bce1fef328941499dc0acb76cc7fd7ac90449c2f
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Aug 31 22:13:51 2023 -0400

    convert : fix another python 3.8 issue (#2949)

commit 528134dd0267838d9c0250cf1d9621631dff09b2
Author: slaren <slarengh@gmail.com>
Date:   Fri Sep 1 01:32:09 2023 +0200

    remove convert-llama-7b-pth-to-gguf.py and convert-llama-hf-to-gguf.py (#2906)

commit aeefac4ff760acea5afe66fbfe8d7eca1937b79c
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Thu Aug 31 16:49:24 2023 -0600

    scripts: Use local gguf package when running from repo (#2927)
    
    * scripts: Use local gguf when running from repo

commit e8422de39e4aa2f7e50574124b060a80607e654a
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Thu Aug 31 04:21:45 2023 -0700

    @vxiiduu's fix for PrefetchVirtualMemory (#2930)
    
    Reimplement fix for `PrefetchVirtualMemory`.
    Co-authored-by: vxiiduu <73044267+vxiiduu@users.noreply.github.com>

commit 92d0b751a77a089e650983e9f1564ef4d31b32b9
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Aug 31 01:02:23 2023 -0400

    convert : fix python 3.8 support, modernize type annotations (#2916)
    
    * convert : fix python 3.8 support
    
    * convert : sort imports
    
    * convert : fix required parameters in convert-llama-ggmlv3-to-gguf
    
    * convert : fix mypy errors in convert-llama-ggmlv3-to-gguf
    
    * convert : use PEP 585 generics and PEP 604 unions
    
    Now that we have `from __future__ import annotations`, we can use this
    modern syntax in Python 3.7 instead of restricting support to Python 3.9
    or 3.10 respectively.
    
    * gguf.py : a tuple is already a tuple
    
    * add mypy.ini
    
    * convert : add necessary `type: ignore` comments
    
    * gguf-py: bump version

commit 8afe2280009ecbfc9de2c93b8f41283dc810609a
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Aug 30 21:46:19 2023 +0200

    CUDA: mul_mat_q=true llama_context_params default (#2912)

commit 71d6975559acfd6c8407a4ef8275a9979c737765
Author: Henri Vasserman <henv@hot.ee>
Date:   Wed Aug 30 19:14:53 2023 +0300

    [Docker] fix tools.sh argument passing. (#2884)
    
    * [Docker] fix tools.sh argument passing.
    
    This should allow passing multiple arguments to containers with
    the full image that are using the tools.sh frontend.
    
    Fix from https://github.com/ggerganov/llama.cpp/issues/2535#issuecomment-1697091734

commit b532a69b2fd08067f34f32f37a2fd9b37678a34a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Aug 30 13:29:40 2023 +0300

    convert.py : use dir name to name the llama

commit c90d135eb433cf0d40fb95e46a48d1391d2352b5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Aug 30 12:52:46 2023 +0300

    examples : fix underscore in beam-search + .gitignore (close #2900)

commit 0d1c706181cd31e7f368dd14eeb16c1a2569e4df
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Wed Aug 30 12:47:40 2023 +0300

    gguf : add workflow for Pypi publishing (#2896)
    
    * gguf : add workflow for Pypi publishing
    
    * gguf : add workflow for Pypi publishing
    
    * fix trailing whitespace

commit 950929442070874d45561d2a4b68b010457767de
Author: alonfaraj <alonfaraj@gmail.com>
Date:   Wed Aug 30 12:42:51 2023 +0300

    make : add test and update CI (#2897)
    
    * build ci: run make test
    
    * makefile:
    - add all
    - add test
    
    * enable tests/test-tokenizer-0-llama
    
    * fix path to model
    
    * remove gcc-8 from macos build test
    
    * Update Makefile
    
    * Update Makefile

commit 35092fb54712d032860f3976a6fc1ae1f84a4a28
Author: Gilad S <giladgd@users.noreply.github.com>
Date:   Wed Aug 30 11:40:12 2023 +0300

    docs : add `node-llama-cpp` to `README.md` (#2885)

commit dc07dc492ef9640bbb82904d7c7679f7bdcf6d76
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Wed Aug 30 02:25:50 2023 -0600

    convert : various script cleanups/fixes + merges and special token handling (#2842)
    
    * convert: Fix permute calls and method/func definitions
    
    * Cleanups for gguf-py
    
    * Minor types cleanups.
    
    * Initial implementation of handling merges and special tokens
    
    * convert: Handle special tokens and merges in vocab only mode
    
    convert: Vocab only mode no longer requires loading model tensors
    
    * gguf: Refactor tensor name mapping
    
    * convert: Fix type hint for special_token_types in SpecialVocab
    
    * Use common special vocab handling in various conversion scripts
    
    * First pass at implementing suggested changes
    
    * Second pass
    
    * gguf: SpecialVocab: Fix issue with special token content not in a dict
    
    gguf: SpecialVocab: Allow skipping handling of merges
    
    * convert-falcon-hf-to-gguf: Support --vocab-only option, bail out if no tokenizer.json
    
    * convert-gptneox-hf-to-gguf and convert: Only handle merges for BPE tokenizer
    
    * gguf: SpecialVocab: Actually set load_merges in object
    
    * Uniform args parsing and vocab only mode for convert examples
    
    * convert.py: Set gpt2 as tokenizer model when using BPE
    
    * Squish last type warning in gguf.py - yay!

commit ad9ddcff6ef322db5cf13785bd7c856b610d242e
Author: chaihahaha <chai836275709@gmail.com>
Date:   Wed Aug 30 14:50:55 2023 +0800

    llm.vim : stop generation at multiple linebreaks, bind to <F2> (#2879)

commit 8341a25957b319a03d4a811176cd5ad7f2b0fbd4
Author: staviq <staviq@gmail.com>
Date:   Wed Aug 30 08:29:32 2023 +0200

    main : log file (#2748)
    
    * initial, base LOG macro
    
    * add *.log to .gitignore
    
    * added basic log file handler
    
    * reverted log auto endline to better mimic printf
    
    * remove atomics and add dynamic log target
    
    * log_enable/disable, LOG_TEE, basic usage doc
    
    * update .gitignore
    
    * mv include to common, params, help msg
    
    * log tostring helpers, token vectors pretty prints
    
    * main: replaced fprintf/LOG_TEE, some trace logging
    
    * LOG_DISABLE_LOGS compile flag, wrapped f in macros
    
    * fix LOG_TEELN and configchecker
    
    * stub LOG_DUMP_CMDLINE for WIN32 for now
    
    * fix msvc
    
    * cleanup main.cpp:273
    
    * fix stray whitespace after master sync
    
    * log : fix compile warnings
    
    - do not use C++20 stuff
    - use PRIu64 to print uint64_t
    - avoid string copies by using const ref
    - fix ", ##__VA_ARGS__" warnings
    - compare strings with == and !=
    
    * log : do not append to existing log + disable file line func by default
    
    * log : try to fix Windows build
    
    * main : wip logs
    
    * main : add trace log
    
    * review: macro f lowercase, str append to sstream
    
    * review: simplify ifs and str comparisons
    
    * fix MSVC, formatting, FMT/VAL placeholders
    
    * review: if/else cleanup
    
    * review: if/else cleanup (2)
    
    * replace _ prefix with _impl suffix
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 849408957c687cde4ab32c147107f643fc55130b
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Wed Aug 30 02:20:26 2023 -0400

    tests : add a C compliance test (#2848)
    
    * tests : add a C compliance test
    
    * make : build C compliance test by default
    
    * make : fix clean and make sure C test fails on clang
    
    * make : move -Werror=implicit-int to CFLAGS

commit 06abf8eebabe086ca4003dee2754ab45032cd3fd
Author: slaren <slarengh@gmail.com>
Date:   Tue Aug 29 23:24:42 2023 +0200

    ggml : add view_src and view_offs to ggml_tensor for views (#2874)
    
    * ggml : add view_src and view_offs
    
    * update ggml-alloc to use view_src
    
    * update ggml_diag_mask to work correctly with automatic inplace
    
    * exclude other ops that set an inplace flag from automatic inplace

commit c03a243abf9f30889f31fefdfa94fe9d7034820c
Author: slaren <slarengh@gmail.com>
Date:   Tue Aug 29 23:17:34 2023 +0200

    remove outdated references to -eps and -gqa from README (#2881)

commit fa3582f509a2715e80a473e79f88dcd1ebff44c2
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Aug 29 23:55:45 2023 +0300

    Tell users attmepting to run perplexity with too few tokens to use more (#2882)
    
    Closes #2858
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit e37e69dcc3d52f21222a63cafed2a71b3f6b53c6
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Aug 29 23:55:03 2023 +0300

    10X faster BPE tokenizer (#2876)
    
    * 10X faster BPE tokenizer
    
    * Remove comment that no longer applies
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 53885d7256909ec3e2176cdc2477f3986c15ec69
Author: maddes8cht <55592906+maddes8cht@users.noreply.github.com>
Date:   Tue Aug 29 15:51:02 2023 +0200

    py : fix "usage" messages (#2873)
    
    convert-to-gguf python scripts

commit bcce96ba4dd95482824700c4ce2455fe8c49055a
Author: jameswu2014 <545426914@qq.com>
Date:   Tue Aug 29 17:48:41 2023 +0800

    convert.py : fix baichuan7B support (#2870)
    
    * [Fix]: convert.py support baichuan7B
    
    * convert.py : fix trailing whitespaces
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 74e0caeb82fc9db77fa2cc93070bb919a9a935dd
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Tue Aug 29 17:30:10 2023 +0800

    readme : add react-native binding (#2869)

commit d4b5e16c32ba9c5fa6bbd035e80a99c113050cde
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Tue Aug 29 04:42:41 2023 -0400

    make : fix clang tests build, add missing examples (#2859)
    
    * make : do not pass headers to the compiler
    
    This fixes building tests with clang.
    
    * make : add missing examples
    
    * make : fix build-info.h dependencies

commit 3a007648f230ea37d6cca5e63850f04ebb12d2cf
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Aug 29 11:33:46 2023 +0300

    metal : add option to disable debug logs (close #2764)

commit 611363ac791435497e66278dfe31ac8a4e11fa4f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Aug 29 10:50:30 2023 +0300

    scripts : add pipefail

commit 95b6e5212f5e4e1419de1d833d7f8d788f9f2227
Author: Marcus Dunn <51931484+MarcusDunn@users.noreply.github.com>
Date:   Mon Aug 28 23:33:27 2023 -0700

    added `struct` to llama_dump_timing_info_yaml's `llama_context` (#2857)
    
    fixes C compat.

commit 44c117f41ee01c5ac8fb86bba041f08d8b87b46d
Author: xaedes <xaedes@gmail.com>
Date:   Mon Aug 28 21:51:47 2023 +0200

    train : mem usage and other improvements  (#2439)
    
    * fix track_max_mem in forward_batch_wo_cache_flash_attn_train
    
    * remove unnecessary Adam(W) optimizer tensors.
    
    reduces optimizer memory overhead from 7*modelsize to 2*modelsize.
    
    additionally allows to optimize models with more than 2^31 parameters by replacing int with int64_t.
    
    bumps training checkpoint file version, but old checkpoints can still be read.
    new version with less tensors is saved.
    
    * add gradient clipping to AdamW
    
    * Fix reset of unused g->nodes and g->grads to NULL
    
    * implement gradient checkpointing for training
    
    reduces memory overhead from O(n_layer) to O(sqrt(n_layer))
    
    as explained in readme of https://github.com/cybertronai/gradient-checkpointing
    
    * remove unused compute buffer 3
    
    * add and use function ggml_build_backward_expand to avoid stack overflows with large maximum number of nodes
    
    GGML_API void ggml_build_backward_expand(struct ggml_context * ctx, struct ggml_cgraph * gf, struct ggml_cgraph * gb, bool keep);
    
    * change AdamW decay parameter to work like the torch AdamW decay parameter
    
    It is now relative to Adam learning rate `alpha*sched`.
    Before that it was relative to `sched` only.
    
    `alpha` being the maximum learning rate and `sched` being a scaling parameter in [0..1]
    
    * change default AdamW weight decay parameter used in training to 0.1 as used in nanoGPT
    
    * change default AdamW weight decay parameter defined in ggml to 0.0, making Adam default instead of AdamW
    
    btw: the default weight decay parameter for torch.optim.AdamW is 0.01
    
    * bug fixes for cross entropy loss
    
    ggml_cross_entropy_loss: sums where not correctly added in workload of each thread
    ggml_cross_entropy_loss_back: simplify backward process, reducing numerical issues
    
    guard usage of exp f16 lookup in cross entropy by #define GGML_CROSS_ENTROPY_EXP_FP16
    
    cross entropy loss is only used once during training, but it is quite sensitive to numerical errors introduced by exp-f16-lookup.
    so exp-f16-lookup for cross entropy loss is disabled by default, trading better gradients for very slightly worse runtime performance.
    
    * fix test-grad0 for cross_entropy_loss
    
    the second argument to cross_entropy_loss must sum up to 1 for each row
    
    * fix test-grad0 for soft_max
    
    dont use only sum as aggregation, because sum of softmax is always 1 -> finite differences should not work
    instead use sum(log(soft_max()*(1-eps)+eps)); use eps to avoid log(0)
    
    * improve finite differences of test-grad0 by using double instead of float
    
    * change cross_entropy_loss to output average over all rows
    
    this helps keeping the loss and gradients in a sane range
    
    * improve gradient checkpointing
    
    sqrt(n_layers) is only the best checkpoint step when mem size of checkpoints and mem size of layers are equal.
    since layers require more memory than the single-tensor-checkpoint we use, the optimal values are compute different:
    
    ```
      given: n, u, v
      objective: minimize(a*u+b*v) where a*b=n, a>0, b>0
      b=n/a
      minimize(a*u+v*n/a)
      diff(a*u+v*n/a, a) = u - (v*n/a)/a
      diff(a*u+v*n/a, a) == 0
      u - (v*n/a)/a == 0
      u == v*n/(a*a)
      u*a*a = v*n
      a*a = v*n/u
      a = sqrt(n*v/u)
    ```
    
    this change results in more checkpoints, requiring less layers to store between checkpoints, overall improving memory usage.
    
    * disable gradient checkpointing debug output
    
    * llama : fix rope usage in train-text-from-scratch after ChatGLM change
    
    * add more training parameters:
    
    --enable-restart N         Only for Adam optimizer. Enable restarts of cos-decay
    --disable-restart N        Only for Adam optimizer. Disable restarts of cos-decay
    --opt-past N               Number of optimization iterations to track for delta convergence test. Disabled when zero.
    --opt-delta N              Maximum delta for delta convergence test. Disabled when <= zero.
    --opt-max-no-improvement N Maximum number of optimization iterations with no improvement. Disabled when <= zero.
    --adam-epsf N              AdamW epsilon for convergence test. Disabled when <= zero.
    --adam-min-alpha N         Adam minimum learning rate alpha, usually 0.1 * alpha
    
    * replace memcpy with reshape operation so that the graph is not cut at the input
    
    this makes it possible to store other values into the input tensor and then simply recompute the graph without rebuilding it
    
    * remove unused function argument from get_example_targets_batch
    
    * measure and print total training time
    
    * add optimization callback to ggml_opt_resume_g
    
    this callback is called before each iteration with custom data and pointer to learning schedule parameter (only used in Adam(W)).
    
    can be used for dynamic learning schedule and setting input data for batches before each iteration
    
    * use optimization callback in training
    
    allows dynamic learning schedule and different batch data for each iteration without relying on low n_iter and high n_examples parameters
    
    reduces runtime by avoiding restart of optimization function and improves training convergence by providing a different batch for each iteration
    
    * add minimum number of tensor dimensions to apply weight decay (default 2)
    
    this allows to not apply weight decay to bias parameters
    
    * rename training parameter cos-decay-alpha to cos-decay-min and clarify that adam-min-alpha also applies to warmup
    
    * fix increase of model.train_samples and model.train_tokens
    
    now that each optimizer iteration gets its own batch we need to multiply by number of opt iterations
    
    * change sampling parameters for prediction after training to defaults of common.h
    
    and clarify what is context for prediction and what are generated tokens
    
    * tighten abs error bounds for cross_entropy_loss in test-grad0
    
    * add conditional compilation of using F16 exp in flash attention
    
    uncomment `// #define GGML_FLASH_ATTN_EXP_FP16` to enable usage of f16 exp in flash attention
    
    * tighten abs error bounds for flash_attn in test-grad0
    
    * tighten abs error bounds for sqrt in test-grad0
    
    * remove out-commented vectorized code of opt_adam
    
    the vectorized code might be bit faster for low number of parameters, but it had a big memory usage overhead
    
    * ggml : update ggml_rms_norm_back with configurable eps
    
    * llama training : fix ggml_rms_norm_back calls to pass configurable eps
    
    * remove trailing whitespace
    
    * add train function using automatic gradient checkpointing backward pass and allocator
    
    * in train function replace add_inplace by regular add
    
    because using add_inplace seems to result in different gradients
    
    * don't use allocate hash_map on context
    
    because the context has no_alloc=True when using memory allocator resulting in NULL data pointers
    
    * correctly clone reshape and permute operations by also cloning tensor->nb values
    
    * fix variable name and add missing type cast
    
    * terminate recursive tensor cloning when reaching tensor without src tensors
    
    * correctly clone view tensors by setting data pointers
    
    without this the checkpointing would only work when being used together with memory allocator
    
    * fix variable names
    
    * swap arguments to commutative ops to be the same as in `forward_batch_wo_cache_flash_attn`
    
    * add input tensors as checkpoints
    
    so that recursive tensor cloning of gradient checkpointing terminates on input tensors
    
    * fix variable name and add missing boolean negation
    
    * make sure some tensors are not reallocated by inserting new temporary nodes depending on them:
    
    output and parameter gradient tensors need to be available at the end of the graph execution
    
    parameter gradient tensors also need to be available before the graph execution because they are set to zero before each optimizer iteration
    
    checkpoint tensors are allocated all together to reduce memory allocator fragmentation
    
    afterwards, in addition to the temporary nodes, we also need to reset the temporary leafs
    
    * fix ASSERT to work with zero layers
    
    * add training options whether to use allocator and/or unified training function
    
    * integrate unified training function which may use memory allocator
    
    the unified training function also supports arguments whether to use flash attention and/or gradient checkpointing
    
    * format name of cloned tensors with " (clone)" suffix
    
    * set names for tensors in unified train function for easier debugging
    
    * allocate graph on context using ggml_new_graph
    
    * remove handwritten training functions
    
    * remove unused training parameters "use_scratch" and "use_unified"
    
    * remove trailing whitespace
    
    * remove unused train params: mem_compute1_gb & mem_compute2_gb
    
    mem_compute_gb is used for compute when automatic memory allocator is not enabled, otherwise it can be very small to only hold the tensor definitions
    mem_compute0_gb is used for automatic memory allocator (as long as measurement of max required size is not implemented)
    
    * remove unused forward_batch function
    
    * add debug asserts in ggml_allocr_alloc to some common pitfalls when using this function directly
    
    * only use ggml_allocr_alloc when tensor has NULL data and is no view
    
    * fix test when to create temporary backward graph
    
    temporary backward graph is only necessary when using checkpointing
    
    * fix memory "leak" in optimizers
    
    each iteration a new cplan with new memory for work data was allocated.
    now cplan creation only happens at the start of optimization, with each iteration reusing the cplan and its work data.
    
    * reverse order of for loop in ggml_build_backward_expand to save memory when using gradient checkpointing and allocator
    
    with this loop order gradient checkpointing with allocator on 16 layer model saves 13% memory; 2 layer memory it saves 2% memory.
    
    the computation results are the same
    
    * add missing lctx argument to get_example_targets_batch
    
    * implement llama model file saving using gguf
    
    checkpoint loading and saving disabled, to be replaced by loading and saving via gguf
    
    * implement loading/saving of checkpointing files using GGUF
    
    * bug fixes
    
    * add checkpoint file version for future compatibility
    
    * update readme with gguf filenames
    
    * save & load opt->just_initialized value
    
    * add first draft for checkpoint conversion script
    
    * add gguf arch and ftype
    
    * save opt parameter counter as uint64
    
    * add gguf key and tensor names for optimizer and training
    
    * add layer_norm_rms_eps to checkpoint convert script
    
    * use same GGUF_GET_KEY macro as in llama.cpp
    
    * use norm_rms_eps, and rope parameters and command line options to set them
    
    * fix memory corruption bug in gguf
    
    ctx->kv and ctx->infos was reallocated using not-aligned realloc, but freed with aligned free.
    to fix this a GGML_ALIGNED_REALLOC was added, but there is no posix_memalign_realloc function.
    so on non-windows and non-mingw32 platforms we fall back to aligned malloc, followed by copying
    and freeing the old data.
    
    * add gguf example cmake file
    
    * bug fixes in tokenize_file
    
    * bug fixes in load_llama_model_gguf
    
    * bug fix: init model when no checkpoint was loaded
    
    * bug fix in read_tensor_by_name
    
    * bug fix in load_opt_context_gguf
    
    * avoid printing lots of spaced on the unusual case that loss gets nan
    
    * set name of tensors with empty name from what was read from gguf
    
    * remove trailing whitespace
    
    * print data checksums before saving and after loading to verify correctness
    
    * bug fixes for convert-train-checkpoint-to-gguf
    
    * temporarily add code to write old checkpoint files
    
    used to verify that old checkpoint files are correctly converted to gguf
    
    * bug fixes for convert-train-checkpoint-to-gguf.py loading checkpoints with opt_version=0
    
    * remove code used to verify correctness of checkpoint file conversion
    
    * remove trailing whitespace
    
    * remove prediction related code
    
    use main for prediction, it is better optimized
    
    * update train-text-from-scratch README.md
    
    * fix non-windows GGML_ALIGNED_REALLOC
    
    * add missing blank line at end of file
    
    * remove GGML_ALIGNED_REALLOC and use normal malloc/realloc/free for gguf ctx->kv & ctx->infos
    
    * train : fix compile warnings
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 43033b7bb4858da4f591715b3babdf906c9b7cbc
Author: slaren <slarengh@gmail.com>
Date:   Mon Aug 28 19:19:18 2023 +0200

    llama-bench : set locale to utf8 (#2832)

commit 6b73ef120114beb5664ea94aab48d07ed248ee52
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Aug 28 17:59:39 2023 +0200

    YAML result logging + preset script (#2657)

commit 75fafcbcccc280a5b3883bc76d0a2dabf474d094
Author: alonfaraj <alonfaraj@gmail.com>
Date:   Mon Aug 28 18:38:35 2023 +0300

    make : fix tests build (#2855)
    
    * makefile:
    - fix test name
    - add missing tests build
    
    * editorconfig : fixes
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit be475f60af1a54e8de81466ccc907d080cf6df1a
Author: grahameth <96447521+grahameth@users.noreply.github.com>
Date:   Mon Aug 28 17:38:12 2023 +0200

    llama.cpp : fix wrong vsnprintf call in MS compiler (#2856)
    
    Co-authored-by: grahameth <->

commit 3af6b86301ddfb11bb68e91dfc030b611b0d8426
Author: Ronny Brendel <ronnybrendel@gmail.com>
Date:   Mon Aug 28 14:51:08 2023 +0200

    ggml : tiny ggml_vec_dot_q4_K_q8_K AVX2 improvement (#2819)

commit 35feac6560387cf0484371af3d9b12bff678e0b9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 28 14:24:53 2023 +0300

    ggml : sync (mem align to header + conv_transpose_2d fixes + ggml_alloc) (#2852)
    
    * ggml : sync (mem align to header + conv_transpose_2d fixes)
    
    ggml-ci
    
    * ggml-alloc : minor fix
    
    * ggml-alloc : sync more fixes

commit 92b1bbd2ec43c82ec0530ba3c8758846c5790c75
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Aug 28 13:23:55 2023 +0200

    CUDA: fix RoPE asserts, block sizes (#2833)

commit dd0dc366dab10e8df28d3924e7f313b5c695e908
Author: igarnier <igarnier@protonmail.com>
Date:   Mon Aug 28 10:19:59 2023 +0200

    llama.h : add missing struct keyword for C compat in callback type (#2847)

commit f55538c3ccba9b926846ef862fa830cea08c433e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 28 10:59:08 2023 +0300

    metal : fix memory leak (#2762)
    
    * metal : fix memory leak
    
    * metal : fix encoders memory leak
    
    * metal : clean up more memory resources
    
    * metal : fix more leaks
    
    * metal : reuse dispatch queue + autoreleasepool
    
    * metal : reuse array for command buffers and encoders
    
    * ggml : assert for odd number of blocks on ARM
    
    15M tinyllama is an example

commit ebcee207b6058b7f695bb5c203ad87b1066a9790
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Mon Aug 28 02:32:25 2023 -0400

    quantize : make output filename optional again (#2823)
    
    * quantize : make output filename optional again
    
    * quantize : fix path parsing on Windows
    
    suggested by @slaren

commit 3e8ff47af620a31e0810c58a41e4b089145982ef
Author: JohnnyB <jboero@users.noreply.github.com>
Date:   Mon Aug 28 07:31:24 2023 +0100

    devops : added systemd units and set versioning to use date. (#2835)
    
    * Corrections and systemd units
    
    * Missing dependency clblast

commit 103cfafc774f6feb3172b5d4d39681c965b17eba
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Aug 27 21:50:22 2023 +0300

    gguf : fix strings to not be null-terminated (#2839)
    
    * gguf : fix strings to not be null-terminated
    
    ggml-ci
    
    * gguf : fix gguf_add_tensor name

commit c10704d01e21e3dbe4d6ca1026ebff85349dd239
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Aug 27 18:55:41 2023 +0300

    llama : fix MPI threads (close #2827)

commit 230d46c723edf5999752e4cb67fd94edb19ef9c7
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Sun Aug 27 15:13:31 2023 +0100

    examples : update llama2.c converter to read vocab and write models in GGUF format (#2751)
    
    * llama2.c: direct gguf output (WIP)
    
    * Simplify vector building logic
    
    * llama2.c gguf conversion: fix token types in converter
    
    * llama2.c: support copying vocab from a llama gguf model file
    
    * llama2.c: update default path for vocab model + readme
    
    * llama2.c: use defines for gguf keys
    
    * llama2.c: escape whitespaces w/ U+2581 in vocab converter the llama.cpp way
    
    * llama2.c converter: cleanups + take n_ff from config

commit 463173a6c0ff353055eb90665794884c888c790f
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Aug 27 16:50:33 2023 +0300

    llama : speedup tokenization (#2831)
    
    * Speedup tokenization
    
    On current master it takes ~3.2 seconds to tokenize
    Wikitext. With this change it becomes ~525 ms.
    
    * Fixit: it was missing the piece after the last found occurence
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit eaa13a48ff4136f01c1cdb79cacd61b67ec53095
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Aug 27 16:40:48 2023 +0300

    falcon : fix CUDA inference by making K and Q contiguous (#2830)
    
    * falcon : fix CUDA inference by making K and Q contiguous
    
    ggml-ci
    
    * cuda : add assert to guard from non-cont ropes

commit da7455d0467b5f5cc2e45d0dcffaf098df13db63
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Aug 27 15:52:34 2023 +0300

    readme : fix headings

commit 25423e9185b7c2a1881ed8f85cc752a12370be9d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Aug 27 15:24:40 2023 +0300

    scripts : helper convert script

commit a6d1189fdd4c1ab4ba23f9d777f8950901dcffb2
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Aug 27 15:19:59 2023 +0300

    k_quants tuning for Falcon-7b (#2816)
    
    * Make ggml-cuda.cu build with QK_K = 64
    
    Using LLAMA_CUDA_FORCE_DMMV = ON and -nommq it runs and produces
    a meaningful result.
    
    * k_quants tuning for Falcon-7b
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit c48c5bb0b06385f6c708339188d2aaf2bc278477
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Aug 27 14:44:35 2023 +0300

    readme : update hot topics

commit d0cee0d36d5be95a0d9088b674dbb27354107221
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Aug 27 14:19:54 2023 +0300

    gguf : add 64-bit support (GGUF v2) (#2821)
    
    * gguf : bump version to 2
    
    * gguf : add support for 64-bit (no backwards comp yet)
    
    * gguf : v1 backwards comp
    
    * gguf.py : bump GGUF version
    
    * gguf.py : uint64_t on all lengths, sizes and counts, enums still uint32_t
    
    * gguf.py : string lengths uint32_t
    
    * gguf : update all counts to 64-bit
    
    * gguf.py : string len uint64_t and n_dims uint32_t
    
    * gguf : fix typo
    
    * llama.cpp : print gguf version
    
    ---------
    
    Co-authored-by: klosax <131523366+klosax@users.noreply.github.com>

commit edd4c1481708fcd788b0e423268304fd26e2b125
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Aug 27 14:19:19 2023 +0300

    llama : more tokenizer fixes (#2810)
    
    * tests : write a Python tokenizer test (wip)
    
    * llama : prefix input text for tokenization with whitespace
    
    * llama : distinguish pieces from decoded text + fix detokenization
    
    * common : add comments
    
    * examples : no longer manually add leading space when tokenizing
    
    * tests : use Python to generate tokenizer tests for C++
    
    * tests : add option to tokenize text files
    
    ggml-ci
    
    * tests : add test-tokenizer-1.py
    
    * llama.cpp : fix LF token
    
    * hellaswag : move the concat space for clarity
    
    * tests : add falcon tests (py + cpp, currently do not pass Unicode)
    
    ggml-ci
    
    * common : temporary separate llama_detokenize calls for SPM and BPE
    
    ---------
    
    Co-authored-by: klosax <131523366+klosax@users.noreply.github.com>

commit 1591e2e590762011b43b10a9b6e04f13f98f2aa5
Author: Przemysław Pawełczyk <przemoc@gmail.com>
Date:   Sun Aug 27 10:10:25 2023 +0200

    ggml : detect SSSE3 (#2825)
    
    * ggml : add ggml_cpu_has_ssse3
    
    * llama : show SSSE3 in system info

commit 789c8c945a2814e1487e18e68823d9926e3b1454
Author: slaren <slarengh@gmail.com>
Date:   Sun Aug 27 09:03:27 2023 +0200

    ci : add LoRA test to CI (#2650)
    
    * ci : add lora test
    
    ggml-ci
    
    * move lora summary to the top, add lora logs
    
    ggml-ci
    
    * ci : decrease CPU ppl runs to 2 to avoide 20 min timeout
    
    ggml-ci
    
    * add 7b lora test
    
    use 1 thread for CUDA generation tests
    
    ggml-ci
    
    * add test with q8_0 (cpu only)
    
    ggml-ci
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit c1ac54b77aaba10d029084d152be786102010eb2
Author: Bruce MacDonald <brucewmacdonald@gmail.com>
Date:   Sat Aug 26 16:11:45 2023 -0700

    server : add `/detokenize` endpoint (#2802)
    
    * Add a /detokenize endpoint to the example server
    
    * remove trailing white-space

commit 730d9c681e339b76407659344e5a2cd50af7d7d5
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Aug 26 14:13:36 2023 -0600

    convert.py : advanced option (#2753)
    
    * Allow convert.py to convert to q8_0
    
    Fix issue with bounded_parallel_map and greedy consuming iterator
    
    Display elapsed time during conversion
    
    * Add --concurrency option
    
    Minor improvements to help text
    
    Clean up bounded_parallel_map function a bit
    
    * Massive speed improvement thanks to Cebtenzzre
    
    * Refactor types

commit c7d92e6dfec3f54849f3a0ba373054d29f321ea2
Author: Tim Miller <drasticactions@users.noreply.github.com>
Date:   Sun Aug 27 03:27:07 2023 +0900

    llama : use Unicode Escape Sequence to replace encoded characters (#2814)
    
    The use of special characters within source files can break compiling on some computers with different region and language settings. Using Unicode escape sequences should allow for the code to be compiled on all setups without needing to change your computers settings or switch regions.

commit 61d1a2895eeca55e0c8b7018492f6ab9c90cff78
Author: Tungsten842 <quantmint@protonmail.com>
Date:   Sat Aug 26 20:19:44 2023 +0200

    flake.nix : add rocm support and cleanup (#2808)

commit 741ca7dd1cec0a0349494742b9083d6ef4cd73c5
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Sat Aug 26 14:17:51 2023 -0400

    llama : move #includes out of _GNU_SOURCE conditional (#2817)

commit 72f895c923ba98b8f2af294440206f35915c0501
Author: Dr. Tom Murphy VII Ph.D <499244+tom7@users.noreply.github.com>
Date:   Sat Aug 26 14:12:56 2023 -0400

    main : fix bug (penalize_nl=false doesn't work) + suppress warning on mingw (#1528)
    
    * Fix bug in main.cpp where penalize_nl=false has no effect. It modifies the underlying logits array, but at this point we are already working on the candidates copy.
    
    * Suppress redefinition warning for NOMINMAX on mingw. In my installation, this macro is already defined by /usr/lib/gcc/x86_64-w64-mingw32/11/include/c++/x86_64-w64-mingw32/bits/os_defines.h:45.
    
    * main : fix indentation
    
    * main : pass ctx to llama_token_nl()
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 50526f37eba0b28336700890242ff282b949cd83
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Sat Aug 26 12:53:52 2023 -0400

    llama : use std::abs in llama_sample_tail_free (#2800)
    
    Plain 'abs' casts the input to int.

commit 04f4b1eb10f3e25750ca3e530265ce2841730e6b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Aug 26 17:37:35 2023 +0300

    k-quants : remove unnecessary tensor shape restrictions (#2811)

commit 7592375403a0bd0456d5ec2cdf8350e591f04fb0
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sat Aug 26 17:27:49 2023 +0300

    Better perplexity for 2- and 3-bit quantization for LLaMA-v2-70B (#2807)
    
    * Better perplexity for 2- and 3-bit quantization for the 70B model
    
    * PR comment
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 771551a793c9976ed9cdfe7b8c69536af32af9f9
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sat Aug 26 16:48:53 2023 +0300

    Fix HellaSwag (#2805)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit f305bad11e10ad09e396faed2e37f4f845f5d566
Author: Volodymyr Vitvitskyi <72226+signalpillar@users.noreply.github.com>
Date:   Sat Aug 26 14:25:39 2023 +0100

    flake : build llama.cpp on Intel with nix (#2795)
    
    Problem
    -------
    `nix build` fails with missing `Accelerate.h`.
    
    Changes
    -------
    - Fix build of the llama.cpp with nix for Intel: add the same SDK frameworks as
    for ARM
    - Add `quantize` app to the output of nix flake
    - Extend nix devShell with llama-python so we can use convertScript
    
    Testing
    -------
    Testing the steps with nix:
    1. `nix build`
    Get the model and then
    2. `nix develop` and then `python convert.py models/llama-2-7b.ggmlv3.q4_0.bin`
    3. `nix run llama.cpp#quantize -- open_llama_7b/ggml-model-f16.gguf ./models/ggml-model-q4_0.bin 2`
    4. `nix run llama.cpp#llama -- -m models/ggml-model-q4_0.bin -p "What is nix?" -n 400 --temp 0.8 -e -t 8`
    
    Co-authored-by: Volodymyr Vitvitskyi <volodymyrvitvitskyi@SamsungPro.local>

commit a2ca4e9de9da45ed0bb1c34935d5ec80cebc22d5
Author: Nigel Bosch <pnigelb@gmail.com>
Date:   Sat Aug 26 07:11:17 2023 -0500

    Handle null rope scaling value (#2793)

commit 2ba83c8685177faea3399db9564f9c52df75c366
Author: klosax <131523366+klosax@users.noreply.github.com>
Date:   Sat Aug 26 13:45:53 2023 +0200

    Fix spm whitespaces (#2806)
    
    * llama.cpp : fix spm whitespace escaping + clean up
    
    * main.cpp : spm - add whitespace in front of prompt
    
    * test-tokenizer-0.cpp : spm - add whitespace in front of prompt

commit bae5c5f679e043371bc2b4dffff8d4964d6cb953
Author: lon <114724657+longregen@users.noreply.github.com>
Date:   Sat Aug 26 10:07:43 2023 +0200

    examples : skip unnecessary external lib in server README.md how-to (#2804)

commit 232caf3c1581a6cb023571780ff41dc2d66d1ca0
Author: Marcus Dunn <51931484+MarcusDunn@users.noreply.github.com>
Date:   Fri Aug 25 09:17:15 2023 -0700

    llama : fix struct decl (#2790)

commit d046dcee081118c9071bbc63dacdb359a58c467a
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Aug 25 19:05:02 2023 +0300

    Faster perplexity computation (#2786)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit c82742ac9cd96fd34aa961978805c1d8a361d589
Author: Matt Pulver <matt.pulver@heavy.ai>
Date:   Fri Aug 25 11:18:48 2023 -0400

    llama : add llama_beam_search() (#2267)
    
    * Add llama_beam_search().
    
    * Add '// Beam search' heading to llama.{h,cpp} after llama_grammar_accept_token().
    
    * Add space around * pointers and & references.
    
    * Add spaces around comparison and assignment operators.
    
    * Prefer west const.
    
    * Use llama_ prefix for structs in global namespace.
    
    * Delete obsolete comment from an earlier revision.
    
    * Change eos to eob in llama_beam and llama_beam_view structs.

commit 28b2c996ca0ab90a5669946084f13443ec98e241
Author: Nigel Bosch <pnigelb@gmail.com>
Date:   Fri Aug 25 09:41:52 2023 -0500

    convert.py : Get rope scale from HuggingFace models (#2772)
    
    * Get rope scale from HF models
    
    * Save rope scale only for linear scaling
    
    * Rewrite for clarity

commit 154725c5436808e5c519685d0279e850596dbe62
Author: slaren <slarengh@gmail.com>
Date:   Fri Aug 25 15:16:19 2023 +0200

    llama-bench : add model sizes (#2771)
    
    * llama-bench : add model sizes
    
    * more compact markdown output
    
    * back to GiB
    
    * adjust column sizes

commit 12e2e33a977af73e75885eeee91c5575a77f4e5f
Author: slaren <slarengh@gmail.com>
Date:   Fri Aug 25 14:08:53 2023 +0200

    convert.py : export rope freq_base when converting CodeLlama from an HF model (#2773)

commit 29674ab4e847fcaba60cc6558f0d46d5f74ae279
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Fri Aug 25 18:32:45 2023 +0800

    server : display token probabilities in the UI (#2489)
    
    * server : add n_probs param in chat UI
    
    * server : keep message data array & show in probabilites component
    
    * server : add simple popover component
    
    * server : fix completion_probabilities undefined if not set n_probs
    
    * server : implement Probabilites
    
    * server : handle bytes
    
    * server : make n_probs max to 10 for easy scroll
    
    * server : adjust for dark/light mode
    
    * server : Fix regenerated prompt
    
    * server : update index.html.hpp
    
    * server : convert prob to percentage + show original value as div title
    
    * server : fix Probabilites not used if included empty str
    
    * server : skip byte pair in display probabilites
    
    * server : remove array check of completion_probabilities in messages
    
    * skip empty array or byte pair (> 1) in Probabilites
    
    * generate index.html.hpp
    
    * fix incorrect prob convert if the str is already a known token
    
    * use final response to show probabilities on stop
    
    * revert unnecessary change
    
    * correct probabilites usage
    
    * remove unused function
    
    * always send partial response for get correct probs of last to_send
    
    * fix typo
    
    * fix content of format_final_response
    
    * refactor probs render & make pColor transparent if not found
    
    * send empty string when got stop_pos in partial
    
    * avoid unnecessary empty data event & send rest of partial tokens on stop
    
    * use <br /> for new line
    
    * skip -1 tok in loop to avoid send '' on end
    
    * trim last new lines on stop
    
    * revert unnecessary change

commit 5439a0ab57c16b556ffa91a0953df5e46b1e7fb4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Aug 25 13:03:25 2023 +0300

    ci : pip install gguf in editable mode (#2782)
    
    ggml-ci

commit 8194cd8772c58b8a43aa07a2fc468f5366d7e320
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Fri Aug 25 12:43:41 2023 +0300

    gguf : export objects to user code (#2780)
    
    * gguf export more objects to user code
    
    * gguf export all objects to user code for now
    
    * gguf : bump version

commit 6bbc598a632560cb45dd2c51ad403bda8723b629
Author: Henri Vasserman <henv@hot.ee>
Date:   Fri Aug 25 12:09:42 2023 +0300

    ROCm Port (#1087)
    
    * use hipblas based on cublas
    * Update Makefile for the Cuda kernels
    * Expand arch list and make it overrideable
    * Fix multi GPU on multiple amd architectures with rocblas_initialize() (#5)
    * add hipBLAS to README
    * new build arg LLAMA_CUDA_MMQ_Y
    * fix half2 decomposition
    * Add intrinsics polyfills for AMD
    * AMD assembly optimized __dp4a
    * Allow overriding CC_TURING
    * use "ROCm" instead of "CUDA"
    * ignore all build dirs
    * Add Dockerfiles
    * fix llama-bench
    * fix -nommq help for non CUDA/HIP
    
    ---------
    
    Co-authored-by: YellowRoseCx <80486540+YellowRoseCx@users.noreply.github.com>
    Co-authored-by: ardfork <134447697+ardfork@users.noreply.github.com>
    Co-authored-by: funnbot <22226942+funnbot@users.noreply.github.com>
    Co-authored-by: Engininja2 <139037756+Engininja2@users.noreply.github.com>
    Co-authored-by: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
    Co-authored-by: jammm <2500920+jammm@users.noreply.github.com>
    Co-authored-by: jdecourval <7315817+jdecourval@users.noreply.github.com>

commit 3f460a2b723c8b936ac29ecfd02f244b3adeba55
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Aug 25 11:55:59 2023 +0300

    cuda : add RoPE kernel for mode == 2 (NeoX) (#2760)
    
    * cuda : add RoPE kernel for mode == 2 (NeoX)
    
    * falcon : do not offload the embeddings layer

commit 87e3733f24a85d894cc16e1cbdfa1ea1e81a76f3
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Fri Aug 25 09:26:05 2023 +0300

    gguf : make gguf pip-installable
    
    * gitignore : add dist and rm pyproject.toml
    
    * gguf: prepare as Pip package
    
    * gguf: prepare as Pip package
    
    * gguf : fix line endings
    
    * requirements : add gguf
    
    * gguf : update readme with build notes
    
    * gguf : update readme with build notes
    
    * gguf : add notes for tests

commit b91ad7f46134d0d051dc516eb59a76f402de55c2
Author: Shouzheng Liu <lshzh.hi@gmail.com>
Date:   Fri Aug 25 01:58:00 2023 -0400

    ggml-alloc : enlarge size of parse_seq (#2776)
    
    Since we also store barriers in this array, we need to double its size.

commit 2e5f70a25fc4576e9ed78603fe493eb7702c37a3
Author: Marcus Dunn <51931484+MarcusDunn@users.noreply.github.com>
Date:   Thu Aug 24 14:49:30 2023 -0700

    Added `enum` to `llama_token_get_type` return type (#2774)

commit d0f77b1353fc820d1ff1e6b87bc6bedde315938d
Author: slaren <slarengh@gmail.com>
Date:   Thu Aug 24 21:10:39 2023 +0200

    convert.py : try to determine n_ctx automatically for CodeLlama (#2770)

commit 0d3094f0c742ce61f84feb6e4f0b59beee6194d7
Author: slaren <slarengh@gmail.com>
Date:   Thu Aug 24 20:04:05 2023 +0200

    gguf : add rope_freq_base parameter for CodeLlama (#2769)

commit 01f2224682b08185af609b28b1268b95c8b4cfa2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Aug 24 19:58:30 2023 +0300

    falcon : write file type

commit 38b16dfca6e5032e6cfb90c1653bf1ba4cf647b4
Author: Shouzheng Liu <lshzh.hi@gmail.com>
Date:   Thu Aug 24 12:27:25 2023 -0400

    metal : bug-fix when enable ggml-alloc (#2757)
    
    * metal: better memory alloc w/ concurrency dispatch
    
    The ggml-alloc should only free tensors at memory barriers.
    
    * ggml-alloc: avoid return silently
    
    In certain cases, the allocate_node() function may silently return
    without performing any memory allocation.

commit 8f8c28e89cb9531211783da697d6e7c445e2af1d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Aug 24 19:26:19 2023 +0300

    convert : auto-determine model name based on dir + scripts update

commit 7694adda8d1111b3cf758ad6c91d754a0a4cacff
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Thu Aug 24 10:11:13 2023 -0600

    Fix for main example getting stuck when -n -2 and --interactive (#2767)
    
    * Fix for main example getting stuck when -n -2 and --interactive
    
    * Add a comment so future generations may suffer less.

commit fea95c682d0028fdd25853bea58035794a0c964d
Author: slaren <slarengh@gmail.com>
Date:   Thu Aug 24 17:44:11 2023 +0200

    fix convert.py for codellama, add llama 34B to the list of recognized models (#2768)

commit ef955fbd230c571cc1cda0d19baaeec347523175
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Thu Aug 24 06:58:02 2023 -0700

    Tag release with build number (#2732)
    
    * Modified build.yml to use build number for release
    
    * Add the short hash back into the tag
    
    * Prefix the build number with b

commit d67777c202c03bcb74372690599ef3c03affb3ba
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Aug 24 16:19:57 2023 +0300

    metal : add Q8_0 support (#2763)
    
    * metal : add dequantize_q8_0 kernel
    
    * metal : add mul_mat_q8_0_f32 kernel
    
    * metal : add Q8_0 mul_mm kernel

commit c3e53b421a9910548be0345f85712c535f467a98
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Aug 24 12:26:01 2023 +0300

    llama : escape all U+2581 in a string (#2750)

commit 6e91a1b0706c2e0e52b9d9be7ee82d3c1e7a33c1
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Thu Aug 24 00:07:13 2023 -0400

    llama : fix grammar sometimes generating null char (#2756)

commit 44d5462b5cddc1c5cbcd7647646f7b55b175b01f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Aug 23 23:44:19 2023 +0300

    readme : fix link

commit c7868b075377c8c3fa916ea7c1aca600f44bed55
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Aug 23 23:43:00 2023 +0300

    minor : fix trailing whitespace

commit 79da24b58c1ea72340e64f799a4717d372207676
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Aug 23 23:41:16 2023 +0300

    readme : update hot topics

commit cf658adc832badaaa2ca119fe86070e5a830f8f6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Aug 23 23:08:04 2023 +0300

    llm : add Falcon support (#2717)
    
    * llama : refactor GGUF constants into static maps
    
    * llama : check if model architecture is known
    
    * llama : refactor llama_model_load_internal()
    
    * gguf : add KV constant maps
    
    * llm : read arch-specific KVs
    
    * convert : add dummy scores + types
    
    * falcon : load tensor data (CPU only)
    
    * llama : fix loading progress bar
    
    * llama : add arch member to llama_model
    
    * falcon : CPU inference working
    
    * falcon : support non-40B models
    
    * falcon : minor
    
    * llama : minor updates
    
    ggml-ci
    
    * convert-falcon-hf-to-gguf.py : fix special token mapping
    
    * llama.cpp : llama default UNK token = id 0
    
    * llama.cpp : fix bpe tokenizer
    
    * llama.cpp : fix the fix of bpe tokenizer
    
    * ggml : pass eps to ggml_norm
    
    * metal : implement RoPE (mode = 2) + avoid ggml_repeat
    
    * ggml : ggml_repeat always creates new tensor
    
    * falcon : copy-paste self-attention from LLaMA
    
    * metal : print extra compute pipeline info
    
    * falcon : minor changes (still chasing the Metal problem)
    
    * llama.cpp : fix linefeed token
    
    * metal : fix GELU kernel numerical stability by using precise::tanh
    
    * metal : temporary workaround for the concurrency optimization bug
    
    * falcon : add CUDA offloading (#2739)
    
    * llama : better model naming and size reporting
    
    * llama : prep new tokenizer support
    
    * llama : advanced BPE tokenizer based on ggllm.cpp imlpementation
    
    * llama : remove oboslete comment
    
    ggml-ci
    
    * common : remove obsolete BPE API + disable test-tokenizer-1
    
    * llama : revert BPE special-case in llama_byte_to_token()
    
    * cuda : add TODOs for RoPE NeoX implementation
    
    * llama : default special tokens based on vocab type
    
    * perplexity : add log for start of tokenization
    
    ---------
    
    Co-authored-by: klosax <131523366+klosax@users.noreply.github.com>
    Co-authored-by: slaren <slarengh@gmail.com>

commit a192860cfec89a38d59a943623bf595b1fe4495b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Aug 23 22:37:39 2023 +0300

    minor : fix trailing whitespace

commit 95385241a91a616788a3bb76d12c9b7b2379ca2d
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Wed Aug 23 20:33:05 2023 +0100

    examples : restore the functionality to import llama2.c models (#2685)
    
    * Fix import of llama2.c models that don't share weights between embedding layers
    
    * llama2c: reinstate ggmlv3 conversion output + update readme w/ gguf conv
    
    * llama2.c: comment out legacy "load from ggml model" logic
    
    * llama2.c: convert special-cased "<0xXX>" single byte tokens from tokenizer.bin

commit 335acd2ffd7b04501c6d8773ab9fcee6e7bf8639
Author: slaren <slarengh@gmail.com>
Date:   Wed Aug 23 16:46:54 2023 +0200

    fix convert-lora-to-ggml.py (#2738)

commit 5290c38e6e9b66ee2b543e560e301c1a1a90929c
Author: klosax <131523366+klosax@users.noreply.github.com>
Date:   Wed Aug 23 16:46:03 2023 +0200

    main : insert bos if no tokens (#2727)
    
    * main.cpp : insert bos if no tokens
    
    * Update examples/main/main.cpp
    
    * Update examples/main/main.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit cc34dbda9681418a2b18382446b90cdcec398d82
Author: akawrykow <142945436+akawrykow@users.noreply.github.com>
Date:   Wed Aug 23 07:31:34 2023 -0700

    gitignore : fix for windows (#2729)

commit 7c2227a1972a4add4b5c118e4914c086513d0382
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Wed Aug 23 10:29:09 2023 -0400

    chmod : make scripts executable (#2675)

commit f19dca04ea5fbf9a0b2753091d93464585d5c73b
Author: JohnnyB <jboero@users.noreply.github.com>
Date:   Wed Aug 23 15:28:22 2023 +0100

    devops : RPM Specs (#2723)
    
    * Create llama-cpp.srpm
    
    * Rename llama-cpp.srpm to llama-cpp.srpm.spec
    
    Correcting extension.
    
    * Tested spec success.
    
    * Update llama-cpp.srpm.spec
    
    * Create lamma-cpp-cublas.srpm.spec
    
    * Create lamma-cpp-clblast.srpm.spec
    
    * Update lamma-cpp-cublas.srpm.spec
    
    Added BuildRequires
    
    * Moved to devops dir

commit 8207214b6a37a46526cee9e72d4c9092b9d1872f
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Wed Aug 23 12:57:12 2023 +0300

    Fix values shown in the quantize tool help (#2735)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 62959e740e8759d246ac8d09036950efde09981c
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Wed Aug 23 12:56:42 2023 +0300

    Strided perplexity (#2714)
    
    * Implementing strided computation of perplexity
    
    * Alternative way to output PPL results
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 7f7ddd5002040804e33fcdbde44aa22f8635f57d
Author: IgnacioFDM <ignaciofdm@gmail.com>
Date:   Wed Aug 23 06:31:09 2023 -0300

    Fix ggml to gguf conversion on Windows (#2733)
    
    This fixes `RuntimeWarning: overflow encountered in long_scalars`
    
    Credit: anon (not mine)

commit b8ad1b66b23f9b2e6e4531e9a62753323036a556
Author: Xiao-Yong Jin <jinxiaoyong@gmail.com>
Date:   Wed Aug 23 02:12:12 2023 -0500

    server : allow json array in prompt or content for direct token input (#2306)
    
    * server: allow json array in prompt or content
    
    We accept an array of strings and numbers representing tokens,
    in addition to the current string valued prompt or content.
    
    This allows direct token input, so that any special tokens
    can be processed and used at the frontend during the construction
    of the json data, before sending to the server. And the server
    does not need to know or parse special tokens from textual input.
    
    With this, we can use EOS and BOS used in llama-2-chat models.
    
    * server: use tokenizePrompt(json) and default "" if empty prompt
    
    * server: fix prompt check
    
    * server: tokenize endpoint no longer adds BOS

commit f5fe98d11bdf9e7797bcfb05c0c3601ffc4b9d26
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Tue Aug 22 21:01:57 2023 -0400

    docs : add grammar docs (#2701)
    
    * docs : add grammar docs
    
    * tweaks to grammar guide
    
    * rework GBNF example to be a commented grammar

commit 777f42ba18b29f25c71ff8de3ecf97b8017304c0
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Tue Aug 22 17:39:39 2023 -0600

    Improve handling of special tokens in GGML to GGUF converter (#2725)
    
    * Improve UNK, BOS, EOS token handling when converting without metadata.
    
    * Allow importing as a module.
    
    * Remove some obsolete code and minor cleanups.
    
    * Set default UNK token mapping from -1 to 0 in llama.cpp
    
    * Try to handle overflow due to buggy Windows Python with a better error message

commit 46ef5b5fcf4c366e1fb27726b6394adbbf8fd0ea
Author: goerch <jhr.walter@t-online.de>
Date:   Tue Aug 22 23:10:42 2023 +0200

    llama : fix whitespace escaping in tokenizer (#2724)

commit c63bb1d16a70c03440671b76954bb767513cead8
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Aug 22 22:47:05 2023 +0200

    CUDA: use mul_mat_q kernels by default (#2683)

commit 3b6cfe7c927df178ca3c11643c3ec93e143471c9
Author: Alex Petenchea <alex.petenchea@gmail.com>
Date:   Tue Aug 22 21:58:16 2023 +0300

    convert.py : clarifying error message (#2718)

commit 800c9635b4a9390126f397870f3a825fc7455bd1
Author: Jiahao Li <liplus17@163.com>
Date:   Wed Aug 23 02:27:06 2023 +0800

    Fix CUDA softmax by subtracting max value before exp (#2665)

commit deb7dfca4b9725cd295d1426db75fe8e0a6d5312
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Aug 22 20:05:59 2023 +0300

    gguf : add ftype meta info to the model (#2710)
    
    * llama : add ftype meta info to the model
    
    ggml-ci
    
    * convert.py : add ftype when converting (does not work)
    
    * convert.py : fix Enum to IntEnum
    
    ggml-ci

commit bac66994cf356cf488078c056831396eb4ce31d5
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Aug 22 19:14:09 2023 +0300

    Quantization imrovements for k_quants (#2707)
    
    * Improve LLaMA-2 2-, 3- and 4-bit quantization
    
    * Q3_K_S: use Q5_K for 1st 2 layers of attention.wv and feed_forward.w2
    * Q4_K_S: use Q6_K for 1st 2 layers of attention.wv and feed_forward.w2
    * Q2_K and Q3_K_M: use Q5_K instead of Q4_K for 1st 2 layers of
      attention.wv and feed_forward.w2
    
    This leads to a slight model sized increase as follows:
    Q2_K  : 2.684G vs 2.670G
    Q3_K_S: 2.775G vs 2.745G
    Q3_K_M: 3.071G vs 3.057G
    Q4_K_S: 3.592G vs 3.563G
    
    LLaMA-2 PPL for context 512 changes as follows:
    Q2_K  : 6.6691 vs 6.8201
    Q3_K_S: 6.2129 vs 6.2584
    Q3_K_M: 6.0387 vs 6.1371
    Q4_K_S: 5.9138 vs 6.0041
    
    There are improvements for LLaMA-1 as well, but they are
    way smaller than the above.
    
    * Minor 4-bit quantization improvement
    
    For the same model size as previus commit, we get
    PPL = 5.9069 vs 5.9138.
    
    * Some more fine tuning
    
    * Adding make_qkx2_quants
    
    With it, we get PPL = 5.8828 for L2-7B Q4_K_S.
    
    * Another minor improvement
    
    * Q2_K improvement
    
    Smaller model, lower perplexity.
     7B: file size = 2.632G, PPL = 6.3772 vs original 2.670G PPL = 6.8201
    12B: file size = 5.056G, PPL = 5.4577 vs original 5.130G PPL = 5.7178
    
    It is mostly Q3_K except for tok_embeddings, attention.wq, attention.wk,
    which are Q2_K
    
    * Iterating
    
    * Revert Q5_K back to make_qkx1_quants
    
    * Better Q6_K
    
    * make_qkx2_quants is better for Q5_K after all
    
    * Fix after rebasing on master
    
    * Fix for changed tensor names
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 519c981f8b65ee6c87c2965539685ced0a17223b
Author: slaren <slarengh@gmail.com>
Date:   Tue Aug 22 16:03:12 2023 +0200

    embedding : evaluate prompt in batches (#2713)

commit 1123f7fbdfb8012e46f05e903e6f675922916378
Author: slaren <slarengh@gmail.com>
Date:   Tue Aug 22 15:25:19 2023 +0200

    ggml-cuda : use graph allocator (#2684)
    
    use a different function for no_alloc to avoid breaking backwards compat, fixes lora
    
    remove 512 n_batch limit
    
    fixed 2048 batch size
    
    cleanup
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

commit ef3f333d3775600d1646a9fa249aca532d15fb89
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Aug 22 14:22:08 2023 +0300

    ggml : sync latest (SAM + SD operators, CUDA alibi) (#2709)
    
    * ggml : sync latest (SAM + SD operators, CUDA alibi)
    
    ggml-ci
    
    * ggml : fix tabs

commit 8e4364f2af9cd5d57240f23e83c0e29bc068bc02
Author: slaren <slarengh@gmail.com>
Date:   Tue Aug 22 09:56:03 2023 +0200

    llama-bench : minor fixes (#2695)

commit 1e3bc523d8053a77df3ac7126a84d0297ee97ef6
Author: Kylin <56434533+KyL0N@users.noreply.github.com>
Date:   Tue Aug 22 15:14:23 2023 +0800

    ggml : support CUDA's half type for aarch64(#1455) (#2670)
    
    * ggml: support CUDA's half type for aarch64(#1455)
    support CUDA's half type for aarch64 in ggml_fp16_t definition
    
    * ggml: use __CUDACC__ to recognise nvcc compiler

commit 14b1d7e6f720dee41ce5a826376df738096d9033
Author: Shouzheng Liu <lshzh.hi@gmail.com>
Date:   Tue Aug 22 02:18:40 2023 -0400

    metal : add missing barriers for mul-mat (#2699)

commit 226255b44ef2c2794bfac48d101d35a9c2dbb965
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Tue Aug 22 08:32:00 2023 +0800

    server : fallback to default if client param is null (#2688)
    
    * server : fallback to default if client param is null
    
    * server : do not overwrite 404 if status is 500 from exception_handler

commit 930523c8e1cbbee5449c055daa894917fac6805e
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Mon Aug 21 18:01:34 2023 -0600

    Fix convert-llama-ggmlv3-to-gguf.py vocab conversion (#2698)
    
    When converting without metadata, the hex value for bytes entries weren't 0 padded to 2 digits.

commit c8dba409e6d6a754090f08e6a862c5ffdd52e421
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 21 23:40:22 2023 +0300

    py : remove obsolete script

commit 6381d4e110bd0ec02843a60bbeb8b6fc37a9ace9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 21 23:07:43 2023 +0300

    gguf : new file format with flexible meta data (beta) (#2398)
    
    * gguf : first API pass
    
    * gguf : read header + meta data
    
    * gguf : read tensor info
    
    * gguf : initial model loading - not tested
    
    * gguf : add gguf_get_tensor_name()
    
    * gguf : do not support passing existing ggml_context to gguf_init
    
    * gguf : simplify gguf_get_val
    
    * gguf : gguf.c is now part of ggml.c
    
    * gguf : read / write sample models
    
    * gguf : add comments
    
    * refactor : reduce code duplication and better API (#2415)
    
    * gguf : expose the gguf_type enum through the API for now
    
    * gguf : add array support
    
    * gguf.py : some code style changes
    
    * convert.py : start a new simplified implementation by removing old stuff
    
    * convert.py : remove GGML vocab + other obsolete stuff
    
    * GGUF : write tensor (#2426)
    
    * WIP: Write tensor
    
    * GGUF : Support writing tensors in Python
    
    * refactor : rm unused import and upd todos
    
    * fix : fix errors upd writing example
    
    * rm example.gguf
    
    * gitignore *.gguf
    
    * undo formatting
    
    * gguf : add gguf_find_key (#2438)
    
    * gguf.cpp : find key example
    
    * ggml.h : add gguf_find_key
    
    * ggml.c : add gguf_find_key
    
    * gguf : fix writing tensors
    
    * gguf : do not hardcode tensor names to read
    
    * gguf : write sample tensors to read
    
    * gguf : add tokenization constants
    
    * quick and dirty conversion example
    
    * gguf : fix writing gguf arrays
    
    * gguf : write tensors one by one and code reuse
    
    * gguf : fix writing gguf arrays
    
    * gguf : write tensors one by one
    
    * gguf : write tensors one by one
    
    * gguf : write tokenizer data
    
    * gguf : upd gguf conversion script
    
    * Update convert-llama-h5-to-gguf.py
    
    * gguf : handle already encoded string
    
    * ggml.h : get array str and f32
    
    * ggml.c : get arr str and f32
    
    * gguf.py : support any type
    
    * Update convert-llama-h5-to-gguf.py
    
    * gguf : fix set is not subscriptable
    
    * gguf : update convert-llama-h5-to-gguf.py
    
    * constants.py : add layer norm eps
    
    * gguf.py : add layer norm eps and merges
    
    * ggml.h : increase GGML_MAX_NAME to 64
    
    * ggml.c : add gguf_get_arr_n
    
    * Update convert-llama-h5-to-gguf.py
    
    * add gptneox gguf example
    
    * Makefile : add gptneox gguf example
    
    * Update convert-llama-h5-to-gguf.py
    
    * add gptneox gguf example
    
    * Update convert-llama-h5-to-gguf.py
    
    * Update convert-gptneox-h5-to-gguf.py
    
    * Update convert-gptneox-h5-to-gguf.py
    
    * Update convert-llama-h5-to-gguf.py
    
    * gguf : support custom alignment value
    
    * gguf : fix typo in function call
    
    * gguf : mmap tensor data example
    
    * fix : update convert-llama-h5-to-gguf.py
    
    * Update convert-llama-h5-to-gguf.py
    
    * convert-gptneox-h5-to-gguf.py : Special tokens
    
    * gptneox-main.cpp : special tokens
    
    * Update gptneox-main.cpp
    
    * constants.py : special tokens
    
    * gguf.py : accumulate kv and tensor info data + special tokens
    
    * convert-gptneox-h5-to-gguf.py : accumulate kv and ti + special tokens
    
    * gguf : gguf counterpart of llama-util.h
    
    * gguf-util.h : update note
    
    * convert-llama-h5-to-gguf.py : accumulate kv / ti + special tokens
    
    * convert-llama-h5-to-gguf.py : special tokens
    
    * Delete gptneox-common.cpp
    
    * Delete gptneox-common.h
    
    * convert-gptneox-h5-to-gguf.py : gpt2bpe tokenizer
    
    * gptneox-main.cpp : gpt2 bpe tokenizer
    
    * gpt2 bpe tokenizer (handles merges and unicode)
    
    * Makefile : remove gptneox-common
    
    * gguf.py : bytesarray for gpt2bpe tokenizer
    
    * cmpnct_gpt2bpe.hpp : comments
    
    * gguf.py : use custom alignment if present
    
    * gguf : minor stuff
    
    * Update gptneox-main.cpp
    
    * map tensor names
    
    * convert-gptneox-h5-to-gguf.py : map tensor names
    
    * convert-llama-h5-to-gguf.py : map tensor names
    
    * gptneox-main.cpp : map tensor names
    
    * gguf : start implementing libllama in GGUF (WIP)
    
    * gguf : start implementing libllama in GGUF (WIP)
    
    * rm binary commited by mistake
    
    * upd .gitignore
    
    * gguf : calculate n_mult
    
    * gguf :  inference with 7B model working (WIP)
    
    * gguf : rm deprecated function
    
    * gguf : start implementing gguf_file_saver (WIP)
    
    * gguf : start implementing gguf_file_saver (WIP)
    
    * gguf : start implementing gguf_file_saver (WIP)
    
    * gguf : add gguf_get_kv_type
    
    * gguf : add gguf_get_kv_type
    
    * gguf : write metadata in gguf_file_saver (WIP)
    
    * gguf : write metadata in gguf_file_saver (WIP)
    
    * gguf : write metadata in gguf_file_saver
    
    * gguf : rm references to old file formats
    
    * gguf : shorter name for member variable
    
    * gguf : rm redundant method
    
    * gguf : get rid of n_mult, read n_ff from file
    
    * Update gguf_tensor_map.py
    
    * Update gptneox-main.cpp
    
    * gguf : rm references to old file magics
    
    * gguf : start implementing quantization (WIP)
    
    * gguf : start implementing quantization (WIP)
    
    * gguf : start implementing quantization (WIP)
    
    * gguf : start implementing quantization (WIP)
    
    * gguf : start implementing quantization (WIP)
    
    * gguf : start implementing quantization (WIP)
    
    * gguf : quantization is working
    
    * gguf : roper closing of file
    
    * gguf.py : no need to convert tensors twice
    
    * convert-gptneox-h5-to-gguf.py : no need to convert tensors twice
    
    * convert-llama-h5-to-gguf.py : no need to convert tensors twice
    
    * convert-gptneox-h5-to-gguf.py : simplify nbytes
    
    * convert-llama-h5-to-gguf.py : simplify nbytes
    
    * gptneox-main.cpp : n_layer --> n_block
    
    * constants.py : n_layer --> n_block
    
    * gguf.py : n_layer --> n_block
    
    * convert-gptneox-h5-to-gguf.py : n_layer --> n_block
    
    * convert-llama-h5-to-gguf.py : n_layer --> n_block
    
    * gptneox-main.cpp : n_layer --> n_block
    
    * Update gguf_tensor_map.py
    
    * convert-gptneox-h5-to-gguf.py : load model in parts to save memory
    
    * convert-llama-h5-to-gguf.py : load model in parts to save memory
    
    * convert : write more metadata for LLaMA
    
    * convert : rm quantization version
    
    * convert-gptneox-h5-to-gguf.py : add file_type key
    
    * gptneox-main.cpp : add file_type key
    
    * fix conflicts
    
    * gguf : add todos and comments
    
    * convert-gptneox-h5-to-gguf.py : tensor name map changes
    
    * Create gguf_namemap.py : tensor name map changes
    
    * Delete gguf_tensor_map.py
    
    * gptneox-main.cpp : tensor name map changes
    
    * convert-llama-h5-to-gguf.py : fixes
    
    * gguf.py : dont add empty strings
    
    * simple : minor style changes
    
    * gguf : use UNIX line ending
    
    * Create convert-llama-7b-pth-to-gguf.py
    
    * llama : sync gguf-llama.cpp with latest llama.cpp (#2608)
    
    * llama : sync gguf-llama.cpp with latest llama.cpp
    
    * minor : indentation + assert
    
    * llama : refactor gguf_buffer and gguf_ctx_buffer
    
    * llama : minor
    
    * gitignore : add gptneox-main
    
    * llama : tokenizer fixes (#2549)
    
    * Merge tokenizer fixes into the gguf branch.
    
    * Add test vocabularies
    
    * convert : update convert-new.py with tokenizer fixes (#2614)
    
    * Merge tokenizer fixes into the gguf branch.
    
    * Add test vocabularies
    
    * Adapt convert-new.py (and fix a clang-cl compiler error on windows)
    
    * llama : sync gguf-llama with llama (#2613)
    
    * llama : sync gguf-llama with llama
    
    * tests : fix build + warnings (test-tokenizer-1 still fails)
    
    * tests : fix wstring_convert
    
    * convert : fix layer names
    
    * llama : sync gguf-llama.cpp
    
    * convert : update HF converter to new tokenizer voodoo magics
    
    * llama : update tokenizer style
    
    * convert-llama-h5-to-gguf.py : add token types
    
    * constants.py : add token types
    
    * gguf.py : add token types
    
    * convert-llama-7b-pth-to-gguf.py : add token types
    
    * gguf-llama.cpp :  fix n_head_kv
    
    * convert-llama-h5-to-gguf.py : add 70b gqa support
    
    * gguf.py : add tensor data layout
    
    * convert-llama-h5-to-gguf.py : add tensor data layout
    
    * convert-llama-7b-pth-to-gguf.py : add tensor data layout
    
    * gptneox-main.cpp : add tensor data layout
    
    * convert-llama-h5-to-gguf.py : clarify the reverse permute
    
    * llama : refactor model loading code (#2620)
    
    * llama : style formatting + remove helper methods
    
    * llama : fix quantization using gguf tool
    
    * llama : simplify gguf_file_saver
    
    * llama : fix method names
    
    * llama : simplify write_header()
    
    * llama : no need to pass full file loader to the file saver
    
    just gguf_ctx
    
    * llama : gguf_file_saver write I32
    
    * llama : refactor tensor names (#2622)
    
    * gguf: update tensor names searched in quantization
    
    * gguf : define tensor names as constants
    
    * gguf : initial write API (not tested yet)
    
    * gguf : write to file API (not tested)
    
    * gguf : initial write API ready + example
    
    * gguf : fix header write
    
    * gguf : fixes + simplify example + add ggml_nbytes_pad()
    
    * gguf : minor
    
    * llama : replace gguf_file_saver with new gguf write API
    
    * gguf : streaming support when writing files
    
    * gguf : remove oboslete write methods
    
    * gguf : remove obosolete gguf_get_arr_xxx API
    
    * llama : simplify gguf_file_loader
    
    * llama : move hparams and vocab from gguf_file_loader to llama_model_loader
    
    * llama : merge gguf-util.h in llama.cpp
    
    * llama : reorder definitions in .cpp to match .h
    
    * llama : minor simplifications
    
    * llama : refactor llama_model_loader (WIP)
    
    wip : remove ggml_ctx from llama_model_loader
    
    wip : merge gguf_file_loader in llama_model_loader
    
    * llama : fix shape prints
    
    * llama : fix Windows build + fix norm_rms_eps key
    
    * llama : throw error on missing KV paris in model meta data
    
    * llama : improve printing + log meta data
    
    * llama : switch print order of meta data
    
    ---------
    
    Co-authored-by: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
    
    * gguf : deduplicate (#2629)
    
    * gguf : better type names
    
    * dedup : CPU + Metal is working
    
    * ggml : fix warnings about unused results
    
    * llama.cpp : fix line feed and compiler warning
    
    * llama : fix strncpy warning + note token_to_str does not write null
    
    * llama : restore the original load/save session implementation
    
    Will migrate this to GGUF in the future
    
    * convert-llama-h5-to-gguf.py : support alt ctx param name
    
    * ggml : assert when using ggml_mul with non-F32 src1
    
    * examples : dedup simple
    
    ---------
    
    Co-authored-by: klosax <131523366+klosax@users.noreply.github.com>
    
    * gguf.py : merge all files in gguf.py
    
    * convert-new.py : pick #2427 for HF 70B support
    
    * examples/gguf : no need to keep q option for quantization any more
    
    * llama.cpp : print actual model size
    
    * llama.cpp : use ggml_elements()
    
    * convert-new.py : output gguf (#2635)
    
    * convert-new.py : output gguf (WIP)
    
    * convert-new.py : add gguf key-value pairs
    
    * llama : add hparams.ctx_train + no longer print ftype
    
    * convert-new.py : minor fixes
    
    * convert-new.py : vocab-only option should work now
    
    * llama : fix tokenizer to use llama_char_to_byte
    
    * tests : add new ggml-vocab-llama.gguf
    
    * convert-new.py : tensor name mapping
    
    * convert-new.py : add map for skipping tensor serialization
    
    * convert-new.py : convert script now works
    
    * gguf.py : pick some of the refactoring from #2644
    
    * convert-new.py : minor fixes
    
    * convert.py : update to support GGUF output
    
    * Revert "ci : disable CI temporary to not waste energy"
    
    This reverts commit 7e82d25f40386540c2c15226300ad998ecd871ea.
    
    * convert.py : n_head_kv optional and .gguf file extension
    
    * convert.py : better always have n_head_kv and default it to n_head
    
    * llama : sync with recent PRs on master
    
    * editorconfig : ignore models folder
    
    ggml-ci
    
    * ci : update ".bin" to ".gguf" extension
    
    ggml-ci
    
    * llama : fix llama_model_loader memory leak
    
    * gptneox : move as a WIP example
    
    * llama : fix lambda capture
    
    ggml-ci
    
    * ggml : fix bug in gguf_set_kv
    
    ggml-ci
    
    * common.h : .bin --> .gguf
    
    * quantize-stats.cpp : .bin --> .gguf
    
    * convert.py : fix HF tensor permuting / unpacking
    
    ggml-ci
    
    * llama.cpp : typo
    
    * llama : throw error if gguf fails to init from file
    
    ggml-ci
    
    * llama : fix tensor name grepping during quantization
    
    ggml-ci
    
    * gguf.py : write tensors in a single pass (#2644)
    
    * gguf : single pass for writing tensors + refactoring writer
    
    * gguf : single pass for writing tensors + refactoring writer
    
    * gguf : single pass for writing tensors + refactoring writer
    
    * gguf : style fixes in simple conversion script
    
    * gguf : refactor gptneox conversion script
    
    * gguf : rename h5 to hf (for HuggingFace)
    
    * gguf : refactor pth to gguf conversion script
    
    * gguf : rm file_type key and method
    
    * gguf.py : fix vertical alignment
    
    * gguf.py : indentation
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * convert-gptneox-hf-to-gguf.py : fixes
    
    * gguf.py : gptneox mapping
    
    * convert-llama-hf-to-gguf.py : fixes
    
    * convert-llama-7b-pth-to-gguf.py : fixes
    
    * ggml.h : reverse GGUF_MAGIC
    
    * gguf.py : reverse GGUF_MAGIC
    
    * test-tokenizer-0.cpp : fix warning
    
    * llama.cpp : print kv general.name
    
    * llama.cpp : get special token kv and linefeed token id
    
    * llama : print number of tensors per type + print arch + style
    
    * tests : update vocab file with new magic
    
    * editorconfig : fix whitespaces
    
    * llama : re-order functions
    
    * llama : remove C++ API + reorganize common source in /common dir
    
    * llama : minor API updates
    
    * llama : avoid hardcoded special tokens
    
    * llama : fix MPI build
    
    ggml-ci
    
    * llama : introduce enum llama_vocab_type + remove hardcoded string constants
    
    * convert-falcon-hf-to-gguf.py : falcon HF --> gguf conversion, not tested
    
    * falcon-main.cpp : falcon inference example
    
    * convert-falcon-hf-to-gguf.py : remove extra kv
    
    * convert-gptneox-hf-to-gguf.py : remove extra kv
    
    * convert-llama-7b-pth-to-gguf.py : remove extra kv
    
    * convert-llama-hf-to-gguf.py : remove extra kv
    
    * gguf.py : fix for falcon 40b
    
    * falcon-main.cpp : fix for falcon 40b
    
    * convert-falcon-hf-to-gguf.py : update ref
    
    * convert-falcon-hf-to-gguf.py : add tensor data layout
    
    * cmpnct_gpt2bpe.hpp : fixes
    
    * falcon-main.cpp : fixes
    
    * gptneox-main.cpp : fixes
    
    * cmpnct_gpt2bpe.hpp : remove non-general stuff
    
    * Update examples/server/README.md
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * cmpnct_gpt2bpe.hpp : cleanup
    
    * convert-llama-hf-to-gguf.py : special tokens
    
    * convert-llama-7b-pth-to-gguf.py : special tokens
    
    * convert-permute-debug.py : permute debug print
    
    * convert-permute-debug-master.py : permute debug for master
    
    * convert-permute-debug.py : change permute type of attn_q
    
    * convert.py : 70b model working (change attn_q permute)
    
    * Delete convert-permute-debug-master.py
    
    * Delete convert-permute-debug.py
    
    * convert-llama-hf-to-gguf.py : fix attn_q permute
    
    * gguf.py : fix rope scale kv
    
    * convert-llama-hf-to-gguf.py : rope scale and added tokens
    
    * convert-llama-7b-pth-to-gguf.py : rope scale and added tokens
    
    * llama.cpp : use rope scale kv
    
    * convert-llama-7b-pth-to-gguf.py : rope scale fix
    
    * convert-llama-hf-to-gguf.py : rope scale fix
    
    * py : fix whitespace
    
    * gguf : add Python script to convert GGMLv3 LLaMA models to GGUF (#2682)
    
    * First pass at converting GGMLv3 LLaMA models to GGUF
    
    * Cleanups, better output during conversion
    
    * Fix vocab space conversion logic
    
    * More vocab conversion fixes
    
    * Add description to converted GGUF files
    
    * Improve help text, expand warning
    
    * Allow specifying name and description for output GGUF
    
    * Allow overriding vocab and hyperparams from original model metadata
    
    * Use correct params override var name
    
    * Fix wrong type size for Q8_K
    
    Better handling of original style metadata
    
    * Set default value for gguf add_tensor raw_shape KW arg
    
    * llama : improve token type support (#2668)
    
    * Merge tokenizer fixes into the gguf branch.
    
    * Add test vocabularies
    
    * Adapt convert-new.py (and fix a clang-cl compiler error on windows)
    
    * Improved tokenizer test
    
    But does it work on MacOS?
    
    * Improve token type support
    
    - Added @klosax code to convert.py
    - Improved token type support in vocabulary
    
    * Exclude platform dependent tests
    
    * More sentencepiece compatibility by eliminating magic numbers
    
    * Restored accidentally removed comment
    
    * llama : add API for token type
    
    ggml-ci
    
    * tests : use new tokenizer type API (#2692)
    
    * Merge tokenizer fixes into the gguf branch.
    
    * Add test vocabularies
    
    * Adapt convert-new.py (and fix a clang-cl compiler error on windows)
    
    * Improved tokenizer test
    
    But does it work on MacOS?
    
    * Improve token type support
    
    - Added @klosax code to convert.py
    - Improved token type support in vocabulary
    
    * Exclude platform dependent tests
    
    * More sentencepiece compatibility by eliminating magic numbers
    
    * Restored accidentally removed comment
    
    * Improve commentary
    
    * Use token type API in test-tokenizer-1.cpp
    
    * py : cosmetics
    
    * readme : add notice about new file format
    
    ggml-ci
    
    ---------
    
    Co-authored-by: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
    Co-authored-by: klosax <131523366+klosax@users.noreply.github.com>
    Co-authored-by: goerch <jhr.walter@t-online.de>
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>

commit dadbed99e65252d79f81101a392d0d6497b86caa
Author: Shouzheng Liu <lshzh.hi@gmail.com>
Date:   Mon Aug 21 06:59:29 2023 -0400

    metal : fix synchronization in new matrix multiplication kernel (#2686)

commit cb1c0727bd59803b439b6a3af121c99e6393ff3d
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Aug 21 11:11:31 2023 +0300

    HellaSwag: split token evaluation into batches if needed (#2681)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 9e232f0234073358e7031c1b8d7aa45020469a3b
Author: slaren <slarengh@gmail.com>
Date:   Sun Aug 20 22:17:53 2023 +0200

    ggml : move all type info to ggml_type_traits (#2663)

commit 5e9ff54a675d163d9f42aad1b5b3e734f17b2701
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Aug 20 16:44:46 2023 +0300

    More efficient Hellaswag implementation (#2677)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 1f0bccb27929e261744c979bc75114955da49e98
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Aug 19 00:45:36 2023 +0300

    server : better default prompt (#2646)

commit f63564adfaa157ca387071d6b9a06cfaef0ef576
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Sat Aug 19 05:41:32 2023 +0800

    server : update xxd usage for older versions compatibility (#2649)
    
    * server : update xxd usage for older versions compatibility
    
    * remove unused $func

commit 2d8b76a110d76ff6b5728ff0af8477531e4db60e
Author: Adrian <smith.adriane@gmail.com>
Date:   Fri Aug 18 12:39:22 2023 -0700

    Add link to clojure bindings to Readme. (#2659)

commit 7af633aec339367e36c867ae709088d6a801aa75
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Aug 18 17:48:31 2023 +0300

    readme : incoming BREAKING CHANGE

commit 097e121e2f17ed3541cf02c55ff7e9febc091b19
Author: slaren <slarengh@gmail.com>
Date:   Fri Aug 18 12:44:58 2023 +0200

    llama : add benchmark example (#2626)
    
    * llama : add benchmark example
    
    * add to examples CMakeLists.txt
    
    * fix msvc build
    
    * add missing include
    
    * add Bessel's correction to stdev calculation
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    * improve markdown formatting
    
    * add missing include
    
    * print warning is NDEBUG is not defined
    
    * remove n_prompt and n_gen from the matrix, use each value separately instead
    
    * better checks for non-optimized builds
    
    * llama.cpp : fix MEM_REQ_SCRATCH0 reusing the value of n_ctx of the first call
    
    * fix json formatting
    
    * add sql output
    
    * add basic cpu and gpu info (linx/cuda only)
    
    * markdown: also show values that differ from the default
    
    * markdown: add build id
    
    * cleanup
    
    * improve formatting
    
    * formatting
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

commit eaf98c2649d7da705de255712f0038ac7e47c610
Author: mdrokz <mohammadmunshi@gmail.com>
Date:   Fri Aug 18 15:47:58 2023 +0530

    readme : add link to Rust bindings (#2656)

commit e9b12c332ec6e215fbac4b2ef165353acbcd8319
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Aug 18 12:48:55 2023 +0300

    perplexity : more meaningful ETA number - 2 decimal points

commit 604b8bdfa6320bbcb018eebcc1252dfede603c6b
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Thu Aug 17 19:54:44 2023 -0400

    Fix unicode in grammars (fixes #2501) (#2553)
    
    * Fix unicode in grammars (fixes #2501)
    
    * add more comments
    
    * fix test-llama-grammar

commit 10151bee2e38b5711335c4a38f6ca93b50223acf
Author: staviq <staviq@gmail.com>
Date:   Thu Aug 17 23:34:01 2023 +0000

    server : support for saving templates in browser LocalStorage (#2486)
    
    * support for templates in browser LocalStorage
    
    * sync accepted #2409 fix from upstream
    
    * convert autosave invocation to useEffect
    
    * Apply suggestions from code review
    
    Co-authored-by: Jhen-Jie Hong <iainst0409@gmail.com>
    
    * Regen index.html.cpp, suggested from code review
    
    ---------
    
    Co-authored-by: Jhen-Jie Hong <iainst0409@gmail.com>

commit 0992a7b8b18a89e29a205efb48ceb559c9a08203
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Aug 17 23:57:59 2023 +0200

    README: fix LLAMA_CUDA_MMV_Y documentation (#2647)

commit 6ddeefad9b634c5c79e6bcf046523493ff1fdf7d
Author: Henri Vasserman <henv@hot.ee>
Date:   Thu Aug 17 23:11:18 2023 +0300

    [Zig] Fixing Zig build and improvements (#2554)
    
    * Fix zig after console.o was split
    
    * Better include and flag management
    
    * Change LTO to option

commit 8dae7ce68437faf1fa96ec0e7687b8700956ef20
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Thu Aug 17 07:29:44 2023 -0600

    Add --cfg-negative-prompt-file option for examples (#2591)
    
    Add --cfg-negative-prompt-file option for examples

commit a73ccf1aa34de49f61bfeb7f8a679c3bfdb3abe3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Aug 17 10:47:09 2023 +0300

    llama : replace (permute + reshape + view_1d) with (view_3d) (#2538)
    
    ggml-ci

commit 7cf54e1f746941279d81d485796777c01f88049c
Author: drbh <david.richard.holtz@gmail.com>
Date:   Thu Aug 17 03:41:01 2023 -0400

    tests : adds simple llama grammar tests (#2618)
    
    * adds simple llama grammar tests
    
    * fix lint and add Makefile
    
    * 0 terminate code_points
    
    * avoid dangling pointers in candidate cleanup
    
    * cleanup grammar at end of test

commit a872a2b28eaefc8d464eaa535c94deeb501666f9
Author: Shouzheng Liu <lshzh.hi@gmail.com>
Date:   Thu Aug 17 03:35:53 2023 -0400

    ggml-alloc : fix discrepency between measure&eval (#2639)
    
    The GGML memory allocator consistently places a tensor within the
    optimal-fit memory block, which is the smallest block capable of
    accommodating the tensor's size. During the measurement phase, the final
    block is generously sized, ensuring it never qualifies as the
    optimal-fit block as long as there exists another block capable of
    accommodating the tensor. Nevertheless, in the evaluation phase, the
    last block is constrained in size and could potentially qualify as the
    optimal-fit block. Consequently, there exists the possibility of a
    tensor being allocated to a different region during evaluation, leading
    to more memory fragmentation in our scratch buffer.
    
    This recent commit guarantees uniform behavior of the allocator across
    both the measurement and evaluation phases, eliminating discrepancies
    between the two.

commit 0919a0f73d95cfb93a1646a1d1741a0615fe2c5e
Author: Kolen Cheung <ickc@users.noreply.github.com>
Date:   Wed Aug 16 21:09:49 2023 +0100

    cmake : install ggml-meta.metal if LLAMA_METAL (#2449)

commit ed53db86c3b0e0815331a96d7a379edb5e62472c
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Thu Aug 17 04:09:03 2023 +0800

    metal : print error of load pipeline state (#2564)
    
    * metal : print error of load pipeline state
    
    * metal : return null if load pipeline failed

commit fc8ef549e50087762a0b4f901cd74b2defcc6ae3
Author: Shouzheng Liu <lshzh.hi@gmail.com>
Date:   Wed Aug 16 16:08:28 2023 -0400

    metal : enable ggml-alloc (#2627)
    
    * metal: enable ggml-alloc
    
    Make ggml-alloc work with concurrently dispatch.
    
    * style-fix
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit bf83bff6742c0f1795b4c18695a13a34ac7adf62
Author: Shouzheng Liu <lshzh.hi@gmail.com>
Date:   Wed Aug 16 16:07:04 2023 -0400

    metal : matrix-matrix multiplication kernel (#2615)
    
    * metal: matrix-matrix multiplication kernel
    
    This commit removes MPS and uses custom matrix-matrix multiplication
    kernels for all quantization types. This commit also adds grouped-query
    attention to support llama2 70B.
    
    * metal: fix performance degradation from gqa
    
    Integers are slow on the GPU, and 64-bit divides are extremely slow.
    In the context of GQA, we introduce a 64-bit divide that cannot be
    optimized out by the compiler, which results in a decrease of ~8% in
    inference performance. This commit fixes that issue by calculating a
    part of the offset with a 32-bit divide. Naturally, this limits the
    size of a single matrix to ~4GB. However, this limitation should
    suffice for the near future.
    
    * metal: fix bugs for GQA and perplexity test.
    
    I mixed up ne02 and nb02 in previous commit.

commit b5ffb2849d23afe73647f68eec7b68187af09be6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Aug 15 10:04:58 2023 +0300

    scripts : add helper script to get wikitext

commit 3ebb00935f3f0522b75df49c2769ab1774b91380
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Tue Aug 15 06:14:14 2023 +0800

    server : add missing /json-schema-to-grammar.mjs (#2616)
    
    fixes #2611

commit d783f7982e0e823a2626a9956359c0d36c1a7e21
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Mon Aug 14 21:37:39 2023 +0800

    metal : return null instead of exit(1) (#2573)

commit d75561df207d22790609ee0ad924302f66ac2599
Author: Cheng Shao <terrorjack@type.dance>
Date:   Mon Aug 14 15:36:42 2023 +0200

    server : add --numa support (#2524)

commit 348acf188c9fbe66396990f2dc83229df367969b
Author: Kamil Tomšík <info@tomsik.cz>
Date:   Mon Aug 14 15:35:16 2023 +0200

    llama : add missing enum keyword in function signatures (#2610)

commit 1cd06fa25eb859b14b3427a1d815a48f25fc3c34
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Aug 14 10:41:22 2023 +0200

    CUDA: launch_bounds, small q4_K, q5_K mmq refactor (#2596)

commit 2feb8934eb75ca63f3c42724229cce1df9579c8e
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Mon Aug 14 16:20:17 2023 +0800

    server : fix default grammar by use empty string in the UI (#2604)

commit 5517d6e69214cdead000a76983b9fe175c3f8329
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Mon Aug 14 15:16:54 2023 +0800

    server : implement json-schema-to-grammar.mjs & add grammar param in the UI (#2588)
    
    * server : implement json-schema-to-grammar.mjs by follow python impl
    
    * server : add grammar support in chat.mjs
    
    * server : implement grammer param in the UI
    
    * server : generate .hpp
    
    * server : remove trailing whitespaces
    
    * server : generate .hpp
    
    * server : fix sort of prop pairs
    
    * server : optimize regex & iteration

commit f31b5397143009d682db90fd2a6cde83f1ef00eb
Author: vxiiduu <73044267+vxiiduu@users.noreply.github.com>
Date:   Mon Aug 14 13:59:16 2023 +1000

    Enhance Windows 7 and below compatibility. (#2592)
    
    * Enhance Windows 7 compatibility.
    * Clean away unnecessary preprocessor conditional

commit ee77efea2a1e3f7d153976b0934522b6bbaa62e6
Author: drbh <david.richard.holtz@gmail.com>
Date:   Sun Aug 13 10:00:48 2023 -0400

    test : add simple grammar parsing tests (#2594)
    
    * adds simple grammar parsing tests
    
    * adds cassert header

commit f64d44a9b9581cd58f7ec40f4fa1c3ca5ca18e1e
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Aug 13 00:24:45 2023 +0200

    CUDA: Fixed OpenLLaMA 3b mmq, reduced compile time (#2590)

commit b19edd54d51cef5e3616c18b1d0d8626895b2cba
Author: byte-6174 <88070277+byte-6174@users.noreply.github.com>
Date:   Fri Aug 11 19:17:25 2023 -0400

    Adding support for llama2.c models (#2559)

commit 53dc399472d5bd35ee739b865e843b1996bd3814
Author: Equim <sayaka@ekyu.moe>
Date:   Sat Aug 12 06:35:14 2023 +0800

    server: fixed wrong variable name in timing json (#2579)
    
    * server: fixed wrong variable name in timing json
    
    * remove redunct entry

commit 9ca4abed893685692f90413e4d43153af12342d9
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Thu Aug 10 13:11:36 2023 -0700

    Handle `ENABLE_VIRTUAL_TERMINAL_PROCESSING` more gracefully on earlier versions of Windows.

commit e59fcb2bc129881f4a269fee748fb38bce0a64de
Author: Christian Demsar <crasm@git.vczf.us>
Date:   Thu Aug 10 10:28:27 2023 -0400

    Add --n-predict -2 for stopping generation on full context (#2565)

commit 1638757767072a4957f52b9e3594f0b67610631b
Author: Martin Krasser <krasserm@googlemail.com>
Date:   Thu Aug 10 12:16:38 2023 +0200

    Fix grammar-based sampling issue in server (#2566)

commit 916a9acdd0a411426690400ebe2bb7ce840a6bba
Author: Sam Spilsbury <smspillaz@gmail.com>
Date:   Wed Aug 9 23:47:42 2023 +0300

    ggml-alloc: Don't try to re-use buffers of external tensors (#2562)
    
    * ggml-alloc: Don't try to re-use buffers of external tensors
    
    They might be weights that came from another context, so we
    have no control over them (and they might be re-used elsewhere
    so writing to them would be a bad idea).
    
    * ggml-alloc: >= when checking for out-of-bounds
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit ea04a4ca1940d92becc0ee26523aa2c4a18cf938
Author: grahameth <96447521+grahameth@users.noreply.github.com>
Date:   Wed Aug 9 22:46:40 2023 +0200

    add log_callback to llama_context_params for custom logging. (#2234)
    
    * add log_callback to llama_context_params for custom logging.
    
    * Fix macro expansion on gcc
    
    * Add struct llama_state for global variables and move log_callback there
    
    * Turn log level into enum and some minor changes.
    
    * Remove model_for_logging parameter (not needed anymore)
    
    * Convert remaining fprintf(stderr, ...) calls to use new macros.
    
    * Fix enum and initialize g_state
    
    * Fix log calls after merge
    
    * Fix missing static
    
    * Add back all the new lines in the logging strings
    
    * Add comment for llama_log_callback and replace remaining printf calls
    
    ---------
    
    Co-authored-by: grahameth <->
    Co-authored-by: Helmut <helmut.buhler@inf.h-brs.de>

commit 25d43e0eb578b6e73046d9d6644a3a14d460600d
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Aug 9 09:42:34 2023 +0200

    CUDA: tuned mul_mat_q kernels (#2546)

commit f5bfea0580e417f99850d5456ca541d871a3e48c
Author: Martin Krasser <krasserm@googlemail.com>
Date:   Tue Aug 8 15:29:19 2023 +0200

    Allow passing grammar to completion endpoint (#2532)
    
    * Allow passing grammar to completion endpoint

commit acfc5478ff3446ca3b54553967a3dea09b7c771a
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Aug 8 14:38:16 2023 +0200

    CUDA: tighter VRAM scratch size for 65b/70b (#2551)

commit 7ed8d1fe7f8cbe6a6763e6b46759795ac8d21e12
Author: chaihahaha <chai836275709@gmail.com>
Date:   Tue Aug 8 20:07:02 2023 +0800

    llm.vim : multiline autocompletion, get rid of "^@" (#2543)

commit e7f94d6fdc83b41ba449b4b8c80821673dd12ffc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Aug 8 15:05:30 2023 +0300

    vim : bring back simple llm.vim example

commit 2d7baaf50f3277e65cf71071f61ea34823d14c30
Author: AustinMroz <austinmroz@utexas.edu>
Date:   Tue Aug 8 06:44:48 2023 -0500

    vim : streaming and more (#2495)
    
    * Update Vim plugin
    
    * Remove getbufoneline usage, Add input bind example.
    
    getbufoneline() appears to be a recently added function and has been
    replaced with getbufline for compatibility.
    
    An additional example that explains how to add a keybind that works in
    insert mode was added.

commit f3c3b4b1672d860800639c87d3b5d17564692469
Author: klosax <131523366+klosax@users.noreply.github.com>
Date:   Mon Aug 7 19:07:19 2023 +0200

    Add --rope-scale parameter (#2544)
    
    * common.cpp : Add --rope-scale parameter
    * README.md : Add info about using linear rope scaling

commit 93356bdb7a324a8f6570f99d02af392cd4c45796
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 7 14:25:58 2023 +0300

    ggml : mul mat tweaks (#2372)
    
    * ggml : mul mat wip
    
    ggml-ci
    
    * ggml : alternative thread distribution for mul_mat
    
    ggml-ci
    
    * ggml : mul_mat block tiling attempt
    
    * ggml : mul_mat threads yield
    
    ggml-ci

commit 60baff7c8584ec369e53469cad5f92e102b1efe4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 7 14:24:42 2023 +0300

    ggml : pad result of ggml_nbytes()

commit 9082b5dfbfae01243a0b822dcd2812877e63bf1b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 7 13:55:18 2023 +0300

    ggml : change params pointer (style change) (#2539)
    
    ggml-ci

commit 99d29c0094476c4962023036ecd61a3309d0e16b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 7 13:20:09 2023 +0300

    ggml : sync (custom ops) (#2537)
    
    ggml-ci

commit 3d9a55181603e85a26378a850a14068034e5002d
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Aug 7 10:09:40 2023 +0200

    Fixed mmap prefetch for GPU offloading (#2529)

commit f6f9896ac3d2ff207e18f87dab85d126ceef5236
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 7 10:52:57 2023 +0300

    metal : fix out-of-bounds access + inc concurrency nodes (#2416)
    
    * metal : fix out-of-bounds access + style changes
    
    * metal : increase concurrency nodes to 2*GGML_MAX_NODES

commit 34a14b28ff7f3c98730339bacee035091b2a812a
Author: GiviMAD <GiviMAD@users.noreply.github.com>
Date:   Sun Aug 6 23:21:46 2023 -0700

    [Makefile] Move ARM CFLAGS before compilation (#2536)

commit 7297128db8159c7b12db4c28a4532b993025c2e5
Author: Henri Vasserman <henv@hot.ee>
Date:   Mon Aug 7 08:35:53 2023 +0300

    [Zig] Rewrite build for Zig 0.11 (#2514)
    
    * zig build fixes
    
    * Disable LTO on Windows.

commit 86c32198954a2bc482025703d6875e11f1c2a574
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Sat Aug 5 23:49:34 2023 -0700

    console : fix issue related to Windows 11 PowerShell console mode persistence (#2521)

commit 2e8265ae1764d6288aab0e2df641909072e2d58e
Author: Keiichi Tabata <keiichi.tabata@outlook.com>
Date:   Sun Aug 6 15:34:05 2023 +0900

    convert.py : add missing abstract methods for quantized data (#2491)

commit f514d1b306e1114c2884fcb25dd9bd48ae64ba32
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Aug 5 18:20:44 2023 +0200

    CUDA: faster k-quant mul_mat_q kernels (#2525)

commit 332311234a0aa2974b2450710e22e09d90dd6b0b
Author: Jonas Wunderlich <32615971+jonas-w@users.noreply.github.com>
Date:   Fri Aug 4 20:16:11 2023 +0000

    fix firefox autoscroll (#2519)

commit 182af739c4ce237f7579facfe8f94dc53a1f573f
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Aug 4 15:00:57 2023 -0400

    server: regenerate completion.js.hpp (#2515)

commit 4329d1acb01c353803a54733b8eef9d93d0b84b2
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Aug 4 11:35:22 2023 -0400

    CUDA: use min compute capability of GPUs actually used (#2506)

commit 02f9d96a866268700b8d8e7acbbcb4392c5ff345
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Aug 4 11:34:32 2023 -0400

    CUDA: check if event is NULL before cudaStreamWaitEvent (#2505)
    
    Fixes #2503

commit 3498588e0fb4daf040c4e3c698595cb0bfd345c0
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Fri Aug 4 08:20:12 2023 -0700

    Add --simple-io option for subprocesses and break out console.h and cpp (#1558)

commit 5f631c26794b6371fcf2660e8d0c53494a5575f7
Author: Stephen Nichols <snichols@users.noreply.github.com>
Date:   Fri Aug 4 06:37:24 2023 -0500

    Fixing race condition in server and partial stream handling in frontend. (#2391)
    
    * Fixing race condition in server.cpp and partial stream handling in completion.js
    
    * Reverting assert edits.
    
    * Adding newline to eof

commit 415e99fec27be5a2e4283f1937afd17eb33fbd66
Author: l3utterfly <gc.pthzfoldr@gmail.com>
Date:   Fri Aug 4 19:29:52 2023 +0800

    Stream save llama context data to file instead of allocating entire buffer upfront (#2488)
    
    * added stream saving context data to file to avoid allocating unnecessary amounts of memory
    
    * generalised copying state data to file or buffer
    
    * added comments explaining how copy_state_data works
    
    * fixed trailing whitespaces
    
    * fixed save load state example
    
    * updated save load state to use public function in llama.cpp
    
    * - restored breakage of the llama_copy_state_data API
    - moved new logic for copying llama state data to internal function
    
    * fixed function declaration order
    
    * restored save load state example
    
    * fixed whitepace
    
    * removed unused llama-util.h include
    
    * Apply suggestions from code review
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Apply code review suggestions
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

commit ff966e7ca6af127c9405523cdb07ef8fa01bf6d6
Author: Borislav Stanimirov <b.stanimirov@abv.bg>
Date:   Fri Aug 4 13:07:21 2023 +0300

    build : fix several cast and printf warnings (#2499)

commit 8183159cf3def112f6d1fe94815fce70e1bffa12
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Wed Aug 2 22:05:44 2023 -0400

    examples : generate JSON according to schema (#1887)
    
    * examples : add JSON schema grammars
    
    * complete JSON grammar
    
    * ensure primitive types can be used as root of schema
    
    * support integer type and adjust usage text

commit 468ea24fb4633a0d681f7ac84089566c1c6190cb
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Aug 2 18:04:04 2023 +0200

    CUDA: faster non k-quant mul_mat_q kernels (#2483)

commit 4f6b60c77652cdfc9d5545fe247ae5d764815598
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Aug 2 16:48:10 2023 +0200

    CUDA: Fix models with output size != 32000 (#2480)

commit 220d9318647a8ce127dbf7c9de5400455f41e7d8
Author: ldwang <ftgreat@163.com>
Date:   Wed Aug 2 16:21:11 2023 +0800

    readme : add Aquila-7B model series to supported models (#2487)
    
    * support bpe tokenizer in convert
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    
    * support bpe tokenizer in convert
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    
    * support bpe tokenizer in convert, fix
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    
    * Add Aquila-7B models in README.md
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    
    * Up Aquila-7B models in README.md
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    
    ---------
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    Co-authored-by: ldwang <ftgreat@gmail.com>

commit 81844fbcfd93a162b7aeaea9e4f2ab1358f7f97e
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Wed Aug 2 04:06:19 2023 -0400

    tests : Fix compilation warnings (Linux/GCC) (#2451)
    
    * fix hellaswag print format, cast away warning in test-double-float
    
    * c++11 cannot use designated initializers
    
    * add static to test-grad0.c internal functions
    
    * use memcpy in test-double-float.c
    
    * port c tests to c++
    
    * use initializer list for ggml_init_params

commit a312193e184b919047f33a5e844d846c5538dbe6
Author: Yiming Cui <conandiy@vip.qq.com>
Date:   Wed Aug 2 14:18:31 2023 +0800

    readme : Add Chinese LLaMA-2 / Alpaca-2 to supported models (#2475)
    
    * add support for chinese llama-2 / alpaca-2
    
    * remove white spaces

commit c574bddb368424b5996cbee2ec45ec050967d404
Author: Bono Lv <lvscar@users.noreply.github.com>
Date:   Tue Aug 1 20:54:28 2023 +0800

    fix a typo in examples/server/README.md (#2478)

commit 86aeb27734481751592abd85590020b40d9224c8
Author: ebraminio <ebraminio@gmail.com>
Date:   Tue Aug 1 01:56:23 2023 -0700

    server : Support dark mode (#2414)
    
    * server : Support dark mode
    
    So it respects user system light / dark settings.
    
    * Update index.html.hpp by running ./deps.sh

commit 1873ff586bd8499a18f763632711bf15d253585e
Author: Matteo Boschini <12133566+mbosc@users.noreply.github.com>
Date:   Tue Aug 1 09:43:12 2023 +0200

    metal : add gqa8 kernel to allow llama-2-70B on metal (#2459)
    
    * Added gqa8 kernel to allow llama-2-70B on metal
    
    * Update ggml-metal.m
    
    Co-authored-by: Cebtenzzre <cebtenzzre@gmail.com>
    
    * Extend kernel_mul_mat_f16_f32 to handle gqa broadcast
    
    * Added ne03==ne13 assertion
    
    ---------
    
    Co-authored-by: Cebtenzzre <cebtenzzre@gmail.com>

commit 49e7cb5bb1f75c91dd5db7d2d88cbc11bd9ee0c5
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jul 31 21:02:19 2023 +0200

    CUDA: fixed LLAMA_FAST compilation option (#2473)

commit b772bba42e3bbca3cdab224456f8ff2ce427fd0b
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jul 31 19:52:22 2023 +0200

    CUDA: fixed cmake F16 option (#2471)

commit 0728c5a8b9569183ffca0399caac099afef87595
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jul 31 15:44:35 2023 +0200

    CUDA: mmq CLI option, fixed mmq build issues (#2453)

commit 1215ed7d5ccf854a55eccb52661427bb985e7f2c
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jul 31 14:32:30 2023 +0200

    CUDA: Implemented row flattening for non-glm RoPE (#2468)

commit 2dbf518911926ef5a30f43aa83a0b1b1cdeaaa11
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jul 31 13:18:51 2023 +0200

    CUDA: fewer memory bank conflicts for mul_mat_q (#2458)

commit 9d2382b3e45b5815fc6a054045a2f2c2b18c22a2
Author: slaren <slarengh@gmail.com>
Date:   Mon Jul 31 11:02:53 2023 +0200

    Fix Metal backend broken from the allocator changes (#2455)
    
    * fix Metal backend broken from the allocator changes

commit a113689571420fb4d6540f1a324d12965781356a
Author: slaren <slarengh@gmail.com>
Date:   Sun Jul 30 15:58:01 2023 +0200

    ggml : add graph tensor allocator (#2411)
    
    * ggml : add graph tensor allocator
    
    * ggml : don't calculate data pointer of unallocated tensors when creating a view with an offset
    
    * ggml : refactor ggml_view_Nd into ggml_view_tensor_offset

commit 11f3ca06b8c66b0427aab0a472479da22553b472
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jul 29 23:04:44 2023 +0200

    CUDA: Quantized matrix matrix multiplication (#2160)
    
    * mmq implementation for non k-quants
    
    * q6_K
    
    * q2_K
    
    * q3_k
    
    * q4_K
    
    * vdr
    
    * q5_K
    
    * faster q8_1 loading
    
    * loop unrolling
    
    * add __restrict__
    
    * q2_K sc_high
    
    * GGML_CUDA_MMQ_Y
    
    * Updated Makefile
    
    * Update Makefile
    
    * DMMV_F16 -> F16
    
    * Updated README, CMakeLists
    
    * Fix CMakeLists.txt
    
    * Fix CMakeLists.txt
    
    * Fix multi GPU out-of-bounds

commit 9baf9ef304f330009d5a93b7390280a0fd27c9a1
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jul 29 23:04:10 2023 +0200

    CUDA: faster multi GPU synchronization (#2448)

commit 8a88e5855c3b93024be0f93290b01a4206b65b38
Author: klosax <131523366+klosax@users.noreply.github.com>
Date:   Fri Jul 28 20:25:36 2023 +0200

    perplexity : add Hellaswag calculation (#2389)
    
    * common.h : add hellaswag / remove perplexity-lines
    
    * common.cpp : add hellaswag / remove perplexity-lines
    
    * perplexity.cpp : add hellswag scores / remove perplexity-lines
    
    * perplexity.cpp : clean up
    
    * common.h : change default param value
    
    * common.cpp : Change default param
    
    * perplexity.cpp : alter wording
    
    * common.h : alter wording
    
    * common.cpp : alter wording

commit a9559bf77b903d94eb21614ceae5ae8950f0f1fc
Author: Lee <44310445+lx200916@users.noreply.github.com>
Date:   Sat Jul 29 02:17:45 2023 +0800

    ggml : workaround for missing _mm256_setr_m128i in GCC < 8 in k_quants.c (#2405)

commit ee1b497c985f61d6ec519c39fcfed78a3c6f1d06
Author: eric8607242 <e0928021388@gmail.com>
Date:   Sat Jul 29 02:10:05 2023 +0800

    llama : support more diverse tokenizers? (#2420)
    
    * supporting more diverse tokenizers
    
    * Update llama.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit d73b8d48b45d6e2c0ae9bb8c39623c4024adc275
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 28 21:05:08 2023 +0300

    examples : fix whitespace

commit 34ae1caf7fdea4c4c09087002e15e6230102e6a9
Author: nhamanasu <45545786+nhamanasu@users.noreply.github.com>
Date:   Sat Jul 29 03:02:10 2023 +0900

    examples : server chat mode with llama2 (#2400)
    
    * add: server chat mode with llama2
    
    * fix: remove the unnecessary last \n

commit d91f3f0c55663719ea03b76311e8c36ed55eb0e2
Author: Weird Constructor <weirdconstructor@gmail.com>
Date:   Fri Jul 28 10:44:43 2023 +0200

    readme : fix the description of the Tail free sampling (TFS) method (#2431)

commit 65cdf34bdc469fa86248e667a5880992684ef114
Author: Rand Xie <randxiexyy29@gmail.com>
Date:   Fri Jul 28 01:42:53 2023 -0700

    llama : use n_embd_gqa instead of n_embd to handle llama-2 70B (#2433)

commit edcc7ae7d26007bbf83136e9d33f863fcad9b871
Author: niansa/tuxifan <tuxifan@posteo.de>
Date:   Fri Jul 28 03:14:11 2023 +0200

    Obtaining LLaMA 2 instructions (#2308)
    
    * Obtaining LLaMA 2 instructions
    
    * Removed sharing warning for LLaMA 2
    
    * Linked TheBloke's GGML repos
    
    * Add LLaMA 2 to list of supported models
    
    * Added LLaMA 2 usage instructions
    
    * Added links to LLaMA 2 70B models

commit 7c529cede6e84054e77a3eceab31c53de7b2f55b
Author: mj-shifu <77107165+mj-shifu@users.noreply.github.com>
Date:   Thu Jul 27 22:39:17 2023 +0200

    convert.py : Update to support 70B HF format model files (#2427)
    
    * convert.py : fix llama 2 70b conversion from Huggingface

commit 1a941869cbef8e9cc351a6c6987e4ae3b0f021f7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jul 27 11:00:54 2023 +0300

    metal : disable graph concurrency optimization due to bug (#2413)

commit b5472ea0ada081a6e1c06998ebbc9a24aa2cd4a4
Author: slaren <slarengh@gmail.com>
Date:   Wed Jul 26 23:57:23 2023 +0200

    ggml : fix assert in ggml_set_unary_op (#2410)

commit 6df1f5940f889adde32fe47dc8881f010dcf9aba
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Wed Jul 26 14:00:04 2023 -0400

    make : build with -Wmissing-prototypes (#2394)

commit 5488fb789ea5692268309baa76f67598155060be
Author: slaren <slarengh@gmail.com>
Date:   Wed Jul 26 15:56:53 2023 +0200

    ggml : allocate graphs in a context (#2392)
    
    * ggml : graph allocation in contexts
    
    * allocate work buffer as a ggml_object in ggml_graph_compute_with_ctx
    
    * llama.cpp : allocate graph in the context
    
    * add GGML_PAD
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit eb542d39324574a6778fad9ba9e34ba7a14a82a3
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Jul 25 18:35:53 2023 +0300

    Add LLAMA_DEFAULT_RMS_EPS so we can change the default (#2384)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 07aaa0f63fccaeab099b3a732abda20b921bc5a5
Author: slaren <slarengh@gmail.com>
Date:   Tue Jul 25 16:20:12 2023 +0200

    ggml : fix ggml_flash_attn to use op_params (#2387)
    
    * ggml : fix ggml_flash_attn to use op_params

commit fce48caf9a6b9930eee9e2a5971428cdff403ba8
Author: ldwang <ftgreat@163.com>
Date:   Tue Jul 25 21:22:09 2023 +0800

    convert.py : support bpe tokenizer (#2228)
    
    * support bpe tokenizer in convert
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    
    * support bpe tokenizer in convert
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    
    * support bpe tokenizer in convert, fix
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    
    ---------
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    Co-authored-by: ldwang <ftgreat@gmail.com>

commit 875086bdb95ce1f3294439811536533199e3b579
Author: Jiahao Li <liplus17@163.com>
Date:   Tue Jul 25 20:58:32 2023 +0800

    ggml : relax contiguous constraints in activation function (#2371)

commit da1889834a036a63ead2b0ca5c9ed8967712568c
Author: slaren <slarengh@gmail.com>
Date:   Tue Jul 25 14:32:20 2023 +0200

    ggml : improve graph build time via hash table lookup (#2329)
    
    * improve graph build time
    
    * ggml_tensor : use 1 bit per flag
    
    * use a hash table instead

commit 82552b7f5403ca13957ac9a2cdc1732470057b62
Author: Hesen Peng <hesen.peng@gmail.com>
Date:   Tue Jul 25 05:24:09 2023 -0700

    build : fix line breaking error in build-info.sh (#2349)
    
    * fix line breaking
    
    * build number line break removal

commit 0c06204fb39aa5560e883e0ae74be9518c57d88e
Author: Xiao-Yong Jin <jinxiaoyong@gmail.com>
Date:   Tue Jul 25 07:19:11 2023 -0500

    main : add `--in-prefix-bos` to prefix BOS to user inputs; keep EOS (#2304)
    
    * add `--in-prefix-bos` to prefix BOS to user inputs; keep EOS
    
    The BOS precedes the string specified by `--in-prefix`.
    Model generated EOS is now kept in the context.
    
    It provides a way to strictly following the prompt format used in
    Llama-2-chat.
    
    The EOS handling also benefits some existing finetunes that uses
    EOS to mark the end of turn.
    
    * examples/common: move input_prefix_bos to other bools

commit 1fed755b1fb9babb6dbc1b4023e492950cd5a5be
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Tue Jul 25 08:16:13 2023 -0400

    ci : add non-AVX scalar build/test (#2356)
    
    * noavx build and test
    
    * we don't need to remove f16c in windows

commit be2301bcdaf6c9f0921141bd071de7788e2a351a
Author: katsu560 <118887472+katsu560@users.noreply.github.com>
Date:   Tue Jul 25 21:13:41 2023 +0900

    k_quants : add AVX support to dot functions with QK_K as 64 (#2339)
    
    * add AVX to ggml_vec_dot_q2_K_q8_K()
    
    * add AVX to ggml_vec_dot_q3_K_q8_K()
    
    * add AVX to ggml_vec_dot_q4_K_q8_K()
    
    * add AVX to ggml_vec_dot_q5_K_q8_K()
    
    * add AVX to ggml_vec_dot_q6_K_q8_K()
    
    * refactor AVX code in ggml_vec_dot_q6_K_q8_K()

commit 1aa18ef994a6a2b531434eb13251ef48e56d345b
Author: Shouzheng Liu <lshzh.hi@gmail.com>
Date:   Tue Jul 25 08:00:19 2023 -0400

    metal : concurrently dispatch commands (#2358)
    
    * metal: concurrently dispatch commands
    
    Function `ggml_metal_graph_find_concurrency` will run and write
    commands that can be issued concurrently to metal context `concur_list`
    array, when `ggml_metal_graph_compute` is called for the first time.
    
    * metal: don't call find_concurrency automatically.
    
    * metal : code style changes
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 9a08eaf3c4010962d0126e9e5bfbe9af64b2ac90
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Jul 25 13:48:29 2023 +0300

    Another speed gain for Q4_0 and Q4_1 on Metal (#2375)
    
    * Another speed gain for Q4_0 and Q4_1 on Metal
    
    * Have N_DST, etc., be template parameters
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 129d844c87d90e74aafc23dcc84c980fd408def4
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Jul 25 13:48:04 2023 +0300

    Fix Q4_K and Q5_K for QK_K = 64 on CUDA (#2359)
    
    * Fix Q4_K and Q5_K for QK_K = 64
    
    * Very slightly better Q5_K bit fiddling
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit d5512b782b27ff698007dcd175da18959d5f163f
Author: slaren <slarengh@gmail.com>
Date:   Tue Jul 25 11:36:17 2023 +0200

    server: add rms_norm_eps parameter (#2380)

commit c798308e3a425eae050a1f249a576fa8c6433327
Author: Henri Vasserman <henv@hot.ee>
Date:   Tue Jul 25 10:27:34 2023 +0300

    [Server] Escape HTML in webchat (#2368)
    
    * escape HTML in webchat
    * add amp

commit 41c674161fb2459bdf7806d1eebead15bc5d046e
Author: slaren <slarengh@gmail.com>
Date:   Mon Jul 24 17:57:12 2023 +0200

    make rms_norm_eps a parameter (#2374)
    
    * make rms_norm_eps a parameter
    
    * add rms_norm_eps to command line
    
    * fix baby llama, test-grad0
    
    * use scientific notation for eps param in the help
    
    ggml-ci

commit b3f138d05849ccbce67303ac17b50ebbc268128a
Author: Aarni Koskela <akx@iki.fi>
Date:   Mon Jul 24 17:54:22 2023 +0300

    Chat UI extras (#2366)
    
    * makefile: correct deps for server
    
    * server: tighten settings layout a little
    
    * server: expose all currently configured generation params in UI
    
    * server: expose remaining generation params, for the adventurous
    
    * server: embetter mirostat fields

commit 5b2b2dc6ae8086bff7c9b3c17fb435cf319b7185
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jul 24 14:46:21 2023 +0300

    ggml : sync (unary ops refactor, static-correctness) (#2370)
    
    * ggml : sync (unary ops, tests)
    
    ggml-ci
    
    * tests : remove unnecessary funcs

commit 42f70cb2f6a8089e0a0560a459e4ba317bac4d49
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jul 24 12:55:02 2023 +0300

    Fix scalar version of Q5_K when QK_K = 64 (#2362)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 84e09a7d8bc4ab6d658b5cd81295ac0add60be78
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Sun Jul 23 23:58:10 2023 -0400

    llama : add grammar-based sampling (#1773)
    
    * llama, main : constrain sampling to grammar
    
    * allow loading grammar from file
    
    * fix whitespace errors
    
    * handle & print parser errors
    
    * add comments to grammar syntax and allow newlines where unambiguous
    
    * add missing include
    
    * support alternates in root rule
    
    * fix bugs with empty token and EOS
    
    * adjust JSON grammar
    
    * remove swp file
    
    * rewrite ternary expressions
    
    Co-authored-by: Henri Vasserman <henv@hot.ee>
    
    * use struct for grammar elements and add Unicode support
    
    * add unicode escapes
    
    * add inverse char ranges
    
    * only sample full tokens (no peeking or truncation)
    
    * llama : minor style changes
    
    blindly applied in online editor - hopefully I didn't break something
    
    * update help text
    
    * add warning message if EOS is disabled
    
    ---------
    
    Co-authored-by: Henri Vasserman <henv@hot.ee>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 2f9cf974a066ac0e03fbb235d834b01b0164d743
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jul 24 00:19:47 2023 +0300

    Some more Q4_K and Q5_K speedup on CUDA (#2346)
    
    * Faster Q5_K on CUDA
    
    * Small Q5_K improvement on older GPUs
    
    * Spped up Q4_K on CUDA
    
    GTX1660: 29.5 ms/t -> 25.6 ms/t
    RTX4080: 8.40 ms/t -> 8.25 ms/t
    
    * Spped up Q4_K on CUDA
    
    GTX1660: 36.7 ms/t -> 35.6 ms/t
    RTX4080:  9.8 ms/t ->  9.5 ms/t
    
    * Address PR comments
    
    * Add some comments to satisfy PR reviewer
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 4f06592cc6b83979e4b442e8cb97b3948c857188
Author: IgnacioFDM <ignaciofdm@gmail.com>
Date:   Sun Jul 23 17:31:17 2023 -0300

    Add gqa parameter support to the server (#2351)
    
    * Add gqa parameter support to the server
    * Change help from stderr to stdout

commit 70d26ac3883009946e737525506fa88f52727132
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Jul 23 17:49:06 2023 +0200

    Fix __dp4a documentation (#2348)

commit 57921ca6db53e08eb90010fba99add85be28b5a1
Author: wzy <32936898+Freed-Wu@users.noreply.github.com>
Date:   Sun Jul 23 21:33:02 2023 +0800

    common : n_threads == -1 uses std::thread::hardware_concurrency() (#2347)
    
    * Fix #2345, fix incorrect n_threads
    
    * Update examples/common.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 3602ac4255fd4cd589821346290b464a64b955d1
Author: slaren <slarengh@gmail.com>
Date:   Sun Jul 23 15:19:39 2023 +0200

    fix n_tasks (#2342)
    
    ggml-ci

commit 95a6c595e7ca8dbe47ccf8824e04213e10357f9a
Author: slaren <slarengh@gmail.com>
Date:   Sun Jul 23 14:36:02 2023 +0200

    ggml: move op parameters from tensors to ggml_tensor::op_params (#2333)
    
    * ggml: move op parameters from tensors to ggml_tensor::op_params
    
    * alibi: use memcpy for float params
    
    * remove `src[1] = NULL` in ops

commit e76d630df17e235e6b9ef416c45996765d2e36fb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jul 23 15:09:47 2023 +0300

    llama : grouped-query attention + LLaMAv2 70B support (#2276)
    
    * CUDA: GQA implementation
    
    * llama : support for GQA and LLaMAv2 70B
    
    ggml-ci
    
    * py : fix hparams parsing (if-else blocks)
    
    ggml-ci
    
    * py : oh boy ..
    
    ggml-ci
    
    * help : fix gqa value for 70B
    
    ggml-ci
    
    ---------
    
    Co-authored-by: JohannesGaessler <johannesg@5d6.de>

commit 1d0824b2476e7fda09751a0235c9e571b76d6f2c
Author: maddes8cht <55592906+maddes8cht@users.noreply.github.com>
Date:   Sun Jul 23 13:59:48 2023 +0200

    llama : print help to stdout (#2338)

commit bc3ec2cdc9ea20b0faba2e1b4576fab3a911e4d1
Author: wzy <32936898+Freed-Wu@users.noreply.github.com>
Date:   Sun Jul 23 19:57:02 2023 +0800

    flake : support `nix build '.#opencl'` (#2337)

commit a940458e4814e87bd0d3fbdb3f3d2733b4a3ccb1
Author: Christian Demsar <christian@github.email.demsar.us>
Date:   Sun Jul 23 07:56:34 2023 -0400

    llama : print max tensor size to stderr (#2336)

commit 91171b8072f6f0c8ae3a61e23451acb538bb9ece
Author: Jose Maldonado <63384398+yukiteruamano@users.noreply.github.com>
Date:   Sun Jul 23 07:52:08 2023 -0400

    make : fix CLBLAST compile support in FreeBSD (#2331)
    
    * Fix Makefile for CLBLAST compile support and instructions for compile llama.cpp FreeBSD
    
    * More general use-case for CLBLAST support (Linux and FreeBSD)

commit 355c80f49e32b0b15c0a457f3bad380e57f5b9ac
Author: AustinMroz <austinmroz@utexas.edu>
Date:   Sun Jul 23 06:16:48 2023 -0500

    examples : simplify vim plugin (#2327)
    
    Uses builtin json_encode and json_decode functions to simplify escaping
    Removes the need for temp files

commit 83a00ce69bef9124c0702424a012ea799128b77d
Author: Jiahao Li <liplus17@163.com>
Date:   Sun Jul 23 19:00:37 2023 +0800

    metal : support bcast add & dup & cont op (#2323)

commit d2a43664f93ba30a84e42713bb69f936cbdacf2a
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Jul 23 08:49:20 2023 +0300

    Speed up Q4_K (#2322)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit b9b7d94fc10a8039befd1bc3af4f4b09c620c351
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jul 22 21:27:34 2023 +0200

    CUDA: Fixed 7b q3_K_S with mul_mat_vec_q (#2313)

commit b47b8a9cfeb439d271bf997fb985fd6d82b3af5e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jul 22 21:17:57 2023 +0300

    llama : optimize memory buffers (#2325)

commit b5fe67f8c69113bd9354bc1adcfe2df6be323740
Author: klosax <131523366+klosax@users.noreply.github.com>
Date:   Sat Jul 22 14:21:24 2023 +0200

    Perplexity: Compute scores correlated to HellaSwag (#2312)
    
    * Add parameter --perplexity-lines to perplexity.cpp

commit 24baa54ac1ff3d4156a2360deb1473af04a9b1a2
Author: whoreson <139810751+whoreson@users.noreply.github.com>
Date:   Sat Jul 22 12:34:51 2023 +0200

    examples : basic VIM plugin
    
    VIM plugin for server exe

commit dd6c67d3cbb7b360747f776412bf01976aa32f4b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jul 22 12:00:56 2023 +0300

    ci : fix args

commit 5d500e8ccf5eee3de3ae66685cc3be75e43e08b9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jul 22 11:48:22 2023 +0300

    ci : add 7B CUDA tests (#2319)
    
    * ci : add 7B CUDA tests
    
    ggml-ci
    
    * ci : add Q2_K to the tests
    
    * ci : bump CUDA ppl chunks
    
    ggml-ci
    
    * ci : increase CUDA TG len + add --ignore-eos
    
    * ci : reduce CUDA ppl cunks down to 4 to save time

commit 7d5f18468ceabd7a38f414f9f21b26b0c137f994
Author: Richard Roberson <richardr1126@gmail.com>
Date:   Fri Jul 21 13:01:10 2023 -0600

    examples : add easy python script to create quantized (k-bit support) GGML models from local HF Transformer models (#2311)
    
    * Resync my fork with new llama.cpp commits
    
    * examples : rename to use dash instead of underscore
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit d924522a46c5ef097af4a88087d91673e8e87e4d
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Jul 21 17:27:51 2023 +0300

    Custom RoPE + bettter memory management for CUDA (#2295)
    
    * Custom RoPE + bettter memory management for CUDA
    
    * Adjusted look ahead in ggml_cuda_pool_malloc to 5%
    
    This is sufficient it seems.
    We end up using about 200 MB less VRAM that way when running
    the 13B model with context 8192.
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 4d76a5f49b9b5382dba5d13d92edb9159536c225
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Jul 21 17:05:30 2023 +0300

    Faster Q3_K implementation on Metal (#2307)
    
    * Faster Q3_K on Metal
    
    * Additional Q3_K speedup on Metal
    
    * Q3_K for QK_K = 64
    
    * Better Q3_K for QK_K = 64
    
    21.6 ms/t -> 21.1 ms/t
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 0db14fef06836caaa13cc123c0a24dc598bdb9f0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 21 15:16:55 2023 +0300

    ggml : fix the rope fix (513f8619535a64fa9ace808cdcbcf66211535f5c)

commit 03e566977b277937c5f706180171c5d12b597b0b
Author: Ikko Eltociear Ashimine <eltociear@gmail.com>
Date:   Fri Jul 21 20:53:07 2023 +0900

    examples :  fix typo in minigpt4.py (#2298)
    
    promt -> prompt

commit 513f8619535a64fa9ace808cdcbcf66211535f5c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 21 14:51:34 2023 +0300

    ggml : fix rope args order + assert (#2054)

commit 3973b25a64a37a47eac156a3fd28f83c16f14bf2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 21 14:42:41 2023 +0300

    gitignore : fix final newline

commit ab0e26bdfb7b3adb1e3145c61a0fa92d1abd21d0
Author: Guillaume "Vermeille" Sanchez <Guillaume.V.Sanchez@gmail.com>
Date:   Fri Jul 21 12:58:36 2023 +0200

    llama : remove cfg smooth factor as it is only a reparameterization of the guidance scale (#2280)

commit 73643f5fb1136dc2b65ae910bdc5a431520d70a2
Author: Jose Maldonado <63384398+yukiteruamano@users.noreply.github.com>
Date:   Fri Jul 21 06:53:27 2023 -0400

    gitignore : changes for Poetry users + chat examples (#2284)
    
    A fix in Makefile for FreeBSD users. In the platfrom x86_64 is amd64. This fix resolve compilation using CFLAGS and CXXFLAGS with -march=native and -mtune=native
    Add two examples for interactive mode using Llama2 models (thx TheBloke for models)
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit a814d04f81121e0429b39a61fe4afd946cd42046
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 21 13:50:55 2023 +0300

    make : fix indentation

commit 4c013bb7385a0e52ce721480c40c45bec5ef103f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 21 13:48:18 2023 +0300

    ci : fix MNT realpath usage (#2250)

commit 42c7c2e2e9cae79330f57456fbc0eae1eaff17fa
Author: Sky Yan <skyan83@gmail.com>
Date:   Fri Jul 21 18:38:57 2023 +0800

    make : support customized LLAMA_CUDA_NVCC and LLAMA_CUDA_CCBIN (#2275)
    
    Under certain environment, nvcc and gcc is installed under customized path but not standard path
    
    Co-authored-by: Yan Lin <yanlin@baidu.com>

commit 78a3d13424b01c3f8ea94ea7e59650ab0501e902
Author: wzy <32936898+Freed-Wu@users.noreply.github.com>
Date:   Fri Jul 21 18:26:34 2023 +0800

    flake : remove intel mkl from flake.nix due to missing files (#2277)
    
    NixOS's mkl misses some libraries like mkl-sdl.pc. See #2261
    Currently NixOS doesn't have intel C compiler (icx, icpx). See https://discourse.nixos.org/t/packaging-intel-math-kernel-libraries-mkl/975
    So remove it from flake.nix
    
    Some minor changes:
    
    - Change pkgs.python310 to pkgs.python3 to keep latest
    - Add pkgconfig to devShells.default
    - Remove installPhase because we have `cmake --install` from #2256

commit ae178ab46bfd6ecb2422da5dad441a4e2fef8b7e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 21 13:10:51 2023 +0300

    llama : make tensor_split ptr instead of array (#2272)

commit 54e3bc76fed914f8d4a30a7a50c19867cccb1338
Author: Jiří Podivín <66251151+jpodivin@users.noreply.github.com>
Date:   Fri Jul 21 12:09:16 2023 +0200

    make : add new target for test binaries (#2244)
    
    Programs in the tests directory are now build with target tests
    and placed in the same location.
    
    * clean target was expanded to remove new binaries
    
    * test target binaries are listed in a variable
    
    * Locations of binaries were added to the .gitignore
    
    Signed-off-by: Jiri Podivin <jpodivin@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 019fe257bbf699f400231683a8b816ad90281275
Author: Hatsune Miku <129688334+at8u@users.noreply.github.com>
Date:   Fri Jul 21 08:13:18 2023 +0000

    MIKU MAYHEM: Upgrading the Default Model for Maximum Fun 🎉 (#2287)
    
    * Miku.sh: Set default model to llama-2-7b-chat
    
    * Miku.sh: Set ctx_size to 4096
    
    * Miku.sh: Add in-prefix/in-suffix opts
    
    * Miku.sh: Switch sampler to mirostat_v2 and tiny prompt improvements

commit e68c96f7fee8fc22814a4a1209ffc97bbf35f7bd
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Jul 21 10:44:40 2023 +0300

    Faster Q2_K on Metal (#2297)
    
    * Faster Q2_K on Metal
    
    * Deleting unnoticed and dangereous trailing white space
    
    * Fixed bug in new metal Q2_K implementation
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 9cf022a1889e50113fd348dc96b4557fc75a6296
Author: Przemysław Pawełczyk <przemoc@gmail.com>
Date:   Fri Jul 21 09:42:21 2023 +0200

    make : fix embdinput library and server examples building on MSYS2 (#2235)
    
    * make : fix embdinput library and server examples building on MSYS2
    
    * cmake : fix server example building on MSYS2

commit e782c9e735f93ab4767ffc37462c523b73a17ddc
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Jul 20 18:19:45 2023 +0300

    Faster Q5_K and Q6_K on Metal (#2294)
    
    * Faster Q6_K on Metal
    
    * Faster Q5_K on Metal
    
    * Another Q5_K speedup
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 785829dfe8baf0213f2ff66963d28c62f92d7930
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Jul 20 15:18:43 2023 +0300

    Faster Q4_K on Metal (#2290)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit fff0e0eafe817eef429ecb64f892ab7bdae31846
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jul 20 13:47:26 2023 +0300

    llama : fix regression from #2000 - could not load no-mmap models

commit 417a85a0010519224cf154eb85d383ffeafeeead
Author: Shouzheng Liu <lshzh.hi@gmail.com>
Date:   Thu Jul 20 06:32:22 2023 -0400

    metal: minor q4 optimization and reduce code size (#2248)
    
    * metal: use uint16_t instead of uint8_t.
    
    Apple GPU doesn't like uint8_t. For every operation on uint8_t
    the gpu need to copy the uint8_t to an empty 16 bit register, then
    it can issue other instructions.
    
    For the matrix-vector multiplication kernel only, we observed a
    340~350 GB/s memory read speed on M1 Max after this commit, which is
    very close to the reported hardware limit.
    
    * metal: update rms_norm kernel
    
    This commit double the speed of rms_norm operations by using 512 threads
    per threadgroup, combining with SIMD primitives to minimize the need for
    thread group barriers.
    
    * metal: use template to reduce size
    
    Revert modifications on block_q4_0 and block_q4_1.

commit 294f424554c1599784ac9962462fc39ace92d8a5
Author: Rinne <AsakusaRinne@gmail.com>
Date:   Wed Jul 19 15:06:40 2023 +0800

    llama : extend API to get max devices at runtime (#2253)

commit 45a1b07e9b20c33d71d8c849ff27d693a75a0269
Author: wzy <32936898+Freed-Wu@users.noreply.github.com>
Date:   Wed Jul 19 15:01:55 2023 +0800

    flake : update flake.nix (#2270)
    
    When `isx86_32 || isx86_64`, it will use mkl, else openblas
    
    According to
    https://discourse.nixos.org/t/rpath-of-binary-contains-a-forbidden-reference-to-build/12200/3,
    add -DCMAKE_SKIP_BUILD_RPATH=ON
    
    Fix #2261, Nix doesn't provide mkl-sdl.pc.
    When we build with -DBUILD_SHARED_LIBS=ON, -DLLAMA_BLAS_VENDOR=Intel10_lp64
    replace mkl-sdl.pc by mkl-dynamic-lp64-iomp.pc

commit b1f429095328a34556c0e9a7a2fefced3db3368c
Author: wzy <32936898+Freed-Wu@users.noreply.github.com>
Date:   Wed Jul 19 15:01:11 2023 +0800

    cmake : install targets (#2256)
    
    fix #2252

commit d01bccde9f759b24449fdaa16306b406a50eb367
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jul 18 14:24:43 2023 +0300

    ci : integrate with ggml-org/ci (#2250)
    
    * ci : run ctest
    
    ggml-ci
    
    * ci : add open llama 3B-v2 tests
    
    ggml-ci
    
    * ci : disable wget progress output
    
    ggml-ci
    
    * ci : add open llama 3B-v2 tg tests for q4 and q5 quantizations
    
    ggml-ci
    
    * tests : try to fix tail free sampling test
    
    ggml-ci
    
    * ci : add K-quants
    
    ggml-ci
    
    * ci : add short perplexity tests
    
    ggml-ci
    
    * ci : add README.md
    
    * ppl : add --chunks argument to limit max number of chunks
    
    ggml-ci
    
    * ci : update README

commit 6cbf9dfb32f0e23ed3afd02d30ab066ed53e2c4d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jul 18 11:50:49 2023 +0300

    llama : shorten quantization descriptions

commit 7568d1a2b206331412106ea66da3f871025e0c3c
Author: Jiahao Li <liplus17@163.com>
Date:   Tue Jul 18 01:39:29 2023 +0800

    Support dup & cont ops on CUDA (#2242)

commit b7647436ccc80970b44a270f70f4f2ea139054d1
Author: Alex Klinkhamer <git@grencez.dev>
Date:   Sun Jul 16 14:01:45 2023 -0700

    llama : fix t_start_sample_us initialization warning (#2238)

commit 672dda10e4d8ac79df5d5970da7fb69d242ca9a7
Author: Qingyou Meng <meng.qingyou@gmail.com>
Date:   Mon Jul 17 03:57:28 2023 +0800

    ggml : fixed runtime bugs and compile errors related to GGML_PERF and GGML_DEBUG (#2219)
    
    * fixed runtime bugs and compile errors related to GGML_PERF and GGML_DEBUG
    
    * remove ifdef GGML_PERF; update fmt

commit 27ab66e437797aedbb23b3599385756b6c26ac39
Author: Jiří Podivín <66251151+jpodivin@users.noreply.github.com>
Date:   Sun Jul 16 21:54:47 2023 +0200

    py : turn verify-checksum-models.py into executable (#2245)
    
    README.md was adjusted to reflect the change.
    
    Signed-off-by: Jiri Podivin <jpodivin@gmail.com>

commit 6e7cca404748dd4b1a3affd0d1296e37f4ac0a6f
Author: Xiao-Yong Jin <jinxiaoyong@gmail.com>
Date:   Sat Jul 15 06:34:16 2023 -0400

    llama : add custom RoPE (#2054)
    
    * Implement customizable RoPE
    
    The original RoPE has pre-defined parameters
    
    theta_i = 10000^(−2(i−1)/d), for i in [1, 2, ..., d/2]
    
    Our customizable RoPE, ggml_rope_custom_inplace, uses
    
    theta_i = scale * base^(−2(i−1)/d), for i in [1, 2, ..., d/2]
    
    with the default matches the original
    
    scale = 1.0
    base = 10000
    
    The new command line arguments
    --rope-freq-base
    --rope-freq-scale
    set the two new RoPE parameter.
    
    Recent researches show changing these two parameters extends the context limit with minimal loss.
    
    1. Extending Context to 8K
       kaiokendev
       https://kaiokendev.github.io/til#extending-context-to-8k
    
    2. Extending Context Window of Large Language Models via Positional Interpolation
       Shouyuan Chen, Sherman Wong, Liangjian Chen, Yuandong Tian
       https://arxiv.org/abs/2306.15595
    
    3. NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation.
       https://www.reddit.com/user/bloc97
       https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/
    
    For the bold, try adding the following command line parameters to your favorite model:
    -c 16384 --rope-freq-base 80000 --rope-freq-scale 0.5
    
    * ggml-metal: fix custom rope
    
    * common: fix argument names in help
    
    * llama: increase MEM_REQ_EVAL for MODEL_3B
    
    It avoids crashing for quantized weights on CPU.
    Better ways to calculate the required buffer size would be better.
    
    * llama: make MEM_REQ_EVAL depend on n_ctx
    
    * server: use proper Content-Type in curl examples
    
    Without the header Content-Type: application/json, curl will POST with
    Content-Type: application/x-www-form-urlencoded
    
    Though our simple server doesn't care, the httplib.h used has a limit
    with CPPHTTPLIB_FORM_URL_ENCODED_PAYLOAD_MAX_LENGTH 8192
    
    With Content-Type: application/json, we can send large json data.
    
    * style : minor fixes, mostly indentations
    
    * ggml : fix asserts
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit a6803cab946c817fb7aaf2a40b317f5d3e373bd1
Author: Dave Della Costa <ddellacosta+github@gmail.com>
Date:   Fri Jul 14 15:13:38 2023 -0400

    flake : add runHook preInstall/postInstall to installPhase so hooks function (#2224)

commit 7dabc66f3c63f8ea0f61bac346fa138e01df675f
Author: wzy <32936898+Freed-Wu@users.noreply.github.com>
Date:   Sat Jul 15 03:05:08 2023 +0800

    make : use pkg-config for OpenBLAS (#2222)

commit 7cdd30bf1f84339c55a5e3de29384f6bbdebb61c
Author: Bach Le <bach@bullno1.com>
Date:   Sat Jul 15 03:00:58 2023 +0800

    cuda : allocate all temporary ggml_tensor_extra_gpu from a fixed-size buffer (#2220)

commit e8035f141e1f71d739fa5cfc9c01531cdee6fc16
Author: Evan Miller <emmiller@gmail.com>
Date:   Fri Jul 14 14:55:56 2023 -0400

    ggml : fix static_assert with older compilers #2024 (#2218)

commit 7513b7b0a1c11faa00ad5a34d22681e5f07d32e4
Author: Bach Le <bach@bullno1.com>
Date:   Sat Jul 15 02:55:24 2023 +0800

    llama : add functions that work directly on model (#2197)
    
    * Remove vocab reference from context
    
    * Add functions that works directly with model

commit de8342423d9600cf6e15455c1a27bae441262b45
Author: Ali Chraghi <63465728+alichraghi@users.noreply.github.com>
Date:   Fri Jul 14 11:50:58 2023 -0700

    build.zig : install config header (#2216)

commit c48c525f8711780f3f7c59bf92f1760f38317218
Author: Shangning Xu <32517059+xushangning@users.noreply.github.com>
Date:   Sat Jul 15 02:40:05 2023 +0800

    examples : fixed path typos in embd-input (#2214)

commit 206e01de117cc65ed8713ac9fdebc57ba4532ec3
Author: Jiahao Li <liplus17@163.com>
Date:   Sat Jul 15 02:38:24 2023 +0800

    cuda : support broadcast add & mul (#2192)
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 4304bd3cded73c867a882ea5ca4517e3995cc996
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Jul 14 19:44:08 2023 +0200

    CUDA: mul_mat_vec_q kernels for k-quants (#2203)

commit 229aab351c375899debad45fcb213bf0565bba4e
Author: James Reynolds <magnusviri@users.noreply.github.com>
Date:   Fri Jul 14 11:34:40 2023 -0600

    make : fix combination of LLAMA_METAL and LLAMA_MPI (#2208)
    
    Fixes https://github.com/ggerganov/llama.cpp/issues/2166 by moving commands after the CFLAGS are changed.

commit 697966680b27d9b4f05668605b863cb9aea3e15f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 14 16:36:41 2023 +0300

    ggml : sync (ggml_conv_2d, fix mul_mat bug, CUDA GLM rope)

commit 27ad57a69b85bf12420a27e9945e580cc280be57
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Jul 14 12:46:21 2023 +0300

    Metal: faster Q4_0 and Q4_1 matrix x vector kernels (#2212)
    
    * 3-5% faster Q4_0 on Metal
    
    * 7-25% faster Q4_1 on Metal
    
    * Oops, forgot to delete the original Q4_1 kernel
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 32c54116318929c90fd7ae814cf9b5232cd44c36
Author: Howard Su <howard0su@gmail.com>
Date:   Thu Jul 13 21:58:25 2023 +0800

    Revert "Support using mmap when applying LoRA (#2095)" (#2206)
    
    Has perf regression when mlock is used.
    
    This reverts commit 2347463201a9f4159ae95b737e1544dd300569c8.

commit ff5d58faecf1f02b05bd015bdfc6a394cf2bc9ba
Author: Howard Su <howard0su@gmail.com>
Date:   Thu Jul 13 21:58:09 2023 +0800

    Fix compile error on Windows CUDA (#2207)

commit b782422a3e090d0aeab84bfa03ba008dcd1c2a3d
Author: Bodo Graumann <mail@bodograumann.de>
Date:   Thu Jul 13 15:49:14 2023 +0200

    devops : add missing quotes to bash script (#2193)
    
    This prevents accidentally expanding arguments that contain spaces.

commit 1cbf561466e957b25f0e8163c2386683f8674369
Author: Shouzheng Liu <61452103+lshzh-ww@users.noreply.github.com>
Date:   Wed Jul 12 16:10:55 2023 -0400

    metal : new q4_0 matrix-vector kernel (#2188)
    
    Prefetch data to improve GPU utilization. ~48% faster for 33B model.

commit 975221e9548ef6d9f4af8d39cdffc4811c050beb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jul 12 20:51:29 2023 +0300

    ggml : broadcast mul_mat + conv batch support (#2199)
    
    * ggml : broadcast mul_mat + conv batch support
    
    * ggml : apply mul_mat broadcast fix by @jploski

commit 4523d10d0cf8c088f1b26c76d38d73290eb3b444
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jul 12 20:27:03 2023 +0300

    ggml : add ggml_pool_1d and ggml_pool_2d

commit 680e6f91775f972f0df34f56807f30826370db59
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jul 12 20:26:18 2023 +0300

    cuda : add gelu support

commit 4e7464ef88885cb3532738b03cac890f4077fa20
Author: Howard Su <howard0su@gmail.com>
Date:   Wed Jul 12 20:18:40 2023 +0800

    FP16 is supported in CM=6.0 (#2177)
    
    * FP16 is supported in CM=6.0
    
    * Building PTX code for both of 60 and 61
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

commit 2b5eb72e109577ed84e44bb8fa47e4956f337300
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jul 12 10:38:52 2023 +0200

    Fixed __dp4a compute capability: 6.0 -> 6.1 (#2189)

commit f7d278faf308cb989c221895968f2a26f14b2155
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jul 12 10:54:19 2023 +0300

    ggml : revert CUDA broadcast changes from #2183 (#2191)

commit 20d7740a9b45f6e5b247fa3738fdda35e18c2e8a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jul 11 22:53:34 2023 +0300

    ggml : sync (abort callback, mul / add broadcast, fix alibi) (#2183)

commit 5bf2a2771886ee86137e01dbc7492f78fb392066
Author: Spencer Sutton <spencersutton@users.noreply.github.com>
Date:   Tue Jul 11 12:31:10 2023 -0400

    ggml : remove src0 and src1 from ggml_tensor and rename opt to src (#2178)
    
    * Add ggml changes
    
    * Update train-text-from-scratch for change
    
    * mpi : adapt to new ggml_tensor->src
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit c9c74b4e3f9dcfab8b0032749ff8a579ab4e4d8d
Author: Bach Le <bach@bullno1.com>
Date:   Wed Jul 12 00:18:43 2023 +0800

    llama : add classifier-free guidance (#2135)
    
    * Initial implementation
    
    * Remove debug print
    
    * Restore signature of llama_init_from_gpt_params
    
    * Free guidance context
    
    * Make freeing of guidance_ctx conditional
    
    * Make Classifier-Free Guidance a sampling function
    
    * Correct typo. CFG already means context-free grammar.
    
    * Record sampling time in llama_sample_classifier_free_guidance
    
    * Shift all values by the max value before applying logsoftmax
    
    * Fix styling based on review

commit 3ec7e596b2ba3f43c22f441254ca2bcfa91102ba
Author: Jinwoo Jeong <33892306+williamjeong2@users.noreply.github.com>
Date:   Wed Jul 12 01:12:35 2023 +0900

    docker : add '--server' option (#2174)

commit 917831c63a4138814d23da1917bf2b5d5b9faa6c
Author: Chad Brewbaker <crb002@gmail.com>
Date:   Tue Jul 11 11:03:06 2023 -0500

    readme : fix zig build instructions (#2171)

commit 2347463201a9f4159ae95b737e1544dd300569c8
Author: Howard Su <howard0su@gmail.com>
Date:   Tue Jul 11 22:37:01 2023 +0800

    Support using mmap when applying LoRA (#2095)
    
    * Support using mmap when applying LoRA
    
    * Fix Linux
    
    * Update comment to reflect the support lora with mmap

commit bbef28218fe827265716b66977719b9ee2b21165
Author: LostRuins <39025047+LostRuins@users.noreply.github.com>
Date:   Tue Jul 11 22:01:08 2023 +0800

    Possible solution to allow K-quants on models with n_vocab!=32000 (#2148)
    
    * This allows LLAMA models that were previously incompatible with K quants to function mostly as normal. This happens when a model has a vocab != 32000, e.g 32001 which means it's not divisible by 256 or 64. Since the problematic dimensions only apply for `tok_embeddings.weight` and `output.weight` (dimentions 4096 x n_vocab), we can simply quantize these layers to Q8_0 whereas the majority of the hidden layers are still K-quanted since they have compatible dimensions.
    
    * Fix indentation
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * As an alternative, to avoid failing on Metal due to lack of Q8_0 support, instead quantize tok_embeddings.weight to Q4_0 and retain output.weight as F16. This results in a net gain of about 55mb for a 7B model compared to previous approach, but should minimize adverse impact to model quality.
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 5656d10599bd756dc0f17284e418e704200b43f3
Author: Evan Miller <emmiller@gmail.com>
Date:   Mon Jul 10 11:49:56 2023 -0400

    mpi : add support for distributed inference via MPI (#2099)
    
    * MPI support, first cut
    
    * fix warnings, update README
    
    * fixes
    
    * wrap includes
    
    * PR comments
    
    * Update CMakeLists.txt
    
    * Add GH workflow, fix test
    
    * Add info to README
    
    * mpi : trying to move more MPI stuff into ggml-mpi (WIP) (#2099)
    
    * mpi : add names for layer inputs + prep ggml_mpi_graph_compute()
    
    * mpi : move all MPI logic into ggml-mpi
    
    Not tested yet
    
    * mpi : various fixes - communication now works but results are wrong
    
    * mpi : fix output tensor after MPI compute (still not working)
    
    * mpi : fix inference
    
    * mpi : minor
    
    * Add OpenMPI to GH action
    
    * [mpi] continue-on-error: true
    
    * mpi : fix after master merge
    
    * [mpi] Link MPI C++ libraries to fix OpenMPI
    
    * tests : fix new llama_backend API
    
    * [mpi] use MPI_INT32_T
    
    * mpi : factor out recv / send in functions and reuse
    
    * mpi : extend API to allow usage with outer backends (e.g. Metal)
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 1d1630996920f889cdc08de26cebf2415958540e
Author: oobabooga <112222186+oobabooga@users.noreply.github.com>
Date:   Sun Jul 9 05:59:53 2023 -0300

    llama : remove "first token must be BOS" restriction (#2153)

commit db4047ad5cd8eae04db3b2efe0245e69a376601a
Author: Nigel Bosch <pnigelb@gmail.com>
Date:   Sun Jul 9 03:56:18 2023 -0500

    main : escape prompt prefix/suffix (#2151)

commit 18780e0a5e17348236230bbe891901b9b5718709
Author: JackJollimore <130917767+JackJollimore@users.noreply.github.com>
Date:   Sun Jul 9 05:20:43 2023 -0300

    readme : update Termux instructions (#2147)
    
    The file pathing is significant when running models inside of Termux on Android devices. llama.cpp performance is improved with loading a .bin from the $HOME directory.

commit 3bbc1a11f04a9adc0d0e08c2940ba4d2978755ab
Author: clyang <clyang@clyang.net>
Date:   Sun Jul 9 16:12:20 2023 +0800

    ggml : fix buidling with Intel MKL but ask for "cblas.h" issue (#2104) (#2115)
    
    * Fix buidling with Intel MKL but ask for "cblas.h" issue
    
    * Use angle brackets to indicate the system library

commit 2492a53fd0d8372ecc67f49f07b581905175eea8
Author: rankaiyx <rankaiyx@rankaiyx.com>
Date:   Sun Jul 9 15:38:42 2023 +0800

    readme : add more docs indexes (#2127)
    
    * Update README.md to add more docs indexes
    
    * Update README.md to add more docs indexes

commit 64639555ff93c8ead2b80becb49cc6b60aeac240
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jul 8 20:01:44 2023 +0200

    Fixed OpenLLaMA 3b CUDA mul_mat_vec_q (#2144)

commit 061f5f8d2109bb7adcbd40f1b456d887c5a1df25
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jul 8 00:25:15 2023 +0200

    CUDA: add __restrict__ to mul mat vec kernels (#2140)

commit 84525e7962bee0abef91108948bbf7f7bfdcf421
Author: dylan <canardleteer@users.noreply.github.com>
Date:   Fri Jul 7 11:25:25 2023 -0700

    docker : add support for CUDA in docker (#1461)
    
    Co-authored-by: canardleteer <eris.has.a.dad+github@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit a7e20edf2266169ccd97a4eb949a593d628fbd64
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 7 21:23:57 2023 +0300

    ci : switch threads to 1 (#2138)

commit 1d656d6360359cfdaaf5d64ed9690047b600dbcb
Author: Qingyou Meng <meng.qingyou@gmail.com>
Date:   Sat Jul 8 00:24:01 2023 +0800

    ggml : change ggml_graph_compute() API to not require context (#1999)
    
    * ggml_graph_compute: deprecate using ggml_context, try resolve issue #287
    
    * rewrite: no longer consider backward compitability; plan and make_plan
    
    * minor: rename ctx as plan; const
    
    * remove ggml_graph_compute from tests/test-grad0.c, but current change breaks backward
    
    * add static ggml_graph_compute_sugar()
    
    * minor: update comments
    
    * reusable buffers
    
    * ggml : more consistent naming + metal fixes
    
    * ggml : fix docs
    
    * tests : disable grad / opt + minor naming changes
    
    * ggml : add ggml_graph_compute_with_ctx()
    
    - backwards compatible API
    - deduplicates a lot of copy-paste
    
    * ci : enable test-grad0
    
    * examples : factor out plan allocation into a helper function
    
    * llama : factor out plan stuff into a helper function
    
    * ci : fix env
    
    * llama : fix duplicate symbols + refactor example benchmark
    
    * ggml : remove obsolete assert + refactor n_tasks section
    
    * ggml : fix indentation in switch
    
    * llama : avoid unnecessary bool
    
    * ggml : remove comments from source file and match order in header
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 72421402834141df6cbdcf595fe46dbd11874dce
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 7 18:36:37 2023 +0300

    ggml : remove sched_yield() call in ggml_graph_compute_thread() (#2134)

commit 3e08ae99ceb143d67f9273fda47541e9d98ff23f
Author: Aarni Koskela <akx@iki.fi>
Date:   Fri Jul 7 16:12:49 2023 +0300

    convert.py: add mapping for safetensors bf16 (#1598)
    
    Fixes #1473

commit 481f793acc3882a09d45d8d2c3076ad3d1c60cfc
Author: Howard Su <howard0su@gmail.com>
Date:   Fri Jul 7 11:34:18 2023 +0800

    Fix opencl by wrap #if-else-endif with \n (#2086)

commit dfd9fce6d65599bf33df43e616e85aa639bdae4c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jul 6 19:41:31 2023 +0300

    ggml : fix restrict usage

commit 36680f6e40e4440c3ec3385d0b7e5ca8bb6c37f7
Author: Judd <foldl@users.noreply.github.com>
Date:   Fri Jul 7 00:23:49 2023 +0800

    convert : update for baichuan (#2081)
    
    1. guess n_layers;
    2. relax warnings on context size;
    3. add a note that its derivations are also supported.
    
    Co-authored-by: Judd <foldl@boxvest.com>

commit a17a2683d8fdb899ba497d0c28ccafb28c62efb6
Author: tslmy <tslmy@users.noreply.github.com>
Date:   Thu Jul 6 09:17:50 2023 -0700

    alpaca.sh : update model file name (#2074)
    
    The original file name, `ggml-alpaca-7b-q4.bin`, implied the first-generation GGML. After the breaking changes (mentioned in https://github.com/ggerganov/llama.cpp/issues/382), `llama.cpp` requires GGML V3 now. Those model files are named `*ggmlv3*.bin`. We should change the example to an actually working model file, so that this thing is more likely to run out-of-the-box for more people, and less people would waste time downloading the old Alpaca model.

commit 31cfbb1013a482e89c72146e2063ac4362becae7
Author: Tobias Lütke <tobi@shopify.com>
Date:   Wed Jul 5 16:51:13 2023 -0400

    Expose generation timings from server & update completions.js (#2116)
    
    * use javascript generators as much cleaner API
    
    Also add ways to access completion as promise and EventSource
    
    * export llama_timings as struct and expose them in server
    
    * update readme, update baked includes
    
    * llama : uniform variable names + struct init
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 983b555e9ddb36703cee4d22642afe958de093b7
Author: Jesse Jojo Johnson <williamsaintgeorge@gmail.com>
Date:   Wed Jul 5 18:03:19 2023 +0000

    Update Server Instructions (#2113)
    
    * Update server instructions for web front end
    * Update server README
    * Remove duplicate OAI instructions
    * Fix duplicate text
    
    ---------
    
    Co-authored-by: Jesse Johnson <thatguy@jessejojojohnson.com>

commit ec326d350c72afd23709a409944728a607188cc0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jul 5 20:44:11 2023 +0300

    ggml : fix bug introduced in #1237

commit 1b6efeab829f3eeda5b39bd47624bb60b3531b88
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jul 5 20:20:05 2023 +0300

    tests : fix test-grad0

commit 1b107b8550dced48dc5f41184640061354226b96
Author: Stephan Walter <stephan@walter.name>
Date:   Wed Jul 5 16:13:06 2023 +0000

    ggml : generalize `quantize_fns` for simpler FP16 handling (#1237)
    
    * Generalize quantize_fns for simpler FP16 handling
    
    * Remove call to ggml_cuda_mul_mat_get_wsize
    
    * ci : disable FMA for mac os actions
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 8567c76b5326e862be0755a8dc1dd988223fcae3
Author: Jesse Jojo Johnson <williamsaintgeorge@gmail.com>
Date:   Wed Jul 5 15:13:35 2023 +0000

    Update server instructions for web front end (#2103)
    
    Co-authored-by: Jesse Johnson <thatguy@jessejojojohnson.com>

commit 924dd22fd3ba93e097f8d19ba5cda919ca2fe2fb
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jul 5 14:19:42 2023 +0200

    Quantized dot products for CUDA mul mat vec (#2067)

commit 051c70dcd55709c9cbbfa849af035951fe720433
Author: Howard Su <howard0su@gmail.com>
Date:   Wed Jul 5 18:31:23 2023 +0800

    llama: Don't double count the sampling time (#2107)

commit 9e4475f5cf639315f61ed7b8da6258bb0c7c5ca9
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jul 5 08:58:05 2023 +0200

    Fixed OpenCL offloading prints (#2082)

commit 7f0e9a775ecc4c6ade271c217f63d6dc93e79eaa
Author: Nigel Bosch <pnigelb@gmail.com>
Date:   Tue Jul 4 18:33:33 2023 -0500

    embd-input: Fix input embedding example unsigned int seed (#2105)

commit b472f3fca558b6335adbd87210ed58cfb5da37cb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jul 4 22:25:22 2023 +0300

    readme : add link web chat PR

commit ed9a54e5129a11c2a5b555e1dc65e875e3c37b4f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jul 4 21:54:11 2023 +0300

    ggml : sync latest (new ops, macros, refactoring) (#2106)
    
    - add ggml_argmax()
    - add ggml_tanh()
    - add ggml_elu()
    - refactor ggml_conv_1d() and variants
    - refactor ggml_conv_2d() and variants
    - add helper macros to reduce code duplication in ggml.c

commit f257fd255044decffad93dee2502875ce66ad80c
Author: jwj7140 <32943891+jwj7140@users.noreply.github.com>
Date:   Wed Jul 5 03:06:12 2023 +0900

    Add an API example using server.cpp similar to OAI. (#2009)
    
    * add api_like_OAI.py
    * add evaluated token count to server
    * add /v1/ endpoints binding

commit 7ee76e45afae7f9a7a53e93393accfb5b36684e1
Author: Tobias Lütke <tobi@shopify.com>
Date:   Tue Jul 4 10:05:27 2023 -0400

    Simple webchat for server (#1998)
    
    * expose simple web interface on root domain
    
    * embed index and add --path for choosing static dir
    
    * allow server to multithread
    
    because web browsers send a lot of garbage requests we want the server
    to multithread when serving 404s for favicon's etc. To avoid blowing up
    llama we just take a mutex when it's invoked.
    
    
    * let's try this with the xxd tool instead and see if msvc is happier with that
    
    * enable server in Makefiles
    
    * add /completion.js file to make it easy to use the server from js
    
    * slightly nicer css
    
    * rework state management into session, expose historyTemplate to settings
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit acc111caf93fc6681450924df9f99679c384c59e
Author: Henri Vasserman <henv@hot.ee>
Date:   Tue Jul 4 15:38:04 2023 +0300

    Allow old Make to build server. (#2098)
    
    Also make server build by default.
    
    Tested with Make 3.82

commit 23c7c6fc9182b041f006b86ea1e7f99911ecf344
Author: ZhouYuChen <zhouyuchen@naver.com>
Date:   Tue Jul 4 20:15:16 2023 +0800

    Update Makefile: clean simple (#2097)

commit 698efad5fbbf326f01288649b123eff5f79b417e
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Tue Jul 4 01:50:12 2023 +0200

    CI: make the brew update temporarily optional. (#2092)
    
    until they decide to fix the brew installation in the macos runners.
    see the open issues. eg https://github.com/actions/runner-images/pull/7710

commit 14a2cc71f62e45803ae70890ffbdeb0a172e6210
Author: Govlzkoy <gotope@users.noreply.github.com>
Date:   Tue Jul 4 07:50:00 2023 +0800

    [ggml] fix index for ne03 value in ggml_cl_mul_f32 (#2088)

commit 1cf14ccef12e19c5a5b0b17ab456242d1f8c7fdd
Author: Henri Vasserman <henv@hot.ee>
Date:   Tue Jul 4 00:05:23 2023 +0300

    fix server crashes (#2076)

commit cc45a7feb8412e84ff292207621412fffc0d3d51
Author: Howard Su <howard0su@gmail.com>
Date:   Tue Jul 4 02:43:55 2023 +0800

    Fix crash of test-tokenizer-0 under Debug build (#2064)
    
    * Fix crash of test-tokenizer-0 under Debug build
    
    * Change per comment

commit 55dbb915cc2a95048f56e667b09dfad38d840421
Author: Howard Su <howard0su@gmail.com>
Date:   Mon Jul 3 19:58:58 2023 +0800

    [llama] No need to check file version when loading vocab score (#2079)

commit d7d2e6a0f0c74f7a570dae384dfff371ac744d2a
Author: WangHaoranRobin <56047610+WangHaoranRobin@users.noreply.github.com>
Date:   Mon Jul 3 05:38:44 2023 +0800

    server: add option to output probabilities for completion (#1962)
    
    * server: add option to output probabilities for completion
    * server: fix issue when handling probability output for incomplete tokens for multibyte character generation
    * server: fix llama_sample_top_k order
    * examples/common.h: put all bool variables in gpt_params together

commit 46088f72318981341a2d646f12f6eee6aec06d65
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jul 2 09:46:46 2023 +0300

    ggml : fix build with OpenBLAS (close #2066)

commit 0bc2cdfc875fa7877d8e01c8bb17066f99c08f21
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jul 1 21:49:44 2023 +0200

    Better CUDA synchronization logic (#2057)

commit befb3a35627432473f143c90994557d78ff5bc67
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jul 1 21:47:26 2023 +0200

    Test-based VRAM scratch size + context adjustment (#2056)

commit b2132270678c473f7cd9ba871b03d694126bc33a
Author: Daniel Drake <drake@endlessos.org>
Date:   Sat Jul 1 20:31:44 2023 +0200

    cmake : don't force -mcpu=native on aarch64 (#2063)
    
    It's currently not possible to cross-compile llama.cpp for aarch64
    because CMakeLists.txt forces -mcpu=native for that target.
    
    -mcpu=native doesn't make sense if your build host is not the
    target architecture, and clang rejects it for that reason, aborting the
    build. This can be easily reproduced using the current Android NDK to build
    for aarch64 on an x86_64 host.
    
    If there is not a specific CPU-tuning target for aarch64 then -mcpu
    should be omitted completely. I think that makes sense, there is not
    enough variance in the aarch64 instruction set to warrant a fixed -mcpu
    optimization at this point. And if someone is building natively and wishes
    to enable any possible optimizations for the host device, then there is
    already the LLAMA_NATIVE option available.
    
    Fixes #495.

commit 2f8cd979ecd1fa582852e7136e92ff8990b98fd8
Author: Aaron Miller <apage43@ninjawhale.com>
Date:   Sat Jul 1 11:14:59 2023 -0700

    metal : release buffers when freeing metal context (#2062)

commit 471aab6e4cb89d8ef6d043f1bc93acb6eb78ab67
Author: Judd <foldl@users.noreply.github.com>
Date:   Sun Jul 2 01:00:25 2023 +0800

    convert : add support of baichuan-7b (#2055)
    
    Co-authored-by: Judd <foldl@boxvest.com>

commit 463f2f4c4f8dd5ca9594b7d65849f346f0effe05
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jul 1 19:05:09 2023 +0300

    llama : fix return value of llama_load_session_file_internal (#2022)

commit cb44dbc7de287b3d17772cfb1aa49d55e082ce5b
Author: Rand Xie <randxiexyy29@gmail.com>
Date:   Sun Jul 2 00:02:58 2023 +0800

    llama : catch llama_load_session_file_internal exceptions (#2022)
    
    * convert checks in llama_load_session_file to throw and handle them
    
    * make llama_load_session_file_internal static
    
    * address feedbacks to avoid using exceptions

commit 79f634a19d1c32a6cfb1befc21551ee684fced6b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jul 1 18:46:00 2023 +0300

    embd-input : fix returning ptr to temporary

commit 04606a159947566b27810508433e6ca5dbc684ba
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jul 1 18:45:44 2023 +0300

    train : fix compile warning

commit b1ca8f36a9cdbcee5f5c425df717611a1040a897
Author: Qingyou Meng <meng.qingyou@gmail.com>
Date:   Sat Jul 1 23:42:43 2023 +0800

    ggml : disable GGML_TASK_INIT and GGML_TASK_FINALIZE by default (#1995)
    
    Will not be scheduled unless explicitly enabled.

commit b8c8dda75fdf5fdea49c80af36818e7c30fe0ddf
Author: Howard Su <howard0su@gmail.com>
Date:   Thu Jun 29 21:15:15 2023 +0800

    Use unsigned for random seed (#2006)
    
    * Use unsigned for random seed. Keep -1 as the value to use a time based seed.
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 96a712ca1b7f427e3bd7ffc0c70b2105cfc7fbf1
Author: LostRuins <39025047+LostRuins@users.noreply.github.com>
Date:   Thu Jun 29 11:56:43 2023 +0800

    Porting the improved K-Quant CUDA kernels to OpenCL (#1966)
    
    * Added broken new q4k quant
    
    * xx + ib0
    
    * Fix q2_k fast kernel
    
    * Use preprocessor for QK_K
    
    * Add q6_k fast matmul kernel
    
    * ported q3k speedup successfully
    
    * ported q2k and q5k speedups
    
    * remove old dot kernels and template
    
    * fixed global const struct types
    
    * fixing address spaces
    
    * fixed string too long CI issue
    
    ---------
    
    Co-authored-by: 0cc4m <picard12@live.de>

commit d3494bb86bf7ad5b0b60aae0220ea576f273b5c0
Author: m3ndax <adrian.goessl@outlook.com>
Date:   Wed Jun 28 20:39:08 2023 +0200

    llama : replacing auto &kv with const auto &kv (#2041)
    
    * Replacing auto &kv with const auto &kv
    
    * Create codacy.yml
    
    * Delete codacy.yml

commit 5b351e94d041742cd50ffcf2d44718d63bab398a
Author: Salvador E. Tropea <stropea@inti.gob.ar>
Date:   Wed Jun 28 14:27:31 2023 -0300

    cuda : remove nchannels_x argument from mul_mat_vec_nc_f16_f32 (#2028)
    
    - Not used

commit 6432aabb6dc887436e4d57414b63116189c3b13b
Author: Salvador E. Tropea <stropea@inti.gob.ar>
Date:   Wed Jun 28 14:26:26 2023 -0300

    cuda : fix missing const qualifier in casts (#2027)

commit b922bc351b69770cec2d35d2aa50fa052b95ca93
Author: Howard Su <howard0su@gmail.com>
Date:   Wed Jun 28 10:13:02 2023 -0700

    llama : remove shards weight file support (#2000)
    
    * Remove multiple shards
    
    * Remove multiple file loaders
    
    * Remove llama_load_tensor_shard class
    
    * Simplify load logic
    
    * Remove dead code guess_n_parts function
    
    * Remove vocab_only from constructor of llama_model_loader
    
    * Remove alignment_prevents_mmap which is not more needed.
    
    * Remove useless check

commit 7f9753fa1263c4eded9a3de19778562f0e1093d7
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jun 28 18:35:54 2023 +0200

    CUDA GPU acceleration for LoRAs + f16 models (#1970)

commit cfa0750bc9dbc2d957a91b8ed09ab0035d8f3d4e
Author: ningshanwutuobang <ningshanwutuobang@gmail.com>
Date:   Wed Jun 28 23:53:37 2023 +0800

    llama : support input embeddings directly  (#1910)
    
    * add interface for float input
    
    * fixed inpL shape and type
    
    * add examples of input floats
    
    * add test example for embd input
    
    * fixed sampling
    
    * add free for context
    
    * fixed add end condition for generating
    
    * add examples for llava.py
    
    * add READMD for llava.py
    
    * add READMD for llava.py
    
    * add example of PandaGPT
    
    * refactor the interface and fixed the styles
    
    * add cmake build for embd-input
    
    * add cmake build for embd-input
    
    * Add MiniGPT-4 example
    
    * change the order of the args of llama_eval_internal
    
    * fix ci error

commit 9d23589d638dc74577d5ff880e6d4248b795f12e
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Tue Jun 27 19:06:33 2023 +0200

    fix pthreads setaffinity usage on android (#2020)

commit 0be54f75a6c3e9a09ea71bdfcdabf9a996a0549b
Author: Howard Su <howard0su@gmail.com>
Date:   Tue Jun 27 13:07:13 2023 +0800

    baby-llama : fix build after ggml_rope change (#2016)

commit 181e8d975528a4e27eabb8ae6e9865f9ceae4b37
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 27 00:37:13 2023 +0300

    llama : fix rope usage after ChatGLM change

commit d9779021bd59ed96daae75e820a5ac5da47ca8ff
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 27 00:06:51 2023 +0300

    ggml : add support for ChatGLM RoPE

commit d38e45157862b58a1824387e64860d68ca3533a7
Author: Roman Parykin <donderom@gmail.com>
Date:   Mon Jun 26 22:47:59 2023 +0300

    readme : add Scala 3 bindings repo (#2010)

commit eaa6ca5a61b8c9501df9ebe3d264f45b75a5f8aa
Author: David Yang <davidyang6us@gmail.com>
Date:   Tue Jun 27 03:45:32 2023 +0800

    ggml : increase max tensor name + clean up compiler warnings in train-text (#1988)
    
    * Clean up compiler warnings in train-text
    
    Some brackets to disambiguate order of operations
    
    * Increase GGML_MAX_NAME
    
    Avoiding strncpy danger in train-text-from-scratch and reducing potential future name length issues

commit aa777abbb73655c4e1e9237b7c0ad66745e8e48c
Author: Gustavo Rocha Dias <91472747+gustrd@users.noreply.github.com>
Date:   Mon Jun 26 16:34:45 2023 -0300

    readme : LD_LIBRARY_PATH complement for some Android devices when building with CLBlast inside Termux (#2007)
    
    * docs - Alternative way to build at Android, with CLBlast.
    
    * doc - LD_LIBRARY_PATH complement for some Android devices when building with CLBlast inside Termux.
    
    * doc- fix typo

commit c824d2e368d193d9f564ff29880a51cda9f90527
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 26 21:03:59 2023 +0300

    ggml : avoid conv 2d kernel round up

commit b853d456018b10820686362af41b2f2f75f1eec6
Author: zrm <trustiosity.zrm@gmail.com>
Date:   Mon Jun 26 13:57:59 2023 -0400

    ggml : add NUMA support (#1556)
    
    * detect NUMA systems and pin work threads to nodes (linux)
    
    * disable mmap prefetch/readahead for NUMA systems
    
    * avoid sending finalize op to thread pool if it does nothing
    
    * silence robot
    
    * fix args
    
    * make --numa a param
    
    * recommendation that n_nodes evenly divide n_threads did not warrant such aggressive enforcement
    
    * lower synchronization overhead
    
    * statically allocate
    
    * move numa state to g_state
    
    * add description for --numa
    
    * ggml : minor style changes
    
    * ggml : minor style + try fix sanitizer build
    
    * llama : allow to initialize backend with NUMA support
    
    * llama : avoid ggml include in llama-util.h
    
    * ggml : style / formatting
    
    * ggml : fix handling of ops with n_threads > n_tasks > 1
    
    * server : utilize numa parameter
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 9225baef71407d799a6f7f563b77fd7f82791416
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 26 20:10:52 2023 +0300

    k-quants : fix indentation

commit a84ab1da8dc6a59a5b67420ae1322f09503ffc72
Author: katsu560 <118887472+katsu560@users.noreply.github.com>
Date:   Tue Jun 27 01:47:02 2023 +0900

    tests : fix quantize perf (#1990)
    
    * fix test quantize perf
    
    * avoid the global state

commit 5743ca80928d8410754ec64a5673d5c2dd6cfbb7
Author: katsu560 <118887472+katsu560@users.noreply.github.com>
Date:   Tue Jun 27 01:46:07 2023 +0900

    k-quants : add AVX support to dot functions (#1916)
    
    * k_quants : add AVX support
    
    * k_quants : apply review comments

commit 412c60e4739367144e51e59add5dc7749d084115
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 26 19:45:09 2023 +0300

    readme : add link to new k-quants for visibility

commit 6769e944c727c63612dcafbef52009d21ae00fff
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jun 26 19:43:07 2023 +0300

    k-quants : support for super-block size of 64 (#2001)
    
    * k_quants: WIP super-blocks with 64 weights
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q6_K scalar and AVX2 works
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q4_K scalar and AVX2 works
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q2_K scalar and AVX2 works. Q2_K is way too slow (it is actually slower
    than the scalar implementation)
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q3_K scalar and AVX2 works.
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q5_K scalar and AVX2 works, and with that all
    k_quants are done on AVX2 and scalar
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q6_K working on CUDA. Cannot make it run quite as gast as
    with super-blocks with 256 weigths: 8% slower on 4080,
    20% slower on the 1660 (but there we fit 1 less layer on the
    GPU because pf the larger model size), so some fraction of
    these 20% is due to that,
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q4_K working on CUDA. ~10% slower on GTX-1660,
    16% slower on 4080.
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q2_K working on CUDA. ~3% slower on GTX-1660,
    10% slower on 4080.
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q3_K working on CUDA.
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q5_K working on CUDA, and with this CUDA is done.
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q6_K working on ARM_NEON
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q4_K working on ARM_NEON, but quite a bit slower than 256 weights
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q2_K working on ARM_NEON, but quite a bit slower than 256 weights
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q3_K working on ARM_NEON, but quite a bit slower than 256 weights.
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q5_K working on ARM_NEON, but quite a bit slower than 256 weights.
    
    With that, we have full support for ARM_NEON, although
    performance is not quite there.
    
    * k_quants: WIP super-blocks with 64 weights
    
    Slightly more efficient Q3_K and Q5_K
    
    * k_quants: WIP super-blocks with 64 weights
    
    Another small improvement for Q3_K and Q5_K on ARM_NEON
    
    * k_quants: WIP super-blocks with 64 weights
    
    Yet another speedup for Q5_K on ARM_NEON.
    We are now within 10% of the QK_K = 256 version.
    
    * k_quants: WIP super-blocks with 64 weights
    
    * We are able to pass preprocessor macros to the Metal
      compiler
    * Q6_K works and is actually slightly more efficient than
      the QK_K = 256 version (25.2 ms vs 25.8 ms)
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q4_K works on Metal and is actually slightly faster
    than QK_K = 256 (21.95 ms vs 24.0 ms).
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q2_K works on Metal and is very slightly faster
    than QK_K = 256 (23.8 ms vs 24.2 ms).
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q3_K works on Metal and is slightly faster
    than QK_K = 256 (26.6 ms vs 28.3 ms).
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q5_K works on Metal and is slightly faster
    than QK_K = 256 (23.7 ms vs 26.3 ms).
    
    * k_quants: call them _K, not _k, also on Metal
    
    * k_quants: correctly define QK_K in llama.cpp
    
    * Fixed bug in q4_K quantization added with the 64-block addition
    
    * Simplify via lambda
    
    * k_quants: swicth Q3_K to 4-bit scales when QK_K = 64
    
    Otherwise there isn't much benefit from this
    quantization type. There is some very slight loss
    in accuracy, but we reduce size by ~7%.
    E.g., for OpenLLaMA-3B, Q3_K_S perplexity is
    8.6131 with 8-bit scales and 8.6352 with 4-bit,
    while file size decreases from 1.53G to 1.44G.
    
    * k_quants: switch Q4_K to 4-bit scales when QK_K = 64
    
     Here the loss in accuracy is greater than for Q3_K,
     but the Q4_K points still move further to the left on
     the perplexity vs size curve.
    
    * k_quants: forgot to add the Metal changes in last commit
    
    * k_quants: change Q5_K to be type 0 when QK_K = 64
    
    Still needs AVX2 implementation
    
    * k_quants: AVX2 implementation for new 64-weight Q5_K
    
    * k_quants: 10% faster ARM_NEON Q5_K dot product
    
    * k_quants: fixed issue caused by merging with master
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit cbebf61ca7584e9709265395f0127ae7fc0f1882
Author: Howard Su <howard0su@gmail.com>
Date:   Mon Jun 26 23:15:47 2023 +0800

    Fix assert when free invalid cuda pointer (#2005)
    
    Fix assert via initializing extra structure always.
    CUDA error 1 at C:\GPT\llama.cpp\ggml-cuda.cu:2536: invalid argument

commit 447ccbe8c39332fcdd0d98a041b6e2ff6f06219d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 25 16:08:12 2023 +0300

    readme : add new roadmap + manifesto

commit bd34cdde38f8fd661890ddd5f57ca30bf279877b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 25 14:25:08 2023 +0300

    ggml : sync latest ggml (custom operators)

commit c2a08f87b8d180115d04b8688f383d1b2761b16d
Author: anon998 <131767832+anon998@users.noreply.github.com>
Date:   Sun Jun 25 08:48:36 2023 +0000

    fix server sampling: top k sampler first (#1977)
    
    Co-authored-by: anon <anon@example.org>

commit 66a2555ba6cab954c56d653b29c27bfbbacfbfb1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 25 09:07:03 2023 +0300

    readme : add Azure CI discussion link

commit e65ca7e14ac76c4046091da39d41a9017abaa9b3
Author: sjinzh <sjinzh@gmail.com>
Date:   Sun Jun 25 13:45:44 2023 +0800

    zig : upgrade build system support (#1981)
    
    * upgrade zig build system support
    
    * zig : add new line at the end of the file
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 5ec8dd5a3c6a9a109351d2257bb9d53869bd0a94
Author: Robyn <robyngraf@users.noreply.github.com>
Date:   Sun Jun 25 04:10:29 2023 +1000

    #1869 Fix null reference errors when training from scratch with CUDA (#1907)
    
    * #1869 Fix null reference errors when training from scratch with CUDA build
    
    Calling ggml_compute_forward when node->src0 was null was causing train-text-from-scratch.exe to terminate unexpectedly.
    
    * ggml : do not dereference src0 if NULL
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 65bdd52a867539691007f85c5508146d507f72c1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jun 24 19:40:18 2023 +0300

    tests : sync test-grad0 from ggml

commit fdd18609113862dc6eb34dfc44a093d54c59ff1f
Author: Rowan Hart <rowanbhart@gmail.com>
Date:   Sat Jun 24 04:07:08 2023 -0700

    flake : fix ggml-metal.metal path and run nixfmt (#1974)

commit c943d823c14cef33092205ca3944de6fdf7abf99
Author: AN Long <aisk@users.noreply.github.com>
Date:   Sat Jun 24 19:02:06 2023 +0800

    convert : fix invalid params in write_vocab_only (#1975)

commit f2c754e1c38936fdde74e4848ac468a696eb73c6
Author: slaren <slarengh@gmail.com>
Date:   Sat Jun 24 12:57:18 2023 +0200

    ggml : improve ggml_graph_dump_dot, add ggml_format_name (#1978)
    
    * Improve ggml_graph_dump_dot, add ggml_format_name
    
    * add more automatic names to view ops
    
    * fix name of copies

commit 11da1a85cd69af84b5861134738c7e9e20907470
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jun 24 13:38:18 2023 +0300

    readme : fix whitespaces

commit 235b610d650cbfed6dbd5d671f750d35fc18cd7d
Author: Alberto <57916483+albbus-stack@users.noreply.github.com>
Date:   Sat Jun 24 12:32:13 2023 +0200

    readme : fixed termux instructions (#1973)

commit b061ba9e2a7a2c335a200df8c11aed5e31e4ccbb
Author: Alex Renda <alexrenda@users.noreply.github.com>
Date:   Sat Jun 24 03:15:01 2023 -0700

    llama : fix top-p sampling to match the canonical definition (#1953)
    
    * Fix top-p sampling to match the standard definition (smallest set that has probability mass at least p, not largest set with probability mass less than p)
    
    * top-p: correct gt to gte
    
    * add test for correct top-p behavior

commit 527b6fba1d237befb324fd846bda7418c0fa394d
Author: Didzis Gosko <didzis@users.noreply.github.com>
Date:   Sat Jun 24 11:47:58 2023 +0300

    llama : make model stateless and context stateful (llama_state) (#1797)
    
    * llama : make model stateless and context stateful
    
    * llama : minor cleanup
    
    * llama : update internal API declaration
    
    * Apply suggestions from code review
    
    fix style
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Missing model memory release
    
    * Fix style
    
    * Add deprecated warning for public API function llama_init_from_file
    
    * Update public API use cases: move away from deprecated llama_init_from_file
    
    * Deprecate public API function llama_apply_lora_from_file
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit d7b7484f74d486f77feb4c0b7af7e1718ed91651
Author: eiery <19350831+eiery@users.noreply.github.com>
Date:   Fri Jun 23 04:38:01 2023 -0400

    Add OpenLLaMA instructions to the README (#1954)
    
    * add openllama to readme

commit 7487137227eb32ed9b12156338b865cb29b2dfd1
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Thu Jun 22 14:20:47 2023 +0200

    rework convert.py to read hyper-parameters from config.json (#1958)
    
    * Read hyper-parameters from HuggingFace-transformer config.json, if they exist, and fall back to guessing, like before otherwise.
      This allows converting open_llama 3B and other non-standard model designs.

commit bbca06e26949686d61a5126332680ba3cccf235c
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jun 21 23:49:25 2023 +0200

    cmake: revert CUDA arch default to 52, 61 if f16 (#1959)

commit fb98254f99d769fcbbf20966ef386abdb48ef601
Author: Rahul Vivek Nair <68507071+RahulVivekNair@users.noreply.github.com>
Date:   Thu Jun 22 03:18:43 2023 +0530

    Fix typo in README.md (#1961)

commit 049aa16b8c5c6d086246e4e6b9feb18de4fbd663
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 20 19:05:54 2023 +0300

    readme : add link to p1

commit 2322ec223a21625dfe9bd73ee677444a98a24ac9
Author: Xiake Sun <xiake.sun@intel.com>
Date:   Tue Jun 20 05:42:40 2023 -0700

    Fix typo (#1949)

commit aacdbd40562684665b6f7b8ba6695b7a2088bbb0
Author: Ettore Di Giacinto <mudler@users.noreply.github.com>
Date:   Tue Jun 20 03:24:39 2023 +0200

    llama : fix params struct slignment (#1936)
    
    * Workaround struct misalignment during value-copy
    
    Signed-off-by: mudler <mudler@localai.io>
    
    * Move booleans at the bottom of the structure
    
    Signed-off-by: mudler <mudler@localai.io>
    
    * Add comment
    
    Signed-off-by: mudler <mudler@localai.io>
    
    ---------
    
    Signed-off-by: mudler <mudler@localai.io>

commit 20568fe60f00155fa25e92eb3a7f6b911d557967
Author: Henri Vasserman <henv@hot.ee>
Date:   Tue Jun 20 01:12:39 2023 +0300

    [Fix] Reenable server embedding endpoint (#1937)
    
    * Add back embedding feature
    
    * Update README

commit 18b35625c3c19c64b7818a12460ba5ddb006dfdc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 19 20:43:30 2023 +0300

    ggml : fix bug in LBFGS optimizer (found by ggml tests)

commit ba4e85a8339b9dd7cdffad31838235f2fe45a8ea
Author: l3utterfly <gc.pthzfoldr@gmail.com>
Date:   Mon Jun 19 23:20:06 2023 +0800

    llama : use aligned memory during ggml_init call from loading saved sessions (#1934)
    
    * fixed issue: memory is not guaranteed to be aligned properly during ggml_init call from loading saved sessions
    
    * - removed commented out old code from fix
    - updated another instance of same issue below original

commit 23fc5c219a9aebd57c8af3fac454062cc4622980
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 19 18:18:34 2023 +0300

    cmake : fix trailing whitespaces

commit cb40dfca694b5cb849837548fd69932117c78362
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jun 19 18:17:03 2023 +0300

    llama : only use Q6_K for output weights if tensor size is multiple of 256 (#1932)
    
    * Only use Q6_K for output weights if tensor size is multiple of 256
    
    * Fixed copy/paste mistake
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit ca7c3f4da5d144d4cd1dd44903552e6ba49b8ec8
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jun 19 18:14:09 2023 +0300

    cuda : faster k-quants on older GPUs (#1930)
    
    * k_quants: hopefully much faster Q4_K on older GPUs
    
    On the GTX-1660 that I have available to represent
    "old GPUs", token prediction drops from 65.5 ms/tok
    to 41.5 ms/tok!
    
    * k_quants: hopefully much faster Q3_K on older GPUs
    
    On the GTX-1660 that I have available to represent
    "old GPUs", token prediction drops from 60.3 ms/tok
    to 41.0 ms/tok!
    
    * k_quants: faster Q2_K on older GPUs
    
    It looks like I didn't need to change anything
    compared to what we already had, so this is just
    adding clarifying comments. But I now measure
    36.3 ms/tok on the GTX-1660, instead fo the
    47.2 ms/tok that I have written in the faster
    k-quants PR.
    
    * k_quants: faster Q5_K on older GPUs
    
    68.5 ms/tok -> 62.0 ms/tok on GTX-1660.
    For some reason the same access pattern that leads
    to such resounding success for Q2_K to Q4_K did not
    work at all for Q5_K.
    
    It is also more difficult to measure because for Q5_K_S
    we only have 32 layers on the GTX-1660, so output, tok embeddings
    and kv cache are done on the CPU.
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit b97ca431db35ec96a339a721acb1219c1dd78bed
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 19 18:12:33 2023 +0300

    ggml : sync latest ggml repo (#1924)
    
    * ggml : sync latest ggml repo
    
    * ggml : remove unused comments
    
    * ggml : asserts

commit 1e3abfcef073e73c2b31e8570cb06c5cb2fd1f55
Author: Howard Su <howard0su@gmail.com>
Date:   Mon Jun 19 23:10:37 2023 +0800

    cmake : fix build shared ggml when CUDA is enabled (#1929)
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 16b9cd193965769089881bb8ec012fccca7b37b6
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jun 19 10:23:56 2023 +0200

    Convert vector to f16 for dequantize mul mat vec (#1913)
    
    * Convert vector to f16 for dmmv
    
    * compile option
    
    * Added compilation option description to README
    
    * Changed cmake CUDA_ARCHITECTURES from "OFF" to "native"

commit b24c3049d96557c24782e4d32feaae65f47277af
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Jun 18 17:41:26 2023 +0200

    Added tokens per second to info prints (#1928)

commit 0ede372a51fd8160688e01b587582666c14e94e5
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Jun 18 16:07:09 2023 +0200

    Fixed incorrectly applying RMS norm twice (#1925)

commit 8596af427722775f0df4a7c90b9af067ba90d4ef
Author: l3utterfly <gc.pthzfoldr@gmail.com>
Date:   Sun Jun 18 19:19:16 2023 +0800

    ggml : fix bug in ggml_compute_forward_add_q_f32 (#1918)

commit e1886cf4fe0d0f31661dda52a4a9f34bd9b9009a
Author: Mike <ytianhui2004@gmail.com>
Date:   Sun Jun 18 16:28:26 2023 +0800

    readme : update Android build instructions (#1922)
    
    Add steps for using termux on android devices to prevent common errors.

commit 8ab8ba62eb27cc340be2edf3418e051b1d967416
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Jun 18 11:13:43 2023 +0300

    llama : prevent usage of k-quants when tensor size is not a multiple of 256 (#1921)
    
    * Fix examples/metal
    
    * k-quants: prevent usage when tensor size is not divisible by 256
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 90cc59d6ab1363a5c69c60c4b94db647d3a54a18
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Jun 18 10:52:10 2023 +0300

    examples : fix examples/metal (#1920)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit ce2c7d72e2d06988b5ddec6811ab923254542077
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 18 09:09:47 2023 +0300

    metal : handle buffers larger than device's maxBufferLength (#1826)
    
    * metal : handle buffers larger than device's maxBufferLength
    
    * metal : print more verbose device info + handle errors
    
    * metal : fix prints for overlapping views
    
    * metal : minimize view overlap to try to utilize device memory better

commit 57cd69460f736031a3fc54af1e97c03f80128478
Author: Howard Su <howard0su@gmail.com>
Date:   Sun Jun 18 12:29:47 2023 +0800

    cmake : add CUDA_ARCHITECTURES to new target ggml_static (#1917)

commit b2416493ab3ab21686d47c96669da6d6c6af08a4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jun 17 20:55:03 2023 +0300

    make : do not print help for simple example

commit 4f9c43e3bd488b7561119785485e1155dba338d7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jun 17 20:24:11 2023 +0300

    minor : warning fixes

commit 2c9380dd2f77e41149340f3ecb09764d793b16db
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jun 17 19:15:02 2023 +0200

    Only one CUDA stream per device for async compute (#1898)

commit 051e1b0e6a6e3aee7d989b47760980e6fda5861c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jun 17 19:30:22 2023 +0300

    llama : fix kv_cache `n` init (close #1903)

commit 86c7571864ff331f8cdb9e092f3abeb123729a56
Author: DaniAndTheWeb <57776841+DaniAndTheWeb@users.noreply.github.com>
Date:   Sat Jun 17 18:17:22 2023 +0200

    make : update for latest Arch (#1701)
    
    With the upcoming change to the openblas package in arch the Makefile workaround is no longer needed.

commit 3d59ec5935ea1d33e9d51060a8dd737169b9b89b
Author: Howard Su <howard0su@gmail.com>
Date:   Sat Jun 17 23:46:15 2023 +0800

    ggml : fix warnings under MSVC (#1908)

commit 0711a5f6dce7f04c2a791b14bc47f7d4cb545408
Author: Aaron Miller <apage43@ninjawhale.com>
Date:   Sat Jun 17 07:37:49 2023 -0700

    metal : add norm, cpy f16->f16, alibi kernels (#1823)

commit fc45a81bc642b9ef33d9004f2b363d558438a6c9
Author: Faez Shakil <faez.shakil@gmail.com>
Date:   Sat Jun 17 17:13:05 2023 +0500

    exposed modules so that they can be invoked by nix run github:ggerganov/llama.cpp#server etc (#1863)

commit 794db3e7b982fee37e3995db9c3a216a57ff65e3
Author: Randall Fitzgerald <randall@dasaku.net>
Date:   Sat Jun 17 07:53:04 2023 -0400

    Server Example Refactor and Improvements (#1570)
    
    A major rewrite for the server example.
    
    Note that if you have built something on the previous server API, it will probably be incompatible.
    Check out the examples for how a typical chat app could work.
    
    This took a lot of effort, there are 24 PR's closed in the submitter's repo alone, over 160 commits and a lot of comments and testing.
    
    Summary of the changes:
    
    - adds missing generation parameters: tfs_z, typical_p, repeat_last_n, repeat_penalty, presence_penalty, frequency_penalty, mirostat, penalize_nl, seed, ignore_eos
    - applies missing top k sampler
    - removes interactive mode/terminal-like behavior, removes exclude parameter
    - moves threads and batch size to server command-line parameters
    - adds LoRA loading and matches command line parameters with main example
    - fixes stopping on EOS token and with the specified token amount with n_predict
    - adds server timeouts, host, and port settings
    - adds expanded generation complete response; adds generation settings, stop reason, prompt truncated, model used, and final text
    - sets defaults for unspecified parameters between requests
    - removes /next-token endpoint and as_loop parameter, adds stream parameter and server-sent events for streaming
    - adds CORS headers to responses
    - adds request logging, exception printing and optional verbose logging
    - adds better stopping words handling when matching multiple tokens and while streaming, or when it finishes on a partial stop string
    - adds printing an error when it can't bind to the host/port specified
    - fixes multi-byte character handling and replaces invalid UTF-8 characters on responses
    - prints timing and build info on startup
    - adds logit bias to request parameters
    - removes embedding mode
    - updates documentation; adds streaming Node.js and Bash examples
    - fixes code formatting
    - sets server threads to 1 since the current global state doesn't work well with simultaneous requests
    - adds truncation of the input prompt and better context reset
    - removes token limit from the input prompt
    - significantly simplified the logic and removed a lot of variables
    
    ---------
    
    Co-authored-by: anon998 <131767832+anon998@users.noreply.github.com>
    Co-authored-by: Henri Vasserman <henv@hot.ee>
    Co-authored-by: Felix Hellmann <privat@cirk2.de>
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    Co-authored-by: Lesaun Harvey <Lesaun@gmail.com>

commit 5ddf7ea1fb42bac21026de2f77e0f9c069b92234
Author: Jiří Podivín <66251151+jpodivin@users.noreply.github.com>
Date:   Sat Jun 17 12:32:48 2023 +0200

    hooks : setting up flake8 and pre-commit hooks (#1681)
    
    Small, non-functional changes were made to non-compliant files.
    These include breaking up long lines, whitespace sanitation and
    unused import removal.
    
    Maximum line length in python files was set to a generous 125 chars,
    in order to minimize number of changes needed in scripts and general
    annoyance. The "txt" prompts directory is excluded from the checks
    as it may contain oddly formatted files and strings for a good reason.
    
    Signed-off-by: Jiri Podivin <jpodivin@gmail.com>

commit bac19927c302737465a1deb14ac0943a221863e8
Author: Gustavo Rocha Dias <91472747+gustrd@users.noreply.github.com>
Date:   Sat Jun 17 06:01:06 2023 -0300

    readme :  alternative way to build for Android with CLBlast. (#1828)

commit b4c6f46f17b6e02f1cd55a81339e7e64f3aaa688
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Jun 17 01:49:42 2023 -0600

    Allow cmake to build ggml as a library (#1896)
    
    * Allow cmake to build ggml as a library
    
    * A ggml_static library will be created
    
    * When BUILD_SHARED_LIBS is enabled, ggml_shared will also be built

commit 92f20d9942c86daeb78637bdad7296a572f4da28
Author: David Yang <davidyang6us@gmail.com>
Date:   Sat Jun 17 14:51:54 2023 +0800

    train : get raw text instead of page with html (#1905)
    
    We probably want to train using just the text of Shakespeare instead of the html of the page displaying his work.

commit d411968e990c37f51328849c96a743dd78f3c3dd
Author: 0cc4m <picard12@live.de>
Date:   Fri Jun 16 20:59:49 2023 +0200

    opencl : support k-quants (#1836)
    
    * Porting q2_k kernel to OpenCL
    
    * Set global and local sizes for kernel calls for dequantizing k-quants
    
    * Added q6_k kernel
    
    * Fix q4_k opencl struct order
    
    * Replace uchar with uint8_t
    
    * Finish dequant kernels
    
    * Added OpenCL DMMV kernels
    
    * Fix q2_k, improve code
    
    * Fix q3_k
    
    * Shorten switch statements
    
    * Improve code formatting
    
    ---------
    
    Co-authored-by: Concedo <39025047+LostRuins@users.noreply.github.com>

commit b41b4cad6f956b5f501db0711dd7007c32b5eee5
Author: SuperUserNameMan <yoann@terminajones.com>
Date:   Fri Jun 16 20:58:09 2023 +0200

    examples : add "simple" (#1840)
    
    * Create `simple.cpp`
    
    * minimalist example `CMakeLists.txt`
    
    * Update Makefile for minimalist example
    
    * remove 273: Trailing whitespace
    
    * removed trailing white spaces simple.cpp
    
    * typo and comments simple.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 13fe9d2d84f30cab613c960bf66ac83916006694
Author: Zenix <zenixls2@gmail.com>
Date:   Sat Jun 17 03:53:04 2023 +0900

    cmake : add auto detection of BLAS_INCLUDE_DIRS (#1886)

commit ac3b8869538c7fbdb48ff141d78c4dea091789f0
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Jun 16 20:25:51 2023 +0200

    llama : fix embd when offloading non-repeating layers (#1891)

commit 5b9ccaf104cc1054d4f8f17bc8a4b8dc949e5527
Author: FrankHB <frankhb1989@gmail.com>
Date:   Sat Jun 17 02:25:01 2023 +0800

    Fixed possible macro redefinition (#1892)
    
    MinGW libstdc++ may define `NOMINMAX` unconditionally. This fixes the case when it is already defined.

commit 9cbf50c041a525d781c7764f493a5443924e4e38
Author: Borislav Stanimirov <b.stanimirov@abv.bg>
Date:   Fri Jun 16 21:23:53 2023 +0300

    build : fix and ignore MSVC warnings (#1889)

commit 3d0112261042b356621e93db3fa4c6798a5d098f
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Jun 16 20:08:44 2023 +0300

    CUDA : faster k-quant dot kernels (#1862)
    
    * cuda : faster k-quant dot kernels
    
    * Imrove Q2_K dot kernel on older GPUs
    
    We now have a K_QUANTS_PER_ITERATION macro, which should be
    set to 1 on older and to 2 on newer GPUs.
    With this, we preserve the performance of the original
    PR on RTX-4080, and are faster compared to master on
    GTX-1660.
    
    * Imrove Q6_K dot kernel on older GPUs
    
    Using the same K_QUANTS_PER_ITERATION macro as last commit,
    we preserve performance on RTX-4080 and speed up
    Q6_K on a GTX-1660.
    
    * Add LLAMA_CUDA_KQUANTS_ITER to CMakeLists.txt and Makefile
    
    Allowed values are 1 or 2. 2 gives the best performance on
    modern GPUs and is set as default. On older GPUs 1 may work
    better.
    
    * PR comments
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 602c748863e15270d80d74aa2c3bf86ab8139e07
Author: Borislav Stanimirov <b.stanimirov@abv.bg>
Date:   Fri Jun 16 09:58:11 2023 +0300

    gitignore : add several entries specific to Visual Studio (#1888)

commit a09f9195be39afb4b023b646c0a6ec8a86915174
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Jun 15 21:49:08 2023 +0200

    Fixed CUDA runtime version check (#1879)

commit bed92756172d4514b23aaf9744cf8e2dc892fc7b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jun 15 21:56:50 2023 +0300

    cmake : remove whitespaces

commit c36e81da62ebfe09a768201cc44fa8d712dd00ed
Author: yangli2 <yangli2@gmail.com>
Date:   Thu Jun 15 11:05:53 2023 -0700

    examples : add chat-vicuna.sh (#1854)
    
    Co-authored-by: Yang Li <yangliyl@google.com>

commit 3559433fecedf365e7aba2fe3d5f89d9abb817c1
Author: Igor Okulist <okigan@gmail.com>
Date:   Thu Jun 15 12:51:26 2023 -0500

    cmake : set include path for OpenBlas (#1830)

commit 69b34a0e80300bfb3e996983ac3ea075f5526675
Author: Frederik Vogel <Schaltfehler@users.noreply.github.com>
Date:   Fri Jun 16 02:47:04 2023 +0900

    swift : Package compile breaks due to ggml-metal.metal (#1831)
    
    * Ignore metal file in spm
    
    * Add ggml.h to spm public Headers
    
    ---------
    
    Co-authored-by: Vogel Frederik <vogel.frederik@linecorp.com>

commit cf267d1c71a781700698f8518e903239c3bcc929
Author: daboe01 <daboe01@googlemail.com>
Date:   Thu Jun 15 19:42:48 2023 +0200

    make : add train-text-from-scratch (#1850)
    
    * make finetuning example accessible
    
    * fixed: targed was in wrong line
    
    * fixed: name of executable was wrong
    
    * fixed: naming of binary
    
    * fixed: model path was wrong
    
    * fixed clean target
    
    * Update examples/train-text-from-scratch/README.md
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 9dda13e5e1f70bdfc25fbc0f0378f27c8b67e983
Author: Srinivas Billa <nivibilla@gmail.com>
Date:   Thu Jun 15 18:36:38 2023 +0100

    readme : server compile flag (#1874)
    
    Explicitly include the server make instructions for C++ noobsl like me ;)

commit 37e257c48e350cf03c353c10d31e777f8d00123d
Author: sandyiscool <sandyiscool@gmail.com>
Date:   Thu Jun 15 23:06:06 2023 +0530

    make : clean *.so files (#1857)

commit 64cc19b4fe3df03bc20e520aa111c30cff3a655e
Author: Howard Su <howard0su@gmail.com>
Date:   Fri Jun 16 01:29:59 2023 +0800

    Fix the validation of main device (#1872)

commit 4bfcc855abdb2c9fcc3c5a84747974521909fa41
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jun 15 20:29:48 2023 +0300

    metal : parallel command buffer encoding (#1860)
    
    * metal : parallel command buffer encoding
    
    * metal : determine number of command buffers based on gf->n_threads

commit 6b8312e7979b852f6b6ac9d29cd51fda16c17948
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Jun 15 19:06:46 2023 +0200

    Better error when using both LoRA + GPU layers (#1861)

commit 254a7a7a5ff4c874ff8488f1f5cbdd7e9c89d682
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jun 14 19:47:19 2023 +0200

    CUDA full GPU acceleration, KV cache in VRAM (#1827)
    
    * Fixed CUDA RoPE
    
    * ggml_cuda_mul_mat_vec_p021
    
    * ggml_cuda_scale
    
    * ggml_cuda_diag_mask_inf
    
    * ggml_is_permuted
    
    * ggml_cuda_cpy
    
    * flatten rows for ggml_cuda_op
    
    * Added a --low-vram option
    
    * Fixed Windows performance
    
    * Fixed LLAMA_CUDA_DMMV_Y > 1 for WizardLM

commit 92549202659fc23ba9fec5e688227d0da9b06b40
Author: 0xspringtime <110655352+0xspringtime@users.noreply.github.com>
Date:   Tue Jun 13 15:37:54 2023 -0400

    baby-llama : fix operator!= (#1821)
    
    * Update baby-llama.cpp
    
    Seems to be an error in the implementation of the operator!= function. It attempts to compare the this pointer (a llama_hparams_lora object) with the other pointer (a llama_hparams object) using memcmp. This can lead to incorrect results because the sizes of the objects being compared (sizeof(llama_hparams) and sizeof(llama_hparams_lora)) are different, should now be able to compare two llama_hparams_lora objects for inequality.
    
    * Update baby-llama.cpp
    
    * Update baby-llama.cpp

commit e32089b2c20b1b87b22912f4a8b93fe01647d5b9
Author: xaedes <xaedes@gmail.com>
Date:   Tue Jun 13 21:04:40 2023 +0200

    train : improved training-from-scratch example (#1652)
    
    * add python wrapper
    
    https://gist.github.com/abetlen/2b90e5f153f6efd00931d098de5c73ce
    
    * fix decoding error. adds errors=ignore parameter
    
    * add python bindings for functions to get and set the whole llama state
    (rng, logits, embedding and kv_cache)
    
    * update python bindings
    
    * add text generating baby-llama from scratch example
    
    * fix race condition bug in ggml_compute_forward_diag_mask_f32
    
    * implement ggml_soft_max_back for more performant backward pass of soft_max
    
    avoids creating big intermediate matrices of size n_embd x n_embd for llama layers and n_vocab x n_vocab for cross entropy loss
    
    * improve softmax backward pass
    
    go from quadratic runtime to linear runtime by simplifying the formulas
    
    * fix race condition bug in non-inplace ggml_compute_forward_diag_mask_f32
    
    memcpy needs to be synchronized across threads to avoid race conditions.
    => do it in INIT phase
    
    * fix bug in ggml_compute_forward_soft_max_back_f32 on DEBUG build
    
    * improve performance of mul_mat backward pass
    
    avoid transpose by using mul_mat with swapped arguments
    
    * avoid printing too much newlines in baby-llama-text
    
    * activate threading in baby-llama-text
    
    * add ggml_out_prod and use it for mul_mat backward pass for improved performance
    
    performance stats report improvement from 37 seconds to 16 seconds runtime during my training tests
    
    * better weight initialization improves training convergence at start
    
    * better weight initialization improves training convergence at start
    
    * improve ggml_out_prod performance
    
    - change iteration order (>15s -> 10s runtime)
    - parallelize over one more dimension: over dst matrix rows (10s -> <5s runtime)
    
    * add llama sampler, shuffle samples and constrain sampling to tokens occurring in train data
    
    * fix get_samples call, add model tensor names, increase model size, start training samples after newline
    
    * save train trained model to checkpoint and load model to be trained from checkpoint
    
    * use inplace functions where possible
    
    * initialize rng with srand
    
    * use different arguments for input and output checkpoint
    
    * ggml fixes to support backward pass on inplace operations
    
    * remove duplicate include
    
    * fix cross entropy loss
    
    - add target probabilities for each sample which is then used in cross entropy loss
    
    * print used memory before and after optimization
    
    * sample with non-greedy sampling parameters at the end of training
    
    * add cmake target for baby-llama-text
    
    * add ggml_add1_inplace to header
    
    * enable gradient propagation for inplace add1 and scale operations
    
    those functions backward passes don't need the original src0, so they also work when forward is inplace
    
    * implement AdamW in ggml_opt_adam by adding weight decay parameter (default 0.001f)
    
    also add a schedule parameter (default 1.0f) that can be used to scale alpha and decay according to learning schedule.
    setting the decay parameter to zero disables AdamW resulting in normal Adam optimizer.
    
    since the difference between Adam and AdamW is minimal it is not implemented as another optimizer, but integrated into the existing Adam optimizer.
    
    * use inplace operations in cross_entropy_loss
    
    * fix random weight initialization scale
    
    * add missing default parameters for adam optimizer
    
    * add ggml_opt_context, so that we can properly resume training
    
    otherwise the optimizer states, tracking statistics about the error function and its derivates,
    will reset to zero each time ggml_opt is called, hindering convergence on resumed training.
    
    now the optimizer context and all its memory is stored in a separate struct.
    
    * fix bug in llama_sample_token_mirostat_v2
    
    when all candidates are filtered out through mu threshold, the following soft_max operation will fail.
    so keep at least one.
    
    * add forward function without using cache, for more performant training
    
    during training on whole samples no cache is required.
    removing the cache and simplifying the remaining code results in performance and memory usage improvement.
    
    * print suppressed newline tokens as string "\n"
    
    printing too much actual newlines is suppressed to avoid flooding the console.
    
    * store optimizer state in training checkpoint and add learning schedule
    
    persistent optimizer state allows to resume training without resetting the optimizer
    learning schedule consists of linear warmup ramp followed by cosine decay with restarts
    
    * remove unused functions
    
    * fix bug in get_samples which corrupted training targets
    
    * save checkpoint only when it was trained
    
    * simplify code
    
    * remove trailing whitespace
    
    * simplify backward pass for SQRT
    
    * replace inefficient repeat backward pass with dedicated repeat_back operation
    
    * add ggml_cross_entropy_loss with backward pass for faster training
    
    cross entropy loss can also be implemented using softmax and log, but as dedicated operation it is faster and especially avoids unnecessary memory overhead.
    
    * add tests for cross_entropy_loss backward pass
    
    finite differences regularly results in estimated gradient of zero, despite the backward pass giving non zero gradient.
    _probably_ the finite differences fails due to numerical issues
    
    * use ggml_cross_entropy_loss in text training example
    
    * remove trailing whitespace
    
    * slightly improve how cross entropy loss is compute
    
    btw: directly implemented cross entropy loss seems to have way lower magnitudes than when implemented with softmax and log.
    probably the input to log gets closer to zero due to float numerics.
    maybe the multiplication by (1.0-eps)/sum is more accurate..
    
    * add llama_get_vocab to get the vocabulary as output parameters
    
    * set default model.type for unknown models with few layers
    
    * add export of training checkpoint to llama compatible model file
    
    * get vocabulary for exporting training checkpoint to llama compatible model file
    
    * implement backward pass of flash attention
    
    * bugfixes for backward pass of flash attention
    
    * test flash attention backward pass
    
    need to set loose error bounds to pass.
    the finitie differences are close to numeric limits and often return quite different values than the backward pass.
    reducing eps further lets the gradients vanish completely.
    likewise setting eps to big results in wronger values.
    the softmax in the middle of the function is probably the most responsible for the numeric issues using finite differences.
    
    * add option to train with flash attention and move options to the top of the main function
    
    training from scratch also works with flash attention
    training convergence and generation results after fix number of iterations are worse than when not using flash attention.
    maybe there still lingers a bug in the flash attention backward pass?
    but training works, just with slower convergence.
    
    flash attention is still worth to use, because it requires way less memory and is faster with high n_ctx
    
    * add train_params and command line option parser
    
    * remove unnecessary comments
    
    * add train params to specify memory size
    
    * remove python bindings
    
    * rename baby-llama-text to train-text-from-scratch
    
    * replace auto parameters in lambda function
    
    * add #include <climits>
    
    * add explicit cast to fix compile error
    
    "error: non-constant-expression cannot be narrowed from type 'int64_t' (aka 'long long') to 'uint32_t' (aka 'unsigned int') in initializer list [-Wc++11-narrowing]"
    
    * remove trailing whitespace
    
    * add ggml_opt_resume_g which accepts forward and backward cgraphs
    
    * fix formulas in comments
    
    * bug fix for ggml_compute_forward_get_rows_back_f32
    
    the result should be set to zero, not to whatever data is in opt0
    
    * improve training memory usage with scratch buffers
    
    instead of relying on the automatic backward pass, we manually create the graph for the backward pass.
    it turns out that all backward pass operations need only temporary memory which can be reused after each layer.
    
    will compute backward pass for ALL model parameters
    
    * add option to use scratch buffers in training or not
    
    make it configurable because currently training with scratch buffers implies flash attention and optimization over all parameters.
    
    * ci : disable temporary
    
    * store view offset and permute axes in opt[0] instead of storing it in padding
    
    use memcpy to store offset, because offset is of type size_t.
    when storing it as int32_t offset would have to be smaller than 2^31 which is not necessarily true.
    
    * minor : fix compile warnings + minor style changes
    
    * fix bug in threaded indices calculation of ggml_compute_forward_flash_attn_back_f32
    
    * store view offset like in master branch
    
    * bug fix in forward_batch_wo_cache_flash_attn_train
    
    * scratch buffer bug fixes in forward_batch_wo_cache_flash_attn_train
    
    data of permute and reshape is the same as their input.
    if we want to preserve the output of permute/reshape, we also need to preserve their inputs.
    
    replace reshape(src0, src1) with reshape_nd calls so that we don't need src1.
    
    replace (temporary) t03 with ggml_repeat(ctx0, layer.attention_norm, t02).
    in the future we could also use the new broadcasting ggml_mul to avoid these repeat calls.
    for this we need backward pass of broadcasting ggml_mul.
    
    * remove unnecessary scratch buffer 0
    
    buf 0 is persistent memory, so we can just disable scratch for this by using buf -1
    
    * avoid creating unnecessary grad tensors
    
    previously we need to create grads for model parameters, so that expand(..) correctly populates cgraph->leafs & cgraph->grads
    this wasted memory, because unnecessary grad for each op were automatically created:
    the automatically generated grad was unnecessary because we later manually set the grad (e.g. t35->grad = expand(gb, ...) ).
    this discarded the automatically generated grad resulting in wasted memory.
    
    improved this by changing expand(..) to not use ggml_build_forward_expand.
    expand set cgraph->nodes but not the leafs.
    cgraph->leafs & cgraph->grads are set in another pass after the last expand call.
    
    * print used training seed
    
    * zero initialize gfbuf and gbbuf
    
    * ci : re-enable workflows + add README for training
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 2347e45e7bdb09c9a7d74b2c0bc86c2b65f0c343
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 13 20:20:07 2023 +0300

    llama : do a warm-up eval at start for better timings (#1824)

commit 74d4cfa3438cb58bd177eed30014e6588694aaa8
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Tue Jun 13 04:23:23 2023 -0600

    Allow "quantizing" to f16 and f32 (#1787)
    
    * Allow "quantizing" to f16 and f32
    
    Fix an issue where quantizing didn't respect LLAMA_NO_K_QUANTS
    
    Add brief help to the list of quantization types in the quantize tool
    
    Ignore case for quantization type arguments in the quantize tool

commit 74a6d922f12ccfe16b0c265f43be8978c6f25e98
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jun 12 22:39:21 2023 +0300

    Metal implementation for all k_quants (#1807)
    
    * metal : improve q4_K
    
    28.3 -> 26.0 ms/token by avoiding a branch in the
    calculation of the scales.
    
    * metal : small improvement for Q4_K
    
    * metal : still optimizing Q4_K
    
    This commit pushes it down to 25.3 ms / token.
    
    The crazy idea of using 6 bits for the scales is really costly on
    Metal: if I remove the bit fiddling necessary to make the block
    scales, time goes almost to the Q4_0 23 ms/token.
    
    Before pushing the k-quants upstream I had a Q4_K variant that
    had used 8-bit scales. It wasn't more accurate, used 0.125 bits more per weight,
    was running slightly slower on the CPU (due to the larger model size
    and being memory bound there), and the difference was entirely
    negligible under CUDA. So, I decided to publish the version with 6-bit
    scales. Perhaps I should re-consider and change to 8-bit scales?
    
    * metal : some more optimizations
    
    Q2_K: 25.4 ms/token
    Q6_K: 27.3 ms/token
    Q4_0: 22.8 ms/token
    Q4_1: 23.1 ms/token
    
    * metal : Q3_K support
    
    Something is not quite right yet.
    
    * metal : Q5_K support
    
    Initial version achieves 31.2 ms/token, 210 GB/s
    
    * metal : still not able to figure out why q3_K does not work
    
    * Minor
    
    * metal : yet another failed attempt to make q3_K work
    
    * metal : optimize Q5_K
    
    31.2 ms -> 27.8 ms.
    250 GB/s.
    
    * metal : q3_K still not working
    
    Adding a heavily commented q3_K metal kernel to explain
    my obviously faulty logic. Perhaps someone could spot the issue?
    
    * metal : q3_K finally working
    
    Not optimized at all.
    
    What was the issue? The scales are not 4-bytes aligned,
    and I was accessing them with a uint32_t pointer.
    When I tried that on CUDA, I got an error (illegal memory access)
    and added a memcpy to a local array of 3 uint32_t's.
    But on Metal it told me there is no memcpy, so I tried
    accessing directly. There is no error, just garbage results.
    At some point I did try accessing the scales with an uint16_t
    pointer (the scales are for sure 2-byte aligned), but was
    still getting garbage. I guess, there must have been another bug.
    
    No access to scales is via a uint16_t pointer and, after starting
    from scratch from the C dequantize function, it finally works.
    
    * metal : Q3_K 1st optimization pass
    
    * metal : Q3_K second optimization pass - 29.6 ms/token
    
    * metal : Q3_K cleanup
    
    * metal : fixed accidentally broken Q2_K
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit e4caa8da59c1c97dc23fa336f4d726984a20560f
Author: slaren <slarengh@gmail.com>
Date:   Mon Jun 12 19:12:47 2023 +0200

    ci : run when changing only the CUDA sources (#1800)

commit 58970a4c39124a647ac2a640d9e178ea6c961e65
Author: Howard Su <howard0su@gmail.com>
Date:   Mon Jun 12 20:44:16 2023 +0800

    Leverage mmap for offloading tensors to GPU (#1597)
    
    * Rebase to latest
    
    * Show progress
    
    * Add assert to make sure we only allocate temp buffer for non-CPU backend tensor
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

commit 8c0a10e64dbf60fd9946c0cd5e6f59690800b123
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jun 12 14:31:36 2023 +0300

    metal : fix failure to load model (#1817)
    
    The number of buffers in the ggml context was left unitialized.
    This leads to sporadic failures to load the model on
    startup. It is actually strange that the failure occurred so
    infrequantly.
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit fa84c4b3e80199a5683438f062009c031a06c4fa
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sun Jun 11 08:19:17 2023 -0600

    Fix issue where interactive mode crashes when input exceeds ctx size (#1789)
    
    * Fix issue where interactive mode in the main example crashes when input exceeds ctx size
    
    * Ensure the context size is at least 8 tokens in the main example.
    
    Closes #1768

commit 12b063f0ecf280e98028e444fc492ee6222cdcdc
Author: Kyle Liang <liangmanlai@gmail.com>
Date:   Sun Jun 11 21:20:52 2023 +0800

    Fixed WSL cuda's OOM error (#1594)
    
    * In the function , add the cuda error bypass.
    
    * remove excessive codes and prints
    
    ---------
    
    Co-authored-by: liang <liangmanlai@126.com>

commit 31d2b5f4a4bae081e59b36ab37c6ff6f5b5940ad
Author: Ryan Landay <rlanday@gmail.com>
Date:   Sun Jun 11 17:38:53 2023 +0800

    Update SHA256SUMS with current hashes for models quantized using q4_0 (#1798)

commit 4de0334f5cabf4696eced2e5d6e279fdfaa6c0f2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jun 10 22:56:53 2023 +0300

    cmake : fix Metal build (close #1791)

commit 3f1223155a462477ac933474ebc4eab0ce3ca264
Author: Artyom Lebedev <vagran.ast@gmail.com>
Date:   Sat Jun 10 22:51:36 2023 +0300

    k-quants : GCC12 compilation fix (#1792)

commit 303f5809f1b4ec49823dbe70cacd2124ec1d0df0
Author: Andrei <abetlen@gmail.com>
Date:   Sat Jun 10 10:47:34 2023 -0400

    metal : fix issue with ggml-metal.metal path. Closes #1769 (#1782)
    
    * Fix issue with ggml-metal.metal path
    
    * Add ggml-metal.metal as a resource for llama target
    
    * Update flake.nix metal kernel substitution

commit 059e99066d95d73d1ca26c3375d47c0e35596229
Author: Aisuko <urakiny@gmail.com>
Date:   Sun Jun 11 00:08:11 2023 +1000

    doc : fix wrong address of BLIS.md (#1772)
    
    Signed-off-by: Aisuko <urakiny@gmail.com>

commit 17c10acfb44ecb7af25e37fb67b9501cbc0034d2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jun 10 12:06:45 2023 +0300

    ggml : force no_alloc == false when creating opt tensors (close #1699)
    
    This is needed to make operators like ggml_view() be able to store their
    parameters in the ggml context's memory and not get discarded when
    no_alloc is true

commit e9b66ee9829039d4ab54550d6222e42a0b31e52a
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sat Jun 10 11:28:11 2023 +0300

    metal : add Q4_1 implementation (#1785)
    
    23.3 ms / token, so just ~1% slower than q4_0.
    Achieves 290 GB/s memory throughput.
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 4f0154b0bad775ac4651bf73b5c216eb43c45cdc
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Jun 10 01:59:17 2023 -0600

    llama : support requantizing models instead of only allowing quantization from 16/32bit (#1691)
    
    * Add support for quantizing already quantized models
    
    * Threaded dequantizing and f16 to f32 conversion
    
    * Clean up thread blocks with spares calculation a bit
    
    * Use std::runtime_error exceptions.

commit ef3171d16241c18581d4d08374f0b9e396ade6b7
Author: Xingchen Song(宋星辰) <xingchensong1996@163.com>
Date:   Sat Jun 10 15:49:40 2023 +0800

    ggml : workaround for missing _mm256_setr_m128i in GCC < 8 (#1638)

commit 555275a693843273759230547001f9ae07fb537e
Author: rankaiyx <rankaiyx@rankaiyx.com>
Date:   Sat Jun 10 14:41:59 2023 +0800

    make : add SSSE3 compilation use case (#1659)

commit 98ed16557432d7a5179c57eddcc3a08a7ae6d54d
Author: Robert Sung-wook Shin <edp1096@users.noreply.github.com>
Date:   Sat Jun 10 01:24:40 2023 +0900

    OpenCL: Add release memory (#1741)
    
    * Add opencl release memory
    
    * Rename function name

commit ae9663f1887513e152839e91f61c513075a19422
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Jun 9 13:58:15 2023 +0200

    Windows nvcc workaround (#1753)
    
    Fix gibberish output on Windows when using CUDA

commit b33dee282f5d8032b5f780152732dc45cbf2d349
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jun 9 11:11:04 2023 +0300

    metal : fix build "tanhf" -> "tanh"

commit 92f44ff7f778ef1b94028b2ba6d39943b5ca0ada
Author: AT <manyoso@users.noreply.github.com>
Date:   Fri Jun 9 04:00:51 2023 -0400

    metal : add GELU implementation (#1770)
    
    Co-authored-by: Adam Treat <adam@nomic.ai>

commit 245fc3c37da5ac5963f9f11a9f4f2ac08d96afc6
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Jun 9 10:39:59 2023 +0300

    metal : faster q4_0 (#1775)
    
    * metal : 8% faster q4_0
    
    Avoid copying into local uchar4 anf float4.
    
    * metal : 17% faster Q4_0
    
    Use 64 threads in a thread group.
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 72ff5282bf0388c60821f504c4c8cc2b1f491aa6
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Jun 8 22:28:21 2023 +0300

    metal : add Q2_K implementation (#1762)
    
    * metal : add Q2_K implementation
    
    27.1 ms / token on M2 Max 30-core GPU, so about the
    same speed as Q4_0. Memory throughput is ~156 GB/s.
    
    The access pattern used in the Q2_K
    CUDA implementation resulted in significantly lower
    performance (~31 ms/token).
    
    * Fixing merge conflicts
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 0bf7cf1b296fc9fca05411b37afdf08a531487d2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jun 8 20:48:14 2023 +0300

    Revert "ggml : load data into int8x16x4_t using vld4q_s8 on arm64 (#1738)"
    
    This reverts commit 8432d4d9f716b25133e3ed671d91e21f6f3be867.

commit 8432d4d9f716b25133e3ed671d91e21f6f3be867
Author: le.chang <cljs118@126.com>
Date:   Fri Jun 9 00:47:56 2023 +0800

    ggml : load data into int8x16x4_t using vld4q_s8 on arm64 (#1738)

commit 0f291e1f65c1d68201e71ce99c89562a36686b6d
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Jun 8 19:46:22 2023 +0300

    metal : Q6_K implementation (#1752)
    
    * Metal implementation for Q4_K
    
    Very slow for now:
    42 ms / token, Q4_0 runs in 28 ms/token on my
    30-core M2 Max GPU.
    
    * Optimizing Q4_K on metal
    
    The first token always takes longer, I guess because
    the metal kernel is being jit-compiled.
    So, using n = 128 to measure time.
    
    At this point Q4_K takes 29.5 ms / token
    compared to 27.2 ms / token for Q4_0.
    Quite a bit better than the initial attempt,
    but still not good enough.
    
    * Optimizing q4_K metal dot some more
    
    For n = 256 it is now 28.1 ms/token compared to
    27 ms/token for q4_0.
    
    * Fix after merge with master
    
    * Metal implementation for Q6_K
    
    Similar to the CUDA implementation.
    No idea if this is the optimum for Metal, but the few
    alternative variants I tried all had a lower performance.
    
    We get 36.5 ms / token on M2 Max with 30 GPU cores.
    This corresponds to ~200 GB/second throughput.
    
    * clang-tidy : add config back
    
    * Much better Q6_K implementation for metal
    
    28.3 ms / token for 7B. Subtracting ~9 ms that is spent in
    other compute graph operations, we are left with ~19 ms
    for the matrix multiplications. The model is ~5.5 GB,
    so we are getting 1000 / 19 * 5.5 = 290 GB/s!
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 8fc8179919a11738910db07a800f2b176f8adf09
Author: qingfengfenga <41416092+qingfengfenga@users.noreply.github.com>
Date:   Thu Jun 8 15:58:53 2023 +0800

    Add llama.cpp docker support for non-latin languages (#1673)
    
    * Modify Dockerfile default character set to improve compatibility (#1673)

commit b50b570ed9d699d3d126d72fc02de92926bcd937
Author: Steven Roussey <sroussey@gmail.com>
Date:   Thu Jun 8 00:12:28 2023 -0700

    ggml : fix fprintf warnings (#1720)

commit 53aba3f393f2e02a78ddaba2e934893a8bbf3246
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jun 8 10:09:08 2023 +0300

    clang-tidy : restore dot file from accidental deletion

commit 4161bdc04debb70bf5f275492b4d89fd9330087c
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Jun 8 10:08:23 2023 +0300

    metal : add Q4_K implementation (#1733)
    
    * Metal implementation for Q4_K
    
    Very slow for now:
    42 ms / token, Q4_0 runs in 28 ms/token on my
    30-core M2 Max GPU.
    
    * Optimizing Q4_K on metal
    
    The first token always takes longer, I guess because
    the metal kernel is being jit-compiled.
    So, using n = 128 to measure time.
    
    At this point Q4_K takes 29.5 ms / token
    compared to 27.2 ms / token for Q4_0.
    Quite a bit better than the initial attempt,
    but still not good enough.
    
    * Optimizing q4_K metal dot some more
    
    For n = 256 it is now 28.1 ms/token compared to
    27 ms/token for q4_0.
    
    * Fix after merge with master
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 0035858273ebe0694926bf4414d279f3e1cd109d
Author: johnson442 <56517414+johnson442@users.noreply.github.com>
Date:   Thu Jun 8 08:02:48 2023 +0100

    k-quants : add missing compile definition to CMakeLists (#1748)

commit 5c64a0952ee58b2d742ee84e8e3d43cce5d366db
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 7 10:59:52 2023 +0300

    k-quants : allow to optionally disable at compile time (#1734)
    
    * k-quants : put behind optional compile flag LLAMA_K_QUANTS
    
    * build : enable k-quants by default

commit 5b57a5b72676540b6a45a3f527126299969ad241
Author: jacobi petrucciani <8117202+jpetrucciani@users.noreply.github.com>
Date:   Wed Jun 7 00:15:31 2023 -0400

    flake : update to support metal on m1/m2 (#1724)

commit 4dc62c545df0af60635d579e9e4dd91bc5afff51
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 7 07:15:08 2023 +0300

    readme : add June roadmap

commit 35a84916fb029905c44746127026079268216e7a
Author: Willy Tarreau <w@1wt.eu>
Date:   Wed Jun 7 04:10:17 2023 +0200

    main: add the possibility to open the prompt cache read-only (#1640)
    
    The prompt cache constitutes a nice speed up when using the same prompt
    prefix across multiple evaluations, but when using it, it will also be
    updated, which is not always desirable. One use case is to have a large
    prompt containing some context and usage rules, and a second part
    containing variable data of the problem being studied. In this case it's
    desirable to be able to save the first part once, and to always reuse it
    as-is without updating it with the second part.
    
    The new argument --prompt-cache-ro enables this read-only mode on the
    prompt cache. The prompt's contents that match the cache are loaded
    from the cache but the rest is not modified. This allowed to reduce a
    total analysis time from 112s to 49.7s here, without having to backup
    and restore a copy of the prompt, which takes significant time at 500
    MB.
    
    Signed-off-by: Willy Tarreau <w@1wt.eu>

commit 2d7bf110edd8c49209401a16132052cba706ffd0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 6 22:54:39 2023 +0300

    llama : fix vram_scratch var

commit 2a4e41a086ce80da68c402457c75c77e52dcc698
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 6 22:41:53 2023 +0300

    llama : fix compile warnings

commit 17366df842e358768c0df7024484fffecfc7865b
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Jun 6 21:33:23 2023 +0200

    Multi GPU support, CUDA refactor, CUDA scratch buffer (#1703)
    
    * CUDA multi GPU + scratch
    
    ggml_cuda_compute_forward
    
    Tensor parallelism
    
    ggml_cuda_add
    
    ggml_cuda_rms_norm
    
    ggml_cuda_silu
    
    CUDA scratch buffer
    
    --main-gpu CLI option

commit 44f906e8537fcec965e312d621c80556d6aa9bec
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 6 20:16:57 2023 +0300

    metal : add f16 support

commit d5b111f53d14972669eb52055f9df2567663ad8b
Author: LostRuins <39025047+LostRuins@users.noreply.github.com>
Date:   Wed Jun 7 01:00:01 2023 +0800

    Clblast fixes + enhancements to save VRAM and offload more layers (#1675)
    
    * Use events instead of clFinish, where possible
    
    * OpenCL: Don't load gpu layers into RAM, add mul_f32 kernel
    
    * Reduce queueing overhead for contiguous tensors by using single mul kernel call
    
    * Adapt to #1612 cl_mem malloc changes
    
    * Reduce code duplication between cuda and opencl branches
    
    * Improve implementation
    
    * Clblast fixes + enhancements to save VRAM:
    
    1. Change all Clblast buffers to CL_MEM_READ_WRITE, as the pool malloc currently doesn't properly handle them.
    2. When recycling buffers in pool malloc, always assign the SMALLEST available buffer that fits, instead of the FIRST available buffer
    3. When failing to recycle a buffer in pool malloc (all too small), instead recycle the largest available free buffer by resizing it.
    
    * change max value size_t to use limits
    
    * removed flags from the CL pool malloc, apply code tidying suggestions.

commit 2d43387dafe9c60f15f57aa23ee0b37864b98b32
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 6 10:18:03 2023 +0300

    ggml : fix builds, add ggml-quants-k.o (close #1712, close #1710)

commit 7ad7750c5c9f6bcea73a1895ffc57e7d21f2ab95
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 6 09:55:10 2023 +0300

    gitignore : add .clang-tidy

commit 7a74dee6b4e0e80862191141c0037abe28967d5c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 6 09:39:38 2023 +0300

    llama : temporary disable Q6_K output quantization (#1711)

commit 590250f7a9847bc9c83aa063dbaac8fa0fea27c8
Author: Spencer Sutton <spencersutton@users.noreply.github.com>
Date:   Mon Jun 5 23:28:17 2023 -0400

    metal : add checks for buffer size (#1706)
    
    Co-authored-by: Spencer Sutton <Spencer.Sutton@precisely.com>

commit f4c55d3bd7e124b101bc974cbbf0e0dbbc32d5a3
Author: Yuval Peled <31162840+Yuval-Peled@users.noreply.github.com>
Date:   Mon Jun 5 23:32:36 2023 +0300

    docs : add performance troubleshoot + example benchmark documentation (#1674)
    
    * test anchor link
    
    * test table
    
    * add benchmarks
    
    * Add performance troubleshoot & benchmark
    
    * add benchmarks
    
    * remove unneeded line
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit f1465624c2cbc8ee65b566e3d87f2af27796d4c4
Author: Foul-Tarnished <107711110+Foul-Tarnished@users.noreply.github.com>
Date:   Mon Jun 5 22:28:37 2023 +0200

    readme : fix typo (#1700)
    
    Fix a typo in a command in README.md

commit c2df36d60dc0ff1576541b965d751eadbacbeada
Author: mgroeber9110 <45620825+mgroeber9110@users.noreply.github.com>
Date:   Mon Jun 5 22:24:29 2023 +0200

    llama : consistently catch and throw only exceptions deriving from std::exception (#1599)
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 9d0693bce38013364b1042568d9083353bfff48f
Author: kiltyj <kiltyj@gmail.com>
Date:   Mon Jun 5 13:24:04 2023 -0700

    metal : use shared buffers between CPU and GPU (#1696)
    
    * Use MTLDevice.newBufferWithBytesNoCopy to share buffers between CPU and GPU
    
    * Page-align buffers used by Metal
    
    * Remove trailing whitespace
    
    * Only import unistd.h for Metal builds
    
    * metal : remove unnecessary copies
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit efe05076323f5c6bafece109e21cce046f5e4b07
Author: grahameth <96447521+grahameth@users.noreply.github.com>
Date:   Mon Jun 5 22:11:49 2023 +0200

    ggml : fix internal overflow in ggml_time_us on Windows (#1702)
    
    Co-authored-by: grahameth <->

commit e7fe66e670537990ccc075cce9286df88bba052a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 5 23:05:05 2023 +0300

    ci : disable auto tidy (#1705)

commit 99009e72f8072fa552eb02efee436be596c71cdd
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jun 5 22:56:18 2023 +0300

    ggml : add SOTA 2,3,4,5,6 bit k-quantizations (#1684)
    
    * Starting to add k-quantization to ggml
    
    I think it is better to have quantization separate from
    ggml. For now just adding the k-quants there, but it would be
    better to also factor out the existing ggml quantizations.
    
    * Adding Q3_K and Q8_K (de)-quantization
    
    * Q3_K now working on CUDA and AVX2/scalar
    
    CUDA is not ideal - ~50% slower than Q4_0 for
    single token prediction, about the same in batch
    mode (perplexity). CPU single token is ~55 ms
    (on Ryzen 7950X).
    
    * Some improvement for Q3_K on CUDA
    
    It is now ~22.5 ms/token on my GPU, so ~30% slower than Q4_0.
    
    * Some more CUDA optimizations for Q3_K
    
    Single token is now 20.5 ms/token (~20% slower than Q4_0).
    Perplexity is on par with Q4_0.
    
    * Adding Q4_K - scalar, AVX2, CUDA
    
    Performance is the same or perhaps very slightly better than Q4_0 on the CPU.
    On the GPU, single token prediction is ~10% better than Q4_0,
    batch mode (perplexity is about the same).
    
    * Adding Q6_K - scalar, AVX2, CUDA
    
    Performance is ~40% lower compared to Q4_K on the CPU.
    This is to be expected, considering that we are memory bound
    on the CPU and the 6-bit model is ~44% larger than the 4-bit.
    On the GPU, single token prediction is ~6% lower than Q4_0,
    batch mode (perplexity) is even closer (but still slower).
    
    * Adding Q5_K - scalar, AVX2, CUDA
    
    Performance is ~20% lower compared to Q4_K on the CPU.
    This is to be expected, considering that we are memory bound
    on the CPU and the 5-bit model is ~22% larger than the 4-bit.
    On the GPU, single token prediction is about the same as Q4_0
    for both, single token and batch prediction.
    
    * Per convention, all QX_K quantizations use Q5_K for output.weight
    
    * Adding quantization mixes
    
    * Quantization mixes: didn't quite get what I wanted in the last commit
    
    * Q4_K dot product for ARM_NEON
    
    * Q6_K dot product for ARM_NEON
    
    * Q5_K dot product for ARM_NEON
    
    * Adding Q3_K dot for ARM_NEON
    
    It is 22% slower than Q4_K, despite the smaller model size.
    On x86_64, where we are memory bound, the Q3_K model is
    quite a bit faster than Q4_K.
    
    * A very slightly faster ARM_NEON Q3_K dot
    
    * Adding Q2_K - just CUDA for now
    
    Token prediction is pretty good - about 15.5 ms on a RTX 4080.
    Perplexity is about the same as Q4_K.
    
    * Adding scalar and AVX2 Q2_K dot
    
    * Adding ARM_NEON Q2_K dot
    
    About the same performance as Q4_K.
    
    * A slightly faster ARM_NEON Q2_K dot
    
    Single token prediction is now ~36 ms on M2 Max.
    The code is much simpler too.
    
    * Fixed bug in Q2_K CUDA dot product kernel
    
    Stranegly enough, for the few prompts I tried with the 7B model
    the responses looked perfectly reasonable. Only realized something
    is not quite right when I tried the larger models and started getting
    nonse back.
    
    In any case, Q2_K single token evaluation time on an RTX 4080 in a Ryzen7950X
    box iusing CUDA and model fully loaded on the GPU are
      ~15.5 ms for 7B, ~25.4 ms for 13B, and ~55.8 ms for 30B.
    The max number of layers that fit in VRAM for The 65B is 32.
    With that, we get ~330 ms per token, which is not that much faster
    than just running on the CPU (~470 ms per token).
    
    * Don't print zeros/NaNs when no count histogram has been collected
    
    * A 10% faster CUDA vector dot kernel for Q3_K
    
    Q3_K is now running at ~18.5 ms / token on CUDA,
    so the gap to Q4_0 is only 10%.
    It seems memory acccess pattern is more important for
    performance than the amount of computation the kernel
    does.
    
    * A slightly daster Q4_K AVX2 dot product
    
    For perplexity, where we are less memory bound, time per
    pass drops by ~5%. Barely measurable difference for single
    token prediction.
    
    * A slightly faster ARM_NEON A4_K dot product
    
    * Minor
    
    * Fix quantization error test
    
    We cannot possibly be expecting rmse < 0.002 for 2- and 3-bit
    quantization variants.
    
    * Fix docker build
    
    I have been sloppy with vector reinterpret casts on ARM_NEON.
    It seems clang is very forgiving in that regard.
    
    * Added forgotten ggml.o dependence on k_quants.h to the Makefile
    
    * Had unintentionally committed the Makefile with -Ofast enabled
    
    * ggml : rename k_quants -> ggml-quants-k, use lowercase in code
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 5220a991a5e92bddad9542267ab445a2c033681c
Author: Henri Vasserman <henv@hot.ee>
Date:   Mon Jun 5 13:43:08 2023 +0300

    Increase 3B scratch buffers. (#1698)
    
    The 128 MB was too optimistic.
    Too bad it is not dynamically computed.

commit d1f563a743a83dabc11e125d4a7d64189c16498c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 5 10:19:03 2023 +0300

    llama : fix Metal KV cache sync (close #1695)

commit 827f5eda91e5b7299848ee2c7179d873bdee0f7b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 4 23:38:19 2023 +0300

    readme : update hot topics

commit ecb217db4fcfa3880300ad08531a5fb6bb142d45
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 4 23:34:30 2023 +0300

    llama : Metal inference (#1642)
    
    * mtl : export the LLaMA computation graph
    
    * ci : disable temporary
    
    * mtl : adapt the MNIST example as starter
    
    * mtl : no need for mtl-export tool, add cli arg for main instead
    
    * mtl : export just a small part of the graph for now to make it easier
    
    * mtl : move MSL code into separate file for easy editing
    
    * mtl : initial get_rows_q4_0 kernel
    
    * mtl : confirmed get_rows_q4_0 is working correctly
    
    * mtl : add rms_norm kernel + confirm working
    
    * mtl : add mul kernel + confirm working
    
    * mtl : initial mul_mat Q4 kernel (wrong results)
    
    * mtl : mul_mat fixes (still wrong)
    
    * mtl : another mul_mat Q4 (still does not work)
    
    * mtl : working mul_mat q4
    
    * ggml : fix handling of "view" ops in ggml_graph_import()
    
    * mtl : add rope kernel
    
    * mtl : add reshape and transpose handling
    
    * ggml : store offset as opt arg for ggml_view_xd() operators
    
    * mtl : add cpy kernel + handle view ops
    
    * mtl : confirm f16 x f32 attention mul mat
    
    * mtl : add scale kernel
    
    * mtl : add diag_mask_inf kernel
    
    * mtl : fix soft_max kernel
    
    * ggml : update ggml_nbytes() to handle non-contiguous tensors
    
    * mtl : verify V tensor contents
    
    * mtl : add f32 -> f32 cpy kernel
    
    * mtl : add silu kernel
    
    * mtl : add non-broadcast mul kernel
    
    * mtl : full GPU inference of the computation graph
    
    * mtl : optimize rms_norm and soft_max kernels
    
    * mtl : add f16 mat x f32 vec multiplication kernel
    
    * mtl : fix bug in f16 x f32 mul mat + speed-up computation
    
    * mtl : faster mul_mat_q4_0_f32 kernel
    
    * mtl : fix kernel signature + roll inner loop
    
    * mtl : more threads for rms_norm + better timing
    
    * mtl : remove printfs from inner loop
    
    * mtl : simplify implementation
    
    * mtl : add save/load vocab to ggml file
    
    * mtl : plug Metal inference into llama.cpp (very quick-n-dirty)
    
    * mtl : make it work with main example
    
    Lots of hacks but at least now it generates text
    
    * mtl : preparing for merge
    
    * mtl : clean-up ggml mtl interface + suport scratch / inplace
    
    * mtl : remove temp / debug code
    
    * metal : final refactoring and simplification
    
    * Revert "ci : disable temporary"
    
    This reverts commit 98c267fc77fe811082f672538fc91bcfc9072d63.
    
    * metal : add comments
    
    * metal : clean-up stuff, fix typos
    
    * readme : add Metal instructions
    
    * readme : add example for main

commit dcb2ed48268e421baf25adc00d602dad0f415564
Author: 0cc4m <picard12@live.de>
Date:   Sun Jun 4 08:12:05 2023 +0200

    OpenCL: Fix duplication of layers in VRAM and RAM, add GPU mul kernel (#1653)
    
    * Use events instead of clFinish, where possible
    
    * OpenCL: Don't load gpu layers into RAM, add mul_f32 kernel
    
    * Reduce queueing overhead for contiguous tensors by using single mul kernel call
    
    * Adapt to #1612 cl_mem malloc changes
    
    * Reduce code duplication between cuda and opencl branches
    
    * Improve implementation

commit d8bd0013e8768aaa3dc9cfc1ff01499419d5348e
Author: Henri Vasserman <henv@hot.ee>
Date:   Sat Jun 3 16:35:20 2023 +0300

    Add info about CUDA_VISIBLE_DEVICES (#1682)

commit b5c85468a3eadf424420af5bf11c2353ff828cda
Author: Jiří Podivín <66251151+jpodivin@users.noreply.github.com>
Date:   Sat Jun 3 14:11:53 2023 +0200

    Docker: change to calling convert.py (#1641)
    
    Deprecation disclaimer was added to convert-pth-to-ggml.py

commit 136476e898fb96c302b0829ee3e79267ae12660f
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Sat Jun 3 07:28:45 2023 -0400

    Fix prompt cache saving and chat-persistent rollover (#1678)
    
    * Fix prompt cache saving and chat-persistent rollover (fixes #1670)
    
    * clang-tidy
    
    Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
    
    ---------
    
    Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>

commit ffb06a345e3a9e30d39aaa5b46a23201a74be6de
Author: Henri Vasserman <henv@hot.ee>
Date:   Tue May 30 21:24:22 2023 +0300

    OpenLLaMA 3B support (#1588)
    
    This adds support to llama.cpp to load the model.
    
    Currently missing are changes that are required from convert.py to convert the model correctly. It needs some changes to start reading the JSON configuration for HF models instead of deriving the values by guessing.
    
    Co-authored-by: FNsi <125447286+FNsi@users.noreply.github.com>

commit 7552ac586380f202b75b18aa216ecfefbd438d94
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 29 19:31:44 2023 +0300

    ggml : sync cgraph import / export API

commit 5d1830b99dfd85bb6279adb4dd94aa444afd5b5e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 29 19:30:49 2023 +0300

    ggml : fix bug in ggml_alibi

commit 248367605ead6fb7c36d2bfb1ebd8f00a23f7c71
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Mon May 29 05:13:40 2023 -0700

    Work around for recalculating logits in cached prompts (Fixes #1585) (#1609)
    
    * Work around for recalculating logits in cached prompts

commit 0e730dd23b0fb5f93dba574e0a48d9a69dc5dbae
Author: Jiří Podivín <66251151+jpodivin@users.noreply.github.com>
Date:   Mon May 29 06:45:50 2023 +0200

    Adding git in container package dependencies (#1621)
    
    Git added to build packages for version information in docker image
    
    Signed-off-by: Jiri Podivin <jpodivin@gmail.com>

commit 3b126f654fceb4f5f195a1c2e825bb18e101188c
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun May 28 21:01:02 2023 +0200

    LLAMA_DEBUG adds debug symbols (#1617)

commit 1b78ed20818b72306edc7208b9bfb69a1a0d3297
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sun May 28 11:48:57 2023 -0600

    Only show -ngl option when relevant + other doc/arg handling updates (#1625)
    
    1. Add a `LLAMA_SUPPORTS_GPU_OFFLOAD` define to `llama.h` (defined when compiled with CLBlast or cuBLAS)
    2. Update the argument handling in the common example code to only show the `-ngl`, `--n-gpu-layers` option when GPU offload is possible.
    3. Add an entry for the `-ngl`, `--n-gpu-layers` option to the `main` and `server` examples documentation
    4. Update `main` and `server` examples documentation to use the new style dash separator argument format
    5. Update the `server` example to use dash separators for its arguments and adds `-ngl` to `--help` (only shown when compiled with appropriate support). It will still support `--memory_f32` and `--ctx_size` for compatibility.
    6. Add a warning discouraging use of `--memory-f32` for the `main` and `server` examples `--help` text as well as documentation. Rationale: https://github.com/ggerganov/llama.cpp/discussions/1593#discussioncomment-6004356

commit 337aea11390221bc925e4acb1f603f1649af2735
Author: Vladimir Zorin <vladimir@deviant.guru>
Date:   Sun May 28 20:14:24 2023 +0300

    examples : add --alias option to gpt_params to set use friendly model name (#1614)

commit bb051d9723d628414b9e929e5264e23262a2f1b2
Author: Howard Su <howard0su@gmail.com>
Date:   Mon May 29 01:13:36 2023 +0800

    opencl : no need to allocate cl_mem on heap (#1612)

commit ca74884f6625b15ef69832f07fc60fe00db5f90c
Author: Howard Su <howard0su@gmail.com>
Date:   Mon May 29 01:09:56 2023 +0800

    opencl : use strstr to check if fp16 supported (#1611)
    
    * Use strstr to check if fp16 supported
    
    * Ensure ext_buffer is null terminated

commit a6704643b62243bc4b6bbcd727d63d44e01a1002
Author: apcameron <37645737+apcameron@users.noreply.github.com>
Date:   Sat May 27 21:03:25 2023 +0100

    ggml : add support for the RISCV architecture (#1616)

commit 0df7d63e5ba0ab8856476e121a03b985d6f15c9d
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat May 27 11:04:14 2023 -0600

    Include server in releases + other build system cleanups (#1610)
    
    Set `LLAMA_BUILD_SERVER` in workflow so the `server` example gets build. This currently only applies to Windows builds because it seems like only Windows binary artifacts are included in releases.
    
    Add `server` example target to `Makefile` (still uses `LLAMA_BUILD_SERVER` define and does not build by default)
    
    Fix issue where `vdot` binary wasn't removed when running `make clean`.
    
    Fix compile warnings in `server` example.
    
    Add `.hpp` files to trigger workflow (the server example has one).

commit 97c9b77c4fc5e2283755c4418759cfc5fc73ad05
Author: Henri Vasserman <henv@hot.ee>
Date:   Sat May 27 18:47:55 2023 +0300

    Add documentation about CLBlast (#1604)
    
    Installing, compiling and using.

commit 0ecb1bbbeb16e36a2ea7a5ce525c6c59ef74312b
Author: Henri Vasserman <henv@hot.ee>
Date:   Sat May 27 17:24:06 2023 +0300

    [CI] Fix openblas (#1613)
    
    * Fix OpenBLAS build
    
    * Fix `LLAMA_BLAS_VENDOR` CMake variable that should be a string and not a boolean.

commit 93618031c7ccdd949d976370f24953d261048575
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 27 16:19:56 2023 +0300

    ggml : add ggml_tensor_overhead()

commit 83c54e6da58f1970556741b143bd26e30b1f46af
Author: Henri Vasserman <henv@hot.ee>
Date:   Sat May 27 15:18:25 2023 +0300

    [CI] CLBlast: Fix directory name (#1606)

commit bdbda1b17afb78e8613d03c8210a57fac632397b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 27 12:22:05 2023 +0300

    ggml : sync ggml core (minor additions, e.g. ggml_get_tensor_by_name())

commit 66874d4fbcc7866377246efbcee938e8cc9c7d76
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Thu May 25 20:18:01 2023 -0600

    Some improvements to loading the session with --prompt-cache (#1550)
    
    Improvements to loading the session with `--prompt-cache` in the `main` example.
    
    1. Fix an issue where the `--seed` parameter was ignored when loading a cached prompt.
    2. When loading a cached prompt, you previously had to specify the saved prompt (or a prefix of it) again. This pull changes that behavior to default to the prompt that was cached if a prompt wasn't specified by the user.

commit 1fcdcc28b119a6608774d52de905931bd5f8a43d
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu May 25 23:07:29 2023 +0200

    cuda : performance optimizations (#1530)
    
    * xor hack
    
    * block y dim
    
    * loop unrolling
    
    * Fixed cmake LLAMA_CUDA_BY option
    
    * Removed hipblas compatibility code
    
    * Define GGML_CUDA_DMMV_BLOCK_Y if not defined
    
    * Fewer iters, more ops per iter
    
    * Renamed DMMV X/Y compilation options

commit ac7876ac20124a15a44fd6317721ff1aa2538806
Author: Henri Vasserman <henv@hot.ee>
Date:   Wed May 24 10:30:09 2023 +0300

    Update CLBlast to 1.6.0 (#1580)
    
    * Update CLBlast to 1.6.0

commit c31bbe934b9666af42f32ce12d32cae9160e5dc4
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Wed May 24 02:24:01 2023 -0400

    readme : add docs for chat-persistent.sh (#1568)
    
    * readme : add docs for chat-persistent.sh
    
    * Update README.md

commit 1359b6aba55d5b0410f6adaa0aa2e49bbfd01d84
Author: Senemu <10880819+Senemu@users.noreply.github.com>
Date:   Wed May 24 06:16:22 2023 +0000

    chat-persistent.sh : use bracket expressions in grep (#1564)

commit 7d873811f31d4d8c909015c946a862c0089cda7d
Author: Maarten ter Huurne <maarten@treewalker.org>
Date:   Tue May 23 18:01:15 2023 +0200

    Fix handling of "invalid property" when creating OpenCL command queue (#1565)
    
    The `clCreateCommandQueue()` function will return the code
    `CL_INVALID_QUEUE_PROPERTIES` when passed unsupported properties,
    not `CL_INVALID_PROPERTY` as the original code was checking for.

commit 2e6cd4b02549e343bef3768e6b946f999c82e823
Author: 0cc4m <picard12@live.de>
Date:   Mon May 22 23:33:24 2023 +0200

    OpenCL Token Generation Acceleration (#1459)
    
    * Move back to C++ for OpenCL
    
    * Refactor OpenCL code to work more like the CUDA code, add missing functions
    
    * Deduplicate dequant kernels
    
    * Add OpenCL compile options
    
    * Use compile args for preprocessing constants
    
    * Restore default platform + device selection by id behavior
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    Co-authored-by: Henri Vasserman <henv@hot.ee>

commit 7e4ea5beff567f53be92f75f9089e6f11fa5dabd
Author: Steward Garcia <57494570+FSSRepo@users.noreply.github.com>
Date:   Sun May 21 11:51:18 2023 -0600

    examples : add server example with REST API (#1443)
    
    * Added httplib support
    
    * Added readme for server example
    
    * fixed some bugs
    
    * Fix the build error on Macbook
    
    * changed json11 to nlohmann-json
    
    * removed some whitespaces
    
    * remove trailing whitespace
    
    * added support custom prompts and more functions
    
    * some corrections and added as cmake option

commit 7780e4f479dc5af106287c164b8e186cd9b6215c
Author: Stefan Sydow <stefan@sydow.email>
Date:   Sun May 21 16:03:44 2023 +0200

    make : .PHONY clean (#1553)

commit 265db9834e761b7c8210ea1888117efcd3262f52
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun May 21 11:56:23 2023 +0300

    ggml : output 3d sizes in ggml_graph_dump_dot()

commit fab49c685e09d95942de34e3eadd72f880de21d5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 20 20:00:41 2023 +0300

    ggml : update WASM SIMD

commit b8ee340abe4a46143eb8cc4a135b976856c9136c
Author: Zenix <zenixls2@gmail.com>
Date:   Sat May 20 23:58:31 2023 +0900

    feature : support blis and other blas implementation  (#1536)
    
    * feature: add blis support
    
    * feature: allow all BLA_VENDOR to be assigned in cmake arguments. align with whisper.cpp pr 927
    
    * fix: version detection for BLA_SIZEOF_INTEGER, recover min version of cmake
    
    * Fix typo in INTEGER
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Fix: blas changes on ci
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 9ecb30f9594f222d8318fb1e803a4c363b0c39e5
Author: Henri Vasserman <henv@hot.ee>
Date:   Sat May 20 17:57:39 2023 +0300

    OpenCL: Fixes for older devices. (#1435)
    
    * Remove `constant`
    
    * Rewrite platform and device selection
    
    * Fix Q8_0

commit 29cf5596fe0c37213f9b74e80d8f631193a93f0f
Author: Juuso Alasuutari <juuso.alasuutari@gmail.com>
Date:   Sat May 20 15:58:15 2023 +0300

    llama : define magic numbers as integer constants (#1518) (#1520)
    
    The underlying representation of multibyte character literals is
    implementation-defined. This could, at least in principle, cause
    cross-build data export/import issues independent of endianness.
    
    Define magic numbers as integer literals to be on the safe side.
    
    Signed-off-by: Juuso Alasuutari <juuso.alasuutari@gmail.com>

commit 3de84b26066d95068409c1dc79bcc41c1eea2a03
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 20 15:34:45 2023 +0300

    ggml : add ggml_clamp() (#1539)
    
    * ggml : add ggml_clamp()
    
    * ggml : indentation

commit affc76edfdefa7b326f526e463cc65ff13fcfb92
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat May 20 14:19:28 2023 +0200

    cuda : loading models directly into VRAM, norm calculation on GPU, broadcasting for ggml_mul (#1483)
    
    * Broadcasting for ggml_mul
    
    * CUDA kernel for ggml_mul, norms in VRAM
    
    * GPU weights not in RAM, direct loading with cuFile
    
    * fixup! GPU weights not in RAM, direct loading with cuFile
    
    * fixup! GPU weights not in RAM, direct loading with cuFile
    
    * define default model path once, sync path with readme (#1366)
    
    * ~7% faster Q5_1 AVX2 code (#1477)
    
    * convert.py: Support models which are stored in a single pytorch_model.bin (#1469)
    
    * Support models in a single pytorch_model.bin
    
    * Remove spurious line with typo
    
    * benchmark-matmul: Print the average of the test results (#1490)
    
    * Remove unused n_parts parameter (#1509)
    
    * Fixes #1511 lambda issue for w64devkit (mingw) (#1513)
    
    * Fix for w64devkit and mingw
    
    * make kv_f16 the default for api users (#1517)
    
    * minor : fix compile warnings
    
    * readme : adds WizardLM to the list of supported models (#1485)
    
    * main : make reverse prompt option act as a stop token in non-interactive mode (#1032)
    
    * Make reverse prompt option act as a stop token in non-interactive scenarios
    
    * Making requested review changes
    
    * Update gpt_params_parse and fix a merge error
    
    * Revert "Update gpt_params_parse and fix a merge error"
    
    This reverts commit 2bb2ff1748513591ad45b175a75ed1d8089d84c8.
    
    * Update gpt_params_parse and fix a merge error take 2
    
    * examples : add persistent chat (#1495)
    
    * examples : add persistent chat
    
    * examples : fix whitespace
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * tests : add missing header
    
    * ggml : use F16 instead of F32 in Q4_0, Q4_1, Q8_0 (#1508)
    
    * ggml : use F16 instead of F32 in Q4_0, Q4_1 and Q8_0
    
    * llama : bump LLAMA_FILE_VERSION to 3
    
    * cuda : update Q4 and Q8 dequantize kernels
    
    * ggml : fix AVX dot products
    
    * readme : update performance table + hot topics
    
    * ggml : fix scalar implementation of Q4_1 dot
    
    * llama : fix compile warnings in llama_set_state_data()
    
    * llama : fix name shadowing and C4146 (#1526)
    
    * Fix name shadowing and C4146
    
    * Fix if macros not using defined when required
    
    * Update llama-util.h
    
    Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
    
    * Update llama-util.h
    
    Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
    
    * Code style
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Fix for mingw (#1462)
    
    * llama : add llama_init_backend() API (close #1527)
    
    * feature : add blis and other BLAS implementation support (#1502)
    
    * feature: add blis support
    
    * feature: allow all BLA_VENDOR to be assigned in cmake arguments. align with whisper.cpp pr 927
    
    * fix: version detection for BLA_SIZEOF_INTEGER, recover min version of cmake
    
    * Fix typo in INTEGER
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Revert "feature : add blis and other BLAS implementation support (#1502)"
    
    This reverts commit 07e9ace0f9da424d82e75df969642522880feb92.
    
    * GPU weights not in RAM, direct loading with cuFile
    
    * llama : code style fixes + progress print fix
    
    * ggml : ggml_mul better broadcast support
    
    * cmake : workarounds for cufile when CMake version < 3.25
    
    * gg rebase fixup
    
    * Loop in llama.cpp, fixed progress callback
    
    * Attempt clang-tidy fix
    
    * llama : fix vram size computation
    
    * Add forgotten fclose()
    
    ---------
    
    Co-authored-by: András Salamon <ott2@users.noreply.github.com>
    Co-authored-by: Ilya Kurdyukov <59548320+ilyakurdyukov@users.noreply.github.com>
    Co-authored-by: Tom Jobbins <784313+TheBloke@users.noreply.github.com>
    Co-authored-by: rankaiyx <rankaiyx@rankaiyx.com>
    Co-authored-by: Stephan Walter <stephan@walter.name>
    Co-authored-by: DannyDaemonic <DannyDaemonic@gmail.com>
    Co-authored-by: Erik Scholz <Green-Sky@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: David Kennedy <dakennedyd@gmail.com>
    Co-authored-by: Jason McCartney <jmac@theroot.org>
    Co-authored-by: Evan Jones <evan.q.jones@gmail.com>
    Co-authored-by: Maxime <672982+maximegmd@users.noreply.github.com>
    Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
    Co-authored-by: Zenix <zenixls2@gmail.com>

commit ea600071cb005267e9e8f2629c1e406dd5fde083
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 20 12:03:48 2023 +0300

    Revert "feature : add blis and other BLAS implementation support (#1502)"
    
    This reverts commit 07e9ace0f9da424d82e75df969642522880feb92.

commit 07e9ace0f9da424d82e75df969642522880feb92
Author: Zenix <zenixls2@gmail.com>
Date:   Sat May 20 18:02:48 2023 +0900

    feature : add blis and other BLAS implementation support (#1502)
    
    * feature: add blis support
    
    * feature: allow all BLA_VENDOR to be assigned in cmake arguments. align with whisper.cpp pr 927
    
    * fix: version detection for BLA_SIZEOF_INTEGER, recover min version of cmake
    
    * Fix typo in INTEGER
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit ec2e10c4443209da56b431b24dd0845b60e757fb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 20 11:06:11 2023 +0300

    llama : add llama_init_backend() API (close #1527)

commit d2c59b8ba498ab01e65203dde6fe95236d20f6e7
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Sat May 20 00:40:02 2023 -0700

    Fix for mingw (#1462)

commit 503db28849d8641d66244385e7e9649608a2e4d0
Author: Maxime <672982+maximegmd@users.noreply.github.com>
Date:   Sat May 20 09:22:37 2023 +0200

    llama : fix name shadowing and C4146 (#1526)
    
    * Fix name shadowing and C4146
    
    * Fix if macros not using defined when required
    
    * Update llama-util.h
    
    Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
    
    * Update llama-util.h
    
    Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
    
    * Code style
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 8a203f9fa1b24e010be8f35ebbbd6786293684cb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 20 10:14:31 2023 +0300

    llama : fix compile warnings in llama_set_state_data()

commit 4fd3e29297e3246a7be291932c115636fadb0f52
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 20 10:13:19 2023 +0300

    ggml : fix scalar implementation of Q4_1 dot

commit 2d5db48371052087a83974abda3767d1aedec598
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 19 22:17:18 2023 +0300

    ggml : use F16 instead of F32 in Q4_0, Q4_1, Q8_0 (#1508)
    
    * ggml : use F16 instead of F32 in Q4_0, Q4_1 and Q8_0
    
    * llama : bump LLAMA_FILE_VERSION to 3
    
    * cuda : update Q4 and Q8 dequantize kernels
    
    * ggml : fix AVX dot products
    
    * readme : update performance table + hot topics

commit 6986c7835adc13ba3f9d933b95671bb1f3984dc6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 19 21:17:28 2023 +0300

    tests : add missing header

commit 943e6081cc939df7584f8f0ab7057a39c2ef3271
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Fri May 19 13:39:51 2023 -0400

    examples : add persistent chat (#1495)
    
    * examples : add persistent chat
    
    * examples : fix whitespace
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 7694b52b9a206b93d59139c3c7c9b55da0f5aa59
Author: Jason McCartney <jmac@theroot.org>
Date:   Fri May 19 10:24:59 2023 -0700

    main : make reverse prompt option act as a stop token in non-interactive mode (#1032)
    
    * Make reverse prompt option act as a stop token in non-interactive scenarios
    
    * Making requested review changes
    
    * Update gpt_params_parse and fix a merge error
    
    * Revert "Update gpt_params_parse and fix a merge error"
    
    This reverts commit 2bb2ff1748513591ad45b175a75ed1d8089d84c8.
    
    * Update gpt_params_parse and fix a merge error take 2

commit 79e3efb0e97b65b6cc72cd9ee970fa8189ad79a4
Author: David Kennedy <dakennedyd@gmail.com>
Date:   Fri May 19 13:16:30 2023 -0400

    readme : adds WizardLM to the list of supported models (#1485)

commit 4b7e245adf63db675c3daab4a9bfddd451ef4097
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 19 20:14:51 2023 +0300

    minor : fix compile warnings

commit 5ea43392731040b454c293123839b90e159cbb99
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Thu May 18 19:31:01 2023 +0200

    make kv_f16 the default for api users (#1517)

commit ee9654138ab0ae5f138f4abddf56ca234ea3c352
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Thu May 18 10:30:40 2023 -0700

    Fixes #1511 lambda issue for w64devkit (mingw) (#1513)
    
    * Fix for w64devkit and mingw

commit dc271c52ed65e7c8dfcbaaf84dabb1f788e4f3d0
Author: Stephan Walter <stephan@walter.name>
Date:   Wed May 17 22:12:01 2023 +0000

    Remove unused n_parts parameter (#1509)

commit c238b5873a1ea496db03ffcfe124c9d0d83afbc6
Author: rankaiyx <rankaiyx@rankaiyx.com>
Date:   Wed May 17 22:47:58 2023 +0800

    benchmark-matmul: Print the average of the test results (#1490)

commit 2b2646931bd2a2eb3e21c6f3733cc0e090b2e24b
Author: Tom Jobbins <784313+TheBloke@users.noreply.github.com>
Date:   Tue May 16 23:04:35 2023 +0100

    convert.py: Support models which are stored in a single pytorch_model.bin (#1469)
    
    * Support models in a single pytorch_model.bin
    
    * Remove spurious line with typo

commit 42627421ece816e632e6a0d757fa75150c687f87
Author: Ilya Kurdyukov <59548320+ilyakurdyukov@users.noreply.github.com>
Date:   Wed May 17 01:36:47 2023 +0700

    ~7% faster Q5_1 AVX2 code (#1477)

commit 9560655409dc80771a9b19e838ff47c5c1df6483
Author: András Salamon <ott2@users.noreply.github.com>
Date:   Tue May 16 16:46:34 2023 +0100

    define default model path once, sync path with readme (#1366)

commit 2a5ee023ad3022bc0b505343394b9754587fb731
Author: sandyiscool <sandyiscool@gmail.com>
Date:   Tue May 16 14:00:15 2023 +0530

    Add alternate include path for openblas (#1476)
    
    In some linux distributions (fedora, for example), the include path for openblas is located at '/usr/local/include'

commit 63d20469b85467c5729cc9a97bd44cc3da63423f
Author: zrm <trustiosity.zrm@gmail.com>
Date:   Sun May 14 22:25:42 2023 -0400

    fix get_num_physical_cores() (#1436)
    
    * fix get_num_physical_cores()
    had been broken on complex topologies because "cpu cores" in /proc/cpuinfo is per-"physical id"
    
    * Add spaces to maintain consistent formatting
    
    ---------
    
    Co-authored-by: slaren <ddevesa@gmail.com>

commit b5c9295eef2b56e307393b35b3a923e3518d226e
Author: slaren <slarengh@gmail.com>
Date:   Sun May 14 22:46:00 2023 +0200

    benchmark-matmul: fix clang-tidy issues, report results in GFLOPS (#1458)
    
    * benchmark-matmul: fix command line parsing, replace macros with functions, report results in GFLOPS

commit eb363627fda5f47de8ab5e9be8abd426049d00df
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun May 14 20:53:23 2023 +0200

    cuda : deduplicated dequantization code (#1453)

commit 79b2d5b69d80be0bf29312fb9a95854876b0a8a5
Author: xaedes <xaedes@gmail.com>
Date:   Sun May 14 17:55:02 2023 +0200

    ggml : alternative fix for race condition bug in non-inplace ggml_compute_forward_diag_mask_f32 (#1454)
    
    * fix race condition bug in non-inplace ggml_compute_forward_diag_mask_f32
    
    memcpy needs to be synchronized across threads to avoid race conditions.
    => do it in INIT phase
    
    * remove trailing whitespace
    
    * Update ggml.c
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 13c351ad7292c5b5ab35db25c7a4f993e75d9cfd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun May 14 18:22:50 2023 +0300

    ggml : various fixes (#1450)
    
    - `ggml_rope()`
    - `ggml_diag_mask_inf()` multi-threaded
    - compatibility with scratch buffers

commit 60f8c361ca26328ef8523dfb08077fe2f1034490
Author: katsu560 <118887472+katsu560@users.noreply.github.com>
Date:   Sun May 14 19:03:51 2023 +0900

    ggml : add AVX support based on AVX2 code (#1430)

commit 601a033475645370483973817d987928ea95f36c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun May 14 10:20:19 2023 +0300

    ggml : add GGML_QNT_VERSION to track quantization format changes
    
    https://github.com/ggerganov/ggml/issues/150#issuecomment-1546625668

commit 08737ef720f0510c7ec2aa84d7f70c691073c35d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 13 17:40:58 2023 +0300

    cuda : fix convert function (#1412)

commit bda4d7c215aa16b2a78e522521dfc0e1c2e8b194
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 13 17:25:09 2023 +0300

    make : fix PERF build with cuBLAS

commit 5a5aeb1e91009c72bf816400b758bb8a305616d7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 13 16:55:14 2023 +0300

    llama : fix unused warning

commit 66841fdb0eaf0cc210757cc7f683d0f4eebadc21
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 13 16:48:03 2023 +0300

    ggml : multi-thread mul and diag_mask ops (#1428)

commit 905d87b70aa189623d500a28602d7a3a755a4769
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat May 13 15:38:36 2023 +0200

    ggml : GPU-accelerated token generation (#1412)
    
    * CUDA kernel for q4_0 dequant. + mat. vec. mult.
    
    * Added q4_1 via template
    
    * Added missing __syncthreads();
    
    * --gpu_layers -> --gpu-layers
    
    * Shorter dequantize_mul_mat_vec line
    
    * q5_0 dequantize_mul_mat kernel
    
    * More readable dequantize_mul_mat_vec logic
    
    * dequantize_mul_mat_vec kernels for q5_1, q8_0, f16
    
    * llama : offload "output" tensor to GPU too + coding style fixes
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit f954edda935a70a14cf0cc45ecc7fe7d60cf3e4b
Author: xaedes <xaedes@gmail.com>
Date:   Sat May 13 14:56:40 2023 +0200

    ggml : implement backward pass for llama + small training-llama-from-scratch example (#1360)
    
    * implement 8 of 14 missing backward pass operations used by llama
    
    - GGML_OP_ADD_AT
    - GGML_OP_CPY
    - GGML_OP_MUL_MAT (src0.grad)
    - GGML_OP_PERMUTE
    - GGML_OP_RESHAPE
    - GGML_OP_SCALE
    - GGML_OP_TRANSPOSE
    - GGML_OP_VIEW
    
    implement additional ggml operation GGML_OP_ADD_AT, which is necessary for backward pass of GGML_OP_VIEW.
    
    this operation adds src1 to src0 with data offset, i.e. to view(src0, ..., offset).
    the values are return in a tensor size of src0. values outside of [data+offset:data+offset+nbytes(src1)] are just the original values from src0.
    
    still missing backward passes for llama:
    
    - GGML_OP_DIAG_MASK_INF
    - GGML_OP_GET_ROWS
    - GGML_OP_RMS_NORM
    - GGML_OP_ROPE
    - GGML_OP_SILU
    - GGML_OP_SOFT_MAX
    
    * implement 5 of 6 missing backward pass operations used by llama
    
    - GGML_OP_DIAG_MASK_INF
    - GGML_OP_GET_ROWS
    - GGML_OP_RMS_NORM
    - GGML_OP_SILU
    - GGML_OP_SOFT_MAX
    
    add necessary ggml operations GGML_OP_ADD1, GGML_OP_SILU_BACK, GGML_OP_RMS_NORM_BACK, GGML_OP_DIAG_MASK_ZERO, and GGML_OP_ROPE_BACK
    
    GGML_OP_ADD1 is necessary to add a scalar value in the backward pass of GGML_OP_SOFT_MAX
    GGML_OP_ADD1 could also be replaced by using GGML_OP_ADD and GGML_OP_REPEAT, but the performance would be worse. additionally GGML_OP_REPEAT will return unexpected value when the the input to GGML_OP_SOFT_MAX contains only a single scalar. in this case GGML_OP_REPEAT will not return the value that should be repeated (src1) but the value which shape the result should take (src0). So in this case it can not replace GGML_OP_ADD1.
    
    GGML_OP_SILU_BACK, GGML_OP_RMS_NORM_BACK and GGML_OP_ROPE_BACK are necessary for backward pass of GGML_OP_SILU, GGML_OP_RMS_NORM and GGML_OP_ROPE. The backward pass for these functions cannot be easily composed of existing operations. Since the backward pass builds a computation graph we need operations forward pass implementations of the the required backward passes. Sounds a bit confusing at first, I know...
    
    GGML_OP_DIAG_MASK_ZERO is necessary for backward pass of GGML_OP_DIAG_MASK_INF.
    
    Some operations where previously inplace-only. for backward pass there needs to be non-inplace variants.
    staying consistent with other operations that have non-inplace and inplace variants, the operations are changed to non-inplace and
    functions with "_inplace" are added which are inplace.
    in llama we need to call the inplace variants so that it is implemented as before.
    for llama backward pass we need to use the non-inplace variants.
    
    still not completely implemented backward passes for llama:
    
    - GGML_OP_ROPE: needs forward pass for GGML_OP_ROPE_BACK
    - GGML_OP_GET_ROWS: only necessary for tokenizer
    
    * norm & rms_norm can not be threaded:
    
    after investigation rms norm for quite some time I come to the conclusion that neither norm, nor rms_norm can be threaded, because we need mean over all items, not just of the slices each thread sees.
    
    * remove already resolved TODO
    
    * implement backward pass of ggml_rope and ggml_rope_back
    
    * implement backward pass for ggml_get_rows and for new operation ggml_get_rows_back
    
    * add test-grad0.c
    
    * use GGML_PRINT_DEBUG for debug messages which will otherwise flood the console
    
    * test both gradients of mul_mat
    
    * disable graph dot export as it floods console
    
    * bug fixes for silu_back
    
    * successfully test silu backward
    
    * bug fix for scale backward pass
    
    use sum instead of mean for gradient of scalar scale parameter
    
    * successfully test scale backward
    
    * improve performance of sum backward pass
    
    use add1(x,y) instead of add(x,repeat(y,x))
    
    * improve performance of sqr backward pass
    
    use scale(x,y) instead of mul(x,repeat(y,x))
    
    * successfully test rope backward
    
    * bug fix for cpy backward pass
    
    * successfully test cpy backward
    
    * bug fix for reshape backward pass
    
    * successfully test reshape backward
    
    * add test-opt.c
    
    this uses ggml_opt to train a,b for minimal e=sum(sqr(c - a*b)) for random initial a,b,c
    
    * correctly implement softmax backward pass using new operation ggml_diag
    
    ggml_diag constructs diagonal matrices with entries.
    ggml_diag(shape[a,1,c,d]) -> shape[a,a,c,d]
    
    * successfully test soft_max backward
    
    * align shape annotations
    
    * add shape annotations for llama
    
    * de-duplicate ggml_forward_dup code taking care of contiguous tensors of same type.
    
    with this we can duplicate tensor of any typ as long as they are contiguous.
    
    * fix ggml_compute_forward_dup_same_cont for when nelements < nthreads
    
    when more threads are used than elements exist ie1 was less than ie0, resulting in invalid negative byte count argument in memcpy
    
    * bug fix for add_at forward
    
    required for view backward pass
    
    src0 values must be copied to dst, because during addition we don't touch all dst elements in contrast to the normal add function.
    
    * successfully test view backward
    
    * minor code format improvement
    
    * fix ggml_forward_add functions to work correctly with transposed tensors
    
    uses the same logic as in ggml_compute_forward_add_q_f32, but make it consistent across all ggml_compute_forward_add_... functions.
    this also slightly changes the mem access pattern of the different threads to works as in ggml_compute_forward_add_q_f32.
    
    * fix ggml_forward_add1 functions to work correctly with transposed tensors
    
    uses the same logic as in ggml_compute_forward_add1_q_f32, but make it consistent across all ggml_compute_forward_add1_... functions.
    this also slightly changes the mem access pattern of the different threads to works as in ggml_compute_forward_add1_q_f32.
    
    * test-grad0.c : add print_elements to help with debugging
    
    * successfully test permute backward
    
    * some minor test-grad0 fixes
    
    * fix sub, mul and div functions to work correctly with transposed tensors
    
    uses the same logic as in add
    
    * implement ggml_cont backward pass
    
    * successfully test transpose backward and permute for all permutations
    
    also test sub, mul and div up to max n_dims
    
    * test-grad0.c add TODO for view_2d and view_3d
    
    add_at (required for view backward pass) is a bit tricky for n_dims > 1.
    
    * fix comments
    
    * successfully test diag_mask_inf and diag_mask_zero backward
    
    * test-grad0 : fix test for div
    
    nargs and ndims was swapped, corrupting the stack
    
    * fix diag_mask to work with non-inplace input
    
    * move dup call into the actual add_at functions
    
    * fix get rows backward pass
    
    * successfully test get_rows backward
    
    * fix view backward pass
    
    add nb parameters to add_at like in view.
    together with offset they define how to view dst and src0 during the add_at operation.
    
    * successfully test backward pass of view_1d, view_2d and view_3d
    
    * fix backward pass for rms_norm
    
    I would have used formulas from other frameworks, but they differed so I could not decide which is correct.
    Instead it was derived here in comment using manual forward-backward automatic differention of rms_norm and simplification.
    
    * successfully test backward pass of rms_norm
    
    some tests may fail when gradients are large.
    could not find a satisfying configuration to check for abs error and relative error that passes all tests while still actually testing the results with tight enough error bounds.
    when looking at the values the "failed" tests look actually ok. for example:
    
    rms_norm: ndims=2, i=0, k=2, x0=0.000153, xm=0.000053, xp=0.000253, f0=0.278594, f1=0.086213, g0=961.905457, g1=966.064941, eps=0.000100, error_abs=4.159485, error_rel=0.004324
    
    it is due to the test logic in check_gradients that they fail.
    
    * add todos for llama backward pass
    
    - implementation for ADD1 backward pass should probably use sum instead of mean (but this backward pass is not required)
    - repeat is not yet tested and looks like it only works for single element src0 inputs.
    
    * add operation ggml_sum_rows
    
    ggml_sum_rows(shape[a,b,c,d]) -> shape[1,b,c,d]
    
    * add missing GGML_OP_SUM_ROWS
    
    * fix backward pass for repeat
    
    requires ggml_sum_rows
    
    * successfully test backward pass of repeat
    
    * update quantization types in switch-case of add_at and add1
    
    * add baby-llama example training a very small llama model from scratch to output a sinusoidal wave.
    
    had to increase maximum number of optimization parameters to train from scratch.
    
    * fix softmax in baby-llama example
    
    * switching from training with adam to lbfgs produces much better results in the baby-llama example
    
    * train with two examples, creating new tensors each time..
    
    * fix bug when using ggml_opt to optimize params in one context and use a renewable context for eval and opt
    
    when not keeping gradients of model parameters they are overwritten by tensors created by opt, which may be invalid after opt context is renewed.
    so we need to keep the original gradients and make dups for opt
    
    * train on multiple examples, generate & print tokens with trained model afterwards
    
    ctx0 for evaluation and optimization is renewed for each sample
    
    * add ggml_reshape_1d, ggml_reshape_4d and ggml_view_4d
    
    * fix soft_max backward pass for input->ne[1] != 1
    
    * add ggml_log operation necessary for cross entropy loss
    
    * add test for ggml_log gradients
    
    * implement backward pass for ggml_sum_rows, necessary for cross entropy loss
    
    * implement ggml_repeat support for rank > 2 tensors
    
    * add test for ggml_sum_rows gradients
    
    * fix training get_example_targets
    
    predict the next token, not the current token!
    
    * add square_error_loss and cross_entropy_loss functions
    
    * optimize loss over multiple samples
    
    this increases computation graph, need parallel batched forward for more efficiency.
    
    * fix backward pass for add_at and change arguments to have same order as in view
    
    * add ggml_set(ctx, a, b) to set b in view of a and return modified a
    
    necessary to set values into kv_self cache and properly propagate the gradients
    
    * fix kv_self gradients for training
    
    use ggml_set instead of ggml_cpy to set kv_self cache with properly propagating gradients
    
    * replace inplace operations for training with copying operations to allow gradient propagation
    
    * add GGML_ASSERT to catch ggml_rope and back value errors
    
    * add trainable lora-only model with all big matrices C split into A,B with A*B=C
    
    this is not a lora-finetune, but the whole model changed to have only low-rank "lora" matrices.
    
    training this instead of the normal model resulted in much worse results though...
    
    * vastly improve training results
    
    instead of logit targets 0 and 1 use -1 and +1.
    
    * shorten code using a variable
    
    * change name of GGML_OP_ADD_AT to GGML_OP_ACC
    
    * smaller default values for baby llama model parameters
    
    * update static assert of GGML_OP_COUNT
    
    * remove shape annotations in llama_eval_internal
    
    * revert disabling of threading for rms_norm and norm
    
    * rename print functions in baby-llama example
    
    * fix call to ggml_set_name
    
    * add missing include for strcmp, etc
    
    * remove trailing whitespace
    
    * reduce number of test-grad0 iterations
    
    avoid exceeding timeout of automated tests
    
    * remove busy loop that was used as sleep for slower sinus wave generation
    
    * disable slow tests grad0 and opt to avoid exceeding timeouts
    
    * c++ in baby-llama example
    
    use c++ includes instead of c includes
    use std::min, std::max instead of MIN, MAX macros
    
    * c++ in baby-llama example
    
    use c++ includes instead of c includes
    use std::min, std::max instead of MIN, MAX macros
    
    * ggml : fix compiler warnings + cosmetic changes
    
    * ggml : fix nullptr derefs in GGML_OP_CONT and GGML_OP_RESHAPE back
    
    * swap arguments to vDSP_vdiv call
    
    documentation for vDSP_vdiv states: "Note that B comes before A!"
    
    * swap arguments to vDSP_vdiv call
    
    documentation for vDSP_vdiv states: "Note that B comes before A!"
    
    * ggml : swap vDSP_vsub args as per documentation
    
    * add parallel batched forward function for baby-llama training
    
    * cleanup code for batched training
    
    * remove trailing whitespace
    
    * minor : fix compiler warnings + indentation style
    
    * ggml : fix null ptr deref in backward pass
    
    * ggml : remove Q4_2 remnants
    
    * ggml : fix clang-tidy warnings
    
    * baby-llama : couple of clang-tidy warnings
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit f048af0230ad5381bb20b9e3c1467ec6fc7debdf
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 13 11:54:33 2023 +0300

    ggml : sync alibi fix from ggml repo

commit ac0cd259d54be7e45ffa2c7b2508812cf4f83ccf
Author: 3ooabkhxtn <31479382+3ooabkhxtn@users.noreply.github.com>
Date:   Sat May 13 10:43:33 2023 +0200

    Adding SSE instructions to ggml_vec_dot_q4_0_q8_0 (#1413)

commit 0cd22e190aeaef867fa5db025b4d274f2fcfdcf6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 13 11:23:15 2023 +0300

    llama : fix various warnings

commit 6456a4eb9f9c1f4de8f6908ef100daa78802ad00
Author: Rinne <AsakusaRinne@gmail.com>
Date:   Sat May 13 15:24:20 2023 +0800

    embedding : remove unused code (#1426)

commit cdd5350892b1d4e521e930c77341f858fcfcd433
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 13 09:12:44 2023 +0300

    readme : update Q4_0 perplexities
    
    I think these were affected by the removal of the `round` during quantization

commit 738ace394a6f8cf0174e90a97185d9e512c0e200
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 13 09:08:52 2023 +0300

    llama : free ggml context in set / copy state data (close #1425)

commit 699b1ad7fe6f7b9e41d3cb41e61a8cc3ea5fc6b5
Author: Henri Vasserman <henv@hot.ee>
Date:   Sat May 13 09:01:15 2023 +0300

    opencl : fix kernels for the new formats (#1422)
    
    * Fix OpenCL kernels for the new formats
    
    * Fix Q5_0 alignment issues.

commit fb62f924336c9746da9976c6ab3c2e6460258d54
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 12 21:44:20 2023 +0300

    llama : fix --mtest option (close #1414)

commit 773ee249fb6c14f791ac39f6ec05536f40dedc54
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri May 12 16:34:55 2023 +0200

    CLI args use - instead of _, backwards compatible (#1416)

commit 553fd4d4b5f3314f338be8006f62a4fdf7670186
Author: slaren <slarengh@gmail.com>
Date:   Fri May 12 15:40:53 2023 +0200

    Add clang-tidy reviews to CI (#1407)

commit 089b1c93ba2b93bc9a605af293730a028fad2c4e
Author: Rinne <liu_yaohui1998@126.com>
Date:   Fri May 12 13:39:40 2023 +0800

    readme : add C#/.NET bindings repo (#1409)

commit b9fd7eee57df101d4a3e3eabc9fd6c2cb13c9ca1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 12 00:23:08 2023 +0300

    ggml : remove bit shuffling (#1405)
    
    * ggml : remove Q4_0 bit shufling (ARM NEON)
    
    * ggml : remove Q4_1 bit shuffling (ARM NEON + reference)
    
    * ggml : nibbles_from_floats() + bytes_from_nibbles() (ARM NEON)
    
    * ggml : remove Q4_2 bit shuffling (WIP, BROKEN)
    
    * ggml : remove Q5_0 bit shuffling (ARM NEON)
    
    * ggml : 2x faster scalar implementations
    
    * ggml : remove Q5_1 bit shuffling (ARM NEON + scalar)
    
    * ggml : simplify scalar dot
    
    * ggml : remove WASM SIMD bit shuffling + remove vzip for ARM 32-bit
    
    * ggml : fix Q4_1 quantization
    
    * ggml : update cuBLAS + normalize variable names
    
    * ggml : remove Q4_2 mode
    
    * ggml : minor formatting
    
    * ggml : fix Q5_0 quantization
    
    * scripts : add script for measuring the time per token
    
    * AVX implementations (#1370)
    
    * ggml : uniform 5th bit extraction
    
    * llama : produce error upon loading old model files
    
    * llama : fix model magic/version write
    
    * ggml : speed-up Q5_0 + Q5_1 at 4 threads
    
    * ggml : preserve old Q4 and Q5 formats
    
    * ggml : simplify Q8_1 - no need for low / high sums anymore
    
    * ggml : fix Q8_0 and Q8_1 rounding
    
    * Revert "AVX implementations (#1370)"
    
    This reverts commit 948d124837f9d287d8490f41338e0e4cceb0814f.
    
    * ggml : fix AVX2 implementation
    
    * sha : update hashes for 7B and 13B
    
    * readme : update timings + remove warning banner
    
    * llama : update v2 PR number to 1405
    
    * ggml : fix WASM comments
    
    * ggml : back to original bit order
    
    * readme : add note that Q4 and Q5 have been changed
    
    * llama : fix return for unknown version
    
    ---------
    
    Co-authored-by: Stephan Walter <stephan@walter.name>

commit b608b55a3ea8e4760c617418538465449175bdb8
Author: CRD716 <crd716@gmail.com>
Date:   Thu May 11 10:10:19 2023 -0500

    prompts : model agnostic DAN (#1304)
    
    * add model-agnostic dan prompt
    
    * quick readme update
    
    * save a token
    
    * Revert "quick readme update"
    
    This reverts commit 8dc342c069cbdca8ce63ad974becec6fc844e1e4.

commit cf348a60e0af3905acd1d297cb064b918265b7ac
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Wed May 10 11:37:14 2023 -0400

    main : add option to save full output to session (#1338)
    
    * main : add option to save full output to session
    
    * split behavior into --session and --prompt-cache
    
    * restore original implementation with new names
    
    * PR comments
    
    * move the check for incompatible parameters to gpt_params_parse
    
    * Fix whitespace
    
    Co-authored-by: DannyDaemonic <DannyDaemonic@gmail.com>
    
    ---------
    
    Co-authored-by: DannyDaemonic <DannyDaemonic@gmail.com>

commit e6a46b0ed1884c77267dc70693183e3b7164e0e0
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Tue May 9 10:53:28 2023 -0700

    Locale fix for Windows (#1379)

commit 9f8dbc4787f32cd9c13b5c647d497d13c1a06db2
Author: Sami Farin <3876865+Safari77@users.noreply.github.com>
Date:   Tue May 9 15:29:20 2023 +0300

    use pause asm insn in busyloop to run the CPU (13600K) 10 °C cooler (#1314)
    
    * use pause asm insn in busyloop to run the CPU (13600K) 10 °C cooler
    
    Tested with a 13B model.
    
    * use _mm_pause() in busyloop
    
    * use _mm_pause() in busyloop on x86_64 to reduce power consumption

commit 41654efea879bbdf4fd794e13335929d4cf0eb90
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Mon May 8 19:45:48 2023 -0700

    Interface improvements and `--multiline-input` (previously `--author-mode`) (#1040)
    
    * Interface improvements
    * Multiline input
    * Track character width
    * Works with all characters and control codes + Windows console fixes

commit 56551bc11f46b2716fdf61bb48ac28414889dc0a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 8 22:52:18 2023 +0300

    readme : add notice about upcoming breaking change

commit fe60904eef4b504685fa0406cb19864ae619fb4f
Author: AlpinDale <52078762+AlpinDale@users.noreply.github.com>
Date:   Mon May 8 21:03:30 2023 +0430

    readme : add TOC and Pygmalion instructions (#1359)

commit 003ba2fb4309e2339487564bd249e4fcc8d7ea01
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Mon May 8 16:48:21 2023 +0200

    llama : fix hparams shadow (#1367)
    
    fixes #1363

commit f9a6364912fd0463fddfdbc9ef9f79fdc281570d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 8 17:41:54 2023 +0300

    llama : require first token to be BOS (#1303)
    
    * llama : require first token to be BOS
    
    * scripts : add ppl-run-all.sh
    
    * perplexity : add BOS for each chunk
    
    * readme : update perplexity values after BOS fix
    
    * perplexity : add clarifying comments

commit 95078cc554fe03d4512363c7e4dec963f0047c72
Author: ubik2 <ubik2@users.noreply.github.com>
Date:   Mon May 8 04:54:26 2023 -0700

    convert: add ability to convert safetensors files (#1276)
    
    * when loading a safetensors file, ignore the metadata header
    * check for safetensors files first, and only use PyTorch versions when safetensors aren't available

commit 1f48b0abcfbd6cc99571e42348e0ec97e4be8b93
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon May 8 02:42:01 2023 +0200

    Documented CUDA reproducibility, added warning (#1346)

commit e1295513a48ae8254d8af5ec0250b56d6eaffefd
Author: Henri Vasserman <henv@hot.ee>
Date:   Sun May 7 14:20:09 2023 +0300

    CI: add Windows CLBlast and OpenBLAS builds (#1277)
    
    * Add OpenCL and CLBlast support
    
    * Add OpenBLAS support
    
    * Remove testing from matrix
    
    * change build name to 'clblast'

commit 1b0fd454650ef4d68a980e3225488b79e6e9af25
Author: swittk <switt1995@gmail.com>
Date:   Sun May 7 10:03:23 2023 +0700

    ggml : Allow usage of CLBlast alongside Accelerate.framework (#1336)
    
    Minor edit in ggml.c which originally would prevent OpenCL from loading completely if GGML_USE_ACCELERATE was defined.
    Minor speedup in prompt eval time.

commit 3924088512d9e12e90ed6dbf28a6c5712481d33e
Author: Jed Fox <git@jedfox.com>
Date:   Sat May 6 17:01:47 2023 -0400

    Remove default arguments from sampling functions (#1343)

commit 173d0e6419e8f8f3c1f4f13201b777f4c60629f3
Author: DaniAndTheWeb <57776841+DaniAndTheWeb@users.noreply.github.com>
Date:   Fri May 5 23:57:14 2023 +0200

    makefile: automatic Arch Linux detection (#1332)
    
    This commit is a port of a detection method used in koboldcpp's Makefile in order to automatically set the -lcblas option on Arch Linux

commit a3b85b28da84c67c3406807aef5e0457bcc4b00f
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Fri May 5 22:56:09 2023 +0200

    ci : add cublas to windows release (#1271)

commit 921dcee00a55d9aba3b3026d0509d31ac8386e2a
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Fri May 5 16:43:36 2023 +0200

    readme: add missing info (#1324)

commit 2d13786e91ec9fd28ddf737053822042a824da78
Author: Ionoclast Laboratories <brigham@ionoclast.com>
Date:   Fri May 5 08:18:21 2023 -0400

    Fix for OpenCL / clbast builds on macOS. (#1329)

commit a90e96b266873ebb5e947c9864b12193bdada0fb
Author: Benjamin Lecaillon <84293038+blecaillon@users.noreply.github.com>
Date:   Fri May 5 02:17:07 2023 +0200

    Convert.py @staticmethod (#1327)
    
    * Line 698 has one #staticmethod and should not
    
    otherwise throw error at unpickle.load() as not callable
    
    * Update convert.py
    
    ---------
    
    Co-authored-by: Ivan Stepanov <ivanstepanovftw@gmail.com>

commit 94c5652fc0f4d04ac54412c4d81e2ebcdafb6ede
Author: slaren <slarengh@gmail.com>
Date:   Fri May 5 00:58:56 2023 +0200

    quantize: make output filename optional, default to ggml-model-<ftype>.bin (#1301)

commit 34d9f22f44c42d345cc72c8f3aa4cb71c5df0acb
Author: Ivan Stepanov <ivanstepanovftw@gmail.com>
Date:   Thu May 4 19:56:27 2023 +0300

    Wrap exceptions in std::exception to verbose output on exception. (#1316)

commit d3e8093e9b5845514b049ede3b12728c8f013eba
Author: Ivan Stepanov <ivanstepanovftw@gmail.com>
Date:   Thu May 4 19:54:37 2023 +0300

    convert: support DT_BF16 tensors (#1309)
    
    
    Co-authored-by: Pavol Rusnak <pavol@rusnak.io>

commit 360cfe5bec852805b84eec799102fc6f45df9fef
Author: 44670 <44670@users.noreply.github.com>
Date:   Fri May 5 00:33:31 2023 +0800

    readme : add OpenBuddy link (#1321)

commit 2edbdb0f99336cb41f0995061c7602ed54beb863
Author: 44670 <44670@users.noreply.github.com>
Date:   Thu May 4 23:41:12 2023 +0800

    main : add --in-suffix option (#1318)
    
    * adding --in-suffix option
    
    * print input suffix before generation

commit 20fbf2a2a08d8edefe9b3435fa86f8b2f63f8588
Author: Ron Jailall <rojailal@gmail.com>
Date:   Thu May 4 11:05:59 2023 -0400

    ggml : change immintrin.h to intrin.h for compatibility (#1307)
    
    * change immintrin.h to intrin.h for compatibility
    
    Building on windows11 arm throws an error on this line. Seems like using intrin.h covers x86 and and arm
    
    * conditional def of intrin.h
    
    * fix typo in ggml.c

commit db1080876a62ec3bb4119d90b16e7dce7594b733
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Thu May 4 05:08:25 2023 -0700

    Only escape prompts when used with `-e` (#1311)

commit c65a7fbfa9c736416a25369cc05d356789df4c15
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Thu May 4 03:02:59 2023 -0700

    Update main's README.md with new features (#1296)

commit f647ce040ff06348d2ceaa5443a6a7a8b80c70c9
Author: Tomas <tom.tomas.36478119@gmail.com>
Date:   Thu May 4 17:02:30 2023 +0700

    fix #1224 reverse prompt and multi line (#1297)
    
    * fix reverse prompt and multi line
    
    * Code Formatting
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 799fdc1b5d888b8a8682baf112e1c2a2df0df1c4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 3 23:24:20 2023 +0300

    ggml : vectorize Q8_0 quantization
    
    https://github.com/ggerganov/ggml/pull/127#issuecomment-1533648531

commit 6daa09d87926fe654385c2887e39ec3eeaa58120
Author: khimaros <me@khimaros.com>
Date:   Wed May 3 10:58:11 2023 -0700

    examples : read chat prompts from a template file (#1196)

commit bca9ad938a2a43621cf406d993b755cc91728dd5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 3 20:09:42 2023 +0300

    minor : fix whitespaces (#1302)

commit e2a937ca6abadc7e01e139db31e6db9dce16e3e9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 3 18:43:23 2023 +0300

    minor : fix trailing whitespaces

commit b0c71c7b6dc0c0adb507d78f401e95e7ab0f5a38
Author: KASR <karim.asrih@gmail.com>
Date:   Wed May 3 17:31:28 2023 +0200

    scripts : platform independent script to verify sha256 checksums (#1203)
    
    * python script to verify the checksum of the llama models
    
    Added Python script for verifying SHA256 checksums of files in a directory, which can run on multiple platforms. Improved the formatting of the output results for better readability.
    
    * Update README.md
    
    update to the readme for improved readability and to explain the usage of the python checksum verification script
    
    * update the verification script
    
    I've extended the script based on suggestions by @prusnak
    
    The script now checks the available RAM, is there is enough to check the file at once it will do so. If not the file is read in chunks.
    
    * minor improvment
    
    small change so that the available ram is checked and not the total ram
    
    * remove the part of the code that reads the file at once if enough ram is available
    
    based on suggestions from @prusnak i removed the part of the code that checks whether the user had enough ram to read the entire model at once. the file is now always read in chunks.
    
    * Update verify-checksum-models.py
    
    quick fix to pass the git check

commit a8a2efdc8161d4f69a0dd863e741c11fbd5df85c
Author: CRD716 <crd716@gmail.com>
Date:   Wed May 3 10:26:47 2023 -0500

    examples : various prompt and example fixes (#1298)
    
    * fix dan.txt
    
    * miku prompt improvements
    
    * use common characters

commit e216aa04633892b972d013719e38b59fd4917341
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Tue May 2 22:26:13 2023 -0400

    llama : only copy used KV cache in get / set state (#1272)
    
    * llama : only copy used KV cache in get / set state
    
    * switch to ggml for copying k, v
    
    * avoid designated initializers

commit 2485d7a4d39406cd0f468e35551b472cceb5bd61
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Tue May 2 18:46:20 2023 -0700

    Process escape sequences given in prompts (#1173)

commit 13b0c68ed7a9948db0720f7393df094ab1005b14
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Tue May 2 18:01:57 2023 -0700

    Handle signals properly on Windows (#1123)

commit 55bc5f0900d925c539488901c5538b637d68665c
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Tue May 2 17:52:35 2023 -0700

    Call sh on build-info.sh (#1294)

commit 9daff419f6be818595ddbea293a0ea7283af726e
Author: kuvaus <22169537+kuvaus@users.noreply.github.com>
Date:   Wed May 3 03:43:43 2023 +0300

    fix build-info.h for git submodules (#1289)
    
    
    * make git build info work with submodules
    
    ---------
    
    Co-authored-by: Green Sky <green@g-s.xyz>

commit bf4b22ffe433dc5e2fba7588c4485a7e51b1a30d
Author: slaren <slarengh@gmail.com>
Date:   Wed May 3 01:36:45 2023 +0200

    fix missing parameters in `llama_init_from_gpt_params` (#1293)

commit 67c77799e025a8425c23a6a0599c007f46ded590
Author: Ron Evans <ron@hybridgroup.com>
Date:   Tue May 2 22:39:51 2023 +0200

    examples : add llama_init_from_gpt_params() common function (#1290)
    
    Signed-off-by: deadprogram <ron@hybridgroup.com>

commit 0e6cbff1b7509628c588e661166f6e187137734d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 2 23:09:08 2023 +0300

    llama : fix compile warnings

commit 5d5817ca603d4cb451bed26594aa3dcd93f4ec56
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 2 22:14:50 2023 +0300

    ggml : fix 32-bit ARM

commit 8c9be35ff998cbb4178b0fedcb9afd85cb6852e2
Author: Ron Evans <ron@hybridgroup.com>
Date:   Tue May 2 19:53:52 2023 +0200

    examples : improve vertical alignment of a few variables (#1286)
    
    Signed-off-by: deadprogram <ron@hybridgroup.com>

commit cc0bb7235c72e50a621800e366d0e4fe315f0e11
Author: Marvin Gießing <marvin.giessing@gmail.com>
Date:   Tue May 2 18:42:16 2023 +0200

    ggml : fix ppc64le build error and make cmake detect Power processors (#1284)
    
    * Fix ppc64le build issue
    
    * Added support to detect ppc64* processors

commit 2bb992f034689e2ddd7b9ac05163b0359a5957b3
Author: Robert Brisita <986796+rbrisita@users.noreply.github.com>
Date:   Tue May 2 12:23:44 2023 -0400

    llama : allow 0 as a seed number. (#1275)

commit e2cd5069999181a9e4a22cf420e0491878b3062f
Author: Ron Evans <ron@hybridgroup.com>
Date:   Tue May 2 18:13:26 2023 +0200

    main : switch input_noecho to input_echo to remove negation (#979)
    
    Signed-off-by: deadprogram <ron@hybridgroup.com>

commit 2d099e5193d73f800b646c39e2fad08c1c1f1096
Author: slaren <slarengh@gmail.com>
Date:   Tue May 2 16:03:00 2023 +0200

    ggml: add names to tensors (#1268)
    
    * ggml: add names to tensors
    
    * minor improvements to dot file formatting

commit f4cef87edfd1b2f8d5befd4fde54ca2e03987bea
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Mon May 1 09:23:47 2023 -0700

    Add git-based build information for better issue tracking (#1232)
    
    * Add git-based build information for better issue tracking
    
    * macOS fix
    
    * "build (hash)" and "CMAKE_SOURCE_DIR" changes
    
    * Redo "CMAKE_CURRENT_SOURCE_DIR" and clearer build messages
    
    * Fix conditional dependency on missing target
    
    * Broke out build-info.cmake, added find_package fallback, and added build into to all examples, added dependencies to Makefile
    
    * 4 space indenting for cmake, attempt to clean up my mess in Makefile
    
    * Short hash, less fancy Makefile, and don't modify build-info.h if it wouldn't change it

commit 58b367c2d757c0ea12aec672382462b42204c724
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Mon May 1 18:11:07 2023 +0200

    cuBLAS: refactor and optimize f16 mat mul performance (#1259)
    
    * cuBLAS: refactor, convert fp16 to fp32 on device
    
    * cuBLAS: use multiple streams, choose smartly between mul_mat_q and mul_mat_f16
    
    * fix build
    
    * cuBLAS: update block_q5_1

commit ea3a0ad6b6b5ca4693b94acd4cb32e2803f66fae
Author: xloem <0xloem@gmail.com>
Date:   Mon May 1 08:58:51 2023 -0400

    llama : update stubs for systems without mmap and mlock (#1266)
    
    Co-authored-by: John Doe <john.doe@example.com>

commit 2bdc09646d8c6cb74a6f573e9081586b4b83b9d1
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Mon May 1 05:56:07 2023 -0600

    ggml : fix ggml_used_mem() (#1264)

commit 70269cae37538461ff816e714afbb3ebcdcdc26b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 1 14:54:59 2023 +0300

    llama : fix session load / save (#1263)

commit b925f1f1b082319ee69943f8d1a83ac9b6ff09ca
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Mon May 1 13:32:22 2023 +0200

    cuBLAS: fall back to pageable memory if pinned alloc fails (#1233)
    
    * cuBLAS: fall back to pageable memory if pinned alloc fails
    
    * cuBLAS: do not use pinned memory if env variable GGML_CUDA_NO_PINNED is set

commit 90b19bd6eee943832584f9cac0b6f9ea29cc42a4
Author: Alex Klinkhamer <git@grencez.dev>
Date:   Mon May 1 00:24:20 2023 -0700

    llama : let context be const when accessing const data (#1261)

commit 7ff0dcd32091c703a12adb0c57c32c565ce17664
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 30 22:28:51 2023 +0300

    ggml : fix UB (int << 31)

commit 6f796992869f306c48484d62a39f2a181ae2fd6f
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Sun Apr 30 20:48:38 2023 +0200

    build: add armv{6,7,8} support to cmake (#1251)
    
    - flags copied from Makefile
    - updated comments in both CMakeLists.txt and Makefile to match reality

commit a5d30b1f53677cb50791fec41c43e93274347303
Author: jon-chuang <9093549+jon-chuang@users.noreply.github.com>
Date:   Sun Apr 30 14:41:35 2023 -0400

    common : better default number of threads (#934)
    
    * commit
    
    * fix
    
    * try-catch
    
    * apply code review
    
    * improve
    
    * improve
    
    * add macos headers
    
    * done
    
    * remove color
    
    * fix windows
    
    * minor
    
    * fix
    
    * Apply suggestions from code review
    
    Co-authored-by: DannyDaemonic <DannyDaemonic@gmail.com>
    
    * remove
    
    * minor
    
    * minor
    
    ---------
    
    Co-authored-by: jon-chuang <jon-chuang@users.noreply.github.com>
    Co-authored-by: DannyDaemonic <DannyDaemonic@gmail.com>

commit 76a884920aa1d2fc0dc7a7ac12dfc5ec5816377c
Author: 0cc4m <picard12@live.de>
Date:   Sun Apr 30 20:34:52 2023 +0200

    ggml : add CLBlast q5_0, q5_1, q8_0 dequant kernels (#1225)
    
    * Implement q5_0, q5_1 and q8_0
    
    * Work around q5_0 OpenCL issue
    
    * Fix q8_0 dequant kernel
    
    * Move cl kernels into ggml-opencl.c
    
    * Use two memcpy calls for q5_0 buffer transfer

commit 6bc4400e67e6bc4faad3ad3d5e9d8a6576a9752d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 30 19:07:00 2023 +0300

    ggml : add Q5 WASM SIMD + GGML_FTYPE

commit f0d70f147d969e41fa410b8af2965a27aa901eb9
Author: Stephan Walter <stephan@walter.name>
Date:   Sun Apr 30 12:32:37 2023 +0000

    Various fixes to mat_mul benchmark (#1253)

commit 3e5aa8a1c44051153d6d7b3eeca2f4b4e5fb310c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 30 10:25:46 2023 +0300

    ggml : fix labels for GGML_OP_ALIBI

commit c3ca7a5f0546c561eb278be3f2fe335795679e01
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 29 21:34:23 2023 +0300

    ggml : fix 32-bit ARM NEON

commit e8c051611abfc9a7f37fd4bba48217180893bd68
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 29 21:12:56 2023 +0300

    ggml : use vzip instead of vuzp for consistency

commit 0b5a9350993e6fc8be45dc2a3eafc1fd0812d392
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 29 19:28:36 2023 +0300

    ggml : fix visibility and unused warnings

commit ec728e44d7488c2da3560970317708b2b12b9c04
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 29 18:43:42 2023 +0300

    ggml : fix #if for f32_f32 mul_mat (CLBlast) (#1229)

commit 214b6a35702a489e3738acd81fad6d46182d3036
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 29 18:43:28 2023 +0300

    ggml : adjust mul_mat_f16 work memory (#1226)
    
    * llama : minor - remove explicity int64_t cast
    
    * ggml : reduce memory buffer for F16 mul_mat when not using cuBLAS
    
    * ggml : add asserts to guard for incorrect wsize

commit 305eb5afd51325e3142c01c17431febb7c67de87
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 29 13:53:12 2023 +0300

    build : fix reference to old llama_util.h

commit 84ca9c2ecf3391d911589d0fe2b483cbfb4b82a6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 29 13:48:11 2023 +0300

    examples : fix save-load-state + rename llama-util.h

commit 334637e43e3a0529b4b50e2c22968b1ed1633353
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 29 09:51:06 2023 +0300

    common : change default parameters to pre-#1126 (#1223)

commit dd7eff57d8491792010b1002b8de6a4b54912e5c
Author: Ivan Stepanov <ivanstepanovftw@gmail.com>
Date:   Sat Apr 29 08:34:41 2023 +0300

    llama : new sampling algorithms (#1126)
    
    * Sample interface, new samplers.
    
    New samplers:
    - locally typical sampling
    - tail free sampling
    - frequency and presence penalty
    - mirostat
    
    Ignore EOS fix: -inf should be used.
    
    * mirostat
    
    * Added --logit-bias and --no-penalize-nl, removed std::span
    
    * Use C++11, clarify llama API documentation, rename Mirostat parameters to --mirostat_lr and --mirostat_ent, add temperature sampling for Mirostat, simplify Mirostat sampling API parameters (removed N and *k)
    
    Use C++11, clarify llama API documentation, rename Mirostat parameters to --mirostat_lr and --mirostat_ent, add temperature sampling for Mirostat, simplify Mirostat sampling API parameters (removed N and *k)
    
    * Save and load example adjust
    
    * Tests
    
    * Windows build fix
    
    * Windows test fix

commit 7fc50c051ae8a78e9643fdf172d12e20f2dd9b6c
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Sat Apr 29 02:04:18 2023 +0200

    cuBLAS: use host pinned memory and dequantize while copying (#1207)
    
    * cuBLAS: dequantize simultaneously while copying memory
    
    * cuBLAS: use host pinned memory
    
    * cuBLAS: improve ggml_compute_forward_mul_mat_f16_f32 with pinned memory
    
    * cuBLAS: also pin kv cache
    
    * fix rebase

commit b1ee8f59b4101b46999a0995d9a34506f7285466
Author: Henri Vasserman <henv@hot.ee>
Date:   Sat Apr 29 02:31:56 2023 +0300

    cuBLAS: non-contiguous tensor support (#1215)
    
    * Cuda: non-contiguous tensor support
    
    * remove extra stuff
    
    * rename
    
    * fix error
    
    * more fixes, now OpenBLAS and CLBlast build too
    
    * now then?

commit 36d19a603b221d1bd7897fcb10e823e2103b052d
Author: Stephan Walter <stephan@walter.name>
Date:   Fri Apr 28 23:10:43 2023 +0000

    Remove Q4_3 which is no better than Q5 (#1218)

commit 7f15c5c477d9933689a9d1c40794483e350c2f19
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Apr 28 21:32:52 2023 +0300

    readme : update hot topics

commit 55390bcaf2579a5435564d7267ae3ed367837fd6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Apr 28 20:37:43 2023 +0300

    ggml : sync ggml (ggml_alibi)

commit 5fba3c016bfd1d73a070e7c93dac14162ce118d0
Author: CRD716 <crd716@gmail.com>
Date:   Fri Apr 28 11:13:33 2023 -0500

    examples : add Jeopardy example (#1168)
    
    * Basic Setup
    
    * Prevent Results.txt from coming up
    
    * Prefixes, Line separators, etc
    
    * editorcheck
    
    * introduction to give more consistent results
    
    * Basic graph thing
    
    * Grading, ready for testing!
    
    * Y'all ready to get funky?
    
    * fix column removal stuff
    
    * missed a few

commit 1481a9cf25ea2e4abef6b13a57660a35f3e66af1
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Fri Apr 28 11:59:37 2023 -0400

    llama : add session file format and saved sessions in main (#1169)

commit 11d902364b0e3b503a02a4e757ee2dc38aacb68f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Apr 28 17:58:44 2023 +0300

    ggml : add helper debug printf in soft_max

commit 7296c961d9303010a2b98379f738da2a8a55aa1b
Author: 0cc4m <picard12@live.de>
Date:   Fri Apr 28 16:57:16 2023 +0200

    ggml : add CLBlast support (#1164)
    
    * Allow use of OpenCL GPU-based BLAS using ClBlast instead of OpenBLAS for context processing
    
    * Improve ClBlast implementation, avoid recreating buffers, remove redundant transfers
    
    * Finish merge of ClBlast support
    
    * Move CLBlast implementation to separate file
    
    Add buffer reuse code (adapted from slaren's cuda implementation)
    
    * Add q4_2 and q4_3 CLBlast support, improve code
    
    * Double CLBlast speed by disabling OpenBLAS thread workaround
    
    Co-authored-by: Concedo <39025047+LostRuins@users.noreply.github.com>
    Co-authored-by: slaren <2141330+slaren@users.noreply.github.com>
    
    * Fix device selection env variable names
    
    * Fix cast in opencl kernels
    
    * Add CLBlast to CMakeLists.txt
    
    * Replace buffer pool with static buffers a, b, qb, c
    
    Fix compile warnings
    
    * Fix typos, use GGML_TYPE defines, improve code
    
    * Improve btype dequant kernel selection code, add error if type is unsupported
    
    * Improve code quality
    
    * Move internal stuff out of header
    * Use internal enums instead of CLBlast enums
    * Remove leftover C++ includes and defines
    * Make event use easier to read
    
    Co-authored-by: Henri Vasserman <henv@hot.ee>
    
    * Use c compiler for opencl files
    
    * Simplify code, fix include
    
    * First check error, then release event
    
    * Make globals static, fix indentation
    
    * Rename dequant kernels file to conform with other file names
    
    * Fix import cl file name
    
    ---------
    
    Co-authored-by: Concedo <39025047+LostRuins@users.noreply.github.com>
    Co-authored-by: slaren <2141330+slaren@users.noreply.github.com>
    Co-authored-by: Henri Vasserman <henv@hot.ee>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 78ec543733d10a1629f984fd0302fdaa4e87fe66
Author: Folko-Ven <71110216+Folko-Ven@users.noreply.github.com>
Date:   Fri Apr 28 19:22:48 2023 +0500

    Correcting link to w64devkit (#1214)
    
    Correcting link to w64devkit (change seeto to skeeto).

commit 92a6e13a31ba052abd9062af6cb8df2a293ce661
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Apr 28 15:40:32 2023 +0200

    Add Manjaro CUDA include and lib dirs to Makefile (#1212)

commit 04aaae1d79482cad2564412f3b32e70298ac7789
Author: Yann Follet <131855179+YannFollet@users.noreply.github.com>
Date:   Fri Apr 28 19:59:48 2023 +0800

    add avx2 for dot_q8_0_q8_0, 2x faster than scalar (#1211)

commit 0b2da20538d01926b77ea237dd1c930c4d20b686
Author: Stephan Walter <stephan@walter.name>
Date:   Wed Apr 26 20:26:42 2023 +0000

    ggml : slightly faster AVX2 implementation for Q5 (#1197)

commit f9be42add0bf3ce61814b7ede0e6d0dda9ff22c6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 26 23:24:42 2023 +0300

    readme : add quantization info

commit 574406dc7e350ddbffaeca33bf0392b7bfeb1436
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 26 23:14:13 2023 +0300

    ggml : add Q5_0 and Q5_1 quantization (#1187)
    
    * ggml : add Q5_0 quantization (cuBLAS only)
    
    * ggml : fix Q5_0 qh -> uint32_t
    
    * ggml : fix q5_0 histogram stats
    
    * ggml : q5_0 scalar dot product
    
    * ggml : q5_0 ARM NEON dot
    
    * ggml : q5_0 more efficient ARM NEON using uint64_t masks
    
    * ggml : rename Q5_0 -> Q5_1
    
    * ggml : adding Q5_0 mode
    
    * quantize : add Q5_0 and Q5_1 to map
    
    * ggml : AVX2 optimizations for Q5_0, Q5_1 (#1195)
    
    ---------
    
    Co-authored-by: Stephan Walter <stephan@walter.name>

commit 87a6f846d3e929632c45916dd08f1e2a9c72d2a3
Author: Ásgeir Bjarni Ingvarsson <asgeir@fundinn.org>
Date:   Wed Apr 26 20:08:43 2023 +0000

    Allow setting the rng seed after initialization. (#1184)
    
    The llama_set_state_data function restores the rng state to what it
    was at the time llama_copy_state_data was called. But users may want
    to restore the state and proceed with a different seed.

commit ea3ad7eb60cfb44526a58122e8019850f437cd1b
Author: DaniAndTheWeb <57776841+DaniAndTheWeb@users.noreply.github.com>
Date:   Wed Apr 26 22:03:03 2023 +0200

    Updating build instructions to include BLAS support (#1183)
    
    * Updated build information
    
    First update to the build instructions to include BLAS.
    
    * Update README.md
    
    * Update information about BLAS
    
    * Better BLAS explanation
    
    Adding a clearer BLAS explanation and adding a link to download the CUDA toolkit.
    
    * Better BLAS explanation
    
    * BLAS for Mac
    
    Specifying that BLAS is already supported on Macs using the Accelerate Framework.
    
    * Clarify the effect of BLAS
    
    * Windows Make instructions
    
    Added the instructions to build with Make on Windows
    
    * Fixing typo
    
    * Fix trailing whitespace

commit 859fee6dfb00fab7ce6bc215b4adae78d82f4759
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Wed Apr 26 18:43:27 2023 +0200

    quantize : use `map` to assign quantization type from `string` (#1191)
    
    instead of `int` (while `int` option still being supported)
    
    This allows the following usage:
    
    `./quantize ggml-model-f16.bin ggml-model-q4_0.bin q4_0`
    
    instead of:
    
    `./quantize ggml-model-f16.bin ggml-model-q4_0.bin 2`

commit 4afcc378698e057fcde64e23eb664e5af8dd6956
Author: Stephan Walter <stephan@walter.name>
Date:   Tue Apr 25 21:41:56 2023 +0000

    Update SHA256SUMS after quantization change (#1181)
    
    
    Co-authored-by: Pavol Rusnak <pavol@rusnak.io>

commit 667c501334ace706e3abc3f7a37cf1d6b4228745
Author: ostix360 <55257054+ostix360@users.noreply.github.com>
Date:   Tue Apr 25 23:33:08 2023 +0200

    py : cast lora_alpha to int in convert-lora-to-ggml (#1170)
    
    
    Co-authored-by: Pavol Rusnak <pavol@rusnak.io>

commit bb98e77be704584fb40b0400394b4c16ae75f8e2
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Tue Apr 25 23:19:57 2023 +0200

    nix: use convert.py instead of legacy wrapper convert-pth-to-ggml.py (#981)

commit 7a32fcb3b29f4db8aed8a85dc58eb958fb118153
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 25 23:40:51 2023 +0300

    ggml : add Q8_0 quantization format (rename the old one to Q8_1) (ARM NEON) (#1179)
    
    * ggml : add Q8_0 quantization format (rename the old one to Q8_1)
    
    * tests : fix test-quantize-fns
    
    * ggml : finalize Q8_0 implementation
    
    * ggml : use q4_0_q8_0 and q4_2_q8_0
    
    * ggml : fix Q8_0 dot product bug (ARM)
    
    * ggml : Q8_0 unroll x2
    
    * ggml : fix bug - using wrong block type
    
    * ggml : extend quantize_fns_t with "vec_dot_type"
    
    * ggml : fix Q8_0 to use 255 values out of 256
    
    * ggml : fix assert using wrong QK4_2 instead of QK4_3

commit dd0eabc049fb1efc631cab8eb0a646808d704e18
Author: unbounded <haakon@likedan.net>
Date:   Tue Apr 25 19:20:46 2023 +0200

    ggml : use full range for Q4_0 and Q4_2 quantization (#729)
    
    * Use full range for q4_0 quantization
    
    By keeping the sign of the highest magnitude, we can make sure the
    highest value maps to -8, which is currently unused.
    This is a bit of a freebie since it is fully backwards compatible with
    the current format.
    
    * Update quantize_row_q4_0 for AVX/AVX2
    
    * Update quantize_row_q4_0 for WASM
    
    Untested
    
    * Update quantize_row_q4_0 for Arm NEON
    
    * Update quantize_row_q4_0 for PowerPC
    
    Untested
    
    * Use full range for q4_2 quantization

commit 54bb60e26858be251a0eb3cb70f80322aff804a0
Author: xaedes <xaedes@gmail.com>
Date:   Mon Apr 24 23:02:02 2023 +0200

    ggml : fix bug in ggml_compute_forward_sum_f32 (#1162)
    
    The sum over all rows is now computed instead of just the last row

commit 8a0f8673ba1cdc6aa6df27a9fbc698431ca70e8d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 24 22:18:25 2023 +0300

    ggml : export symbols (#1155)

commit 0c5692345d5c046dbc6a7d311a00ae5842ac39c3
Author: xaedes <xaedes@gmail.com>
Date:   Mon Apr 24 18:23:31 2023 +0200

    examples : add save_load_state example (#1150)
    
    * add save_load_state example
    
    * use <cstdio> instead of <iostream> and fprintf / printf instead of cout
    
    * renamed save-load-state example files replacing underscores by dashes

commit 957c8ae21d1e7052ea45a40ee8c0407b909e90cc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 24 18:47:03 2023 +0300

    llama : increase scratch buffer size for 65B (ref #1152)
    
    Temporary solution

commit 9b0a4d421459f4e5e1af735c9784c3247b379025
Author: mgroeber9110 <45620825+mgroeber9110@users.noreply.github.com>
Date:   Mon Apr 24 17:45:32 2023 +0200

    examples/main README improvements and some light refactoring (#1131)

commit 2ec83428de7a876ecbbe484e1de42b73b5a40e25
Author: Stephan Walter <stephan@walter.name>
Date:   Mon Apr 24 15:38:26 2023 +0000

    Fix build for gcc 8 and test in CI (#1154)

commit e4cf982e0d4fcfbb4b977a52dbeacd115da10c3b
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Mon Apr 24 17:29:58 2023 +0200

    Fix cuda compilation (#1128)
    
    * Fix: Issue with CUBLAS compilation error due to missing -fPIC flag
    
    ---------
    
    Co-authored-by: B1gM8c <89020353+B1gM8c@users.noreply.github.com>

commit c4fe84fb0d28851a5c10e5a633f82ae2ba3b7fae
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 24 07:40:02 2023 +0300

    llama : refactor get / set state + remove redundant kv cache API (#1143)

commit 1d78fecdab4087028a38517e86ed129f077174d8
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Sun Apr 23 23:03:44 2023 +0200

    Fix LoRA acronym (#1145)

commit 284685f1692258c2bcf08b86b723b80ba2e66c7a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 23 19:57:09 2023 +0300

    scripts : add helper scripts to synch ggml repo

commit edce63baa9dbd3963c3441bce07ee0acbb635697
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Sun Apr 23 08:37:02 2023 -0700

    Added README.md for main with examples and explanations (#1139)

commit ec9cdb6752dd96b3cc74d90ad1adeba5b4fa2b0e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 23 18:32:52 2023 +0300

    ggml : do not print perf ops that have not been used at all

commit e4422e299c10c7e84c8e987770ef40d31905a76b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 23 18:15:39 2023 +0300

    ggml : better PERF prints + support "LLAMA_PERF=1 make"

commit 53c8434398b3cba7ac6298cdd44abd40f0e640b1
Author: Stephan Walter <stephan@walter.name>
Date:   Sun Apr 23 11:01:03 2023 +0000

    Improve AVX2 for vec_dot_q4_3_q8_0 (#1138)

commit c6524f46eb93fdb949330293a8469fd70080bd5a
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Sun Apr 23 10:21:26 2023 +0200

    readme : update gpt4all instructions (#980)

commit c9e2c26f413377b352845f442cdab976ce85a05d
Author: Yishuo Wang <MeouSker77@outlook.com>
Date:   Sun Apr 23 15:57:05 2023 +0800

    A better `packNibbles` and `mul_sum_i8_pairs_float` implementation using AVX512 (#1119)

commit 0e018fe008eacebdbcfa2d61b6c988c245c961cd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 22 16:31:56 2023 +0300

    ggml : fix Q4_3 cuBLAS

commit 857308d1e8fb6afe33edb481d48560eee8fe7d7c
Author: Stephan Walter <stephan@walter.name>
Date:   Sat Apr 22 13:12:29 2023 +0000

    ci : trigger CI for drafts, but not most PR actions (#1125)

commit c50b628810f36a3e6e0324371f6db579eacefa0e
Author: Stephan Walter <stephan@walter.name>
Date:   Sat Apr 22 10:54:13 2023 +0000

    Fix CI: ARM NEON, quantization unit tests, editorconfig (#1122)

commit 5f939498d517b4dddbe904f202e895a3ecfb9dc4
Author: unbounded <haakon@likedan.net>
Date:   Sat Apr 22 11:10:39 2023 +0200

    ggml : unit test for quantization functions (#953)
    
    * Unit test for quantization functions
    
    Use the ggml_internal_get_quantize_fn function to loop through all
    quantization formats and run a sanity check on the result.
    
    Also add a microbenchmark that times these functions directly without
    running the rest of the GGML graph.
    
    * test-quantize-fns: CI fixes
    
    Fix issues uncovered in CI
     - need to use sizes divisible by 32*8 for loop unrolling
     - use intrinsic header that should work on Mac
    
    * test-quantize: remove
    
    Per PR comment, subsumed by test-quantize-fns
    
    * test-quantize: fix for q8_0 intermediates

commit 36b4f7e06406eed8a605cc9f2921d9244ef6a8e5
Author: wbpxre150 <100937007+wbpxre150@users.noreply.github.com>
Date:   Sat Apr 22 16:56:35 2023 +0800

    llama : print timings on ctrl+c exit (#1021)
    
    * print timings on ctrl+c exit
    
    * remove redundant free memory call.
    
    * add global pointer to ctx.

commit 10f19c1121068ce3dab9bece03a8b9caaea2db36
Author: eiery <19350831+eiery@users.noreply.github.com>
Date:   Sat Apr 22 04:27:05 2023 -0400

    llama : have n_batch default to 512 (#1091)
    
    * set default n_batch to 512 when using BLAS
    
    * spacing
    
    * alternate implementation of setting different n_batch for BLAS
    
    * set n_batch to 512 for all cases

commit 7e312f165c5047d6e16680d1eebc83055e95c313
Author: Howard Su <howard0su@gmail.com>
Date:   Sat Apr 22 16:18:20 2023 +0800

    cmake : fix build under Windows when enable BUILD_SHARED_LIBS (#1100)
    
    * Fix build under Windows when enable BUILD_SHARED_LIBS
    
    * Make AVX512 test on Windows to build the shared libs

commit 872c365a9176a011b13d31269bb3121fa89c37e1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 22 11:08:12 2023 +0300

    ggml : fix AVX build + update to new Q8_0 format

commit 955ef9a5d53d8f911fe00580ac9bd0caa56430af
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 22 10:55:35 2023 +0300

    ggml : alternative Q4_3 implementation using modified Q8_0 (#1109)
    
    * ggml : prefer vzip to vuzp
    
    This way we always use the same type of instruction across all quantizations
    
    * ggml : alternative Q4_3 implementation using modified Q8_0
    
    * ggml : fix Q4_3 scalar imlpementation
    
    * ggml : slight improvement of Q4_3 - no need for loop unrolling
    
    * ggml : fix AVX paths for Q8_0 quantization

commit c5aa5e577741d0359ad26ec50b9e21a74c65d911
Author: Stephan Walter <stephan@walter.name>
Date:   Sat Apr 22 07:37:05 2023 +0000

    ggml : AVX2 optimization for vec_dot_q4_3_q8_0 and refactoring (#1099)
    
    * AVX2 optimization for vec_dot_q4_3_q8_0 and refactoring
    
    * finish AVX vectorization of quantize_row_q8_0
    
    * Rename hsum_int_8 to hsum_i32_8

commit e9a9cb0c54461ffbda75b7b2f99f3ea5562291c2
Author: Clint Herron <hanclinto@gmail.com>
Date:   Sat Apr 22 02:54:33 2023 -0400

    examples : Improve Alpaca Default Repeat Penalty: Better Match Alpaca.cpp Experience (#1107)
    
    * Moving parameters to separate lines for readability.
    
    * Increasing repeate_penalty to 1.1 to make alpaca more usable by default.
    
    * Adding trailing newline.

commit b6e7f9b09e9c340ec97a2fae61c1eb8db861f2f9
Author: xaedes <xaedes@gmail.com>
Date:   Sat Apr 22 08:21:32 2023 +0200

    llama : add api for getting/setting the complete state: rng, logits, embedding and kv_cache (#1105)
    
    * reserve correct size for logits
    
    * add functions to get and set the whole llama state:
    
    including rng, logits, embedding and kv_cache
    
    * remove unused variables
    
    * remove trailing whitespace
    
    * fix comment

commit 50cb666b8a2e35a49b08c0f6bc81138c8f6f2ac1
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Fri Apr 21 21:59:17 2023 +0200

    Improve cuBLAS performance by using a memory pool (#1094)
    
    * Improve cuBLAS performance by using a memory pool
    
    * Move cuda specific definitions to ggml-cuda.h/cu
    
    * Add CXX flags to nvcc
    
    * Change memory pool synchronization mechanism to a spin lock
    General code cleanup

commit 25d7abbd1f73582b7e0fdc422a936e8541c0780b
Author: apaz <aarpazdera@gmail.com>
Date:   Fri Apr 21 13:48:06 2023 -0500

    llama : fixed rlimit error message (#888)

commit 018f2279f5fe3ef743bd8254b23ea8f0efae7e73
Author: 源文雨 <41315874+fumiama@users.noreply.github.com>
Date:   Sat Apr 22 02:27:06 2023 +0800

    cmake : link threads publicly to ggml (#1042)
    
    * fix: ld link test-tokenizer-0 error
    
    ```
    cmake3 --build . --config Release
    [  5%] Built target ggml
    [ 16%] Built target llama
    [ 22%] Linking CXX executable ../bin/test-tokenizer-0
    ../libllama.a(ggml.c.o)：在函数‘ggml_graph_compute’中：
    ggml.c:(.text+0xf2db)：对‘pthread_create’未定义的引用
    ggml.c:(.text+0xf9d4)：对‘pthread_join’未定义的引用
    collect2: error: ld returned 1 exit status
    gmake[2]: *** [bin/test-tokenizer-0] 错误 1
    gmake[1]: *** [tests/CMakeFiles/test-tokenizer-0.dir/all] 错误 2
    gmake: *** [all] 错误 2
    ```
    
    * Update CMakeLists.txt
    
    * Update CMakeLists.txt
    
    * Update CMakeLists.txt

commit 9411288271ab548216902a029f42a0a38ebcedb7
Author: Alex Klinkhamer <from.github.com.917@grencez.dev>
Date:   Fri Apr 21 11:18:09 2023 -0700

    main : evaluate tokens in batches after swapping context (#1014)
    
    * examples : evaluate tokens in batches after swapping context
    
    * Update examples/main/main.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 8687c1f2581d059cd5b6a9502f89bd343566062a
Author: xaedes <xaedes@googlemail.com>
Date:   Fri Apr 21 17:25:21 2023 +0200

    llama : remember and restore kv cache data pointers (#1104)
    
    because their value is stored in buf and overwritten by memcpy

commit 1bfc153e2f35ddd9d64b084e8d1a5e6fa57ad1c9
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Apr 21 17:18:26 2023 +0200

    ggml : a faster version for Q4_1 x Q8_0 dot products (#1083)
    
    * A faster version for Q4_1 x Q8_0 dot products
    
    The idea nehind being that Q8_0 quantized
    values get used many times in the matrix multiplications
    where they are involved. In the current implementations,
    when we are evaluating the dot products, we need to compute
    the sum of the quants in the Q8_0 vector, so the same
    operation is repeated many times. Here we pre-compute
    the sum during Q8_0 quantization, store it in the
    now modified block_q8_0 struct, and then reuse this
    result in the subsequent dot products.
    
    In a synthetic benchmark (just compute a bunch of dot
    products), this change speeds up the Q4_1 * Q8_0 dot
    product by 80%, making the performance identical to
    Q4_0 * Q8_0.
    
    In practical application, I see a ~15% gain in speed for
    token prediction on M2, and ~5% gain on Ryzen 7950X.
    The speed gain in the prompt evaluation is much bigger
    (around 50%).
    
    I have only done the change for the scalar version,
    ARM_NEON, and AVX2, so we still need an AVX implementation.
    
    * Cleaning up
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 3d59769c3bb7e72c915646ddb1e239b1face19f5
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Fri Apr 21 14:57:57 2023 +0200

    Show perplexity ETA in hours and minutes (#1096)

commit d40fded93e1a533e969768e1e335c15c61c296ce
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Apr 21 10:23:36 2023 +0300

    llama : fix comment for "output.weight" tensor

commit 2510c1831fac874f32e272f6079f01b5461f3986
Author: Stephan Walter <stephan@walter.name>
Date:   Thu Apr 20 21:56:44 2023 +0000

    Add ggml-model-*.bin checksums for 7B, 13B, 30B, 65B (#1088)
    
    * Add ggml-model-*.bin checksums for 7B, 13B, 30B
    * Add ggml-model-*.bin checksums for 65B
    
    ---------
    
    Co-authored-by: Pavol Rusnak <pavol@rusnak.io>

commit 12b5900dbc9743dee3ce83513cf5c3a44523a1b6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 20 23:32:59 2023 +0300

    ggml : sync ggml (add GPT-NeoX RoPE implementation)

commit 9ff334f3c9b960a44c5e149b08c748a2914fb882
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 20 21:58:05 2023 +0300

    ggml : fix bug in ggml_compute_forward_dup_f32()

commit 2005469ea130cf920c50175d4f47a87bfd8aaf4d
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Thu Apr 20 20:49:53 2023 +0200

    Add Q4_3 support to cuBLAS (#1086)

commit 8a1756abdf1f48cb4dcb898bc8fbe9102ef49dc6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 20 21:43:50 2023 +0300

    ggml : do not break cuBLAS build (Q4_3 is not yet implemented)

commit 66aab46079609972ee1f7bd6f319d826205a2fbd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 20 20:44:05 2023 +0300

    ggml : fix Q4_3 quantization
    
    Broke it during conflict resolution in last PR

commit 38de86a7114c97ecf3644e3a60159f1ed893e1b0
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Apr 20 19:42:27 2023 +0200

    llama : multi-threaded quantization (#1075)
    
    * Multi-threading quantization.
    
    Not much gain for simple quantizations, bit it will be important
    for quantizations that require more CPU cycles.
    
    * Multi-threading for quantize-stats
    
    It now does the job in ~14 seconds on my Mac for
    Q4_0, Q4_1 and Q4_2. Single-threaded it was taking
    more than 2 minutes after adding the more elaborate
    version of Q4_2.
    
    * Reviewer comments
    
    * Avoiding compiler confusion
    
    After changing chunk_size to const int as suggested by
    @ggerganov, clang and GCC starting to warn me that I don't
    need to capture it in the lambda. So, I removed it from the
    capture list. But that makes the MSVC build fail. So,
    making it a constexpr to make every compiler happy.
    
    * Still fighting with lambda captures in MSVC
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit e0305ead3a072db9c08b35c9600c49273b38a4b5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 20 20:35:53 2023 +0300

    ggml : add Q4_3 quantization (#1082)

commit 6a9661ea5ad72166b700ae5e87976e4452499dda
Author: Ivan Komarov <Ivan.Komarov@dfyz.info>
Date:   Thu Apr 20 17:15:18 2023 +0200

    ci : remove the LLAMA_ACCELERATE matrix dimension from Ubuntu builds in the CI (#1074)
    
    [Accelerate](https://developer.apple.com/documentation/accelerate) is an Apple framework which can only be used on macOS, and the CMake build [ignores](https://github.com/ggerganov/llama.cpp/blob/master/CMakeLists.txt#L102) the `LLAMA_ACCELERATE` variable when run on non-Apple platforms. This implies setting `LLAMA_ACCELERATE` is a no-op on Ubuntu and can be removed.
    
    This will reduce visual noise in CI check results (in addition to reducing the number of checks we have to run for every PR). Right now every sanitized build is duplicated twice for no good reason (e.g., we have `CI / ubuntu-latest-cmake-sanitizer (ADDRESS, Debug, ON)` and `CI / ubuntu-latest-cmake-sanitizer (ADDRESS, Debug, OFF)`).

commit 5addcb120cf2682c7ede0b1c520592700d74c87c
Author: 源文雨 <41315874+fumiama@users.noreply.github.com>
Date:   Thu Apr 20 21:28:43 2023 +0800

    fix: LLAMA_CUBLAS=1 undefined reference 'shm_open' (#1080)

commit c8c2c524827be8fd681a63f0e5a697b0bf4c587b
Author: Stephan Walter <stephan@walter.name>
Date:   Thu Apr 20 06:45:41 2023 +0000

    AVX2 optimization for vec_dot_q4_2_q8_0 (#1068)

commit 02d6988121510c067e06d498a273a351a888f5b9
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Thu Apr 20 03:14:14 2023 +0200

    Improve cuBLAS performance by dequantizing on the GPU (#1065)

commit 834695fe3a3ed2a962e774c9615e3f7b41d360a8
Author: CRD716 <crd716@gmail.com>
Date:   Wed Apr 19 14:52:14 2023 -0500

    Minor: Readme fixed grammar, spelling, and misc updates (#1071)

commit f7d05095b404b5500b4a702ea16f67fc22446e49
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Wed Apr 19 20:20:14 2023 +0200

    Q4_2 quantization with rmse-optimized scale and quants (#1062)
    
    * Q4_2 quantization with rmse-optimized scale and quants
    
    For quantize-stats we get
    q4_2: rmse 0.00159301, maxerr 0.17480469, 95pct<0.0030, median<0.0012
    
    For 7B perplexity with BLAS enabled we get 6.2038 after 655 chunks.
    
    Quantization is slow (~90 seconds on my Mac for 7B) as not
    multi-threaded as in PR #896.
    
    * ggml : satisfy the sanitizer builds
    
    Not sure why this makes them fail
    
    * Better follow ggml conventions for function names
    
    * Fixed type as per reviewer comment
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 884e7d7a2bfd7325b107442d6758983f5886ed3d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 19 20:10:08 2023 +0300

    ggml : use 8-bit precision for Q4_1 intermediate results (#1047)
    
    * ggml : use 8-bit precision for Q4_1 intermediate results (ARM)
    
    * ggml : optimize ggml_vec_dot_q4_1_q8_0() via vmalq_n_f32
    
    56 ms/token with Q4_1 !
    
    * ggml : AVX2 implementation of ggml_vec_dot_q4_1_q8_0 (#1051)
    
    * gitignore : ignore ppl-*.txt files
    
    ---------
    
    Co-authored-by: slaren <2141330+slaren@users.noreply.github.com>

commit 7cd5c4a3e9106151d48f328bb3c94c298a211f18
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 19 19:07:54 2023 +0300

    readme : add warning about Q4_2 and Q4_3

commit f3d4edf504c19ee9d1381c5727fe38667205d979
Author: Stephan Walter <stephan@walter.name>
Date:   Wed Apr 19 16:06:37 2023 +0000

    ggml : Q4 cleanup - remove 4-bit dot product code (#1061)
    
    * Q4 cleanup
    
    * Remove unused AVX512 Q4_0 code

commit 8944a1329648c57bb7d66851170938230587a52c
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Wed Apr 19 11:22:45 2023 +0200

    Add NVIDIA cuBLAS support (#1044)

commit 66674012389f9537140044290f8517bc4a5e0b74
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Wed Apr 19 00:53:24 2023 +0200

    Multi-threaded ggml_cpy (#1035)
    
    * Multi-threaded ggml_cpy
    
    * Update ggml.c
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Also fix wdata offset in ggml_compute_forward_add_q_f32
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 77a73403ca8eaced2590559d0f9cebd2b3649d32
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 18 23:54:57 2023 +0300

    ggml : add new Q4_2 quantization (ARM only) (#1046)
    
    * ggml : Q4_2 ARM
    
    * ggml : add ggml_is_quantized()
    
    * llama : update llama_type_name() with Q4_2 entry
    
    * ggml : speed-up q4_2
    
    - 4 threads: ~100ms -> ~90ms
    - 8 threads:  ~55ms -> ~50ms
    
    * ggml : optimize q4_2 using vmlaq_n_f32 + vmulq_n_f32

commit 50a8a2af97cb92e53e7a3195aa201c3d87da5415
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 18 23:11:23 2023 +0300

    ggml : scratch that - vmlaq_n_f32 is always better
    
    Had a background process that was messing with the timings

commit 4caebf6d408b91c2d29d0abc7b1e867b5de64db5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 18 23:00:08 2023 +0300

    gitignore : vdot

commit dcdd65e2969bc03c91a1ebd1160162d5054e6923
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 18 22:59:17 2023 +0300

    ggml : optimize ggml_vec_dot_q4_0_q8_0() using vectorized accumulators

commit 5ecff35151156118c2df74899637ad34ee384b9b
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Apr 18 21:00:14 2023 +0200

    Adding a simple program to measure speed of dot products (#1041)
    
    On my Mac, the direct Q4_1 product is marginally slower
    (~69 vs ~55 us for Q4_0). The SIMD-ified ggml version
    is now almost 2X slower (~121 us).
    
    On a Ryzen 7950X CPU, the direct product for Q4_1 quantization
    is faster than the AVX2 implementation (~60 vs ~62 us).
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

commit 7faa7460f03bdd88becf1e659cf359f274055404
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 18 20:10:26 2023 +0300

    readme : update hot topics about new LoRA functionality

commit 5af8e32238c7d9c4cdb7fc640472c9a26538b9da
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 17 18:00:10 2023 +0300

    ci : do not run on drafts

commit 42747220b4cac548b6e3059b66b3e960b517cfa4
Author: Ivan Komarov <Ivan.Komarov@dfyz.info>
Date:   Tue Apr 18 03:15:50 2023 +0200

    Do not close file after mmap (Windows version) (#1034)

commit e9298af3896b536d0c6d740447dc764e56b22102
Author: Atsushi Tatsuma <yoshoku@outlook.com>
Date:   Tue Apr 18 04:34:35 2023 +0900

    readme : add Ruby bindings (#1029)

commit 4ad73137a1aadc40a62f7101aab883f69e7172c6
Author: Cameron <csteele@steelecameron.com>
Date:   Mon Apr 17 11:26:23 2023 -0700

    add 4_0 to default outfile namestr dict (#1031)
    
    this came up when trying to convert the gpt4all-lora-unfiltered-quantized.bin file

commit 315a95a4d30db726fb7d244dd3b9e90a83fb1616
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Mon Apr 17 17:28:55 2023 +0200

    Add LoRA support (#820)

commit efd05648c88a0923a55f56e7ce1b0f9c33410afb
Author: Arik Poznanski <arikpoz@users.noreply.github.com>
Date:   Mon Apr 17 17:41:53 2023 +0300

    llama : well-defined static initialization of complex objects (#927)
    
    * Replaced static initialization of complex objects with a initialization on first use. This prevents an undefined behavior on program run, for example, crash in Release build, works in Debug build
    
    * replaced use of auto with exact type to avoid using -std=c++14
    
    * Made the assessors functions for static maps be static const

commit eb17a026fd23d1c1b612fa4600f7f5c58e501a28
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 17 17:31:06 2023 +0300

    quantize-stats : fix bug in --type argument

commit 69b740289f9b3756ea9dd2a23f241c6f688d88b9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 17 16:16:23 2023 +0300

    ggml : avoid using ggml_fp16_to_fp32() and ggml_fp32_to_fp16() in ggml.c

commit f266259ad9a2bce5a34d919592310147af23f3dc
Author: Ivan Komarov <Ivan.Komarov@dfyz.info>
Date:   Mon Apr 17 15:10:57 2023 +0200

    Speedup the AVX-512 implementation of ggml_vec_dot_q4_0() (#933)

commit 47f61aaa5f76d04286792e2fbd0c95b659ab2af0
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Sun Apr 16 21:27:38 2023 +0200

    Fix: do not close file on mmap (#1017)

commit 3173a62eb9f90b94fb3184131032c1c8b7aa8d86
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 16 13:58:48 2023 +0300

    stdout : vertical align outputs for better readibility

commit 489537e6cf6c93b74a029a11533dbcaa89791dcc
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Sun Apr 16 12:13:00 2023 +0200

    examples: add missing <ctime> include for time() (#1011)

commit 2d3481c72125cd388258864c7ad8d7d36777bad7
Author: nanahi <130121847+na-na-hi@users.noreply.github.com>
Date:   Sun Apr 16 17:13:42 2023 +0800

    Fix msys2 build error and warnings (#1009)

commit 74f5899df4a6083fc467b620baa1cf821e37799d
Author: comex <comexk@gmail.com>
Date:   Sat Apr 15 14:53:21 2023 -0700

    convert.py: Fix loading safetensors and ggml format on Windows (#991)
    
    Calling `mmap.mmap` on Windows apparently resets the file offset of the
    raw file object (and makes the BufferedReader return a *negative* file
    offset).  For safetensors, avoid using the file offset after calling
    mmap.  For GGML format, explicitly save and restore the offset.
    
    Fixes #966.

commit 2f7c8e014e3c0ceaf39688845c2ff6f919fb03b7
Author: Stephan Walter <stephan@walter.name>
Date:   Sat Apr 15 18:28:56 2023 +0000

    Fix potential int8 overflow in non-SIMD vec_dot (#986)

commit 0ad964631f9b3970f1936008fcfb1eadef59c7ed
Author: Stephan Walter <stephan@walter.name>
Date:   Sat Apr 15 16:25:38 2023 +0000

    Refactor ggml.c for future tensor types (#1001)

commit e95b6554b493e71a0275764342e09bd5784a7026
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 15 17:53:22 2023 +0300

    ggml : add Q8_0 quantization for intermediate results (#951)
    
    * ggml : add Q8_0 quantization for intermediate results
    
    * quantize-stats : fix test + add it to Makefile default
    
    * Q8: use int8_t, AVX/AVX2 optimizations
    
    * ggml : fix quantize_row_q8_0() ARM_NEON rounding
    
    * minor : updates after rebase to latest master
    
    * quantize-stats : delete obsolete strings
    
    * ggml : fix q4_1 dot func
    
    ---------
    
    Co-authored-by: Stephan Walter <stephan@walter.name>

commit aa485cee334e84437e21681c14b6f80b65876d8b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 15 14:25:45 2023 +0300

    ggml : use posix_memalign on non-Windows env

commit c12b14b77fced0ce9a0e2d81f670c3a746dec251
Author: Ivan Komarov <Ivan.Komarov@dfyz.info>
Date:   Sat Apr 15 07:51:54 2023 +0200

    benchmark : fix result validation in benchmark-q4_0-matmult (#987)

commit 106faaf2971d6c89d6010279a9a95737772470ef
Author: katsu560 <118887472+katsu560@users.noreply.github.com>
Date:   Sat Apr 15 14:51:11 2023 +0900

    cmake : add finding the OpenBLAS header file (#992)

commit c85e03d12e4b8af22cb13aa9c618dcd5935862fd
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Fri Apr 14 21:58:43 2023 +0200

    Revert "main : alternative instruct mode (Vicuna support, etc.) (#863)" (#982)
    
    This reverts commit f4d277ae17247ee51129ef1a9ff74d377cc90b1b.

commit 489093548c89c67520109ab25c4df4a4614a32a0
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Fri Apr 14 21:46:49 2023 +0200

    py : bump sentencepiece to 0.1.98 to support Python 3.11 (#976)

commit 93265e988af32b8be314bfed334f795a3037555d
Author: Stephan Walter <stephan@walter.name>
Date:   Fri Apr 14 19:39:48 2023 +0000

    make : fix dependencies, use auto variables (#983)

commit c56b7152690ca25cfd66b20210b3629e6c1e739b
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Fri Apr 14 20:05:37 2023 +0200

    Expose type name from ggml (#970)
    
    Avoid duplication of type names in utils
    
    Co-authored-by: Håkon H. Hitland <haakon@likedan.net>

commit f4d277ae17247ee51129ef1a9ff74d377cc90b1b
Author: Tomáš Pazdiora <tomas.pazdiora@gmail.com>
Date:   Fri Apr 14 17:19:17 2023 +0200

    main : alternative instruct mode (Vicuna support, etc.) (#863)
    
    * Add support for configs, add configurable prefixes / suffixes, deprecate instruct mode, add stop prompt
    
    * Add multiline mode, update text input.
    
    * bugfix
    
    * update implementation
    
    * typos
    
    * Change --multiline implementation to be toggled by EOF.
    
    * bugfix
    
    * default multiline mode
    
    * add more configs
    
    * update formating
    
    * update formatting
    
    * apply suggestions

commit c9a59b70a54e0bc05777df287feaea3dbe0310c4
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Fri Apr 14 08:43:55 2023 -0600

    ggml : add unary and binary map operations (#874)
    
    * GGML map ops proof of concept.
    
    * Various cleanups.
    
    Add handling for task setting.
    
    Add handling for ggml_compute_backward.
    
    Rename functions to ggml_map_unary_f32 and ggml_map_binary_f32
    
    Fix compiler warnings related to casting function pointers and `void *`
    
    Reorder functions and definitions based on the GGML op number.
    
    Use typedefs for map op function pointer types.
    
    * Fix position of map ops cases in ggml_compute_forward

commit a32f7acc9f54dba1c728cb1e596bd00bf3b4eb5f
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Fri Apr 14 15:37:11 2023 +0200

    py : cleanup dependencies (#962)
    
    after #545 we do not need torch, tqdm and requests in the dependencies

commit 43ffdefb7424f79a3d510c199e2ea86684b4f824
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Fri Apr 14 14:23:21 2023 +0200

    py : fix flake8 and isort nitpicks (#960)

commit 1623a6e9b46453bff30afd7d0f6c3fd188499c2f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Apr 14 13:31:29 2023 +0300

    ggml : minor

commit c14e0d2f23e6d1e785255f4da8c253c1b4723659
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Apr 14 13:31:15 2023 +0300

    ggml : always allocate buffers with size multiple of GGML_MEM_ALIGN

commit 723dac55fa2ba7adc6e3fc8609781d1ad0378906
Author: comex <comexk@gmail.com>
Date:   Fri Apr 14 00:03:03 2023 -0700

    py : new conversion script (#545)
    
    Current status: Working, except for the latest GPTQ-for-LLaMa format
      that includes `g_idx`.  This turns out to require changes to GGML, so
      for now it only works if you use the `--outtype` option to dequantize it
      back to f16 (which is pointless except for debugging).
    
      I also included some cleanup for the C++ code.
    
      This script is meant to replace all the existing conversion scripts
      (including the ones that convert from older GGML formats), while also
      adding support for some new formats.  Specifically, I've tested with:
    
      - [x] `LLaMA` (original)
      - [x] `llama-65b-4bit`
      - [x] `alpaca-native`
      - [x] `alpaca-native-4bit`
      - [x] LLaMA converted to 'transformers' format using
            `convert_llama_weights_to_hf.py`
      - [x] `alpaca-native` quantized with `--true-sequential --act-order
            --groupsize 128` (dequantized only)
      - [x] same as above plus `--save_safetensors`
      - [x] GPT4All
      - [x] stock unversioned ggml
      - [x] ggmh
    
      There's enough overlap in the logic needed to handle these different
      cases that it seemed best to move to a single script.
    
      I haven't tried this with Alpaca-LoRA because I don't know where to find
      it.
    
      Useful features:
    
      - Uses multiple threads for a speedup in some cases (though the Python
        GIL limits the gain, and sometimes it's disk-bound anyway).
    
      - Combines split models into a single file (both the intra-tensor split
        of the original and the inter-tensor split of 'transformers' format
        files).  Single files are more convenient to work with and more
        friendly to future changes to use memory mapping on the C++ side.  To
        accomplish this without increasing memory requirements, it has some
        custom loading code which avoids loading whole input files into memory
        at once.
    
      - Because of the custom loading code, it no longer depends in PyTorch,
        which might make installing dependencies slightly easier or faster...
        although it still depends on NumPy and sentencepiece, so I don't know
        if there's any meaningful difference.  In any case, I also added a
        requirements.txt file to lock the dependency versions in case of any
        future breaking changes.
    
      - Type annotations checked with mypy.
    
      - Some attempts to be extra user-friendly:
    
          - The script tries to be forgiving with arguments, e.g. you can
            specify either the model file itself or the directory containing
            it.
    
          - The script doesn't depend on config.json / params.json, just in
            case the user downloaded files individually and doesn't have those
            handy.  But you still need tokenizer.model and, for Alpaca,
            added_tokens.json.
    
          - The script tries to give a helpful error message if
            added_tokens.json is missing.

commit 0f07cacb05f49704d35a39aa27cfd4b419eb6f8d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Apr 14 09:45:42 2023 +0300

    ggml : fix q4_1 dot product types

commit c5d70f5c9ea5a8f0f6b0d6aa741455978a1dabfd
Author: Howard Su <howard0su@gmail.com>
Date:   Fri Apr 14 14:24:52 2023 +0800

    ggml : optimize rope function to avoid call powf in the tight loop (#807)

commit be87b6ed20a5f7528bf491a83e759a9fc6a24fea
Author: Gary Linscott <glinscott@gmail.com>
Date:   Thu Apr 13 14:50:42 2023 -0700

    perplexity : add support for batch size to `--perplexity` (#407)
    
    * Add support to batch size for perplexity
    
    * Revert "Fix memory allocation issues and seg faults"
    
    This reverts commit 4870e455b3653f7d7769fa5772b2c90ffad088df.
    
    * update from merge
    
    * Remove perplexity from main
    
    * updates
    
    * Update batch size for efficiency

commit 0e07e6a8399fd993739a3ba3c6f95f92bfab6f58
Author: CRD716 <crd716@gmail.com>
Date:   Thu Apr 13 10:39:25 2023 -0500

    common : remove unnecessary includes (#947)

commit a3a2a0eda8828b60436e9f69d9ac2c1060d03e7a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 13 18:36:40 2023 +0300

    ggml : add GGML_DEFAULT_N_THREADS

commit d990e3fffc5b0f5448e90a16c79a4f2675100af0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 13 18:32:36 2023 +0300

    ggml : speed-up ggml_vec_dot_q4_1() ARM_NEON + 32-bit ARM support (#900)
    
    * ggml : speed-up q4_1 ARM_NEON by ~5%
    
    * ggml : implement vaddvq when missing
    
    * ggml : implement vminvq and vmaxvq when missing
    
    * ggml : implement vzip when missing
    
    * ggml : fix comment
    
    * ggml : try to use correct ifdef

commit 9190e8eac8bdc108c40d2d7505e9b45fa773251f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 13 18:04:45 2023 +0300

    llama : merge llama_internal.h into llama.h
    
    Hide it behind an #ifdef

commit c85980acd04631a7c43d13676276f76ec72f5dfe
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 13 18:01:22 2023 +0300

    gitignore : benchmark

commit 6232f2d7fd7a22d5eeb62182b2f21fcf01359754
Author: Stephan Walter <stephan@walter.name>
Date:   Thu Apr 13 14:59:50 2023 +0000

    ggml : optimize non-SIMD Q4_0 vector dot product (#703)

commit 6c248707f51c8a50f7792e7f7787ec481881db88
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Thu Apr 13 16:08:32 2023 +0200

    ggml : introduce GGML_ALIGNED_MALLOC/GGML_ALIGNED_FREE macros (#884)
    
    which allows us to use aligned_alloc or _aligned_malloc functions

commit 8cda5c981d0bf4dcb7664194b2cb9a06e2dbdd54
Author: CRD716 <crd716@gmail.com>
Date:   Thu Apr 13 09:03:57 2023 -0500

    fix whitespace (#944)

commit ec29272175d7a79681d9919f3e755b1bcefa0478
Author: CRD716 <crd716@gmail.com>
Date:   Thu Apr 13 08:59:53 2023 -0500

    readme : remove python 3.10 warning (#929)

commit 7e941b95eba067cb5b92785e642fd803657376ee
Author: Genkagaku.GPT <hlhr202@163.com>
Date:   Thu Apr 13 21:54:27 2023 +0800

    readme : llama node binding (#911)
    
    * chore: add nodejs binding
    
    * chore: add nodejs binding

commit c729ff730a46a135817a3d9988a097e3678a9722
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Thu Apr 13 15:49:05 2023 +0200

    flake.nix: add all binaries from bin (#848)

commit 4579af95e8e16910f6dbab0994917a5b3901f0cf
Author: Judd <foldl@users.noreply.github.com>
Date:   Thu Apr 13 21:43:22 2023 +0800

    zig : update build.zig (#872)
    
    * update
    
    * update readme
    
    * minimize the changes.
    
    ---------
    
    Co-authored-by: zjli2019 <zhengji.li@ingchips.com>

commit 8c3ffc2f048a372639906fb30ec3c2070288d3be
Author: Vladimir <bogdad@gmail.com>
Date:   Thu Apr 13 15:24:30 2023 +0200

    ggml : update cblas_sgemm columns var to be more reasonable (#838)

commit 107980d970808c2ccf9334ad033e2782a560b911
Author: niansa/tuxifan <anton-sa@web.de>
Date:   Thu Apr 13 15:03:39 2023 +0200

    examples : add -n to alpaca and gpt4all scripts (#706)

commit 585d91a156794d30eec16ebe67c8d7a1d41406c1
Author: anzz1 <anzz1@live.com>
Date:   Thu Apr 13 15:48:21 2023 +0300

    cmake : add explicit F16C option (x86) (#576)
    
    Fixes building for x86 processors missing F16C featureset
    MSVC not included, as in MSVC F16C is implied with AVX2/AVX512

commit 95ea26f6e92d620a5437f576b80868aee7f808d6
Author: SebastianApel <13675545+SebastianApel@users.noreply.github.com>
Date:   Thu Apr 13 14:46:23 2023 +0200

    benchmark : add tool for timing q4_0 matrix multiplication (#653)
    
    * Initial version of q4_0 matrix multiplication benchmark
    
    * Bugfix: Added dependency to ggml.o to benchmark
    
    * Reviewer requests: added parameter for threads, switched to ggml_time_us()
    
    * Reviewer input: removed rtsc, use epsilon for check
    
    * Review comment: Removed set_locale
    
    * Feature: Param for numer of iterations, Bugfix for use of parameter threads
    
    * Reviewer suggestion: Moved to examples
    
    * Reviewer feedback: Updated clean: and benchmark: sections
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 82d146df9b43cf677e0dbce20b03cf864958a0cc
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Thu Apr 13 11:33:16 2023 +0200

    do not force the prompt file to end with a new line (#908)

commit e7f6997f897a18b6372a6460e25c5f89e1469f1d
Author: Stephan Walter <stephan@walter.name>
Date:   Wed Apr 12 15:06:16 2023 +0000

    Don't crash on ftype (formerly f16) == 4 (#917)

commit f76cb3a34d6a6b03afb96650e39495f201eac042
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 12 14:48:57 2023 +0300

    readme : change "GPU support" link to discussion

commit 782438070f7568380755ffc7bf5e09b20c1e8272
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 12 14:31:12 2023 +0300

    readme : update hot topics with link to "GPU support" issue

commit 4dbbd407500cf500ca5f47e4e947635797997c05
Author: Nicolai Weitkemper <kontakt@nicolaiweitkemper.de>
Date:   Wed Apr 12 08:46:20 2023 +0200

    readme: link to sha256sums file (#902)
    
    This is to emphasize that these do not need to be obtained from elsewhere.

commit 8b679987cdce292ff36bd741f6715e4927e26f9b
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Tue Apr 11 21:45:44 2023 +0200

    Fix whitespace, add .editorconfig, add GitHub workflow (#883)

commit 3e6e70d8e8917b5bd14c7c9f9b89a585f1ff0b31
Author: Stephan Walter <stephan@walter.name>
Date:   Tue Apr 11 15:03:51 2023 +0000

    Add enum llama_ftype, sync ggml_type to model files (#709)

commit 2663d2c6784ad7b77998c6874df25648d597f74b
Author: comex <comexk@gmail.com>
Date:   Tue Apr 11 06:19:54 2023 -0700

    Windows fixes (#890)
    
    Mostly for msys2 and mingw64 builds, which are different from each other
    and different from standard Visual Studio builds.  Isn't Windows fun?
    
    - Define _GNU_SOURCE in more files (it's already used in ggml.c for
      Linux's sake).
    
    - Don't use PrefetchVirtualMemory if not building for Windows 8 or later
      (mingw64 doesn't by default).  But warn the user about this situation
      since it's probably not intended.
    
    - Check for NOMINMAX already being defined, which it is on mingw64.
    
    - Actually use the `increment` variable (bug in my `pizza` PR).
    
    - Suppress unused variable warnings in the fake pthread_create and
      pthread_join implementations for Windows.
    
    - (not Windows-related) Remove mention of `asprintf` from comment;
      `asprintf` is no longer used.
    
    Fixes #871.

commit a0caa34b162449b5c13b8d604573053300ff54a1
Author: qouoq <qouoq@fastmail.com>
Date:   Tue Apr 11 04:41:53 2023 +0800

    Add BAIR's Koala to supported models (#877)

commit 461ba9e66ed3885f80680d71495e055580573c74
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 10 23:20:01 2023 +0300

    ggml : fix WASM build

commit c3ac702e5ee3533457e0489df4906ee112fe88e7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 10 22:40:28 2023 +0300

    ggml : add ggml_cont() + optimize ggml_cpy() for contiguous dst

commit 9d634ef452d0fc24fcd49592952d13d0ab0f41b7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 10 19:32:45 2023 +0300

    ggml : remove trailing whitespaces

commit d9a239c4104c888eafda672c1e42c9bbc5084cb8
Author: Marco Matthies <71844+marcom@users.noreply.github.com>
Date:   Mon Apr 10 19:57:59 2023 +0200

    Simplify to include lower-case windows.h always, fix compile on mingw32 (#747)

commit 684da25926e5c505f725b4f10b5485b218fa1fc7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 10 19:29:48 2023 +0300

    ggml : fix quantize_row_q4_1() ARM_NEON (close #876)

commit 180b693a47b6b825288ef9f2c39d24b6eea4eea6
Author: comex <comexk@gmail.com>
Date:   Sat Apr 8 13:08:21 2023 -0700

    Print model version.
    
    Also improve model type printing, and fix indentation of an unrelated
    switch statement.

commit f963b63afa0e057cfb9eba4d88407c6a0850a0d8
Author: comex <comexk@gmail.com>
Date:   Sat Apr 8 12:24:37 2023 -0700

    Rewrite loading code to try to satisfy everyone:
    
    - Support all three formats (ggml, ggmf, ggjt).  (However, I didn't
      include the hack needed to support GPT4All files without conversion.
      Those can still be used after converting them with convert.py from my
      other PR.)
    
    - Support both mmap and read (mmap is used by default, but can be
      disabled with `--no-mmap`, and is automatically disabled for pre-ggjt
      files or on platforms where mmap is not supported).
    
    - Support multi-file models like before, but automatically determine the
      number of parts rather than requiring `--n_parts`.
    
    - Improve validation and error checking.
    
    - Stop using the per-file type field (f16) entirely in favor of just
      relying on the per-tensor type/size fields.  This has no immediate
      benefit, but makes it easier to experiment with different formats, and
      should make it easier to support the new GPTQ-for-LLaMa models in the
      future (I have some work in progress on that front).
    
    - Support VirtualLock on Windows (using the same `--mlock` option as on
      Unix).
    
        - Indicate loading progress when using mmap + mlock.  (Which led me
          to the interesting observation that on my Linux machine, with a
          warm file cache, mlock actually takes some time, whereas mmap
          without mlock starts almost instantly...)
    
          - To help implement this, move mlock support from ggml to the
            loading code.
    
    - madvise/PrefetchVirtualMemory support (based on #740)
    
    - Switch from ifstream to the `fopen` family of functions to avoid
      unnecessary copying and, when mmap is enabled, allow reusing the same
      file descriptor for both metadata reads and mmap (whereas the existing
      implementation opens the file a second time to mmap).
    
    - Quantization now produces a single-file output even with multi-file
      inputs (not really a feature as much as 'it was easier this way').
    
    Implementation notes:
    
    I tried to factor the code into more discrete pieces than before.
    
    Regarding code style: I tried to follow the code style, but I'm naughty
    and used a few advanced C++ features repeatedly:
    
    - Destructors to make it easier to ensure everything gets cleaned up.
    
    - Exceptions.  I don't even usually use exceptions when writing C++, and
      I can remove them if desired... but here they make the loading code
      much more succinct while still properly handling a variety of errors,
      ranging from API calls failing to integer overflow and allocation
      failure.  The exceptions are converted to error codes at the
      API boundary.)
    
    Co-authored-by: Pavol Rusnak <pavol@rusnak.io> (for the bit I copied from #740)

commit aaf3b23debc1fe1a06733c8c6468fb84233cc44f
Author: Tomáš Pazdiora <tomas.pazdiora@gmail.com>
Date:   Sat Apr 8 17:49:39 2023 +0200

    fix for windows utf-8 input (#840)
    
    Use UTF-16 as input on Windows, since UTF-8 does not work and reads multibyte characters as zeros

commit f2d1c472946dee2aba9077e8df73346796752b10
Author: eiery <19350831+eiery@users.noreply.github.com>
Date:   Sat Apr 8 07:15:17 2023 -0400

    cmake should link openblas properly with -lopenblas like how it's done in the makefile (#839)

commit 317fb12fbd7cef5d86476574bffe0e904af884ca
Author: lon <114724657+longregen@users.noreply.github.com>
Date:   Sat Apr 8 07:04:23 2023 -0300

    Add new binaries to flake.nix (#847)

commit 62cfc54f77e519057110265b52b0d614fa363e2a
Author: unbounded <haakon@likedan.net>
Date:   Sat Apr 8 00:09:18 2023 +0200

    Add quantize-stats command for testing quantization (#728)
    
    Command that calculates some statistics over the errors introduced by
    quantization, like mean square error, max error and some percentile errors for layer
    weights. Should be useful for testing quantization improvements.
    
    Exposes some internal state from ggml and llama for testing

commit 698f7b5d6316a1f8453b3b32fd0d637d24952ffd
Author: bhubbb <79117352+bhubbb@users.noreply.github.com>
Date:   Sat Apr 8 02:11:58 2023 +1000

    make : add libllama.so target for llama-cpp-python (#797)
    
    I was able to get llama-cpp-python working but only when I build libllama.so with make.

commit c1950c343109ab1fd15fc2ae1c83650c85d4eeef
Author: iacore <74560659+iacore@users.noreply.github.com>
Date:   Fri Apr 7 16:05:29 2023 +0000

    zig : don't link examples/common.cpp for non-example (#814)

commit 4953e9007f86327aabc8312a7211c18019a3a40e
Author: Ivan Stepanov <ivanstepanovftw@gmail.com>
Date:   Fri Apr 7 19:02:12 2023 +0300

    llama : always sort logits before nucleus sampling (#812)
    
    * Always sort logits before nucleus sampling
    
    * remove second normalization
    
    - fix windows build
    - remove normalization since std::discrete_distribution does not require it

commit cc9cee8e9e7598bd280295f6264f36d3a9224006
Author: Sergey Alirzaev <zl29ah@gmail.com>
Date:   Thu Apr 6 17:59:11 2023 +0200

    Do not crash when it has nothing to say. (#796)
    
    Otherwise observing this in the interactive mode:
    /usr/lib/gcc/x86_64-pc-linux-gnu/12/include/g++-v12/bits/stl_vector.h:1230: reference std::vector<int>::back() [_Tp = int, _Alloc = std::allocator<int>]: Assertion '!this->empty()' failed.

commit d2beca95dcfcd6f1145886e914b879ffc3604b7a
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Thu Apr 6 08:56:58 2023 +0200

    Make docker instructions more explicit (#785)

commit eeaa7b0492fc79baab8bb1fe195d6c87159f2bd3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 5 22:11:03 2023 +0300

    ggml : multi-thread ggml_rope() (~3-4 times faster on M1) (#781)

commit 986b6ce9f99503c51ec5afd8a10baa32359434c6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 5 22:07:33 2023 +0300

    ggml, llama : avoid heavy V transpose + improvements (#775)
    
    ggml :
    
    - added ggml_view_3d()
    - ggml_view_tensor() now inherits the stride too
    - reimplement ggml_cpy() to account for dst stride
    - no longer require tensor->data to be memory aligned
    
    llama :
    
    - compute RoPE on 32-bit tensors (should be more accurate)
    - store RoPE-ed K in the KV cache
    - store transposed V in the KV cache (significant speed-up)
    - avoid unnecessary Q copy

commit 34162989297fdfe3ab7305451ce55bc87e3f4c9c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 5 19:54:30 2023 +0300

    Update README.md

commit 5a8c4f624077373a198cd562146ffa67b02ebc75
Author: Ivan Stepanov <ivanstepanovftw@gmail.com>
Date:   Wed Apr 5 19:20:05 2023 +0300

    llama : define non-positive top_k; top_k range check (#779)
    
    * Define non-positive top_k; top_k range check
    
    * minor : brackets
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit ff05d05c960076b42f027e4d318cbd6ea59b3030
Author: at8u <129688334+at8u@users.noreply.github.com>
Date:   Wed Apr 5 15:59:13 2023 +0000

    miku.sh : add executable bit (#780)

commit 62b3e81aaeafb282934de8b21de13b0104f12f8c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 5 18:58:06 2023 +0300

    media : add logos and banners

commit 8d10406d6ee230e6c1a96590cc8273f86ae606f8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 5 18:56:20 2023 +0300

    readme : change logo + add bindings + add uis + add wiki

commit ed1c214e667d58ac442b3c6664831fe0eed2941d
Author: iacore <74560659+iacore@users.noreply.github.com>
Date:   Wed Apr 5 15:06:02 2023 +0000

    zig : add build.zig (#773)
    
    Co-authored-by: Locria Cyber <74560659+locriacyber@users.noreply.github.com>

commit 0c44427df10ee024b4e7ef7bfec56e993daff1db
Author: Ivan Stepanov <ivanstepanovftw@gmail.com>
Date:   Wed Apr 5 17:38:37 2023 +0300

    make : missing host optimizations in CXXFLAGS (#763)

commit 594cc95fabab0b662dabba3ea619ca5dca18bf6b
Author: Adithya Balaji <adithya.b94@gmail.com>
Date:   Wed Apr 5 16:36:12 2023 +0200

    readme : update with CMake and windows example (#748)
    
    * README: Update with CMake and windows example
    
    * README: update with code-review for cmake build

commit 88ed5761b869a221bc26847ff3f6977e7ee6425e
Author: at8u <129688334+at8u@users.noreply.github.com>
Date:   Wed Apr 5 14:32:42 2023 +0000

    examples : add Miku.sh (#724)
    
    * Add Miku.sh to examples
    
    * Add missing line to prompt in Miku.sh
    
    * Add --keep param to Miku.sh
    
    * Remove '[end_of_conversation]' line from Miku.sh
    
    No longer is necessary.

commit 58c438cf7dfbbef710b1905a453a38a8a9ced07d
Author: Andrew Duffy <a10y@users.noreply.github.com>
Date:   Wed Apr 5 11:44:24 2023 +0100

    Add Accelerate/BLAS when using Swift (#765)

commit 53dbba769537e894ead5c6913ab2fd3a4658b738
Author: mgroeber9110 <45620825+mgroeber9110@users.noreply.github.com>
Date:   Mon Apr 3 18:00:55 2023 +0200

    Windows: reactive sigint handler after each Ctrl-C (#736)

commit 437e77855a54e69c86fe03bc501f63d9a3fddb0e
Author: SebastianApel <13675545+SebastianApel@users.noreply.github.com>
Date:   Mon Apr 3 09:52:28 2023 +0200

    10+% performance improvement of ggml_vec_dot_q4_0 on AVX2 (#654)
    
    * Performance improvement of AVX2 code
    * Fixed problem with MSVC compiler
    * Reviewer comments: removed double semicolon, deleted empty line 1962

commit cd7fa956904cb8e321b72b3499f4a3a82e43c266
Author: Ivan Stepanov <ivanstepanovftw@gmail.com>
Date:   Mon Apr 3 03:19:04 2023 +0300

    Define non-positive temperature behavior (#720)

commit a0c05164168297c04737936ad0cad849a512547a
Author: bsilvereagle <bsilvereagle@users.noreply.github.com>
Date:   Sun Apr 2 15:13:03 2023 -0700

    Remove torch GPU dependencies from the Docker.full image (#665)
    
    By using `pip install torch --index-url https://download.pytorch.org/whl/cpu`
    instead of `pip install torch` we can specify we want to install a CPU-only version
    of PyTorch without any GPU dependencies. This reduces the size of the Docker image
    from 7.32 GB to 1.62 GB

commit d8d4e865cd481b18f10508ffee35db903767ef5c
Author: Thatcher Chamberlin <j.thatcher.c@gmail.com>
Date:   Sun Apr 2 06:48:57 2023 -0400

    Add a missing step to the gpt4all instructions (#690)
    
    `migrate-ggml-2023-03-30-pr613.py` is needed to get gpt4all running.

commit e986f94829bae0b9e66b326acbbba179931c84f1
Author: Christian Falch <875252+chrfalch@users.noreply.github.com>
Date:   Sun Apr 2 12:23:04 2023 +0200

    Added api for getting/setting the kv_cache (#685)
    
    The api provides access methods for retrieving the current memory buffer for the kv_cache and its token number.
    It also contains a method for setting the kv_cache from a memory buffer.
    
    This makes it possible to load/save history - maybe support --cache-prompt paramater as well?
    
    Co-authored-by: Pavol Rusnak <pavol@rusnak.io>

commit c0bb1d3ce21005ab21d686626ba87261a6e3a660
Author: Marian Cepok <marian.cepok@gmail.com>
Date:   Sun Apr 2 12:21:31 2023 +0200

    ggml : change ne to int64_t (#626)

commit 6e7801d08d81c931a5427bae46f00763e993f54a
Author: Leonardo Neumann <leonardo@neumann.dev.br>
Date:   Sun Apr 2 04:56:20 2023 -0300

    examples : add gpt4all script (#658)

commit 81040f10aae3160317c5787c9c59acb219927826
Author: Stephan Walter <stephan@walter.name>
Date:   Sun Apr 2 07:18:53 2023 +0000

    llama : do not allocate KV cache for "vocab_only == true" (#682)
    
    Fixes sanitizer CI

commit c4f89d8d73aab4318a6c61e3835135adfcf55407
Author: Fabian <cmdrf@users.noreply.github.com>
Date:   Sun Apr 2 09:17:05 2023 +0200

    make : use -march=native -mtune=native on x86 (#609)

commit 5b70e7de4c0b8186669d0c5609ba61a2d46de562
Author: Murilo Santana <mvrilo@gmail.com>
Date:   Sat Apr 1 23:41:12 2023 -0300

    fix default params for examples/main (#697)

commit a717cba8440b380f43cd3e2510862fc1ea3de9a2
Author: Ikko Eltociear Ashimine <eltociear@gmail.com>
Date:   Sun Apr 2 01:38:18 2023 +0900

    py: huggingface -> Hugging Face (#686)

commit d0a7f742e76bb48c0bd852f0b3bf09ec0b75b200
Author: rimoliga <53384203+rimoliga@users.noreply.github.com>
Date:   Sat Apr 1 11:57:30 2023 -0300

    readme: replace termux links with homepage, play store is deprecated (#680)

commit 0d054e292e5492981867be69c788edd04dc8adeb
Author: Slaren <2141330+slaren@users.noreply.github.com>
Date:   Fri Mar 31 20:03:48 2023 +0200

    Show error message when -f fails

commit 3525899277d2e2bdc8ec3f0e6e40c47251608700
Author: Stephan Walter <stephan@walter.name>
Date:   Fri Mar 31 19:19:16 2023 +0000

    Enable -std= for cmake builds, fix warnings (#598)

commit 1d08882afa647c44195f4f6495a68ea455650cae
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Fri Mar 31 17:55:52 2023 +0200

    Optimize AVX2 ggml_vec_dot_q4_0 (#642)

commit 02c5b27e91a6d18cf1043d3a2d8dbc59610ac257
Author: perserk <perserk@gmail.com>
Date:   Fri Mar 31 16:55:44 2023 +0500

    Add AVX acceleration (#617)
    
    * ggml : add AVX quantize_row_q4_0()
    
    * ggml : add AVX ggml_vec_dot_q4_0()
    
    * ggml : refactor AVX part of ggml_vec_dot_q4_0()
    
    https://github.com/ggerganov/llama.cpp/pull/617#issuecomment-1489985645

commit cbef542879962fdc491656cd0c8cadd65a5f1356
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Wed Mar 29 21:31:24 2023 +0200

    py : cleanup the code
    
    - use f-strings where possible
    - drop first param of encode/decode functions since "utf-8" is the default

commit 9733104be5389ebb1ff05095eca2a70280cd875a
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Fri Mar 31 00:52:06 2023 +0200

    drop quantize.py (now that models are using a single file)

commit 3df890aef432ce68143cfafcd7caf828bc4c3e55
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 30 22:31:54 2023 +0300

    readme : update supported models

commit ee0c40dd6de8c3c658ae43199939ef40bb1cf408
Author: Justine Tunney <jtunney@gmail.com>
Date:   Thu Mar 30 05:42:56 2023 -0700

    Introduce GGML migration tool for new file format
    
    If you deleted your old Meta LLaMA .pth files, then the
    migrate-ggml-2023-03-30-pr613.py script will allow you to convert your
    old ggml files into the new mmap()'able format.
    
    See #613

commit 6f23ba5ee235cbcb1eedd63b98422dd8d4392a78
Author: Justine Tunney <jtunney@gmail.com>
Date:   Thu Mar 30 01:53:36 2023 -0700

    Ensure --mlock works properly with mmap() support

commit 78ca9838ee36660a776e97e3391b6fb5dcaacf7f
Author: Justine Tunney <jtunney@gmail.com>
Date:   Wed Mar 29 13:51:37 2023 -0700

    Make loading weights 10-100x faster
    
    This is a breaking change that's going to give you three benefits:
    
    1. Your inference commands should load 100x faster
    2. You may be able to safely load models 2x larger
    3. You can run many concurrent inference processes
    
    This was accomplished by changing the file format so we can mmap()
    weights directly into memory without having to read() or copy them
    thereby ensuring the kernel can make its file cache pages directly
    accessible to our inference processes; and secondly, that the file
    cache pages are much less likely to get evicted (which would force
    loads to hit disk) because they're no longer competing with memory
    pages that were needlessly created by gigabytes of standard i/o.
    
    The new file format supports single-file models like LLaMA 7b, and
    it also supports multi-file models like LLaMA 13B. Our Python tool
    now merges the foo.1, foo.2, etc. files back into a single file so
    that the C++ code which maps it doesn't need to reshape data every
    time. That's made llama.cpp so much simpler. Much of its load code
    has now been deleted.
    
    Furthermore, this change ensures that tensors are aligned properly
    on a 32-byte boundary. That opens the door to seeing if we can get
    additional performance gains on some microprocessors, by using ops
    that require memory alignment.
    
    Lastly note that both POSIX and the Windows platform are supported
    
    Fixes #91

commit a017390358cdb23fffb30988dc84bb190d0403ca
Author: Slaren <2141330+slaren@users.noreply.github.com>
Date:   Wed Mar 29 22:22:36 2023 +0200

    Initial windows support (untested)

commit ac184d514723902f9b05b688703b1be6e8dc65de
Author: Slaren <2141330+slaren@users.noreply.github.com>
Date:   Wed Mar 29 08:53:14 2023 +0200

    Always initialize mm_addr and mm_length in llama_model

commit 276e5b781155e3bbe6834472c58f03dfe62efabe
Author: Slaren <2141330+slaren@users.noreply.github.com>
Date:   Wed Mar 29 08:31:26 2023 +0200

    Unmap the file in llama_free

commit d68c5dc4356c8f49e933df210f2ceca5002a8118
Author: Slaren <2141330+slaren@users.noreply.github.com>
Date:   Wed Mar 29 06:18:18 2023 +0200

    Make mmap_file static

commit 64bde3ffd4aef799acb790a3eedddbd0a0612108
Author: Slaren <2141330+slaren@users.noreply.github.com>
Date:   Wed Mar 29 05:38:57 2023 +0200

    Fix ggml_init_params in quantize

commit c03ae8dca1d7c451054754979e60a6de1f64c3cd
Author: Slaren <2141330+slaren@users.noreply.github.com>
Date:   Wed Mar 29 02:03:43 2023 +0200

    Add mmap support for model files

commit 3bcc129ba881c99795e850b0a23707a4dfdabe9d
Author: Stephan Walter <stephan@walter.name>
Date:   Thu Mar 30 17:56:59 2023 +0000

    cmake : properly invoke CTest (#629)

commit a4755cf288deb83df646f91f8fc98613271322db
Author: Casey Primozic <me@ameo.link>
Date:   Thu Mar 30 10:53:35 2023 -0700

    Remove unused variable (#607)
    
    * It seems some new warning were added recently that exposed this.  I wrote the code that included this unused variable originally and it is indeed not needed.

commit 1f0414feecc336482163af6c1e5650f9373ed8c9
Author: david raistrick <keen99@users.noreply.github.com>
Date:   Thu Mar 30 13:34:45 2023 -0400

    make : fix darwin f16c flags check (#615)
    
    ...there was no check.  ported upstream from https://github.com/zanussbaum/gpt4all.cpp/pull/2 (I dont see any clean path for upstream patches)

commit 77efdf5a501b1140801da5cd8751e9f9b259ec32
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 30 20:27:32 2023 +0300

    ggml : fix NEON signs (close #620, #622)

commit ed3c680bcd0e8ce6e574573ba95880b694449878
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Thu Mar 30 11:16:30 2023 +0200

    Fix GGML_F32Cx8_STORE in AVX without F16C path (#619)

commit 9cbc404ba6699a9ba4925ea25a60552b13491c7a
Author: anzz1 <anzz1@live.com>
Date:   Wed Mar 29 23:44:39 2023 +0300

    ci : re-enable AVX512 testing (Windows-MSVC) (#584)
    
    * CI: Re-enable AVX512 testing (Windows-MSVC)
    
    Now with 100% less base64 encoding
    
    * plain __cpuid is enough here

commit b51c717d5cf9181c33afcb84554e47f6d539c891
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 29 22:15:34 2023 +0300

    ggml : init time on first ggml_init() call

commit 0ba76c1e73ae21038b80bfb5a746157376c88173
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 29 22:13:12 2023 +0300

    llama : fix compile warnings when reading the vocab

commit cea1c859483a5cfc7e2b31a06f8561d7a7604870
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 29 22:10:01 2023 +0300

    ggml : add ARM_NEON dequantize_row_q4_1()

commit f202ada131f60059112a948f660b2e0ac93d049a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 29 22:03:02 2023 +0300

    ggml : add ARM_NEON quantize_row_q4_1()

commit 3b44d30d9b618f0f2eb9abcfe912770a4e7d85d4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 29 21:47:33 2023 +0300

    ggml : add ARM_NEON ggml_vec_dot_q4_1()

commit 61cbfff5c95e45236883b1b60e025f8f6fa8c8a3
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Wed Mar 29 20:09:25 2023 +0200

    rename convert_ggml_to_pth.py -> convert-ggml-to-pth.py (#600)
    
    to match filenames of other converters

commit d9ad104440d84a0cc0734bff47ef0ba41ba740c4
Author: Thérence <13496987+Royalphax@users.noreply.github.com>
Date:   Wed Mar 29 19:21:09 2023 +0200

    Create chat-13B.bat (#592)
    
    * Create chat-13B.bat
    
    Same script than chat-13B.sh, but for windows users.
    Tested and working on windows 10/11 v 22H2
    
    * Apply suggestions from code review
    
    ---------
    
    Co-authored-by: anzz1 <anzz1@live.com>

commit b467702b87461543c75013207e9adc6d20dcc01d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 29 19:38:31 2023 +0300

    readme : fix typos

commit 516d88e75c9e768c0001a452dbad212494c586b3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 29 19:37:20 2023 +0300

    readme : add GPT4All instructions (close #588)

commit 53635c081c49321d523567112f9fddfbba6b787b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 29 19:29:26 2023 +0300

    py : add GPT4All conversion script
    
    For now: copy-paste
    Too much time for me to deduplicate the python code

commit 41318d708ed196ff727dce14d263a64b23c7333d
Author: Maël Kerbiriou <m431.kerbiriou@gmail.com>
Date:   Wed Mar 29 18:10:07 2023 +0200

    llama : use the same threshold for OpenBLAS and ggml thread limiting (#577)

commit a6956b25a1c783e5e96fe06c9c00438f846ef047
Author: Tobias Lütke <tobi@shopify.com>
Date:   Wed Mar 29 17:10:24 2023 +0200

    add example of re-act pattern (#583)
    
    * add example of re-act pattern
    
    * spelling...
    
    * fixed whitespace in reverse prompt issue

commit 83df5639eb182ed7c122382907691d8baa3c32df
Author: anzz1 <anzz1@live.com>
Date:   Wed Mar 29 16:20:07 2023 +0300

    Fix GCC warning about binary literal (#595)
    
    0b10101010 -> 0xAA /* 0b10101010 */

commit a5c42c4b13b3be9e58fe8f9adbb6ee60417674a6
Author: anzz1 <anzz1@live.com>
Date:   Wed Mar 29 16:19:29 2023 +0300

    Fix typo in llama.h (#593)

commit 5a5f8b1501fbb34367225544010ddfc306d6d2fe
Author: anzz1 <anzz1@live.com>
Date:   Tue Mar 28 22:44:29 2023 +0300

    Enable Fused-Multiply-Add (FMA) and F16C/CVT16 vector extensions on MSVC (#375)
    
    * Enable Fused-Multiply-Add (FMA) instructions on MSVC
    
    __FMA__ macro does not exist in MSVC
    
    * Enable F16C/CVT16 vector extensions on MSVC
    
    __F16C__ macro does not exist in MSVC, but is implied with AVX2/AVX512
    
    * MSVC cvt intrinsics
    
    * Add __SSE3__ macro for MSVC too because why not
    
    even though it's not currently used for anything when AVX is defined

commit f1217055eaedfc7214be93d98e529cae89830430
Author: anzz1 <anzz1@live.com>
Date:   Tue Mar 28 22:43:25 2023 +0300

    CI: fix subdirectory path globbing (#546)
    
    - Changes in subdirectories will now be detecter properly
    - (Windows-MSVC) AVX512 tests temporarily disabled

commit 7f4c5c66514227c3870c2bd189fb0609fdd0de10
Author: anzz1 <anzz1@live.com>
Date:   Tue Mar 28 21:23:09 2023 +0300

    llama : fix linkage with mingw (#551)
    
    * Revert 7e53955 (#542)
    
    Still needs to be fixed properly
    
    * Fix linking on mingw32

commit 2a98bc18ea34dbf15f261a0df37080e588a189d1
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Tue Mar 28 20:06:03 2023 +0200

    ggml : add AVX2 implementation of quantize_row_q4_1 (#515)
    
    * Add AVX2 implementation of quantize_row_q4_1
    
    * Actually use AVX2
    
    * Make quantize_row_q4_1 static
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit d0aaff571cd5c316b68e3e11d57e274bfd2bd457
Author: thement <40525767+thement@users.noreply.github.com>
Date:   Tue Mar 28 19:55:42 2023 +0200

    py : add temporary script to convert old ggml files to newer version (#539)
    
    Co-authored-by: Jakub Horak <jakub.horak@ibawizard.net>

commit d0330fd783d7c67349cdcce4a56604ef0aeccdb5
Author: Tai Duc Nguyen <taiducnguyen.drexel@gmail.com>
Date:   Tue Mar 28 13:51:29 2023 -0400

    py : add capabiliy to convert from ggml back to torch or hf format for further consumption/training/finetuning (#403)

commit 99c5b2765422232ebb4414f5a63693d734406a7f
Author: Stephan Walter <stephan@walter.name>
Date:   Tue Mar 28 17:13:01 2023 +0000

    ggml : refactor quantized processing functions (#509)
    
    * Refactor quantized processing functions
    
    * ggml : minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 692ce3164ef1201ecb9cfad315cc0a08b965adb8
Author: DooWoong Lee (David) <manics99@naver.com>
Date:   Wed Mar 29 02:02:34 2023 +0900

    py : removed unused `model` variable and verified that the code functions correctly with `vocab_only` setting. Also confirmed that the code works as expected after running with reduced memory usage due to deletion of no-longer-needed variable. (#547)

commit 96f9c0506fa81cada6f96f45768c34f45406c4bb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 28 20:01:09 2023 +0300

    ci : make ctest verbose, hopefully we see what is wrong with the sanitizer

commit d502bc7c9d9d6dfb3a09aea404395b666d7b374d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 28 19:51:55 2023 +0300

    tests : free llama context at the end of the test

commit 436e56193199a1625f8c561069f702e8840a9e08
Author: Stephan Walter <stephan@walter.name>
Date:   Tue Mar 28 16:48:20 2023 +0000

    all : be more strict about converting float to double (#458)
    
    * Be more strict about converting float to double
    
    * Test equivalence of round, SILU implementations
    
    Test module is commented out in CMakeLists.txt because the tests may
    take a long time, depending on how much the compiler optimizes.
    
    * Fix softmax in perplexity.cpp
    
    * all : prefer float over double where appropriate
    
    * perplexity : add <cmath>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 20e1e84884376b3fb44ffbfd48d478b2934b0b5e
Author: Jed Fox <git@jedfox.com>
Date:   Tue Mar 28 11:39:01 2023 -0500

    deploy : add a Package.swift for SwiftPM support (#393)
    
    * Add a Package.swift for SwiftPM support
    
    * Swap from exclusions to allowlist

commit c1f885067c61191a07a1aedf684168dda62f3f71
Author: Stephan Walter <stephan@walter.name>
Date:   Tue Mar 28 15:56:03 2023 +0000

    ggml : introduce structs for the q4 data blocks (#356)
    
    * Introduce structs for the q4 data blocks
    
    * ggml : rename quant struct variables + fix ARM_NEON
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit e0670260fb50a882b37074112b1881fb0820cf77
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 28 18:34:35 2023 +0300

    gitignore : add "embedding"

commit 28ba975aea1dcae2f31770516f5d542ff177771e
Author: dotpy314 <33351922+dotpy314@users.noreply.github.com>
Date:   Tue Mar 28 23:06:28 2023 +0800

    Check the existence of f16_model_path_base in quantize.py (#574)
    
    Co-authored-by: Jincheng Miao <jincheng.miao@gmail.com>

commit a6bdc47cba23713a22ade47dd65b6afeb8009ff4
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Tue Mar 28 16:26:55 2023 +0200

    Fix usage of F16C intrinsics in AVX code (#563)
    
    * Fix usage of F16C intrinsics in AVX code when F16C is not defined

commit 7b8dbcb78b2f65c4676e41da215800d65846edd0
Author: anzz1 <anzz1@live.com>
Date:   Tue Mar 28 17:09:55 2023 +0300

    main.cpp fixes, refactoring (#571)
    
    - main: entering empty line passes back control without new input in interactive/instruct modes
    - instruct mode: keep prompt fix
    - instruct mode: duplicate instruct prompt fix
    - refactor: move common console code from main->common

commit 4b8efff0e3945090379aa2f897ff125c8f9cdbae
Author: RJ Adriaansen <adriaansen@eshcc.eur.nl>
Date:   Tue Mar 28 08:11:09 2023 +0200

    Add embedding example to Makefile (#540)

commit 7e5395575a3360598f2565c73c8a2ec0c0abbdb8
Author: Marco Matthies <71844+marcom@users.noreply.github.com>
Date:   Mon Mar 27 06:55:26 2023 +0200

    Fix missing ggml link in cmake for examples/* on w64-mingw32 (#542)

commit 34c1072e497eb92d81ee7c0e12aa6741496a41c6
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Sun Mar 26 17:48:40 2023 +0200

    ci: add debug build to sanitizer build matrix (#527)

commit 939ad2d3a56815f480b6fd5ea432a7ee576a7e6b
Author: Stephan Walter <stephan@walter.name>
Date:   Sun Mar 26 15:34:02 2023 +0000

    Fix undefined variables in debug build, remove unused variables (#531)

commit 8c2ec5e21d580c99e257c3cfddcf21fa53229aa4
Author: Juan Calderon-Perez <835733+gaby@users.noreply.github.com>
Date:   Sun Mar 26 10:48:42 2023 -0400

    Add support for linux/arm64 platform during Docker Builds (#514)
    
    * Add support for linux/arm64 platform
    
    * Add platform to versioned builds

commit b391579db92f095666be1d979899b54ae0981573
Author: Stephan Walter <stephan@walter.name>
Date:   Sun Mar 26 13:14:01 2023 +0000

    Update README and comments for standalone perplexity tool (#525)

commit 7a87d31f4f0c37bbb2ea695929fa4fe3ad579cda
Author: anzz1 <anzz1@live.com>
Date:   Sun Mar 26 16:06:10 2023 +0300

    [main] fix infinite generation (-n == -1) (#523)

commit 348d6926ee31d4476f9b90e1a627b0925a70f847
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 26 10:20:49 2023 +0300

    Add logo to README.md

commit 33e35b8fe8f09adcac0632e9cece62e1dd629f7d
Author: Harald Fernengel <harald.fernengel@here.com>
Date:   Sun Mar 26 07:25:46 2023 +0200

    Exit from interactive mode if input stream is bad (#491)
    
    Allow exiting the interactive prompt also with CTRL-D on Unix and CTRL-Z
    on Windows.

commit 19726169b379bebc96189673a19b89ab1d307659
Author: anzz1 <anzz1@live.com>
Date:   Sun Mar 26 00:13:28 2023 +0200

    CI: Run other sanitizer builds even if one fails (#511)
    
    applies only to sanitizer builds so they wont be cancelled

commit f732695cd57fb41e3a1be625cec4edf5be45b40a
Author: jp-x-g <jpxg-dev@protonmail.com>
Date:   Sat Mar 25 14:53:55 2023 -0700

    Clarify console output in convert-pth-to-ggml.py (#512)
    
    "Processing part 1 of 3" instead of "Processing part 0"

commit 2f7bf7dd7cd7299874d582f7f34834418abf4057
Author: anzz1 <anzz1@live.com>
Date:   Sat Mar 25 23:38:11 2023 +0200

    CMake / CI additions (#497)
    
    * CMake: Add AVX512 option
    
    * CI: Add AVX/AVX512 builds (Windows)
    (AVX512 tests can only be run when the worker happens to support it, building works anyway)
    
    * CMake: Fix sanitizer linkage ( merged #468 )
    
    * CI: Add sanitizer builds (Ubuntu)
    
    * CI: Fix release tagging
    (change @zendesk/action-create-release to @anzz1/action-create-release until upstream PR Added commitish as input zendesk/action-create-release#32 is merged)

commit 34ab5268432fd287caa68d60bdd8aef411def3fa
Author: anzz1 <anzz1@live.com>
Date:   Sat Mar 25 22:29:22 2023 +0200

    (Windows) Set console to UTF-8 on init (#420)
    
    Sets console codepage to 65001 (CP_UTF8) on start for both input and output, should fix problems with UTF-8 characters.

commit c2b25b6912662d2637d9c6e6df3a5de931e0d7ce
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 21:53:39 2023 +0200

    Fix colors enabling on WIN32

commit 79b2b266db6b198b5af450982c3cd61120fac951
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 21:51:41 2023 +0200

    If n_predict == -1, generate forever

commit e2d490dafd860eaaaf9aa8008ab790527d556daf
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 21:36:22 2023 +0200

    Inifinite generation via context swapping (#71)

commit 03f7e335604b3d68f74995aa2ccb4955833ee423
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 20:51:14 2023 +0200

    Cleanup STL headers + fix embedding examples + minor stuff

commit 55ad42af845127bd0eb0c1f36f327ecec83f4bca
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 20:36:52 2023 +0200

    Move chat scripts into "./examples"

commit 459e93cce07cab9052c06b5bf360819893442e1e
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Sat Mar 25 19:31:48 2023 +0100

    Add AVX2 implementation of dequantize_row_q4_1 (#505)

commit a316a425d04027453dc0fd45f003b647c12f66f9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 20:26:40 2023 +0200

    Overhaul the examples structure
    
    - main -> examples
    - utils -> examples (renamed to "common")
    - quantize -> examples
    - separate tools for "perplexity" and "embedding"
    
    Hope I didn't break something !

commit ecbe466a364876927994e2f1ec14f4d82301d201
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 19:47:21 2023 +0200

    Retire the ggml_mul_mat() branch for transposed src0 (#500)
    
    * Retire the ggml_mul_mat() for transposed src0
    
    - It can always be made contiguous with ggml_cpy()
    - The code is now simplified
    - The results are deterministic in respect to num threads
    
    * SIMD-ify dequantize_row_q4_0() for ARM_NEON (#502)
    
    * Attempt to SIMD-ify dequantize_row_q4_0() for ARM_NEON
    
    * Fix dequantization - forgot to interleave the quants

commit 502a400192013d3e95ed87b777e8fa3bec45713c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 17:16:50 2023 +0200

    Disable prompt verbosity by default and add option to enable (#480)

commit 09aecbf6283bbce9449e2d96000073145aaaf5fc
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Sat Mar 25 16:06:49 2023 +0100

    Add AVX2 implementation of dequantize_row_q4_0 (#467)

commit 4640eff23d341a0273587800e17ff4a378132d60
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 17:03:10 2023 +0200

    Don't interefe with BLAS for large prompts by running only 1 thread

commit ab77d7631211b299cb734bea6ad1f74324154150
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 16:47:59 2023 +0200

    Add longer DAN prompt for testing big batch numbers

commit 29b7baab670ae8b76ac0da21c2ded69ff18971ee
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Sat Mar 25 15:34:23 2023 +0100

    Add timings for the prompt evaluation (#478)

commit 4a7129acd2e939b92d70dd568c746f2fa078232c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 16:30:32 2023 +0200

    Remove obsolete information from README

commit 6b6dbc8910c6d53f4d96c46c8fcec70e2cd435d8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 16:22:05 2023 +0200

    Remove obsolete assert and fix compiler warning

commit 2a2e63ce0503d9bf3e55283e40a052c78c1cc3a8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 16:09:54 2023 +0200

    Fix nasty bug in ggml_compute_forward_mul_mat_f32() and reenable BLAS

commit e899bf54b291e8c84173a0e534a2c262f3f63229
Author: anzz1 <anzz1@live.com>
Date:   Sat Mar 25 14:42:09 2023 +0200

    bounds checking for input prefix (#492)

commit fbd4d38c647f82b2598291ea9b8d0c09ac1ffb8c
Author: anzz1 <anzz1@live.com>
Date:   Sat Mar 25 14:03:19 2023 +0200

    feat: '--in-prefix STRING' option (#426)
    
    Prefix user inputs with a string

commit 58e6c9f36f97d0a3e287b97256dc5f6b0e9fb5ae
Author: Jed Fox <git@jedfox.com>
Date:   Sat Mar 25 01:26:28 2023 -0400

    Add support for file load progress reporting callbacks (#434)
    
    * File load progress reporting
    
    * Move llama_progress_handler into llama_context_params
    
    * Renames
    
    * Use seekg to find file size instead
    
    * More correct load progress
    
    * Call progress callback more frequently
    
    * Fix typo

commit 36d07532ef7ccf0bdc12e050472f359a6794957f
Author: Doomsdayrs <38189170+Doomsdayrs@users.noreply.github.com>
Date:   Sat Mar 25 01:21:24 2023 -0400

    Add missing struct annotation (#483)
    
    `llama_sample_top_p_top_k` was missing the struct annotation on line 126.
    
    This causes a compiler issue when being parsed by the Kotlin C interop generator.
    
    This commit fixes the above issue by adding the struct annotation.

commit 6f1ee4b640912211a4b07965c585d327e32e734d
Author: Chris Kuehl <ckuehl@ckuehl.me>
Date:   Fri Mar 24 23:38:14 2023 -0500

    Fix crash for 65B model with pre-allocated memory (#485)

commit 8520fc310eab87f2c4612f2a00d4adbd44a20d0d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 24 23:47:06 2023 +0200

    Disable BLAS altogether - the bug is not just for qunatized mat mul

commit b3f460e94139cb24b0af81cc8bc10eb86269d704
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 24 23:39:17 2023 +0200

    Disable BLAS branch in mul_mat - seems there is a bug

commit 04c6f5ed6fafd63601fa06757877ed5ccf9d5991
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 24 23:17:58 2023 +0200

    Immediately start processing the prompt before user input has been provided (#476)

commit 7a9b6c3a8bdc1cb75fefc826dfaa7331eb63695d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 24 23:17:37 2023 +0200

    Reduce memory usage and allocate enough memory for largest context (#473)
    
    * Reduce memory usage and allocate enough memory for large contexts
    
    * Simpler scratch buffer usage
    
    * Reenable BLAS for quantized mul_mat
    
    * Fix number of layers in 30B and 65B
    
    * Fix KV cache size for F32

commit 31572d966531f7d768eb773322016ab78eb6e835
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 24 18:23:56 2023 +0200

    Temporary bump the memory buffer size - hopefully fix issues from 483bab2e

commit f4f5362edb01b05c383b23f36d7b3489c77061b5
Author: Gary Mulder <gjmulder@gmail.com>
Date:   Fri Mar 24 15:23:09 2023 +0000

    Update README.md (#444)
    
    Added explicit **bolded** instructions clarifying that people need to request access to models from Facebook and never through through this repo.

commit 863f65e2e32dc1e6d23c96a4811bf382d6b2a548
Author: rabidcopy <rabidcopy@yahoo.com>
Date:   Fri Mar 24 10:22:39 2023 -0500

    fix instruct mode (#445)
    
    changes to EOS behavior in interactive and reverse prompt handling broke instruct mode by erroneously injecting instruct mode's reverse prompt and an extra newline.

commit afd220d9c665e4c19107120ace2f0cb742e28aa1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 24 17:21:01 2023 +0200

    Properly free llama_context on failure

commit 481044d50cfe8eaa6cd0c1a1b445680e4b0b3ebc
Author: Cameron Kaiser <classilla@users.noreply.github.com>
Date:   Fri Mar 24 08:19:26 2023 -0700

    additional optimizations for POWER9 (#454)

commit 563cdc391dde140f1084d1012234e8e6f57f881f
Author: comex <comexk@gmail.com>
Date:   Fri Mar 24 08:19:05 2023 -0700

    Support calling mlock() on loaded model data on Linux and macOS (#453)
    
    * Support calling mlock() on loaded model data on Linux and macOS
    
    This is enabled by a new --mlock command line option.
    
    Using mlock() disables swapping and memory compression for the model
    data.  Doing so can be useful on systems where the model takes up a
    large fraction of system RAM.  In my experience, macOS is quite eager to
    start compressing llama.cpp's memory, which then makes it halt for a few
    seconds while it decompresses, even with a model that uses "only" 25GB
    out of 32GB.
    
    Of course, this comes at the cost of forcing the system to swap or
    compress other processes' memory instead, so it needs to be used with
    care and shouldn't be enabled by default.
    
    In theory it should be possible to support this on Windows as well using
    VirtualLock(), but I'm not much of a Windows user.
    
    * Update llama.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 8d4a855c241ecb0f3ddc03447fe56002ebf27a37
Author: Luciano <lucianostrika44@gmail.com>
Date:   Fri Mar 24 08:05:13 2023 -0700

    Add embedding mode with arg flag. Currently working (#282)
    
    * working but ugly
    
    * add arg flag, not working on embedding mode
    
    * typo
    
    * Working! Thanks to @nullhook
    
    * make params argument instead of hardcoded boolean. remove useless time check
    
    * start doing the instructions but not finished. This probably doesnt compile
    
    * Embeddings extraction support
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit b6b268d4415fd3b3e53f22b6619b724d4928f713
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 24 09:13:35 2023 +0200

    Add link to Roadmap discussion

commit 3cd8dde0d1357b7f11bdd25c45d5bf5e97e284a0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 24 06:22:28 2023 +0200

    Revert "Fix memory allocation issues and seg faults"
    
    This reverts commit 4870e455b3653f7d7769fa5772b2c90ffad088df.
    
    Will provide the correct fix later

commit 4870e455b3653f7d7769fa5772b2c90ffad088df
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 24 00:11:53 2023 +0200

    Fix memory allocation issues and seg faults

commit 483bab2e3d4a868fe679d8bb32827d2a4df214dc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 23 23:22:01 2023 +0200

    Avoid the transposed X branch in the Z = X * Y matrix multiplication (#439)
    
    Should make results reproducible for different number of threads and batch sizes

commit 404e1da38ec8025707031a8027da14dc1590f952
Author: Jed Fox <git@jedfox.com>
Date:   Thu Mar 23 16:42:52 2023 -0400

    Fix quantize script not finding models in parent directory (#428)

commit 4cc053b6d5e9df7ac21fa06b7208a70c156d4d7a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 23 22:39:44 2023 +0200

    Remove oboslete command from Docker script

commit 0ba5a3a9a5efedb1aeecbbc70a4e9825542472d5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 23 22:32:02 2023 +0200

    Obsolete

commit 2e17dfd80a473099dacc0f41c9146d233c6a5972
Author: rabidcopy <rabidcopy@yahoo.com>
Date:   Thu Mar 23 15:22:47 2023 -0500

    Replace EOS with newline to prevent context/memory being flushed by EOS in interactive mode (#333)
    
    * Improve interactive mode's coherence after EOS
    
    Aims to improve coherence and ability to resume the interactive session when the user is given input back after an end of text token is reached.
    Not sure what token 13 is or why it seems to help. See conversation for examples.
    
    * Make newline token a constant
    
    * dynamically determine newline token
    
    * relocate previous newline token const
    
    * cleanup whitespace
    
    * print a new line on end of text in interactive
    
    this may need to be looked into further when not using a reverse prompt
    
    * only print manual newline with reverse prompt
    
    fix formatting of reverse prompts so they don't end up at the end of the current line while not introducing unnecessary new lines otherwise
    
    * alternate approach to replace end of text tokens
    
    * Inject the reverse prompt again after eos in interactive mode
    
    * tokenize reverse prompt when needed
    
    makes this PR compatible with https://github.com/ggerganov/llama.cpp/pull/330
    
    * tokenize and inject only first reverse prompt
    
    thanks to tjohnman
    
    * tokenize first reverse prompt once
    
    * add newline token
    
    * add newline token
    
    * tokenize/inject reverse prompt for refactor
    
    this doesn't seem right though
    
    * tokenize nothing for antiprompt if no reverse
    
    * Update main.cpp
    
    * Update main.cpp
    
    * tokenize and inject reverse prompt as needed
    
    this doesn't seem to work if the reverse prompt is tokenized outside earlier on
    
    * not needed
    
    * remove newline token
    
    * remove newline token
    
    * tokenize newline token
    
    * add space to comment
    
    * Update main.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Slaren <2141330+slaren@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 20a1a4e09c522a80e2a0db51643d25fa38326065
Author: Timmy Knight <r2d2fish@gmail.com>
Date:   Thu Mar 23 10:18:13 2023 -1000

    Fix GPTQ converter (#423)
    
    * Fix GPTQ converter
    
    * Fix comment
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit ad072fc5ad6f6905a7224ff6ea07c0644aa075b1
Author: nusu-github <29514220+nusu-github@users.noreply.github.com>
Date:   Fri Mar 24 05:16:48 2023 +0900

    Generate library with CMake (#430)
    
    * Generate library with CMake
    
    BUILD_SHARED_LIBS to allow llama library to be generated.
    
    * Turn ON PIC when BUILD_SHARED_LIBS is ON

commit ea10d3ded2994106596ddf8e4ed02741b3e053e6
Author: anzz1 <anzz1@live.com>
Date:   Thu Mar 23 19:54:28 2023 +0200

    Command line args bounds checking (#424)
    
    * command line args bounds checking
    
    * unknown and invalid param exit codes 0 -> 1

commit a18c19259a3cb9dec332d613e8f15704f678a468
Author: Ben Siraphob <bensiraphob@gmail.com>
Date:   Wed Mar 22 00:37:02 2023 -0500

    Fix Nix build

commit a50e39c6fe36be3de0941b3c05aaf9c37912fd47
Author: Stephan Walter <stephan@walter.name>
Date:   Thu Mar 23 14:15:48 2023 +0000

    Revert "Delete SHA256SUMS for now" (#429)
    
    * Revert "Delete SHA256SUMS for now (#416)"
    
    This reverts commit 8eea5ae0e5f31238a97c79ea9103c27647380e37.
    
    * Remove ggml files until they can be verified
    * Remove alpaca json
    * Add also model/tokenizer.model to SHA256SUMS + update README
    
    ---------
    
    Co-authored-by: Pavol Rusnak <pavol@rusnak.io>

commit a140219e81cfb80356438112cd2290d701b282bb
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Thu Mar 23 05:41:32 2023 -0600

    Fix Makefile echo escape codes (by removing them). (#418)

commit 8a3e5ef801339e57b9b0449220e9ffb11a6648e2
Author: Gary Mulder <gjmulder@gmail.com>
Date:   Thu Mar 23 11:30:40 2023 +0000

    Move model section from issue template to README.md (#421)
    
    * Update custom.md
    
    * Removed Model section as it is better placed in README.md
    
    * Updates to README.md model section
    
    * Inserted text that was removed from  issue template about obtaining models from FB and links to papers describing the various models
    
    * Removed IPF down links for the Alpaca 7B models as these look to be in the old data format and probably shouldn't be directly linked to, anyway
    
    * Updated the perplexity section to point at Perplexity scores #406 discussion

commit 8eea5ae0e5f31238a97c79ea9103c27647380e37
Author: anzz1 <anzz1@live.com>
Date:   Thu Mar 23 12:26:19 2023 +0200

    Delete SHA256SUMS for now (#416)
    
    Delete this for now to avoid confusion since it contains some wrong checksums from the old tokenizer format
    Re-add after #374 is resolved

commit 93208cfb929c2323e5d2ac6bf354e278040e70ed
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 23 10:46:58 2023 +0200

    Adjust repetition penalty ..

commit 03ace14cfd68a1289ac3b76563534c8ee72a2e53
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 23 09:48:51 2023 +0200

    Add link to recent podcast about whisper.cpp and llama.cpp

commit e4412b45e395981068d2850d3fa04cc16c77d70d
Author: anzz1 <anzz1@live.com>
Date:   Thu Mar 23 04:20:34 2023 +0200

    CI: CMake: Separate build and test steps (#376)
    
    * CI: Separate Build and Test steps (CMake)
    
    * CI: Make sure build passes before running tests (CMake)
    
    * CI: Standardise step id names

commit f7dc43bc0d759732815856183246f167111587ad
Author: tjohnman <tjohnman@users.noreply.github.com>
Date:   Thu Mar 23 01:30:23 2023 +0100

    Fix instruct mode broken by PR #354 (#409)
    
    Co-authored-by: Johnman <tjohnman@github>

commit ee8a7887865a893be208e0a92d6d94d2cb66a789
Author: Gary Mulder <gjmulder@gmail.com>
Date:   Wed Mar 22 19:06:18 2023 +0000

    Update issue template so people will use it (#404)

commit 69c92298a9e36dc2363b3bf50452976ce49487b3
Author: Stephan Walter <stephan@walter.name>
Date:   Wed Mar 22 17:29:06 2023 +0000

    Deduplicate q4 quantization functions (#383)
    
    * Deduplicate q4 quantization functions
    
    * Use const; add basic test
    
    * Re-enable quantization test
    
    * Disable AVX2 flags in CI
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 97940520e8fd49c56bb29b71cc350190b723513f
Author: Valentyn Bezshapkin <61702053+valentynbez@users.noreply.github.com>
Date:   Wed Mar 22 18:20:25 2023 +0100

    fix: add POSIX functionality for Linux compilation (#51)
    
    * fix: add POSIX functionality for Linux compilation
    
    * fix: older standard for compatibility

commit 305ba6f0e6daa3796aad9dd18053a1945dd4cc58
Author: tjohnman <tjohnman@users.noreply.github.com>
Date:   Wed Mar 22 18:16:35 2023 +0100

    Don't force immediate interactive without `-i` (#354)
    
    * Don't force immediate interactive without -i
    
    Sometimes we might want to use a reverse prompt but we want to let the
    model generate tokens right after the initial prompt. So we don't force
    user input mode if the -i flag wasn't specified and instead let it run
    until we encounter the reverse prompt.
    
    This gives use some more flexibility, since it doesn't force the user to
    enter a newline if they want to let the model generate text right after
    the initial prompt and only be asked for input if the reverse prompt is
    encountered.
    
    The `--interactive-first` flag is reintroduced to force the old
    behavior. `-r` behaves like `-i` plus introduces a reverse prompt (it
    can be specified more than once).
    
    * Update help output.
    
    ---------
    
    Co-authored-by: Johnman <tjohnman@github>

commit 4122dffff958cd137175b58f1f27c0913528d7ba
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Wed Mar 22 17:37:10 2023 +0100

    cmake: make llama an actual library (#392)

commit 56e659a0b271436e24813a801640d015e7b05328
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Wed Mar 22 17:09:38 2023 +0100

    fix perplexity after c-api refactor (#390)
    
    * preallocate a buffer of fitting size for tokenization (utils.cpp)
    
    * don't create a new std::string (especially here, where it's usually large)

commit 40ea807a972ec7b5a426f034ebfa593b5e7a06ed
Author: Gary Linscott <glinscott@gmail.com>
Date:   Wed Mar 22 08:53:54 2023 -0700

    Add details on perplexity to README.md (#395)

commit d5850c53ca179b9674b98f35d359763416a3cc11
Author: Yusuf Kağan Hanoğlu <hanoglu@yahoo.com>
Date:   Wed Mar 22 11:55:45 2023 +0300

    Add missing header for memcpy (#386)
    
    fixed: memcpy is not defined

commit ae44e23ee36c02da0e37ab508a4b473ace724f8e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 22 07:47:15 2023 +0200

    When seed <= 0 - use the clock to generate one

commit 928480ef5b7b03d7a07e98286aebe3d8b24457d9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 22 07:45:00 2023 +0200

    Init llama_context_params properly from CLI (#370)

commit 56817b1f882b1894daa4051d0de0bf9a0926d315
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 22 07:34:02 2023 +0200

    Remove temporary notice and update hot topics

commit f5a77a629bd0f37ae1696747633ab42a5530ec15
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 22 07:32:36 2023 +0200

    Introduce C-style API (#370)
    
    * Major refactoring - introduce C-style API
    
    * Clean up
    
    * Add <cassert>
    
    * Add <iterator>
    
    * Add <algorithm> ....
    
    * Fix timing reporting and accumulation
    
    * Measure eval time only for single-token calls
    
    * Change llama_tokenize return meaning

commit da0e9fe90ccf6e73597eb19dd0cfc0a28363fb3b
Author: Gary Mulder <gjmulder@gmail.com>
Date:   Mon Mar 20 20:14:06 2023 +0000

    Add SHA256SUMS file and instructions to README how to obtain and verify the downloads
    
    Hashes created using:
    
    sha256sum models/*B/*.pth models/*[7136]B/ggml-model-f16.bin* models/*[7136]B/ggml-model-q4_0.bin* > SHA256SUMS

commit e6c9e0986c79ba1cc8848879b2fcce979f9b4672
Author: anzz1 <anzz1@live.com>
Date:   Tue Mar 21 23:49:24 2023 +0200

    Fix bin dir for win ci

commit 01a297b09932e29f3319d6588977c32a926c7907
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Tue Mar 21 22:34:25 2023 +0100

    specify build type for ctest on windows (#371)

commit 3366853e41fcc818222a0271c76b6106179106fb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 21 22:57:35 2023 +0200

    Add notice about pending change

commit 3f9c6135e45ae3f520b1e17197004cc60c9ca45b
Author: Mathieu Nayrolles <MathieuNls@users.noreply.github.com>
Date:   Tue Mar 21 16:52:27 2023 -0400

    fix typo in chatLLaMa (#368)
    
    The prompt contains a typo where 'alound' is used instead of 'aloud'.

commit 0f6135270839f0715843c4d480c63ae150def419
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 21 19:47:27 2023 +0200

    Update issue templates

commit 353ec251a42491f5192c48561da4b444ef67f23c
Author: Fabio R. Sluzala <Fabio3rs@users.noreply.github.com>
Date:   Tue Mar 21 14:21:50 2023 -0300

    We could use std::unordered_map over std::map (#305)
    
    * Improve performance by changing std::map to std::unordered_map and std::map<id, token> id_to_token; to std::vector<token> id_to_token;
    
    * fix last commit on gpt_vocab_init add vocab.id_to_token.resize(vocab.token_to_id.size());
    
    * Removed include <map>
    
    * Nest struct token score inside gpt_vocab
    
    * renamed token to tok

commit 89d5d90f3b6d25f134da7a8e252c3432bffcf674
Author: Matvey Soloviev <blackhole89@gmail.com>
Date:   Tue Mar 21 18:11:01 2023 +0100

    Fix color codes emitting mid-UTF8 code. (#312)

commit 16ffc013c62f22bdaa3cdc022d7a13fd952d73fc
Author: comex <comexk@gmail.com>
Date:   Tue Mar 21 09:42:25 2023 -0700

    Importer for GPTQ quantized LLaMA models (#301)
    
    * [WIP, broken] Importer for GPTQ quantized LLaMA models
    
    Based on: https://github.com/qwopqwop200/GPTQ-for-LLaMa
    
    Current status: Something is busted.  The output starts out decent, but
    quickly degrades into gibberish.  This doesn't happen with either the
    original GPTQ-for-LLaMa using the same weights, or llama.cpp when using
    weights quantized by its own quantizer.  Is there a bug in the
    conversion script that somehow only comes into play with a large context
    size?
    
    I did notice one potential issue.  It's clearly not the main cause of
    the gibberish, since it doesn't happen when using q4_1 weights quantized
    by llama.cpp itself, but it seems concerning.  When doing a matrix
    multiplication of f16 * f32 => f32 or q4_1 * f32 => f32, at least when
    the multiplication is not done with BLAS, the intermediate results are
    stored in the smaller format rather than f32.  This seems like an
    unnecessary waste of precision, especially in the q4_1 case.
    
    I was originally hoping to validate the results by matching the Python
    implementation's output exactly, but precision and non-associativity
    issues make this very difficult, including when performing matrix
    multiplications and, especially, computing norms.
    
    Anyway, design details:
    
    The models being imported store per-layer weights in essentially q4_1
    format, although the addend and scale are shared across an entire row
    rather than every group of 32 weights.  This script duplicates the
    addend and scale to match ggml's expectations, at the cost of wasting
    some memory.
    
    However, there are two differences which I accommodated changing the
    output format (and adding corresponding support to main.cpp) rather than
    having the script match the existing one:
    
    - The tok_embeddings and output weights (i.e. the weights that aren't
      per-layer) are f16 instead of q4_1.  They could be converted to q4_1,
      and the impact of the loss of precision would probably be low, but
      this would rule out exactly matching the Python implementation's
      output for validation.
    
    - There is no sharding, since the input doesn't have it, and for a
      CPU-only implementation it seems more useful to avoid having to deal
      with multiple files.
    
    The new format is differentiated from existing q4_1 format by changing
    the 'f16' header flag to a new value, 4.  That said, I think a cleaner
    approach would be to change main.cpp to support loading each tensor with
    an arbitrary sharding configuration and type rather than hardcoding
    specific combinations of types.  So far I've wasted too much time
    debugging to try implementing this...
    
    * Add missing permutation.  Now it works.
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 486ae645fd3eda8b9d7413d5ff34fb65a3e337fb
Author: Gary Linscott <glinscott@gmail.com>
Date:   Tue Mar 21 09:27:42 2023 -0700

    Compute perplexity over prompt (#270)
    
    * Compute perplexity over prompt
    
    * More accurate perplexity calculation - over all logits in the context window (so 512x more tokens!)
    
    * Output all perplexitiies
    
    * Add timing/ETA

commit 3ab3e6582f7320c2b6568c892fdfc8215caf7e6c
Author: Jean-Christophe Hoelt <hoelt@fovea.cc>
Date:   Tue Mar 21 18:23:15 2023 +0200

    Add chatLLaMa script (#198)
    
    * Add chatLLaMa script
    
    * Fix shellcheck errors and do some cleanup
    
    * Move chatLLaMa script to `examples` directory
    
    * Reduce chatLLaMa context size to 2048
    
    Ref d7def1a7524f712e5ebb7cd02bab0f13aa56a7f9
    
    * Include n_predict to 2048 in examples/chatLLaMa

commit f157088cb75f23208abc92b473a132ef3f7a7f15
Author: Alex von Gluck IV <kallisti5@unixzen.com>
Date:   Tue Mar 21 11:21:06 2023 -0500

    makefile: Fix CPU feature detection on Haiku (#218)

commit c86ba036e613d46815501a4c6775117c9fc7afce
Author: anzz1 <anzz1@live.com>
Date:   Tue Mar 21 18:14:46 2023 +0200

    Enable ANSI colors on Windows 10+ (#311)
    
    * Enable ANSI colors on Windows 10+
    
    On older versions function will silently fail without any ill effects
    
    * Do not call SetConsoleMode if the mode is already set
    
    * Update main.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 1daf4dd71235dbbf537738e7ad53daad8d97586f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 21 18:10:32 2023 +0200

    Minor style changes

commit dc6a845b8573cd7d06c6b295241d26f311602a1f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 21 18:09:37 2023 +0200

    Add chat.sh script

commit 6a612959e1b6c37b68b6b141329751a2902b1030
Author: tjohnman <tjohnman@users.noreply.github.com>
Date:   Tue Mar 21 17:05:06 2023 +0100

    Check for reverse prompt by characters instead of tokens (#292) (#330)
    
    * Check for reverse prompt by characters instead of tokens (#292)
    
    * Update main.cpp
    
    Wording.
    
    * Cleanup.
    
    * Remove unnecessary use of std::stringstream.
    
    ---------
    
    Co-authored-by: Johnman <tjohnman@github>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit d5f56a5e5a0069329a81f96460221e7afb1daddc
Author: tjohnman <tjohnman@users.noreply.github.com>
Date:   Tue Mar 21 17:04:43 2023 +0100

    Check for reverse prompt by characters instead of tokens (#292) (#330)
    
    * Check for reverse prompt by characters instead of tokens (#292)
    
    * Update main.cpp
    
    Wording.
    
    * Cleanup.
    
    * Remove unnecessary use of std::stringstream.
    
    ---------
    
    Co-authored-by: Johnman <tjohnman@github>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 3bfa3b43b7319b71853bfc7d3cf4e9767c24bbc8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 21 17:59:16 2023 +0200

    Fix convert script, warnings alpaca instructions, default params

commit 715d292ee0e34d27f27af43d7feaad1f1344981d
Author: Kevin Lo <kevlo@kevlo.org>
Date:   Tue Mar 21 09:50:09 2023 -0600

    Add OpenBSD support (#314)

commit c98ae02668a25916954b1653e25a5a35ca048d63
Author: Mack Straight <eiz@users.noreply.github.com>
Date:   Tue Mar 21 08:49:43 2023 -0700

    fix typo in comment (#318)

commit c3b2306b18a087799acc431e485b8a2e3162cd52
Author: Qingyou Meng <meng.qingyou@gmail.com>
Date:   Tue Mar 21 23:44:11 2023 +0800

    Makefile: slightly cleanup for Mac Intel; echo instead of run ./main -h (#335)

commit 975d2cebf97ce888fa0aeee6f5ac774d7135891f
Author: anzz1 <anzz1@live.com>
Date:   Tue Mar 21 17:42:43 2023 +0200

    cmdline option for custom amount of model parts (--n_parts N) (#348)
    
    * cmdline option for custom amount of model parts (--n_parts N)
    
    * Update main.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit e0ffc861fae5ac8b40ce973f822d03db02929d36
Author: Kevin Kwok <antimatter15@gmail.com>
Date:   Tue Mar 21 08:34:49 2023 -0700

    Update IPFS links to quantized alpaca with new tokenizer format (#352)

commit 8f644a0a859938c787d329d27f98e03c58d7df27
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 21 17:32:14 2023 +0200

    Change default repeat_penalty to 1.0
    
    I feel this penalty is not really helping.
    Especially for the example from the README it makes results pretty bad

commit eb34620aeceaf9d9df7fcb19acc17ad41b9f60f8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 21 17:29:41 2023 +0200

    Add tokenizer test + revert to C++11 (#355)
    
    * Add test-tokenizer-0 to do a few tokenizations - feel free to expand
    * Added option to convert-pth-to-ggml.py script to dump just the vocabulary
    * Added ./models/ggml-vocab.bin containing just LLaMA vocab data (used for tests)
    * Added utility to load vocabulary file from previous point (temporary implementation)
    * Avoid using std::string_view and drop back to C++11 (hope I didn't break something)
    * Rename gpt_vocab -> llama_vocab
    * All CMake binaries go into ./bin/ now

commit 2e664f1ff413995506c9a54f3a8d5b8c64e37a91
Author: Casey Primozic <casey@cprimozic.net>
Date:   Tue Mar 21 07:35:42 2023 -0700

    Add initial AVX512 support for dot product on Linux (#320)
    
     * Update Makefile to detect AVX512 support and add compiler flags if it's available
     * Based on existing AVX2 implementation, dot product on one 32-value block of 4-bit quantized ints at a time
     * Perform 8 bit -> 16 bit sign extension and multiply+add on 32 values at time instead of 16
     * Use built-in AVX512 horizontal reduce add to get sum at the end
     * Manual unrolling on inner dot product loop to reduce loop counter overhead

commit 8cf9f34eddc124d4ab28f4d2fe8e99d574510bde
Author: nusu-github <29514220+nusu-github@users.noreply.github.com>
Date:   Tue Mar 21 09:37:16 2023 +0900

    Adding missing features of CMakeLists.txt & Refactoring (#131)
    
    * Functionality addition CMakeLists.txt
    
    Refactoring:
    1. Simplify more options that are negation of negation.
    LLAMA_NO_ACCELERATE -> LLAMA_ACCELERATE
    2. Changed to an optional expression instead of forcing to enable AVX2 in MSVC.
    3. Make CMAKE_CXX_STANDARD, which is different from Makefile, the same.
    4. Use add_compile_options instead of adding options to CMAKE_C_FLAGS.
    5. Make utils use target_link_libraries instead of directly referencing code.
    
    Added features:
    1. Added some options.
    LLAMA_STATIC_LINK,LLAMA_NATIVE,LLAMA_LTO,LLAMA_GPROF,LLAMA_OPENBLAS
    
    * Fix Accelerate link in CMake
    
    * Windows build Fix
    
    * C++11 to C++17
    
    * Reflects C/C++ standard individually
    
    * Change the version to 3.12
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit bd4b46d6ba504b99c936f43fc014529adffb6048
Author: Ben Siraphob <bensiraphob@gmail.com>
Date:   Mon Mar 20 16:44:30 2023 -0500

    Nix flake: set meta.mainProgram to llama

commit 6b6d5b5024faaf82019d08cde5e8a9d69c6ca316
Author: Qingyou Meng <meng.qingyou@gmail.com>
Date:   Tue Mar 21 03:33:10 2023 +0800

    Fixed tokenizer.model not found error when model dir is symlink (#325)

commit a791a68b613b162c88a83f5f0225223bc167c762
Author: Mack Straight <eiz@users.noreply.github.com>
Date:   Mon Mar 20 12:26:01 2023 -0700

    move file magic/version to header, print expected version (#319)

commit 0f1b21cb90ac6b84a9af70cafb8e13b5389e3b32
Author: Bernat Vadell <hounter.caza@gmail.com>
Date:   Mon Mar 20 18:05:20 2023 +0100

    Docker - Fix publish docker image in GitHub Registry (#235)
    
    * fix publish permission
    
    * try to fix docker pipeline using as password github_token & username repository_owner

commit 074bea2eb1f1349a0118239c4152914aecaa1be4
Author: Mack Straight <eiz@users.noreply.github.com>
Date:   Mon Mar 20 03:17:23 2023 -0700

    sentencepiece bpe compatible tokenizer (#252)
    
    * potential out of bounds read
    
    * fix quantize
    
    * style
    
    * Update convert-pth-to-ggml.py
    
    * mild cleanup
    
    * don't need the space-prefixing here rn since main.cpp already does it
    
    * new file magic + version header field
    
    * readme notice
    
    * missing newlines
    
    Co-authored-by: slaren <2141330+slaren@users.noreply.github.com>

commit 5cb63e2493c49bc2c3b9b355696e8dc26cdd0380
Author: Stephan Walter <stephan@walter.name>
Date:   Mon Mar 20 08:24:11 2023 +0000

    Add tqdm to Python requirements (#293)
    
    * Add tqdm to Python requirements
    * Remove torchvision torchaudio, add requests

commit da5303c1ea68aa19db829c634f1e10d08d409680
Author: cocktailpeanut <121128867+cocktailpeanut@users.noreply.github.com>
Date:   Sun Mar 19 17:44:20 2023 -0400

    bugfix: default should not be interactive (#304)

commit 4545539d718cf88f4c3a76669b8ac2e26cd8a1e5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 19 21:58:51 2023 +0200

    Rename script

commit edeba283665591f2f726024a92efe4b0b40434b3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 19 21:57:28 2023 +0200

    Add temporary helper script for Alpaca chat

commit 5c19c70ba631a8f5d54feb6634e0eea178911a84
Author: Rickey Bowers Jr <bitRAKE@gmail.com>
Date:   Sun Mar 19 13:44:30 2023 -0600

    fix coloring of last `n_batch` of prompt, and refactor line input (#221)
    
    * fix coloring of last `n_batch` of prompt, and refactor line input
    * forgot the newline that needs to be sent to the model
    * (per #283) try to force flush of color reset in SIGINT handler

commit 24568371ae0d7caf85164abe4753f36a7dba0288
Author: tjohnman <tjohnman@users.noreply.github.com>
Date:   Sun Mar 19 20:33:06 2023 +0100

    Support for multiple reverse prompts. (#299)
    
    Co-authored-by: Johnman <>
    Co-authored-by: Johnman <tjohnman@github>

commit 7392f1cd2cef4dfed41f4db7c4160ab86c0dfcd9
Author: Suaj Carrot <72162667+SuajCarrot@users.noreply.github.com>
Date:   Sun Mar 19 12:38:44 2023 -0600

    Improved quantize script (#222)
    
    * Improved quantize script
    
    I improved the quantize script by adding error handling and allowing to select many models for quantization at once in the command line. I also converted it to Python for generalization as well as extensibility.
    
    * Fixes and improvements based on Matt's observations
    
    Fixed and improved many things in the script based on the reviews made by @mattsta. The parallelization suggestion is still to be revised, but code for it was still added (commented).
    
    * Small fixes to the previous commit
    
    * Corrected to use the original glob pattern
    
    The original Bash script uses a glob pattern to match files that have endings such as ...bin.0, ...bin.1, etc. That has been translated correctly to Python now.
    
    * Added support for Windows and updated README to use this script
    
    New code to set the name of the quantize script binary depending on the platform has been added (quantize.exe if working on Windows) and the README.md file has been updated to use this script instead of the Bash one.
    
    * Fixed a typo and removed shell=True in the subprocess.run call
    
    Fixed a typo regarding the new filenames of the quantized models and removed the shell=True parameter in the subprocess.run call as it was conflicting with the list of parameters.
    
    * Corrected previous commit
    
    * Small tweak: changed the name of the program in argparse
    
    This was making the automatic help message to be suggesting the program's usage as being literally "$ Quantization Script [arguments]". It should now be something like "$ python3 quantize.py [arguments]".

commit ad5fd5b60cfdfbfb22b0f2bc9e9f6c9692768f8d
Author: tjohnman <tjohnman@users.noreply.github.com>
Date:   Sun Mar 19 19:36:19 2023 +0100

    Make prompt randomization optional. (#300)
    
    Co-authored-by: Johnman <>

commit 368d0c8a9ebae16a20e1c8971b21ee888bdefad5
Author: tjohnman <tjohnman@users.noreply.github.com>
Date:   Sun Mar 19 19:31:17 2023 +0100

    Respect the maximum number of tokens in interactive. (#298)
    
    Co-authored-by: Johnman <johnman@github>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 50fae10d0339f2bd639f69dd679c0201d939a265
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Sun Mar 19 19:22:48 2023 +0100

    Add --ignore-eos parameter (#181)
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 084e2f0ec081c929343d44b09df07ae87cd1ed32
Author: Qingyou Meng <meng.qingyou@gmail.com>
Date:   Mon Mar 20 02:10:00 2023 +0800

    interactive mode: print '\n' in sigint_handler, this flush stdout thus ensure color reset. (#283)

commit 0b366e735729327476ec31da02de3c9c9771ddfb
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Sun Mar 19 18:57:00 2023 +0100

    Command line switch to use F16 for memory_k and memory_v (refactor of #154) (#294)
    
    * Use F16 for memory_k and memory_v
    
    * add command line switch to use f16 instead of f32 for memory k+v
    
    ---------
    
    Co-authored-by: Ty Everett <ty@tyweb.us>

commit 160bfb217da5038ccbd74438f9f16a16012d7866
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 19 19:51:55 2023 +0200

    Update hot topics to mention Alpaca support

commit c494ed5b94b429d3d73721235e78c9f5fa6e5652
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 19 19:46:32 2023 +0200

    Fix off-by-one bug (#115)

commit c1c7026b470ced0b8a6c67e968c04bb47864def1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 19 19:33:18 2023 +0200

    Fix python stuff (#109)

commit 467b149761cd63248b00d6ffb204d50a4cbb451a
Author: qunash <anzoria@gmail.com>
Date:   Sun Mar 19 20:17:39 2023 +0300

    Refactoring `convert-pth-to-ggml.py`: more concise and readable (#109)
    
    * Refactor get_n_parts function to simplify code and improve readability
    
    * Use f-strings instead of concatenation
    
    * Refactoring: more concise and readable
    
    * modularize
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 70f01cb8632f73b5cf70428608b89cd3c0775d23
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 19 19:04:44 2023 +0200

    Drop trailing new line from file prompts (#80)

commit a4e63b73dfa1894387926cc8072b5f36deebf0a5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 19 18:49:50 2023 +0200

    Add instruction for using Alpaca (#240)

commit 9e1707218a24ff758c7b623594f8c0ce5e12eb6c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 19 18:37:02 2023 +0200

    Add "--instruct" argument for usage with Alpaca (#240)
    
    Also start adding prompts in "./prompts"

commit 22213a17b56336bbea384a572a9484ce208c0333
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 19 17:30:00 2023 +0200

    Change RMSNorm eps to 1e-6 (#173)
    
    I think this is what is used in the Python code

commit d7def1a7524f712e5ebb7cd02bab0f13aa56a7f9
Author: Ronsor <ronsor@ronsor.pw>
Date:   Sat Mar 18 17:10:47 2023 -0700

    Warn user if a context size greater than 2048 tokens is specified (#274)
    
    LLaMA doesn't support more than 2048 token context sizes, and going above that produces terrible results.

commit 6f61c18ec9a30416e21ed5abfb1321bdb14979be
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Sat Mar 18 22:39:46 2023 +0100

    Fix typo in readme

commit 1e5a6d088d0f3a967c6e86298a756daec9e8df12
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Sat Mar 18 22:20:04 2023 +0100

    Add note about Python 3.11 to readme

commit 554b54152145c30618bac171efb712cf4a7d1e96
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Sat Mar 18 21:58:46 2023 +0100

    Add memory/disk requirements to readme

commit d3f202d57b694376cef6f381a6b6901825c3f6d9
Author: Alex Nguyen <tiendung@users.noreply.github.com>
Date:   Sat Mar 18 20:51:49 2023 +0700

    Remove unused code since n_vocab is model.hparams.n_vocab (#262)

commit e03e359730c127f888fcf00e93375771bc0a3500
Author: Justin Suess <justin.suess@westpoint.edu>
Date:   Sat Mar 18 07:44:09 2023 -0400

    fixed warning with std::ignore about unused function result (#151)
    
    fixed warning with std::ignore about unused function result

commit a81d0c2a171a4446e6a21a3ec74a0c0768d71184
Author: Gary Linscott <glinscott@gmail.com>
Date:   Sat Mar 18 04:17:19 2023 -0700

    Fix n^2 loop in tokenization (#254)
    
    This causes long prompts to parse very slowly.

commit b2de7f18dfbb93463eeb5b4392117bbe82d5bd1b
Author: anzz1 <anzz1@live.com>
Date:   Sat Mar 18 09:27:12 2023 +0200

    CI Improvements (#230)
    
    * CI Improvements
    
    Manual build feature, autoreleases for Windows
    
    * better CI naming convention
    
    use branch name in releases and tags

commit a29274789309029fd88a9465e6d0832d4632272b
Author: Niklas Korz <niklas@niklaskorz.de>
Date:   Fri Mar 17 23:03:48 2023 +0100

    Nix flake (#40)
    
    * Nix flake
    
    * Nix: only add Accelerate framework on macOS
    
    * Nix: development shel, direnv and compatibility
    
    * Nix: use python packages supplied by withPackages
    
    * Nix: remove channel compatibility
    
    * Nix: fix ARM neon dotproduct on macOS
    
    ---------
    
    Co-authored-by: Pavol Rusnak <pavol@rusnak.io>

commit c9f670a17755311aa28c411f5c7f3c8c05434770
Author: thement <40525767+thement@users.noreply.github.com>
Date:   Fri Mar 17 21:05:58 2023 +0100

    Implement non-greedy tokenizer that tries to maximize token lengths (#242)
    
    * Implement non-greedy tokenizer that tries to maximize token lengths
    
    * Insert single space in front of the prompt
    
    - this is to match original llama tokenizer behavior
    
    ---------
    
    Co-authored-by: Jakub Horak <jakub.horak@ibawizard.net>

commit 4f546091102a418ffdc6230f872ac56e5cedb835
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 17 21:46:46 2023 +0200

    Default to 4 threads (#243)

commit e81b9c81c101f64531ef0fa1ee6b77d562635652
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 17 20:30:04 2023 +0200

    Update Contributing section

commit 367946c668757532deed929e1d78673c6ac6bcb8
Author: Stephan Walter <stephan@walter.name>
Date:   Fri Mar 17 17:47:35 2023 +0000

    Don't tell users to use a bad number of threads (#243)
    
    The readme tells people to use the command line option "-t 8", causing 8
    threads to be started. On systems with fewer than 8 cores, this causes a
    significant slowdown. Remove the option from the example command lines
    and use /proc/cpuinfo on Linux to determine a sensible default.

commit 6b0df5ccf360fe5c015f6607f0375bfc6849005e
Author: mmyjona <jonathan.gonse@gmail.com>
Date:   Sat Mar 18 00:38:24 2023 +0800

    add ptread link to fix cmake build under linux (#114)
    
    * add ptread link to fix cmake build under linux
    
    * add cmake to linux and macos platform
    
    * separate make and cmake workflow
    
    ---------
    
    Co-authored-by: Sebastián A <sebastian.aedo29@gmail.com>

commit 2af23d30434a677c6416812eea52ccc0af65119c
Author: Bernat Vadell <hounter.caza@gmail.com>
Date:   Fri Mar 17 10:47:06 2023 +0100

    🚀 Dockerize llamacpp (#132)
    
    * feat: dockerize llamacpp
    
    * feat: split build & runtime stages
    
    * split dockerfile into main & tools
    
    * add quantize into tool docker image
    
    * Update .devops/tools.sh
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * add docker action pipeline
    
    * change CI to publish at github docker registry
    
    * fix name runs-on macOS-latest is macos-latest (lowercase)
    
    * include docker versioned images
    
    * fix github action docker
    
    * fix docker.yml
    
    * feat: include all-in-one command tool & update readme.md
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 904d2a8d6acd667c9633138d45a361d40fbf76d0
Author: Matvey Soloviev <blackhole89@gmail.com>
Date:   Fri Mar 17 05:48:39 2023 +0100

    Q4_1 quantization (#193)
    
    * Add AVX2 version of ggml_vec_dot_q4_1
    
    * Small optimisations to q4_1 dot product (@Const-me)
    
    * Rearrange Q4_1 quantization to work for multipart models. (Fix #152)
    
    * Fix ggml_vec_mad_q4_1 too
    
    * Fix non-vectorised q4_1 vec mul

commit 721311070e31464ac12bef9a4444093eb3eaebf7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 16 15:00:09 2023 +0200

    Update README.md

commit ac15de789547e5a6e93df552e787379b3a23ef26
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 16 08:55:13 2023 +0200

    Expand "Contributing" section

commit 273abc47ff9dd899b3c4f58acd19d4649e90d6b4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 16 07:12:12 2023 +0200

    Update hot topics - RMSnorm

commit 9b4a15b17d8395eb075379b140fcd0b0283f4ef6
Author: Nebula <infinitewormhole@gmail.com>
Date:   Wed Mar 15 19:29:25 2023 -0400

    Fix RMS norm in GGML (#191)

commit 6eac39ba953acaeec396cea2969dbf413907e2ec
Author: hoangmit <hoangmit@users.noreply.github.com>
Date:   Wed Mar 15 18:41:38 2023 -0400

    Add RMS norm and use it (#187)
    
    * add ggml_rms_norm
    
    * update op num

commit 27944c4206a49bbe003021a2610bacaa3044e619
Author: moritzbrantner <31051084+moritzbrantner@users.noreply.github.com>
Date:   Wed Mar 15 21:35:25 2023 +0100

    fixed typo (#178)

commit 2d15d6c9a959749f954d4fbbf44d711e19c5bdff
Author: Rickey Bowers Jr <bitRAKE@gmail.com>
Date:   Wed Mar 15 13:56:24 2023 -0600

    add SIGINT support for _WIN32 environments (#120)
    
    * add SIGINT support for _WIN32 environments
    
    * perhaps more consistent

commit 2d64715ad475f192a4004a52d134c67ccb6f44ad
Author: Justin Suess <justin.suess@westpoint.edu>
Date:   Wed Mar 15 15:42:40 2023 -0400

    added ctx_size parameter (#148)
    
    * added ctx_size parameter
    
    * added it in more places
    
    * Apply suggestions from code review
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 16b2c61a22f828ea77d9f084ca871c63bc5cc283
Author: Justin Suess <justin.suess@westpoint.edu>
Date:   Wed Mar 15 15:39:38 2023 -0400

    fixed color reset on exit (#149)
    
    * fixed color reset on exit
    
    * added sigint handler for ansi_color_reset
    
    * Update main.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 977295c700a2952c18400026d57467077dcd1a20
Author: Musab Gultekin <musabgultekin@users.noreply.github.com>
Date:   Wed Mar 15 22:39:06 2023 +0300

    Fix potential licensing issue (#126)
    
    * Update README.md
    
    * Update README.md
    
    remove facebook

commit 956dfda8ad8cea7961e22e0384bbc315bf79aed2
Author: Ronsor <ronsor@ronsor.pw>
Date:   Wed Mar 15 12:37:50 2023 -0700

    Use `tokenizer.vocab_size()` instead of hardcoding 32000 in convert-pth-to-ggml.py (#142)
    
    There are ways that special tokens or other new tokens could be added to the tokenizer; therefore it's probably best not to assume the vocabulary is only 32000 tokens.

commit 113e685d18ac4edb20f647fd34b000941556f6a6
Author: hoangmit <hoangmit@users.noreply.github.com>
Date:   Wed Mar 15 15:05:14 2023 -0400

    inline -> static inline for "bytesFromNibbles" (#161)
    
    Without "static" prefix, it fails to compile in clang

commit 47857e564c218a2c38346d0cdd94314632878fcb
Author: Ronsor <ronsor@ronsor.pw>
Date:   Tue Mar 14 12:34:37 2023 -0700

    Don't use vdotq_s32 if it's not available (#139)
    
    * Don't use vdotq_s32 if it's not available
    
    `dotprod` extensions aren't available on some ARM CPUs (e.g. Raspberry Pi 4), so check for them and only use them if they're available.
    
    Reintroduces the code removed in 84d9015 if `__ARM_FEATURE_DOTPROD` isn't defined.
    
    * Update ggml.c
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 60f819a2b10475055a36415bc489e5b55df2d052
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Tue Mar 14 15:30:08 2023 +0200

    Add section to README on how to run the project on Android (#130)

commit 97ab2b257897bfe7e2ae72876a3e50ed41b8c7ce
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 14 09:43:52 2023 +0200

    Add Misc section + update hot topics + minor fixes

commit 2f700a27381e558a4eb5a3f8fd56757f4c7a417c
Author: Sebastián A <sebastian.aedo29@gmail.com>
Date:   Mon Mar 13 17:29:10 2023 -0300

    Add windows to the CI (#98)

commit c09a9cfb06c87d114615c105adda91b0e6273b69
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 13 21:22:15 2023 +0200

    CMake build in Release by default (#75)

commit 7ec903d3c162417c11463f14ad5b773a918fb7f1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 13 19:21:51 2023 +0200

    Update contribution section, hot topics, limitations, etc.

commit 4497ad819c0010a8b19ffeaf8c0428eb7558d3e0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 13 19:15:08 2023 +0200

    Print system information

commit ed6849cc07a8973e5d31947b9df2df2da975ac96
Author: Sebastián A <sebastian.aedo29@gmail.com>
Date:   Mon Mar 13 14:12:33 2023 -0300

    Initial support for CMake (#75)

commit 41be0a3b3d76ee4f254dc81b42bd8ed26ee324e7
Author: Thomas Klausner <wiz@gatalith.at>
Date:   Mon Mar 13 17:40:54 2023 +0100

    Add NetBSD support. (#90)

commit 671d5cac15241b495006f56482bf2d6967dca91f
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Mon Mar 13 17:39:56 2023 +0100

    Use fprintf for diagnostic output (#48)
    
    keep printf only for printing model output
    
    one can now use ./main ... 2>dev/null to suppress any diagnostic output

commit 84d9015c4a91ab586ba65d5bd31a8482baf46ba1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 13 18:36:44 2023 +0200

    Use vdotq_s32 to improve performance (#67)
    
    * 10% performance boost on ARM
    
    * Back to original change

commit 63fd76fbb06f9b723ca11505352387a3148b1814
Author: uint256_t <konndennsa@gmail.com>
Date:   Tue Mar 14 01:33:43 2023 +0900

    Reduce model loading time (#43)
    
    * Use buffering
    
    * Use vector
    
    * Minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 2a20f48efad692a8c2744f10c673bbdbe0c751b7
Author: Val Kharitonov <mail@kharvd.com>
Date:   Mon Mar 13 12:24:18 2023 -0400

    Fix UTF-8 handling (including colors) (#79)

commit d1f224712d78ab2cbb78777acfeb6739f660eb96
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Mon Mar 13 17:15:20 2023 +0100

    Add quantize script for batch quantization (#92)
    
    * Add quantize script for batch quantization
    
    * Indentation
    
    * README for new quantize.sh
    
    * Fix script name
    
    * Fix file list on Mac OS
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 1808ee0500ea674b4bc2911acd0489ee5cbcef87
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 13 09:42:26 2023 +0200

    Add initial contribution guidelines

commit a169bb889cfe7b77a798f04fbc573e67ccb4316a
Author: Matvey Soloviev <blackhole89@gmail.com>
Date:   Mon Mar 13 04:08:01 2023 +0100

    Gate signal support on being on a unixoid system. (#74)

commit 460c48254098b28d422382a2bbff6a0b3d7f7e17
Author: Matvey Soloviev <blackhole89@gmail.com>
Date:   Mon Mar 13 00:35:51 2023 +0100

    Fix token count accounting

commit c80e2a8f2adeda202cbffe76ef800f134e51f03f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 13 01:28:08 2023 +0200

    Revert "10% performance boost on ARM"
    
    This reverts commit 113a9e83ebc0f788f861394437087bf3ca0e019b.
    
    There are some reports for illegal instruction.
    Moved this stuff to vdotq_s32 branch until resolve

commit 54a0e66ea0ed3248e6c95a070a2da0bf5c6d4817
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 13 01:21:03 2023 +0200

    Check for vdotq_s32 availability

commit 543c57e991a23121c666561c2837faa09c4a78ca
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 13 01:05:24 2023 +0200

    Ammend to previous commit - forgot to update non-QRDMX branch

commit 113a9e83ebc0f788f861394437087bf3ca0e019b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 13 00:56:10 2023 +0200

    10% performance boost on ARM

commit 404fac0d623c9eea74ad7a9347da69e33f10984e
Author: Matvey Soloviev <blackhole89@gmail.com>
Date:   Sun Mar 12 23:07:34 2023 +0100

    Fix color getting reset before prompt output done (#65)
    
    (cherry picked from commit 7eb2987619feee04c40eff69b604017d09919cb6)

commit 1a0a74300f35ad4868715d684d0bc0effdaa9d31
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 12 23:39:01 2023 +0200

    Update README.md

commit 96ea727f4780620b60c1897b538654931411a3fd
Author: Matvey Soloviev <blackhole89@gmail.com>
Date:   Sun Mar 12 22:13:28 2023 +0100

    Add interactive mode (#61)
    
    * Initial work on interactive mode.
    
    * Improve interactive mode. Make rev. prompt optional.
    
    * Update README to explain interactive mode.
    
    * Fix OS X build

commit 966195483549e201cff062096a848d9e9833a1a6
Author: Marc Köhlbrugge <subscriptions@marckohlbrugge.com>
Date:   Mon Mar 13 03:30:08 2023 +0700

    Fix typo in README (#45)

commit f385f8dee83d1baf59896b2eb09f1524dc9cde45
Author: Ben Garney <bengarney@users.noreply.github.com>
Date:   Sun Mar 12 13:28:36 2023 -0700

    Allow using prompt files (#59)

commit 02f0c6fe7f9b7be24c7d339aed016e54a92388ea
Author: beiller <beiller@gmail.com>
Date:   Sun Mar 12 16:23:15 2023 -0400

    Add back top_k (#56)
    
    * Add back top_k
    
    * Update utils.cpp
    
    * Update utils.h
    
    ---------
    
    Co-authored-by: Bill Hamilton <bill.hamilton@shopify.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit eb062bb012c4e131818dd757a6d3a757fdee3961
Author: Sebastián A <sebastian.aedo29@gmail.com>
Date:   Sun Mar 12 17:15:00 2023 -0300

    Windows fixes (#31)
    
    * Apply fixes suggested to build on windows
    
    Issue: https://github.com/ggerganov/llama.cpp/issues/22
    
    * Remove unsupported VLAs
    
    * MSVC: Remove features that are only available on MSVC C++20.
    
    * Fix zero initialization of the other fields.
    
    * Change the use of vector for stack allocations.

commit 7027a97837c351e0a7bc48db2027af368de382db
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 12 22:09:26 2023 +0200

    Update README.md

commit 2d555e5b42922cda6dfc0c3ff54df7b1ee4d0ff4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 12 22:08:24 2023 +0200

    Add CI (#60)

commit 7c9e54e55e4106f84688245fb15207f6df917e12
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 12 20:59:01 2023 +0200

    Revert "weights_only" arg - this causing more trouble than help

commit b9bd1d014113b7498f04ad4d28e6021d5f4cddad
Author: Oleksandr Nikitin <oleksandr@tvori.info>
Date:   Sun Mar 12 14:16:33 2023 +0200

    python/pytorch compat notes (#44)

commit 129c7d1ea886e52ac1b87ff6184310bab3158806
Author: beiller <beiller@gmail.com>
Date:   Sun Mar 12 05:27:42 2023 -0400

    Add repetition penalty (#20)
    
    * Adding repeat penalization
    
    * Update utils.h
    
    * Update utils.cpp
    
    * Numeric fix
    
    Should probably still scale by temp even if penalized
    
    * Update comments, more proper application
    
    I see that numbers can go negative so a fix from a referenced commit
    
    * Minor formatting
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit 702fddf5c5c3c1377e169ba9ecdfed4cb16c268b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 12 09:03:25 2023 +0200

    Clarify meaning of hacking

commit 7d86e25bf648eb369a3a8388bf239b6b19f7a789
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 12 08:41:54 2023 +0200

    README: add "Supported platforms" + update hot topics

commit a93120236f99e13d77e4b278e47ffcaad4a899e4
Author: deepdiffuser <112834445+deepdiffuser@users.noreply.github.com>
Date:   Sat Mar 11 22:36:35 2023 -0800

    use weights_only in conversion script (#32)
    
    this restricts malicious weights from executing arbitrary code by restricting the unpickler to only loading tensors, primitive types, and dictionaries

commit 6a9a67f0bee5eed67cf8bf03f74f77619da40d3f
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Sun Mar 12 07:36:03 2023 +0100

    Add LICENSE (#21)

commit da1a4ff01f42d058cfa59806dd5679c0fe5a8604
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 12 01:26:32 2023 +0200

    Update README.md

commit 6b2cb6302ffaf8264e33af1dc52e3ea54003e690
Author: Juraj Bednar <juraj@bednar.io>
Date:   Sat Mar 11 18:32:20 2023 +0100

    Fix a typo in model name (#16)

commit 4235e3d5b3f4a0e6844f6291322ebb42181345c9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 18:10:18 2023 +0200

    Update README.md

commit f1eaff4721153a5a5094fd1bd8cbdae7a3c079cc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 17:58:18 2023 +0200

    Add AVX2 support for x86 architectures thanks to @Const-me !

commit a9e58529ea507ac15cd2df4c39d1b9613d6acb6e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 17:40:14 2023 +0200

    Fix un-initialized FP16 tables on x86 (#15, #2)

commit 7d9ed7b25fe17db3fc8848b5116d14682864ce8e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 12:44:21 2023 +0200

    Bump memory buffer

commit 0c6803321c818f3f2da4a0693d20128b0f79ad28
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 12:31:21 2023 +0200

    Update README.md

commit f60fa9e50afce35e7ebe1fedf34d4a9327353927
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 12:26:46 2023 +0200

    .gitignore models/

commit 7211862c943273fc8ce4b7fdf4c04f9821b7b591
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 12:26:16 2023 +0200

    Update Makefile var + add comment

commit a5c5ae2f545d0d3338f5b2a7840457e35d5bccc1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 11:34:25 2023 +0200

    Update README.md

commit ea977e85ecda7b983f0e7b1db20b509998ddc889
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 11:34:11 2023 +0200

    Update README.md

commit 007a8f6f459c6eb56678fdee4c09219ddb85b640
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 10:47:09 2023 +0200

    Support all LLaMA models + change Q4_0 quantization storage

commit 5f2f970d51a04b783799bc92fd1d006408269f26
Author: Simon Willison <swillison@gmail.com>
Date:   Fri Mar 10 21:47:26 2023 -0800

    Include Python dependencies in README (#6)

commit 73c6ed5e8784a20f89d51b1703a09bc690c68227
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 01:30:47 2023 +0200

    Update README.md

commit 01eeed8fb1437978603a8523c0b8ea2f6280f5d7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 01:22:58 2023 +0200

    Update README.md

commit 6da2df34ee40301d9ecb126968ec4c0c6195f26d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 01:18:10 2023 +0200

    Update README.md

commit 9dcf4dba459ba6482a7a5aced22645a387ec6991
Author: Jean-Michaël Celerier <jeanmichael.celerier+github@gmail.com>
Date:   Fri Mar 10 18:04:06 2023 -0500

    Add missing headers for memcpy and assert (#3)

commit 920a7fe2d94cc7e4fed0e88db830b674c91865c5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 00:55:22 2023 +0200

    Update README.md

commit 3a57ee59de53c2a9d2c3a2c643b609ce07a58a16
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 00:51:46 2023 +0200

    Update README.md

commit b85028522d6e924473159ba0da3543fc174d2ded
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 00:09:19 2023 +0200

    Update README.md

commit 8a01f565ff78cc6c0c5a9fa402787a2f179f2d78
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 10 23:53:11 2023 +0200

    Update README.md

commit 70bc0b8b15b98dca23b28f0c8f5e34b27e424cda
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 10 23:46:39 2023 +0200

    Fix a bug in the rope calculation

commit 18ebda34d67c05f4f5584a9209e7efb949f5fd56
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 10 21:52:27 2023 +0200

    Update README.md

commit 319cdb3e1ffe263cf5b08249c9559e011396c1de
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 10 21:50:46 2023 +0200

    Final touches

commit 775328064e69db1ebd7e19ccb59d2a7fa6142470
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 10 21:47:46 2023 +0200

    Create README.md

commit 26c084662903ddaca19bef982831bfb0856e8257
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 10 20:40:58 2023 +0200

    Initial release
